# Author: Developer Ray
# Date: 2025-03-04
# Last Editor: Developer Ray
# Last Edit Date: 2025-03-11
# Description: Heavily generated by ChatGPT: https://chatgpt.com/share/e/67802aac-275c-8000-8cfc-2d26df911ca8.

# Inherit from the base image
FROM us-central1-docker.pkg.dev/eng-reactor-287421/cloud-run-source-deploy/base AS app

# Copy application code
COPY . .

# Expose the default port
EXPOSE 8080

## **8 workers and 1 thread versus 1 worker and 8 threads**
# | Feature                     | **8 Workers, 1 Thread**                       | **1 Worker, 8 Threads**                       |
# |-----------------------------|-----------------------------------------------|-----------------------------------------------|
# | **CPU Utilization**         | Good for CPU-bound tasks                      | Good for I/O-bound tasks; limited by the GIL  |
# | **Memory Usage**            | Higher (8 independent processes)              | Lower (1 process with shared memory)          |
# | **Startup Time**            | Longer (8 processes initialized)              | Faster (1 process initialized)                |
# | **Concurrency Management**  | Process-based; good for fault tolerance       | Thread-based; efficient for lightweight tasks |
# | **Scaling for I/O**         | Less efficient than threads                   | Better for I/O-bound workloads                |

## **Key Differences Between Workers and Threads**
# | Feature                  | **Workers (Processes)**                                         | **Threads**                                              |
# |--------------------------|-----------------------------------------------------------------|----------------------------------------------------------|
# | **Concurrency Type**     | Process-based concurrency                                       | Thread-based concurrency                                 |
# | **Memory Usage**         | High (each worker has its own memory)                           | Low (threads share memory)                               |
# | **GIL Impact**           | No impact (each process has its own Python interpreter)         | GIL prevents true parallel execution for CPU-bound tasks |
# | **Best For**             | **CPU-bound** workloads (e.g., heavy computation, ML inference) | **I/O-bound** workloads (e.g., DB queries, API calls)    |
# | **Performance Overhead** | Higher (process creation is expensive)                          | Lower (threads are lightweight)                          |
# | **Crash Isolation**      | A worker crash does not affect others                           | A thread crash may crash the worker                      |

## **When to Use More Workers vs. More Threads?**
# | Scenario                                                               | Best Choice                                                       |
# |------------------------------------------------------------------------|-------------------------------------------------------------------|
# | **CPU-bound workload (ML, image processing, computation-heavy tasks)** | More **workers**, fewer threads (e.g., `--workers 8 --threads 1`) |
# | **I/O-bound workload (DB queries, API requests, web scraping)**        | More **threads**, fewer workers (e.g., `--workers 2 --threads 4`) |
# | **Balanced workload (some CPU, some I/O)**                             | Mix of workers and threads (e.g., `--workers 4 --threads 2`)      |

# Decided to use 8 workers and 2 threads because the work is CPU-bound: the servers configured with this Dockerfile do much more heavy computation than DB queries or API calls
# Make sure that the number of workers multipled by the number of threads equals the concurrency limit (specified in `deploy-ficc-servers.sh`) since Google Cloud Run sends the number of requests specified in concurrency limit to each container, and so if the number of processes that can be handled is lesser than the concurrency limit, then there will be requests waiting on the sideline
# The number of workers should not exceed the number of CPUs allocated for the container (specified in `deploy-ficc-servers.sh`) because otherwise the workers will be competing for the CPUs and some processes might have partial CPU access causing slowdown
# Setting `--timeout 3500` to automatically kill workers that take too long (3500 seconds) to process a request; previously the value was set to 0 which meant that Gunicorn would never kill unresponsive workers (intentionally chose 3500 instead of 3600 as the time threshold since this will occur before involving Google Cloud Run [time threshold set to 3600] since behavior of Google Cloud Run termination command is unstable)
# With `--max-requests 1000 --max-requests-jitter 50`, workers restart automatically after 1000 ± 50 requests so workers will not keep growing in memory usage
# Setting `--graceful-timeout 3500` to give each worker 3500 seconds to finish processing a request before shutting down due to the hitting the `--max_requests` ± `--max-requests-jitter` threshold (intentionally chose 3500 instead of 3600 as the time threshold since this will occur before involving Google Cloud Run [time threshold set to 3600] since behavior of Google Cloud Run termination command is unstable)
CMD exec gunicorn --bind :8080 --workers 8 --threads 2 --timeout 3500 --graceful-timeout 3500 --max-requests 1000 --max-requests-jitter 50 main:app
