'''
Last updated by Developer on 2024-05-13.
Description: Goes to S&P website and scrapes the yield to worst values for each index. Then, updates the values in the BigQuery table associated with each index.
'''
import time    # used to rate limit the BigQuery table updates to not get flagged by GCP
import requests
from io import BytesIO
import traceback    # used to print error stack trace
import pandas as pd
from google.cloud import bigquery, secretmanager

import smtplib
from email.mime.text import MIMEText


table_headers = ['date', 'ytw']

PROJECT_ID = 'eng-reactor-287421'

# table names in bigquery
TABLE_IDS = [
    'sp_15plus_year_national_amt_free_index',
    'sp_12_22_year_national_amt_free_index',
    'sp_7_12_year_national_amt_free_municipal_bond_index_yield',
    'sp_high_quality_intermediate_managed_amt_free_municipal_bond_index_yield',
    'sp_high_quality_short_intermediate_municipal_bond_index_yield',
    'sp_high_quality_short_municipal_bond_index_yield',
    'sp_muni_high_quality_index_yield',
    'sp_long_term_national_amt_free_municipal_bond_index_yield',
]

# defining IDs using dictionaries so we don't need to rely on indexing for lists, etc, to remove any ambiguity
# the INDEX_IDS are unique identifiers for each S&P index to request their data from the S&P API, these values were scraped directly from S&P
INDEX_IDS = {
    'sp_15plus_year_national_amt_free_index': 92346704,
    'sp_12_22_year_national_amt_free_index': 946546,
    'sp_7_12_year_national_amt_free_municipal_bond_index_yield': 946545,
    'sp_high_quality_intermediate_managed_amt_free_municipal_bond_index_yield': 92404510,
    'sp_high_quality_short_intermediate_municipal_bond_index_yield': 10001820,
    'sp_high_quality_short_municipal_bond_index_yield': 10001819,
    'sp_muni_high_quality_index_yield': 10001818,
    'sp_long_term_national_amt_free_municipal_bond_index_yield': 946547,
}


###functions to email error message if update fails
def access_secret_version(project_id, secret_id, version_id):
    # Create the Secret Manager client.
    client = secretmanager.SecretManagerServiceClient()
    # Build the resource name of the secret version.
    name = f"projects/{project_id}/secrets/{secret_id}/versions/{version_id}"
    # Access the secret version.
    response = client.access_secret_version(request={"name": name})
    payload = response.payload.data.decode("UTF-8")
    return payload


def send_error_email(subject, error_message):
    sender_email = access_secret_version('eng-reactor-287421', 'notifications_username', 'latest')
    password = access_secret_version('eng-reactor-287421', 'notifications_password', 'latest')
    receiver_email = 'ficcteam@ficc.ai'

    msg = MIMEText(error_message)
    msg['Subject'] = subject
    msg['From'] = sender_email
    msg['To'] = receiver_email

    smtp_server = 'smtp.gmail.com'
    port = 587
    sender_email = 'notifications@ficc.ai'

    with smtplib.SMTP(smtp_server, port) as server:
        try:
            server.starttls()
            server.login(sender_email, password)
            server.sendmail(sender_email, receiver_email, msg.as_string())
        except Exception as e:
            print(e)
        finally:
            server.quit()


### functions to preprocess and update scraped S&P data to bigquery
def convert_types(df):
    df.date = pd.to_datetime(df.date, format='%Y-%m-%d')
    df.ytw = df.ytw * 100
    return df


def get_schema():
    schema = [bigquery.SchemaField('date', 'DATE'),
              bigquery.SchemaField('ytw', 'FLOAT')]
    return schema


def upload_data(df, table_id):
    client = bigquery.Client(project=PROJECT_ID, location='US')
    job_config = bigquery.LoadJobConfig(schema=get_schema(), write_disposition='WRITE_APPEND')
    job = client.load_table_from_dataframe(df, table_id, job_config=job_config)
    try:
        job.result()
    except Exception as e:
        send_error_email('Error uploading S&P index yields in cloud function: `update_all_sp_indices`',
                         f'This error means that there was an issue uploading data to BigQuery\nTable Name: {table_id}\n{type(e)}: {e}\n{traceback.format_exc()}')
        print(e)


def generate_user_agent():
    '''Generated by ChatGPT with query: "can you generate a user-agent that resembles a regular web browser?"'''
    # import random

    # platforms = ['Macintosh', 'Windows', 'X11']
    # browsers = ['chrome', 'firefox', 'safari', 'edge', 'opera']
    
    # platform = random.choice(platforms)
    # browser = random.choice(browsers)
    # version = random.randint(1, 15)
    
    # if browser == 'chrome':
    #     webkit_version = random.randint(500, 599)
    #     return f"Mozilla/5.0 ({platform}; {platform} Intel Mac OS X 10_15_{random.randint(1, 9)}) AppleWebKit/{webkit_version}.36 (KHTML, like Gecko) Chrome/{version}.0.{random.randint(1000, 9999)}.{random.randint(100, 999)} Safari/{webkit_version}.36"
    # elif browser == 'firefox':
    #     return f"Mozilla/5.0 ({platform}; {platform} rv:{version}.0) Gecko/20100101 Firefox/{version}.0"
    # elif browser == 'safari':
    #     webkit_version = random.randint(500, 599)
    #     return f"Mozilla/5.0 ({platform}; {platform} Intel Mac OS X 10_{random.randint(5, 15)}_{random.randint(1, 9)}) AppleWebKit/{webkit_version}.{random.randint(1, 3)} (KHTML, like Gecko) Version/{version}.{random.randint(0, 2)} Safari/{webkit_version}.{random.randint(1, 3)}"
    # elif browser == 'edge':
    #     return f"Mozilla/5.0 (Windows NT 10.0; {platform} Win64; x64) AppleWebKit/{random.randint(500, 599)}.36 (KHTML, like Gecko) Chrome/{version}.0.{random.randint(1000, 9999)}.{random.randint(100, 999)} Safari/{random.randint(500, 599)}.36 Edge/{random.randint(80, 90)}.{random.randint(1000, 9999)}"
    # elif browser == 'opera':
    #     return f"Mozilla/5.0 ({platform}; {platform} {random.choice(['x86_64', 'Win64'])}) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{version}.0.{random.randint(1000, 9999)}.{random.randint(100, 999)} Safari/537.36 OPR/{version}.0.{random.randint(1000, 9999)}.{random.randint(100, 999)}"
    return 'Mozilla/5.0 (Windows NT 10.0; X11 Win64; x64) AppleWebKit/515.36 (KHTML, like Gecko) Chrome/10.0.9265.815 Safari/509.36 Edge/89.2734'    # this is one of the `user_agent`s that worked and so we use this `user_agent` for every API call


def main(args):
    for table in TABLE_IDS:  # for each index, we download index data and upload
        indexId = INDEX_IDS[table]
        link = f'https://www.spglobal.com/spdji/en/idsexport/file.xls?hostIdentifier=48190c8c-42c4-46af-8d1a-0cd5db894797&redesignExport=true&languageId=1&selectedModule=YieldToWorstGraphView&selectedSubModule=Graph&yearFlag=threeYearFlag&indexId={indexId}'    # link leads directly to .xls download, so we can use pd.read_excel directly by specifying the right indexId
        print(f'Attempting to get data for {table} from {link}')

        user_agent = generate_user_agent()
        print('User-Agent:', user_agent)
        headers = {'User-Agent': user_agent}    # used to make the programmatic request seem like it is coming from a regular web browser
        response = requests.get(link, headers=headers)
        print('Response status code:', response.status_code)
        try:
            df = pd.read_excel(BytesIO(response.content))

            # To remove the header and tail description
            df.rename(columns={'Unnamed: 0': 'date', 'Unnamed: 1': 'ytw'}, inplace=True)

            # Removing the header and tail in the excel sheet, where the lengths of the header and tails are fixed; header uses the first 8 lines and the tail; uses the last 4 lines
            df = df[8:-4]
            df = convert_types(df)

            # Get the last row as a dataframe
            df = df[-1:]

            upload_data(df, 'eng-reactor-287421.spBondIndex.' + table)
            time.sleep(15)
            print(df)

        except Exception as e:
            send_error_email('Error downloading S&P index yields in cloud function: `update_all_sp_indices`',
                             f'This error means that there was an issue downloading data from the S&P webpage for {table}\n{type(e)}: {e}\n{traceback.format_exc()}')
            print(e)
            raise e    # this ensures that even after we catch an error and send an email alert, we raise the error to make the function fail. This allows the cloud function instance to fail and retry.

    return 'Success'
