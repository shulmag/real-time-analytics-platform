'''
 # @ Author: Developer Ray
 # @ Create date: 2023-12-18
 # @ Modified by: Developer
 # @ Modified date: 2024-03-29
 '''
import warnings
import os
import numpy as np
import pandas as pd
from pandas.tseries.offsets import BusinessDay
from sklearn import preprocessing
from pickle5 import pickle
from datetime import datetime

from google.cloud import bigquery
from google.cloud import storage

from ficc.utils.gcp_storage_functions import download_data
from ficc.data.process_data import process_data
from ficc.utils.auxiliary_variables import NUM_OF_DAYS_IN_YEAR
from ficc.utils.auxiliary_functions import function_timer, sqltodf, get_ys_trade_history_features, get_dp_trade_history_features
from ficc.utils.nelson_siegel_model import yield_curve_level
from ficc.utils.diff_in_days import diff_in_days_two_dates

from automated_training_auxiliary_variables import NUM_OF_DAYS_IN_YEAR, \
                                                   PREDICTORS, \
                                                   PREDICTORS_DOLLAR_PRICE, \
                                                   YS_VARIANTS, \
                                                   YS_FEATS, \
                                                   DP_VARIANTS, \
                                                   DP_FEATS, \
                                                   YEAR_MONTH_DAY, \
                                                   HOUR_MIN_SEC, \
                                                   QUERY_FEATURES, \
                                                   QUERY_CONDITIONS, \
                                                   ADDITIONAL_QUERY_CONDITIONS_FOR_YIELD_SPREAD_MODEL, \
                                                   ADDITIONAL_QUERY_FEATURES_FOR_DOLLAR_PRICE_MODEL, \
                                                   EASTERN, \
                                                   NUM_TRADES_IN_HISTORY_YIELD_SPREAD_MODEL, \
                                                   NUM_TRADES_IN_HISTORY_DOLLAR_PRICE_MODEL, \
                                                   CATEGORICAL_FEATURES_VALUES, \
                                                   SAVE_MODEL_AND_DATA, \
                                                   HOME_DIRECTORY, \
                                                   BUCKET_NAME, \
                                                   EARLIST_TRADE_DATETIME, \
                                                   CUMULATIVE_DATA_PICKLE_FILENAME_YIELD_SPREAD, \
                                                   CUMULATIVE_DATA_PICKLE_FILENAME_DOLLAR_PRICE, \
                                                   OPTIONAL_ARGUMENTS_FOR_PROCESS_DATA_YIELD_SPREAD, \
                                                   OPTIONAL_ARGUMENTS_FOR_PROCESS_DATA_DOLLAR_PRICE, \
                                                   TTYPE_DICT, \
                                                   LONG_TIME_AGO_IN_NUM_SECONDS


project_id = 'eng-reactor-287421'

WORKING_DIRECTORY = HOME_DIRECTORY 


def get_storage_client():
    return storage.Client(project=project_id)


def get_bq_client():
    return bigquery.Client(project=project_id)


STORAGE_CLIENT = get_storage_client()
BQ_CLIENT = get_bq_client()


D_prev = dict()
P_prev = dict()
S_prev = dict()


def padded_print(message, width=100):
    print(f'{message:=^{width}}')


def get_trade_history_columns(model: str) -> list:
    '''Creates a list of columns.'''
    assert model in ('yield_spread', 'dollar_price'), f'Model should be either yield_spread or dollar_price, but was instead: {model}'
    if model == 'yield_spread':
        variants = YS_VARIANTS
        feats = YS_FEATS
    else:
        variants = DP_VARIANTS
        feats = DP_FEATS
    
    columns = []
    for prefix in variants:
        for suffix in feats:
            columns.append(prefix + suffix)
    return columns

def target_trade_processing_for_attention(row):
    trade_mapping = {'D': [0,0], 'S': [0,1], 'P':[1,0]}
    target_trade_features = []
    target_trade_features.append(row['quantity'])
    target_trade_features = target_trade_features + trade_mapping[row['trade_type']]
    return np.tile(target_trade_features, (1, 1))


def replace_ratings_by_standalone_rating(data: pd.DataFrame) -> pd.DataFrame:
    data.loc[data.sp_stand_alone.isna(), 'sp_stand_alone'] = 'NR'
    data.rating = data.rating.astype('str')
    data.sp_stand_alone = data.sp_stand_alone.astype('str')
    data.loc[(data.sp_stand_alone != 'NR'), 'rating'] = data[(data.sp_stand_alone != 'NR')]['sp_stand_alone'].loc[:]
    return data


def get_yield_for_last_duration(row, nelson_params, scalar_params, shape_parameter):
    if pd.isnull(row['last_calc_date'])or pd.isnull(row['last_trade_date']):
        # if there is no last trade, we use the duration of the current bond
        duration = diff_in_days_two_dates(row['maturity_date'], row['trade_date']) / NUM_OF_DAYS_IN_YEAR
        ycl = yield_curve_level(duration, row['trade_date'].date(), nelson_params, scalar_params, shape_parameter) / 100
        return ycl
    duration =  diff_in_days_two_dates(row['last_calc_date'], row['last_trade_date']) / NUM_OF_DAYS_IN_YEAR
    ycl = yield_curve_level(duration, row['trade_date'].date(), nelson_params, scalar_params, shape_parameter) / 100
    return ycl


@function_timer
def add_yield_curve(data):
    '''Add 'new_ficc_ycl' field to `data`.'''
    nelson_params = sqltodf('SELECT * FROM `eng-reactor-287421.ahmad_test.nelson_siegel_coef_daily` order by date desc', BQ_CLIENT)
    nelson_params.set_index('date', drop=True, inplace=True)
    nelson_params = nelson_params[~nelson_params.index.duplicated(keep='first')]
    nelson_params = nelson_params.transpose().to_dict()

    scalar_params = sqltodf('SELECT * FROM `eng-reactor-287421.ahmad_test.standardscaler_parameters_daily` order by date desc', BQ_CLIENT)
    scalar_params.set_index('date', drop=True, inplace=True)
    scalar_params = scalar_params[~scalar_params.index.duplicated(keep='first')]
    scalar_params = scalar_params.transpose().to_dict()

    shape_parameter = sqltodf('SELECT * FROM `eng-reactor-287421.ahmad_test.shape_parameters` order by Date desc', BQ_CLIENT)
    shape_parameter.set_index('Date', drop=True, inplace=True)
    shape_parameter = shape_parameter[~shape_parameter.index.duplicated(keep='first')]
    shape_parameter = shape_parameter.transpose().to_dict()

    data['last_trade_date'] = data['last_trade_datetime'].dt.date
    data['new_ficc_ycl'] = data[['last_calc_date',
                                 'last_settlement_date',
                                 'trade_date',
                                 'last_trade_date',
                                 'maturity_date']].parallel_apply(lambda row: get_yield_for_last_duration(row, nelson_params, scalar_params, shape_parameter), axis=1)
    data['new_ficc_ycl'] = data['new_ficc_ycl'] * 100
    return data


def decrement_business_days(date: str, num_business_days: int) -> str:
    '''Subtract `num_business_days` from `date`.'''
    return (datetime.strptime(date, YEAR_MONTH_DAY) - BusinessDay(num_business_days)).strftime(YEAR_MONTH_DAY)


def earliest_trade_from_new_data_is_same_as_last_trade_date(new_data: pd.DataFrame, last_trade_date) -> bool:
    '''Checks whether `last_trade_date` is the same as the date of the earliest trade in `new_data`. This 
    situation arises materialized trade history is created in the middle of the day, and so there are trades 
    on the same day that are still coming in. If we do not account for this case, then the automated training 
    fails since it searches for trades to populate the testing set as those after the `last_trade_date`.'''
    return new_data.trade_date.min().date().strftime(YEAR_MONTH_DAY) == last_trade_date


@function_timer
def get_new_data(file_name, model: str, use_treasury_spread: bool = False, optional_arguments_for_process_data: dict = {}):
    assert model in ('yield_spread', 'dollar_price'), f'Invalid value for model: {model}'
    query_features = QUERY_FEATURES
    query_conditions = QUERY_CONDITIONS
    if model == 'yield_spread':
        query_conditions = ADDITIONAL_QUERY_CONDITIONS_FOR_YIELD_SPREAD_MODEL + query_conditions
    else:
        query_features = query_features + ADDITIONAL_QUERY_FEATURES_FOR_DOLLAR_PRICE_MODEL
    
    old_data, last_trade_datetime, last_trade_date = get_data_and_last_trade_datetime(BUCKET_NAME, file_name)
    print(f'last trade datetime: {last_trade_datetime}')
    DATA_QUERY = get_data_query(last_trade_datetime, query_features, query_conditions)
    file_timestamp = datetime.now(EASTERN).strftime(YEAR_MONTH_DAY + '-%H:%M')

    trade_history_features = get_ys_trade_history_features(use_treasury_spread) if model == 'yield_spread' else get_dp_trade_history_features()
    num_features_for_each_trade_in_history = len(trade_history_features)
    num_trades_in_history = NUM_TRADES_IN_HISTORY_YIELD_SPREAD_MODEL if model == 'yield_spread' else NUM_TRADES_IN_HISTORY_DOLLAR_PRICE_MODEL
    raw_data_filepath = f'raw_data_{file_timestamp}.pkl'
    data_from_last_trade_datetime = process_data(DATA_QUERY, 
                                                 BQ_CLIENT, 
                                                 num_trades_in_history, 
                                                 num_features_for_each_trade_in_history, 
                                                 raw_data_filepath, 
                                                 save_data=False, 
                                                 **optional_arguments_for_process_data)
    
    if data_from_last_trade_datetime is not None:
        if earliest_trade_from_new_data_is_same_as_last_trade_date(data_from_last_trade_datetime, last_trade_date):    # see explanation in docstring for `earliest_trade_from_new_data_is_same_as_last_trade_date(...)` as to why this scenario is important to handle
            decremented_last_trade_date = decrement_business_days(last_trade_date, 1)
            warnings.warn(f'Since the earliest trade from the new data is the same as the last trade date, we are decrementing the last trade date from {last_trade_date} to {decremented_last_trade_date}. This occurs because materialized trade history was created in the middle of the work day. If materialized trade history was not created during the middle of the work day, then investigate why we are inside this `if` statement.')
            last_trade_date = decremented_last_trade_date
        
        if model == 'dollar_price': data_from_last_trade_datetime = data_from_last_trade_datetime.rename(columns={'trade_history': 'trade_history_dollar_price'})    # change the trade history column name to match with `PREDICTORS_DOLLAR_PRICE`
    else:
        class EmptyDataFrameError(Exception):
            def __init__(self, message="Dataframe for new trades is empty. If TESTING=False, this is the expected behavior of the component to prevent redundant computation."):
                self.message = message
                super().__init__(self.message)
        raise EmptyDataFrameError
    
    return old_data, data_from_last_trade_datetime, last_trade_date, num_features_for_each_trade_in_history, raw_data_filepath


def remove_old_trades(data: pd.DataFrame, num_days_to_keep: int, most_recent_trade_date: str = None, dataset_name: str = None) -> pd.DataFrame:
    '''Only keep `num_days_to_keep` days from the most recent trade in `data`.'''
    from_dataset_name = f' from {dataset_name}' if dataset_name is not None else ''
    most_recent_trade_date = data.trade_date.max() if most_recent_trade_date is None else pd.to_datetime(most_recent_trade_date)
    days_to_most_recent_trade = diff_in_days_two_dates(most_recent_trade_date, data.trade_date, 'exact')
    print(f'Removing trades{from_dataset_name} older than {num_days_to_keep} days before {most_recent_trade_date}')
    return data[days_to_most_recent_trade < num_days_to_keep]


@function_timer
def combine_new_data_with_old_data(old_data: pd.DataFrame, new_data: pd.DataFrame, model: str) -> pd.DataFrame:
    assert model in ('yield_spread', 'dollar_price'), f'Invalid value for model: {model}'
    if new_data is None: return old_data    # there is new data since `last_trade_date`

    num_trades_in_new_data = len(new_data)
    num_trades_in_old_data = 0 if old_data is None else len(old_data)
    print(f'Old data has {num_trades_in_old_data} trades. New data has {num_trades_in_new_data} trades')
    trade_history_feature_name = 'trade_history' if model == 'yield_spread' else 'trade_history_dollar_price'
    num_trades_in_history = NUM_TRADES_IN_HISTORY_YIELD_SPREAD_MODEL if model == 'yield_spread' else NUM_TRADES_IN_HISTORY_DOLLAR_PRICE_MODEL
    print(f'Restricting history to {num_trades_in_history} trades')
    new_data[trade_history_feature_name] = new_data[trade_history_feature_name].apply(lambda x: x[:num_trades_in_history])
    if old_data is not None: old_data[trade_history_feature_name] = old_data[trade_history_feature_name].apply(lambda x: x[:num_trades_in_history])    # done in case `num_trades_in_history` has decreased from before

    new_data = replace_ratings_by_standalone_rating(new_data)
    new_data['yield'] = new_data['yield'] * 100
    if model == 'yield_spread': new_data = add_yield_curve(new_data)
    new_data['target_attention_features'] = new_data.parallel_apply(target_trade_processing_for_attention, axis=1)

    new_data['trade_history_sum'] = new_data[trade_history_feature_name].parallel_apply(lambda x: np.sum(x))
    new_data.dropna(inplace=True, subset=['trade_history_sum'])
    print(f'Removed {num_trades_in_new_data - len(new_data)} trades, since these have null values in the trade history')
    new_data.issue_amount = new_data.issue_amount.replace([np.inf, -np.inf], np.nan)

    data = pd.concat([new_data, old_data]) if old_data is not None else new_data    # concatenating `new_data` to the original `data` dataframe
    if model == 'yield_spread': data['new_ys'] = data['yield'] - data['new_ficc_ycl']
    print(f'{len(data)} trades after combining new and old data')
    return data


@function_timer
def add_trade_history_derived_features(data: pd.DataFrame, model: str, use_treasury_spread: bool = False) -> pd.DataFrame:
    assert model in ('yield_spread', 'dollar_price'), f'Invalid value for model: {model}'
    data.sort_values('trade_datetime', inplace=True)    # when calling `trade_history_derived_features...(...)` the order of trades needs to be ascending for `trade_datetime`
    trade_history_derived_features = trade_history_derived_features_yield_spread(use_treasury_spread) if model == 'yield_spread' else trade_history_derived_features_dollar_price
    trade_history_feature_name = 'trade_history' if model == 'yield_spread' else 'trade_history_dollar_price'
    
    temp = data[['cusip', trade_history_feature_name, 'quantity', 'trade_type']].parallel_apply(trade_history_derived_features, axis=1)
    cols = get_trade_history_columns(model)
    data[cols] = pd.DataFrame(temp.tolist(), index=data.index)
    del temp

    data.sort_values('trade_datetime', ascending=False, inplace=True)
    return data


@function_timer
def drop_features_with_null_value(df: pd.DataFrame, features: list) -> pd.DataFrame:
    # df = df.dropna(subset=features)
    for feature in features:    # perform the procedure feature by feature to output how many trades are being removed for each feature
        num_trades_before = len(df)
        df = df.dropna(subset=[feature])
        num_trades_after = len(df)
        if num_trades_before != num_trades_after: print(f'Removed {num_trades_before - num_trades_after} trades for having a null value in feature: {feature}')
    return df


@function_timer
def save_data(data: pd.DataFrame, file_name: str) -> None:
    file_path = f'{WORKING_DIRECTORY}/files/{file_name}'
    data = remove_old_trades(data, 390, dataset_name='entire processed data file')    # 390 = 13 * 30, so we are keeping approximately 13 months of data in the file; decided to keep the last 13 months of data to go beyond one year and allow for future experiments with annual patterns without having to re-create the entire dataset
    print(f'Saving data to pickle file with name {file_path}')
    data.to_pickle(file_path)
    padded_print(' SAVE DATA RUNNING ', 50)
    # upload_data(STORAGE_CLIENT, BUCKET_NAME, file_name, file_path)


def get_data_and_last_trade_datetime(bucket_name: str, file_name: str):
    '''Get the dataframe from `bucket_name/file_name` and the most recent trade datetime from this dataframe.'''
    data = download_data(STORAGE_CLIENT, bucket_name, file_name)
    if data is None: return None, EARLIST_TRADE_DATETIME, EARLIST_TRADE_DATETIME[:10]    # get trades starting from `EARLIEST_TRADE_DATETIME` if we do not have these trades already in a pickle file; string representation of datetime has the date as the first 10 characters (YYYY-MM-DD is 10 characters)
    last_trade_datetime = data.trade_datetime.max().strftime(YEAR_MONTH_DAY + 'T' + HOUR_MIN_SEC)
    last_trade_date = data.trade_date.max().date().strftime(YEAR_MONTH_DAY)
    return data, last_trade_datetime, last_trade_date


def get_data_query(last_trade_datetime, features: list, conditions: list) -> str:
    features_as_string = ', '.join(features)
    conditions = conditions + [f'trade_datetime > "{last_trade_datetime}"']
    conditions_as_string = ' AND '.join(conditions)
    return f'''SELECT {features_as_string}
               FROM `eng-reactor-287421.auxiliary_views.materialized_trade_history`
               WHERE {conditions_as_string}
               ORDER BY trade_datetime DESC'''


def update_data(model: str):
    assert model in ('yield_spread', 'dollar_price'), f'Model should be either yield_spread or dollar_price, but was instead: {model}'
    filename = CUMULATIVE_DATA_PICKLE_FILENAME_YIELD_SPREAD if model == 'yield_spread' else CUMULATIVE_DATA_PICKLE_FILENAME_DOLLAR_PRICE
    optional_arguments_for_process_data = OPTIONAL_ARGUMENTS_FOR_PROCESS_DATA_YIELD_SPREAD if model == 'yield_spread' else OPTIONAL_ARGUMENTS_FOR_PROCESS_DATA_DOLLAR_PRICE
    use_treasury_spread = optional_arguments_for_process_data.get('use_treasury_spread', False)
    data_before_last_trade_datetime, data_from_last_trade_datetime, last_trade_date, num_features_for_each_trade_in_history, raw_data_filepath = get_new_data(filename, 
                                                                                                                                                              model, 
                                                                                                                                                              use_treasury_spread=use_treasury_spread, 
                                                                                                                                                              optional_arguments_for_process_data=optional_arguments_for_process_data)
    data = combine_new_data_with_old_data(data_before_last_trade_datetime, data_from_last_trade_datetime, model)
    data = add_trade_history_derived_features(data, model, use_treasury_spread)

    predictors = PREDICTORS if model == 'yield_spread' else PREDICTORS_DOLLAR_PRICE
    data = drop_features_with_null_value(data, predictors)
    data = remove_old_trades(data, 390, dataset_name='entire processed data file')
    # if SAVE_MODEL_AND_DATA: save_data(data, filename)
    return data, last_trade_date, num_features_for_each_trade_in_history, raw_data_filepath


def save_update_data_results_to_pickle_files(model: str):
    '''The function specified in `update_data` is called, and the 3 return values are stored as pickle files. If 
    testing, then first check whether the pickle files exist, before calling `update_data`. `suffix` is appended 
    to the end of the filename for each pickle file.'''
    assert model in ('yield_spread', 'dollar_price'), f'Model should be either yield_spread or dollar_price, but was instead: {model}'
    
    data, last_trade_date, num_features_for_each_trade_in_history, raw_data_filepath = update_data(model)

    # if not os.path.isdir(f'{WORKING_DIRECTORY}/files/'): os.mkdir(f'{WORKING_DIRECTORY}/files/')
    # data_pickle_filepath = f'{WORKING_DIRECTORY}/files/data_from_update_data_{model}.pkl'
    # last_trade_data_from_update_data_pickle_filepath = f'{WORKING_DIRECTORY}/files/last_trade_data_from_update_data_{model}.pkl'
    # num_features_for_each_trade_in_history_pickle_filepath = f'{WORKING_DIRECTORY}/files/num_features_for_each_trade_in_history_{model}.pkl'
    
    # data.to_pickle(data_pickle_filepath)
    # with open(last_trade_data_from_update_data_pickle_filepath, 'wb') as file: pickle.dump(last_trade_date, file)
    # with open(num_features_for_each_trade_in_history_pickle_filepath, 'wb') as file: pickle.dump(num_features_for_each_trade_in_history, file)
    return data, last_trade_date, num_features_for_each_trade_in_history, raw_data_filepath


def fit_encoders(data: pd.DataFrame, categorical_features: list, model: str):
    '''Fits label encoders to categorical features in the data. For a few of the categorical features, the values 
    don't change for these features we use the pre-defined set of values specified in `CATEGORICAL_FEATURES_VALUES`. 
    Outputs a tuple of dictionaries where the first item is the encoders and the second item is the maximum value 
    for each class.'''
    assert model in ('yield_spread', 'dollar_price'), f'Model should be either yield_spread or dollar_price, but was instead: {model}'
    encoders = {}
    fmax = {}
    for feature in categorical_features:
        if feature in CATEGORICAL_FEATURES_VALUES:
            fprep = preprocessing.LabelEncoder().fit(CATEGORICAL_FEATURES_VALUES[feature])
        else:
            fprep = preprocessing.LabelEncoder().fit(data[feature].drop_duplicates())
        fmax[feature] = np.max(fprep.transform(fprep.classes_))
        encoders[feature] = fprep
    
    filename = 'encoders.pkl' if model == 'yield_spread' else 'encoders_dollar_price.pkl'
    with open(f'{WORKING_DIRECTORY}/{filename}', 'wb') as file:
        pickle.dump(encoders, file)
    return encoders, fmax


def _trade_history_derived_features(row, model: str, use_treasury_spread: bool = False) -> list:
    assert model in ('yield_spread', 'dollar_price'), f'Invalid value for model: {model}'
    if model == 'yield_spread':
        variants = YS_VARIANTS
        trade_history_features = get_ys_trade_history_features(use_treasury_spread)
    else:
        variants = DP_VARIANTS
        trade_history_features = get_dp_trade_history_features()

    ys_or_dp_idx = trade_history_features.index(model)
    par_traded_idx = trade_history_features.index('par_traded')
    trade_type1_idx = trade_history_features.index('trade_type1')
    trade_type2_idx = trade_history_features.index('trade_type2')
    seconds_ago_idx = trade_history_features.index('seconds_ago')


    def extract_feature_from_trade(row, name, trade):    # `name` is used solely for debugging
        ys_or_dp = trade[ys_or_dp_idx]
        ttypes = TTYPE_DICT[(trade[trade_type1_idx], trade[trade_type2_idx])] + row.trade_type
        seconds_ago = trade[seconds_ago_idx]
        quantity_diff = np.log10(1 + np.abs(10**trade[par_traded_idx] - 10**row.quantity))
        return [ys_or_dp, ttypes, seconds_ago, quantity_diff]


    global D_prev
    global S_prev
    global P_prev
    
    trade_history_feature_name = 'trade_history' if model == 'yield_spread' else 'trade_history_dollar_price'
    trade_history = row[trade_history_feature_name]
    most_recent_trade = trade_history[0]
    
    D_min_ago_t = D_prev.get(row.cusip, most_recent_trade)
    D_min_ago = LONG_TIME_AGO_IN_NUM_SECONDS        

    P_min_ago_t = P_prev.get(row.cusip, most_recent_trade)
    P_min_ago = LONG_TIME_AGO_IN_NUM_SECONDS
    
    S_min_ago_t = S_prev.get(row.cusip, most_recent_trade)
    S_min_ago = LONG_TIME_AGO_IN_NUM_SECONDS
    
    max_ys_or_dp_t = most_recent_trade
    max_ys_or_dp = most_recent_trade[ys_or_dp_idx]
    min_ys_or_dp_t = most_recent_trade
    min_ys_or_dp = most_recent_trade[ys_or_dp_idx]
    max_qty_t = most_recent_trade
    max_qty = most_recent_trade[par_traded_idx]
    min_ago_t = most_recent_trade
    min_ago = most_recent_trade[seconds_ago_idx]
    
    for trade in trade_history:
        seconds_ago = trade[seconds_ago_idx]
        # Checking if the first trade in the history is from the same block; TODO: shouldn't this be checked for every trade?
        if seconds_ago == 0: continue

        ys_or_dp = trade[ys_or_dp_idx]
        if ys_or_dp > max_ys_or_dp: 
            max_ys_or_dp_t = trade
            max_ys_or_dp =ys_or_dp
        elif ys_or_dp < min_ys_or_dp: 
            min_ys_or_dp_t = trade
            min_ys_or_dp = ys_or_dp

        par_traded = trade[par_traded_idx]
        if par_traded > max_qty: 
            max_qty_t = trade 
            max_qty = par_traded

        if seconds_ago < min_ago:    # TODO: isn't this just the most recent trade not in the same block, and isn't this initialized above already?
            min_ago_t = trade
            min_ago = seconds_ago
            
        side = TTYPE_DICT[(trade[trade_type1_idx], trade[trade_type2_idx])]
        if side == 'D':
            if seconds_ago < D_min_ago: 
                D_min_ago_t = trade
                D_min_ago = seconds_ago
                D_prev[row.cusip] = trade
        elif side == 'P':
            if seconds_ago < P_min_ago: 
                P_min_ago_t = trade
                P_min_ago = seconds_ago
                P_prev[row.cusip] = trade
        elif side == 'S':
            if seconds_ago < S_min_ago: 
                S_min_ago_t = trade
                S_min_ago = seconds_ago
                S_prev[row.cusip] = trade
        else: 
            print('invalid side', trade)
    
    variant_trade_dict = dict(zip(variants, [max_ys_or_dp_t, min_ys_or_dp_t, max_qty_t, min_ago_t, D_min_ago_t, P_min_ago_t, S_min_ago_t]))
    variant_trade_list = []
    for variant_name, variant_trade in variant_trade_dict.items():
        feature_list = extract_feature_from_trade(row, variant_name, variant_trade)
        variant_trade_list += feature_list
    return variant_trade_list


def trade_history_derived_features_yield_spread(use_treasury_spread) -> callable:
    return lambda row: _trade_history_derived_features(row, 'yield_spread', use_treasury_spread)
def trade_history_derived_features_dollar_price(row) -> list:
    return _trade_history_derived_features(row, 'dollar_price')


def remove_lines_with_character(character_to_remove, file_path, new_file_path=None):
    with open(file_path, 'r') as file:    # read the file
        lines = file.readlines()
    filtered_lines = [line for line in lines if character_to_remove not in line]    # filter out lines containing the specified character
    if new_file_path is None: new_file_path = file_path
    with open(new_file_path, 'w') as file:    # write the filtered lines back to the file
        file.writelines(filtered_lines)

def remove_lines_with_character(character_to_remove, file_path, new_file_path=None):
    with open(file_path, 'r') as file:    # read the file
        lines = file.readlines()
    filtered_lines = [line for line in lines if character_to_remove not in line]    # filter out lines containing the specified character
    if new_file_path is None: new_file_path = file_path
    with open(new_file_path, 'w') as file:    # write the filtered lines back to the file
        file.writelines(filtered_lines)