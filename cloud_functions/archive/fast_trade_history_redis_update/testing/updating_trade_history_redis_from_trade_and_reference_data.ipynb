{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook manually updates trades to the trade history redis in the event of missing trades. This has been used due to transient issues with the `get_msrb_trade_messages` cloud function and the `fast_trade_history_redis_update` cloud function. See https://www.notion.so/Research-on-making-sure-we-are-getting-all-the-data-c5147fbaf0494d0abf051b557357ba1b for more details. The particular transient issues for those cloud functions have been handled (by updating `latest_sequence_number` at the end of `fast_trade_history_redis_update` instead of at the beginning) but this notebook may still be valuable in any instance that requires manually updating the trade history redis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# TODO: comment the below line out when not running the function locally\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/user/ficc/mitas_creds.json'\n",
    "\n",
    "import decimal\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from main import get_frequency, \\\n",
    "                 add_restrictions_on_interest_payment_frequency, \\\n",
    "                 typecast_yield, \\\n",
    "                 add_calc_date, \\\n",
    "                 concatenate_date_and_time_objects_into_datetime_object, \\\n",
    "                 typecast_for_bigquery, \\\n",
    "                 update_trade_history_redis, \\\n",
    "                 upload_trade_history_to_trade_history_bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_CLIENT = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqltodf(sql_query, limit=''):\n",
    "    if limit != '': limit = f' ORDER BY RAND() LIMIT {limit}'\n",
    "    return BQ_CLIENT.query(sql_query + limit).result().to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps the query to a pickle filename so that after running the query (which takes about 5 minutes) we do not need to re-run it if there are downstream errors\n",
    "query_to_pickle_filename = {'select * from `auxiliary_views.trades_with_ref_data_pd` where rtrs_control_number = 2024010300517200': '923004TL9.pkl', \n",
    "                            'SELECT b.*EXCEPT (material_event_history,default_event_history) FROM `eng-reactor-287421.jesse_tests.missing_trades` a LEFT JOIN `auxiliary_views.trades_with_ref_data_pd` b ON a.rtrs_control_number = b.rtrs_control_number': 'missing_trades.pkl'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'SELECT b.*EXCEPT (material_event_history,default_event_history) FROM `eng-reactor-287421.jesse_tests.missing_trades` a LEFT JOIN `auxiliary_views.trades_with_ref_data_pd` b ON a.rtrs_control_number = b.rtrs_control_number'\n",
    "pickle_filename = query_to_pickle_filename.get(query, None)\n",
    "if pickle_filename is not None and os.path.isfile(pickle_filename):\n",
    "    all_data = pd.read_pickle(pickle_filename)\n",
    "else:\n",
    "    all_data = sqltodf(query)\n",
    "    all_data['series_id'] = all_data['issue_key']\n",
    "    if pickle_filename is not None: all_data.to_pickle(pickle_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in all_data.columns:\n",
    "    column_value = all_data[column].iloc[0]\n",
    "    if isinstance(column_value, decimal.Decimal): \n",
    "        print(f'{column} is currently type decimal.Decimal, so converting it to float')\n",
    "        all_data[column] = pd.to_numeric(all_data[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ('publish_time', 'time_of_trade'):    # these columns need to be string type instead of datetime.time type since `concatenate_date_and_time_objects_into_datetime_object(...)` expects them to be strings\n",
    "    all_data[column] = all_data[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_after_restrictions = add_restrictions_on_interest_payment_frequency(all_data)\n",
    "\n",
    "if len(all_data_after_restrictions) == 0:    # do not perform any processing if there are no trades after applying restrictions\n",
    "    print(f'No trades left after applying restrictions. Before restrictions:')\n",
    "    print(all_data.to_markdown())\n",
    "else:\n",
    "    all_data = all_data_after_restrictions\n",
    "    all_data['interest_payment_frequency'] = all_data['interest_payment_frequency'].apply(get_frequency)\n",
    "    all_data = typecast_yield(all_data)\n",
    "    all_data = add_calc_date(all_data)\n",
    "    all_data = concatenate_date_and_time_objects_into_datetime_object(all_data)\n",
    "    all_data = typecast_for_bigquery(all_data, {'issue_key': 'Int64'})    # sometimes the numerical data that is supposed to be an integer comes in as a float causing an error when attempting to upload to BigQuery; use `Int64` instead of `int` to allow conversion when there are `None` values: https://stackoverflow.com/questions/26614465/python-pandas-apply-function-if-a-column-value-is-not-null\n",
    "    cusip_trade_history_pairs = update_trade_history_redis(all_data)\n",
    "    upload_trade_history_to_trade_history_bigquery(cusip_trade_history_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pickle_filename is not None and os.path.isfile(pickle_filename): os.remove(pickle_filename)    # remove the pickle file after completing the processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
