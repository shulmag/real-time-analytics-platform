{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a single related trade\n",
    "\n",
    "Past work: https://www.notion.so/Handling-Trade-History-f9071a731fc5496a9af17ccd524d53c9. This notebook asks if we can get value out of using a single related trade. Data file: https://console.cloud.google.com/storage/browser/_details/ficc_training_data_latest/processed_data_2023-08-21-14:53.pkl;tab=live_object?authuser=1&project=eng-reactor-287421. \n",
    "\n",
    "This notebook is heavily based on: https://github.com/Ficc-ai/ficc/blob/ahmad_ml/ml_models/sequence_predictors/yield_spread_model_attention_experiment.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing pandarallel with 5.0 cores\n",
      "INFO: Pandarallel will run on 5 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('mode.chained_assignment', None)    # Do not display SettingWithCopyError\n",
    "import time\n",
    "from functools import wraps\n",
    "import multiprocess as mp\n",
    "\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from sklearn import preprocessing, cluster\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "\n",
    "from ficc.data.process_data import process_data\n",
    "from ficc.utils.auxiliary_variables import PREDICTORS, NON_CAT_FEATURES, BINARY, CATEGORICAL_FEATURES, IDENTIFIERS, PURPOSE_CLASS_DICT\n",
    "from ficc.utils.gcp_storage_functions import upload_data, download_data\n",
    "\n",
    "from ficc.utils.related_trade import append_recent_trade_data, get_appended_feature_name\n",
    "# from ficc.utils.auxiliary_variables import RELATED_TRADE_BINARY_FEATURES, RELATED_TRADE_NON_CAT_FEATURES, RELATED_TRADE_CATEGORICAL_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_SPLIT = 0.85\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 1000\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "DROPOUT = 0.10\n",
    "SEQUENCE_LENGTH = 5\n",
    "NUM_FEATURES = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRADE_TYPE_MAPPING = {'D': (0, 0),\n",
    "                      'S': (0, 1), \n",
    "                      'P': (1, 0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_timer(function_to_time):\n",
    "    '''This function is to be used as a decorator. It will print out the execution time of `function_to_time`.'''\n",
    "    @wraps(function_to_time)    # used to ensure that the function name is still the same after applying the decorator when running tests: https://stackoverflow.com/questions/6312167/python-unittest-cant-call-decorated-test\n",
    "    def wrapper(*args, **kwargs):    # using the same formatting from https://docs.python.org/3/library/functools.html\n",
    "        print(f'Begin execution of {function_to_time.__name__}')\n",
    "        start_time = time.time()\n",
    "        result = function_to_time(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f'Execution time of {function_to_time.__name__}: {timedelta(seconds=end_time - start_time)}')\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the data file to have related trades\n",
    "The related trade that we use is the most recent trade that matches the *TODO: fill in*. *TODO: determine the best related trade criteria*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if the treasury spreads and target attention features are present in `PREDICTORS`\n"
     ]
    }
   ],
   "source": [
    "print('Checking if the treasury spreads and target attention features are present in `PREDICTORS`')\n",
    "if 'ficc_treasury_spread' not in PREDICTORS:\n",
    "    PREDICTORS.append('ficc_treasury_spread')\n",
    "    NON_CAT_FEATURES.append('ficc_treasury_spread')\n",
    "if 'target_attention_features' not in PREDICTORS:\n",
    "    PREDICTORS.append('target_attention_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_processed_file_pickle(filename):\n",
    "    '''Read from `filename`, which should be a pickle file with the trade data after running the query.'''\n",
    "    if not os.path.isfile(filename):\n",
    "        print(f'{filename} not found')\n",
    "        return None\n",
    "    print(f'START: Reading from processed file at {filename}')\n",
    "    data = pd.read_pickle(filename)\n",
    "    print(f'END: Reading from processed file at {filename}')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START: Reading from processed file at ../data/mitas_jan.pkl\n",
      "END: Reading from processed file at ../data/mitas_jan.pkl\n"
     ]
    }
   ],
   "source": [
    "processed_file_pickle = '../data/mitas_jan.pkl'    # '../data/processed_data_2023-08-21-14-53.pkl'\n",
    "data = read_processed_file_pickle(processed_file_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earliest trade date: 2023-01-03 00:00:00. Most recent trade date: 2023-01-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(f'Earliest trade date: {data.trade_date.min()}. Most recent trade date: {data.trade_date.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricting the data to just January 2023\n",
      "828012 from 2023-01-03 00:00:00 to 2023-01-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print('Restricting the data to just January 2023')\n",
    "data = data[(datetime(2022, 12, 31) < data.trade_date) & (data.trade_date < datetime(2023, 2, 1))]\n",
    "print(f'{len(data)} from {data.trade_date.min()} to {data.trade_date.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricting history to 5 trades\n"
     ]
    }
   ],
   "source": [
    "print(f'Restricting history to {SEQUENCE_LENGTH} trades')\n",
    "data.trade_history = data.trade_history.apply(lambda x: x[:SEQUENCE_LENGTH])\n",
    "data.target_attention_features = data.target_attention_features.apply(lambda x: x[:SEQUENCE_LENGTH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting the data by `trade_datetime`\n"
     ]
    }
   ],
   "source": [
    "print('Sorting the data by `trade_datetime`')\n",
    "data.sort_values('trade_datetime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_columns(data):\n",
    "    columns = sorted(list(data.columns))\n",
    "    print(f'The data currently has the following {len(data.columns)} columns:')\n",
    "    for column_idx in range(0, len(columns), 5):\n",
    "        print(columns[column_idx : column_idx + 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data currently has the following 189 columns:\n",
      "['A/E', 'D_min_ago_ago', 'D_min_ago_qdiff', 'D_min_ago_ttypes', 'D_min_ago_ys']\n",
      "['P_min_ago_ago', 'P_min_ago_qdiff', 'P_min_ago_ttypes', 'P_min_ago_ys', 'S_min_ago_ago']\n",
      "['S_min_ago_qdiff', 'S_min_ago_ttypes', 'S_min_ago_ys', 'accrual_date', 'accrued_days']\n",
      "['calc_date', 'calc_day_cat', 'call_timing', 'call_timing_in_part', 'call_to_maturity']\n",
      "['callable', 'callable_at_cav', 'called', 'called_redemption_type', 'capital_type']\n",
      "['coupon', 'coupon_type', 'cusip', 'cusip_mean_last_1', 'cusip_mean_last_10']\n",
      "['cusip_mean_last_15', 'cusip_mean_last_2', 'cusip_mean_last_20', 'cusip_mean_last_30', 'cusip_mean_last_5']\n",
      "['cusip_mean_last_50', 'cusip_series', 'dated_date', 'days_in_interest_payment', 'days_to_call']\n",
      "['days_to_maturity', 'days_to_par', 'days_to_refund', 'days_to_settle', 'de_minimis_threshold']\n",
      "['default_indicator', 'deferred', 'delivery_date', 'dollar_price', 'escrow_exists']\n",
      "['ex_cusip_masked_series_average_1', 'ex_cusip_masked_series_average_10', 'ex_cusip_masked_series_average_15', 'ex_cusip_masked_series_average_2', 'ex_cusip_masked_series_average_20']\n",
      "['ex_cusip_masked_series_average_30', 'ex_cusip_masked_series_average_5', 'ex_cusip_masked_series_average_50', 'extraordinary_make_whole_call', 'federal_tax_status']\n",
      "['ficc_treasury_spread', 'first_coupon_date', 'has_unexpired_lines_of_credit', 'id', 'incorporated_state_code']\n",
      "['instrument_primary_name', 'interest_payment_frequency', 'is_callable', 'is_called', 'is_general_obligation']\n",
      "['is_non_transaction_based_compensation', 'issue_amount', 'issue_price', 'issue_text', 'last_calc_date']\n",
      "['last_calc_day_cat', 'last_dollar_price', 'last_duration', 'last_maturity_date', 'last_next_call_date']\n",
      "['last_par_call_date', 'last_period_accrues_from_date', 'last_real_time_ficc_ycl_to_maturity', 'last_real_time_ficc_ycl_to_next_call', 'last_real_time_ficc_ycl_to_par_call']\n",
      "['last_real_time_ficc_ycl_to_refund', 'last_refund_date', 'last_rtrs_control_number', 'last_seconds_ago', 'last_settlement_date']\n",
      "['last_size', 'last_trade_date', 'last_trade_datetime', 'last_trade_type', 'last_yield']\n",
      "['last_yield_spread', 'make_whole_call', 'masked_cusip_average_5', 'masked_series_average_1', 'masked_series_average_10']\n",
      "['masked_series_average_15', 'masked_series_average_2', 'masked_series_average_20', 'masked_series_average_30', 'masked_series_average_5']\n",
      "['masked_series_average_50', 'maturity_amount', 'maturity_date', 'maturity_description_code', 'max_amount_outstanding']\n",
      "['max_qty_ago', 'max_qty_qdiff', 'max_qty_ttypes', 'max_qty_ys', 'max_ys_ago']\n",
      "['max_ys_qdiff', 'max_ys_ttypes', 'max_ys_ys', 'min_ago_ago', 'min_ago_qdiff']\n",
      "['min_ago_ttypes', 'min_ago_ys', 'min_amount_outstanding', 'min_ys_ago', 'min_ys_qdiff']\n",
      "['min_ys_ttypes', 'min_ys_ys', 'moodys_long', 'muni_issue_type', 'muni_security_type']\n",
      "['new_ficc_ycl', 'new_ys', 'next_call_date', 'next_call_price', 'next_coupon_payment_date']\n",
      "['next_sink_date', 'orig_principal_amount', 'original_yield', 'other_enhancement_type', 'par_call_date']\n",
      "['par_call_price', 'par_price', 'par_traded', 'previous_coupon_payment_date', 'publish_datetime']\n",
      "['purpose_class', 'purpose_sub_class', 'quantity', 'rating', 'real_time_ficc_ycl']\n",
      "['real_time_ficc_ycl_to_maturity', 'real_time_ficc_ycl_to_next_call', 'real_time_ficc_ycl_to_par_call', 'real_time_ficc_ycl_to_refund', 'refund_date']\n",
      "['refund_price', 'rtrs_control_number', 'scaled_accrued_days', 'series_mean_last_1', 'series_mean_last_10']\n",
      "['series_mean_last_100', 'series_mean_last_15', 'series_mean_last_2', 'series_mean_last_20', 'series_mean_last_30']\n",
      "['series_mean_last_5', 'series_mean_last_50', 'series_name', 'settlement_date', 'sink_amount_type']\n",
      "['sink_frequency', 'sinking', 'sp_long', 'sp_stand_alone', 'state_tax_status']\n",
      "['target_attention_features', 'test', 'trade_date', 'trade_datetime', 'trade_history']\n",
      "['trade_history_sum', 'trade_type', 'transaction_type', 'treasury_rate', 'use_of_proceeds']\n",
      "['when_issued', 'whenissued', 'yield', 'zerocoupon']\n"
     ]
    }
   ],
   "source": [
    "print_columns(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating features from trade history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttype_dict = {value: key for key, value in TRADE_TYPE_MAPPING.items()}\n",
    "\n",
    "ys_variants = ['max_ys', 'min_ys', 'max_qty', 'min_ago', 'D_min_ago', 'P_min_ago', 'S_min_ago']\n",
    "ys_feats = ['_ys', '_ttypes', '_ago', '_qdiff']\n",
    "D_prev = dict()\n",
    "P_prev = dict()\n",
    "S_prev = dict()\n",
    "\n",
    "\n",
    "def get_trade_history_columns():\n",
    "    '''This function is used to create a list of columns.'''\n",
    "    YS_COLS = []\n",
    "    for prefix in ys_variants:\n",
    "        for suffix in ys_feats:\n",
    "            YS_COLS.append(prefix + suffix)\n",
    "    return YS_COLS\n",
    "\n",
    "\n",
    "def extract_feature_from_trade(row, name, trade):\n",
    "    yield_spread = trade[0]\n",
    "    ttypes = ttype_dict[(trade[3], trade[4])] + row.trade_type\n",
    "    seconds_ago = trade[5]\n",
    "    quantity_diff = np.log10(1 + np.abs(10**trade[2] - 10**row.quantity))\n",
    "    return [yield_spread, ttypes, seconds_ago, quantity_diff]\n",
    "\n",
    "\n",
    "def trade_history_derived_features(row):\n",
    "    trade_history = row.trade_history\n",
    "    trade = trade_history[0]\n",
    "    \n",
    "    D_min_ago_t = D_prev.get(row.cusip, trade)\n",
    "    D_min_ago = 9        \n",
    "\n",
    "    P_min_ago_t = P_prev.get(row.cusip, trade)\n",
    "    P_min_ago = 9\n",
    "    \n",
    "    S_min_ago_t = S_prev.get(row.cusip, trade)\n",
    "    S_min_ago = 9\n",
    "    \n",
    "    max_ys_t = trade; max_ys = trade[0]\n",
    "    min_ys_t = trade; min_ys = trade[0]\n",
    "    max_qty_t = trade; max_qty = trade[2]\n",
    "    min_ago_t = trade; min_ago = trade[5]\n",
    "    \n",
    "    for trade in trade_history[0:]:\n",
    "        # Checking if the first trade in the history is from the same block\n",
    "        if trade[5] == 0: \n",
    "            continue\n",
    " \n",
    "        if trade[0] > max_ys: \n",
    "            max_ys_t = trade\n",
    "            max_ys = trade[0]\n",
    "        elif trade[0] < min_ys: \n",
    "            min_ys_t = trade; \n",
    "            min_ys = trade[0]\n",
    "\n",
    "        if trade[2] > max_qty: \n",
    "            max_qty_t = trade \n",
    "            max_qty = trade[2]\n",
    "        if trade[5] < min_ago: \n",
    "            min_ago_t = trade; \n",
    "            min_ago = trade[5]\n",
    "            \n",
    "        side = ttype_dict[(trade[3], trade[4])]\n",
    "        if side == 'D':\n",
    "            if trade[5] < D_min_ago: \n",
    "                D_min_ago_t = trade; D_min_ago = trade[5]\n",
    "                D_prev[row.cusip] = trade\n",
    "        elif side == 'P':\n",
    "            if trade[5] < P_min_ago: \n",
    "                P_min_ago_t = trade; P_min_ago = trade[5]\n",
    "                P_prev[row.cusip] = trade\n",
    "        elif side == 'S':\n",
    "            if trade[5] < S_min_ago: \n",
    "                S_min_ago_t = trade; S_min_ago = trade[5]\n",
    "                S_prev[row.cusip] = trade\n",
    "        else: \n",
    "            print('invalid side', trade)\n",
    "    \n",
    "    trade_history_dict = {'max_ys': max_ys_t,\n",
    "                          'min_ys': min_ys_t,\n",
    "                          'max_qty': max_qty_t,\n",
    "                          'min_ago': min_ago_t,\n",
    "                          'D_min_ago': D_min_ago_t,\n",
    "                          'P_min_ago': P_min_ago_t,\n",
    "                          'S_min_ago': S_min_ago_t}\n",
    "\n",
    "    return_list = []\n",
    "    for variant in ys_variants:\n",
    "        feature_list = extract_feature_from_trade(row,variant, trade_history_dict[variant])\n",
    "        return_list += feature_list\n",
    "    \n",
    "    return return_list\n",
    "\n",
    "\n",
    "@function_timer\n",
    "def apply_trade_history_derived_features_to_every_trade(data):\n",
    "    return data[['cusip', 'trade_history', 'quantity', 'trade_type']].parallel_apply(trade_history_derived_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin execution of apply_trade_history_derived_features_to_every_trade\n",
      "Execution time of apply_trade_history_derived_features_to_every_trade: 0:00:23.401240\n",
      "There are 28 trade history derived features which are:\n",
      "['max_ys_ys', 'max_ys_ttypes', 'max_ys_ago', 'max_ys_qdiff', 'min_ys_ys', 'min_ys_ttypes', 'min_ys_ago', 'min_ys_qdiff', 'max_qty_ys', 'max_qty_ttypes', 'max_qty_ago', 'max_qty_qdiff', 'min_ago_ys', 'min_ago_ttypes', 'min_ago_ago', 'min_ago_qdiff', 'D_min_ago_ys', 'D_min_ago_ttypes', 'D_min_ago_ago', 'D_min_ago_qdiff', 'P_min_ago_ys', 'P_min_ago_ttypes', 'P_min_ago_ago', 'P_min_ago_qdiff', 'S_min_ago_ys', 'S_min_ago_ttypes', 'S_min_ago_ago', 'S_min_ago_qdiff']\n",
      "CPU times: user 11 s, sys: 2.88 s, total: 13.9 s\n",
      "Wall time: 30.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "temp = apply_trade_history_derived_features_to_every_trade(data)\n",
    "YS_COLS = get_trade_history_columns()\n",
    "num_columns_for_each_trade_in_temp = len(temp.iloc[0])\n",
    "assert len(YS_COLS) == num_columns_for_each_trade_in_temp, f'The number of columns in YS_COLS, {len(YS_COLS)}, does not match the number of columns in the constructed dataframe from calling `apply_trade_history_derived_features_to_every_trade`, {num_columns_for_each_trade_in_temp}'\n",
    "print(f'There are {num_columns_for_each_trade_in_temp} trade history derived features which are:\\n{YS_COLS}')\n",
    "data[YS_COLS] = pd.DataFrame(temp.tolist(), index=data.index)\n",
    "del temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding trade history features to `PREDICTORS` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in YS_COLS:\n",
    "    if 'ttypes' in col and col not in PREDICTORS:\n",
    "        PREDICTORS.append(col)\n",
    "        CATEGORICAL_FEATURES.append(col)\n",
    "    elif col not in PREDICTORS:\n",
    "        NON_CAT_FEATURES.append(col)\n",
    "        PREDICTORS.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trades after creating `trade_history_sum` feature: 828012\n"
     ]
    }
   ],
   "source": [
    "data['trade_history_sum'] = data.trade_history.parallel_apply(lambda x: np.sum(x))\n",
    "data = data.dropna(subset=['trade_history_sum'])\n",
    "print(f'Number of trades after creating `trade_history_sum` feature: {len(data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `new_ys` for every trade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping trades where `new_ys` is null which is a result of not having a last trade and being unable to have a value for the feature `new_real_time_ficc_ycl`\n"
     ]
    }
   ],
   "source": [
    "data['new_ys'] = data['yield'] - data['new_ficc_ycl']    # data['new_real_time_ficc_ycl']\n",
    "print('Dropping trades where `new_ys` is null which is a result of not having a last trade and being unable to have a value for the feature `new_real_time_ficc_ycl`')\n",
    "data = data.dropna(subset=['new_ys'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting encoders to the categorical features. These encoders are then used to encode the categorical features of the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_feature_values = {'purpose_class' : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, \n",
    "                                                 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, \n",
    "                                                 47, 48, 49, 50, 51, 52, 53], \n",
    "                              'rating' : ['A', 'A+', 'A-', 'AA', 'AA+', 'AA-', 'AAA', 'B', 'B+', 'B-', 'BB', 'BB+', 'BB-', \n",
    "                                          'BBB', 'BBB+', 'BBB-', 'CC', 'CCC', 'CCC+', 'CCC-' , 'D', 'NR', 'MR'], \n",
    "                              'trade_type' : ['D', 'S', 'P'], \n",
    "                              'incorporated_state_code' : ['AK', 'AL', 'AR', 'AS', 'AZ', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA', 'GU', \n",
    "                                                           'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME', 'MI', 'MN', \n",
    "                                                           'MO', 'MP', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM', 'NV', 'NY', 'OH', \n",
    "                                                           'OK', 'OR', 'PA', 'PR', 'RI', 'SC', 'SD', 'TN', 'TX', 'US', 'UT', 'VA', 'VI', \n",
    "                                                           'VT', 'WA', 'WI', 'WV', 'WY']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating\n",
      "incorporated_state_code\n",
      "trade_type\n",
      "purpose_class\n",
      "max_ys_ttypes\n",
      "min_ys_ttypes\n",
      "max_qty_ttypes\n",
      "min_ago_ttypes\n",
      "D_min_ago_ttypes\n",
      "P_min_ago_ttypes\n",
      "S_min_ago_ttypes\n"
     ]
    }
   ],
   "source": [
    "encoders = {}\n",
    "fmax = {}\n",
    "for f in CATEGORICAL_FEATURES:\n",
    "    print(f)\n",
    "    if f in categorical_feature_values:\n",
    "        fprep = preprocessing.LabelEncoder().fit(categorical_feature_values[f])\n",
    "    else:\n",
    "        fprep = preprocessing.LabelEncoder().fit(data[f].drop_duplicates())\n",
    "    fmax[f] = np.max(fprep.transform(fprep.classes_))\n",
    "    encoders[f] = fprep\n",
    "    \n",
    "with open('../data/encoders.pkl', 'wb') as file:\n",
    "    pickle.dump(encoders, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exclusions and partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying basic exclusions:\n",
    "- Invalid issue amount\n",
    "- Callable less than a year into the future\n",
    "- Maturity less than a year in the future and more than 30 years in the future\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trades after applying exclusions: 828012\n"
     ]
    }
   ],
   "source": [
    "data.issue_amount = data.issue_amount.replace([np.inf, -np.inf], np.nan)\n",
    "# data = data[(data.days_to_call == 0) | (data.days_to_call > np.log10(400))]\n",
    "# data = data[(data.days_to_refund == 0) | (data.days_to_refund > np.log10(400))]\n",
    "# data = data[(data.days_to_maturity == 0) | (data.days_to_maturity > np.log10(400))]\n",
    "# data = data[data.days_to_maturity < np.log10(30000)]\n",
    "print(f'Number of trades after applying exclusions: {len(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data):\n",
    "    print(f'Split the data into train and test sets at a {TRAIN_TEST_SPLIT} train/test ratio')\n",
    "    train_size = int(len(data) * TRAIN_TEST_SPLIT)\n",
    "    test_size = len(data) - train_size\n",
    "    print(f'Train set size: {train_size}\\t\\tTest set size: {test_size}')\n",
    "    train_data = data.head(train_size)\n",
    "    test_data = data.tail(test_size)\n",
    "    assert train_data.iloc[-1].trade_datetime <= test_data.iloc[0].trade_datetime, 'Most recent trade from the train data should be before the oldest trade from the test data'\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining *related* by matching certain categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following features will be added to each trade:\n",
      "['related_last_new_ys', 'related_last_ficc_treasury_spread', 'related_last_quantity', 'related_last_seconds_ago']\n"
     ]
    }
   ],
   "source": [
    "related_trade_prefix = 'related_last_'\n",
    "get_related_trade_feature = lambda feature: lambda curr, related_trade: related_trade[feature]\n",
    "RELATED_TRADE_FEATURES_FUNCTIONS = {'new_ys': get_related_trade_feature('new_ys'), \n",
    "                                    'ficc_treasury_spread': get_related_trade_feature('ficc_treasury_spread'), \n",
    "                                    'quantity': get_related_trade_feature('quantity'), \n",
    "                                    # 'trade_type1': lambda curr, related_trade: TRADE_TYPE_MAPPING[related_trade['trade_type']][0], \n",
    "                                    # 'trade_type2': lambda curr, related_trade: TRADE_TYPE_MAPPING[related_trade['trade_type']][1], \n",
    "                                    'seconds_ago': lambda curr, related_trade: np.log10(1 + (curr['trade_datetime'] - related_trade['trade_datetime']).total_seconds())}\n",
    "related_trade_features_functions_and_default_values = {feature: (function, 0) for feature, function in RELATED_TRADE_FEATURES_FUNCTIONS.items()}    # for simplicity, set the default value to 0 for every feature\n",
    "related_trade_features = [get_appended_feature_name(0, feature, related_trade_prefix) for feature in RELATED_TRADE_FEATURES_FUNCTIONS]    # first argument of `get_appended_feature_name(...)` is 0 since we are only adding a single related trade and so it corresponds to index 0; insertion order of the dictionary is preserved for Python v3.7+\n",
    "print(f'The following features will be added to each trade:\\n{related_trade_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_single_recent_trade_data_using_shift(df, features, related_trade_prefix, default_value=0, min_seconds_ago=60, exclude_same_cusip=True):\n",
    "    '''All features in `features` will be added as related features in addition to the seconds ago of the related trade.'''\n",
    "    assert all([feature in df.columns for feature in features + ['trade_datetime']])\n",
    "    datetime_far_in_the_future = pd.to_datetime('2100-01-01 00:00:00')\n",
    "    related_seconds_ago = related_trade_prefix + 'seconds_ago'\n",
    "    related_trade_datetime = related_trade_prefix + 'trade_datetime'\n",
    "    related_cusip = related_trade_prefix + 'cusip'\n",
    "\n",
    "    # first pass\n",
    "    shift_idx = 1\n",
    "    for feature in features:\n",
    "        df[related_trade_prefix + feature] = df[feature].shift(shift_idx, fill_value=default_value)\n",
    "    df[related_cusip] = df['cusip'].shift(shift_idx, fill_value='')\n",
    "    df[related_trade_datetime] = df['trade_datetime'].shift(shift_idx, fill_value=datetime_far_in_the_future)    # filling this with a date in the future so the gap between this value and the current trade_datetime is very negative\n",
    "    df[related_seconds_ago] = (df['trade_datetime'] - df['related_last_trade_datetime']).apply(lambda time_delta: time_delta.total_seconds())\n",
    "\n",
    "    def condition_on_seconds_ago_and_same_cusip(df):\n",
    "        related_seconds_ago_less_than_min_seconds_ago = df[related_seconds_ago] < min_seconds_ago\n",
    "        if exclude_same_cusip:\n",
    "            return related_seconds_ago_less_than_min_seconds_ago | (df['cusip'] == df[related_cusip])\n",
    "        return related_seconds_ago_less_than_min_seconds_ago\n",
    "\n",
    "    while shift_idx < len(df) - 1:    # do not need to shift past the end of the dataframe\n",
    "        condition = condition_on_seconds_ago_and_same_cusip(df)\n",
    "        if condition.sum() == 0 or condition.size - np.argmax(condition[::-1]) - 1 < shift_idx: break    # `condition.size - np.argmax(condition[::-1]) - 1` finds the position of the last `True` value in `condition`\n",
    "        shift_idx += 1\n",
    "        for feature in features:\n",
    "            df.loc[condition, related_trade_prefix + feature] = df[feature].shift(shift_idx, fill_value=default_value)[condition.values]\n",
    "        df.loc[condition, related_cusip] = df['cusip'].shift(shift_idx, fill_value='')\n",
    "        df.loc[condition, related_trade_datetime] = df['trade_datetime'].shift(shift_idx, fill_value=datetime_far_in_the_future)[condition.values]    # filling this with a date in the future so the gap between this value and the current trade_datetime is very negative\n",
    "        df[related_seconds_ago] = (df['trade_datetime'] - df[related_trade_datetime]).apply(lambda time_delta: time_delta.total_seconds())\n",
    "        \n",
    "    # print(f'`append_single_recent_trade_data_using_shift` took {shift_idx} shifts to satisfy conditions for {len(df)} cusips')\n",
    "    df[related_seconds_ago] = df[related_seconds_ago].clip(0)    # clip all of the negative seconds_ago values to 0\n",
    "    return df.drop(columns=[related_trade_datetime])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_single_recent_trade_data_using_shift_caller = lambda header, df: append_single_recent_trade_data_using_shift(df, ['new_ys', 'ficc_treasury_spread', 'quantity'], related_trade_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_with_related_trade_info(data, group_by_features):\n",
    "    if type(group_by_features) == str: group_by_features = [group_by_features]\n",
    "    for feature in group_by_features:\n",
    "        assert feature in data.columns\n",
    "    data_with_related_trade_info = []\n",
    "    # for header, df in data.groupby(group_by_features):\n",
    "    #     data_with_related_trade_info.append(append_single_recent_trade_data_using_shift_caller(header, df))\n",
    "    with mp.Pool() as pool_object:\n",
    "        data_with_related_trade_info = pool_object.starmap(append_single_recent_trade_data_using_shift_caller, data.groupby(group_by_features))\n",
    "    data_with_related_trade_info = pd.concat(data_with_related_trade_info)\n",
    "    return data_with_related_trade_info.sort_values('trade_datetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching on issuer and `trade_type` and a function of `maturity_date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['issuer'] = data['cusip'].str.slice(0, 6)    # issuer is the first 6 digits of the CUSIP; slicing a string in a pandas dataframe: https://medium.com/geekculture/how-do-you-use-slice-method-in-the-pandas-dataframe-on-string-data-type-columns-6a8fd02c15eb#:~:text=slice()%20method%20in%20Pandas,(start%2C%20stop)%20method.\n",
    "data['maturity_year'] = data['maturity_date'].dt.year\n",
    "data['maturity_year_by_2'] = data['maturity_year'] // 2\n",
    "data['maturity_year_by_5'] = data['maturity_year'] // 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique issuers: 17692\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of unique issuers: {data[\"issuer\"].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 36s, sys: 6.47 s, total: 3min 43s\n",
      "Wall time: 3min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = get_data_with_related_trade_info(data, ['issuer', 'maturity_year_by_5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['issuer', 'maturity_year', 'maturity_year_by_2', 'maturity_year_by_5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cusip</th>\n",
       "      <th>yield</th>\n",
       "      <th>is_callable</th>\n",
       "      <th>refund_date</th>\n",
       "      <th>refund_price</th>\n",
       "      <th>accrual_date</th>\n",
       "      <th>dated_date</th>\n",
       "      <th>next_sink_date</th>\n",
       "      <th>coupon</th>\n",
       "      <th>delivery_date</th>\n",
       "      <th>...</th>\n",
       "      <th>ex_cusip_masked_series_average_10</th>\n",
       "      <th>ex_cusip_masked_series_average_15</th>\n",
       "      <th>ex_cusip_masked_series_average_20</th>\n",
       "      <th>ex_cusip_masked_series_average_30</th>\n",
       "      <th>ex_cusip_masked_series_average_50</th>\n",
       "      <th>related_last_new_ys</th>\n",
       "      <th>related_last_ficc_treasury_spread</th>\n",
       "      <th>related_last_quantity</th>\n",
       "      <th>related_last_cusip</th>\n",
       "      <th>related_last_seconds_ago</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115085AY8</td>\n",
       "      <td>440.0</td>\n",
       "      <td>True</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-12-21</td>\n",
       "      <td>2021-12-21</td>\n",
       "      <td>2048-09-01</td>\n",
       "      <td>4.000</td>\n",
       "      <td>2021-12-21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74442PXL4</td>\n",
       "      <td>460.0</td>\n",
       "      <td>True</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>2045-07-01</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>544445YC7</td>\n",
       "      <td>425.0</td>\n",
       "      <td>True</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>NaT</td>\n",
       "      <td>4.000</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>677523EM0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-12-21</td>\n",
       "      <td>2022-12-21</td>\n",
       "      <td>NaT</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2022-12-21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>240523ZF0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-12-14</td>\n",
       "      <td>2022-12-14</td>\n",
       "      <td>NaT</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2022-12-14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828007</th>\n",
       "      <td>64971XUJ5</td>\n",
       "      <td>410.0</td>\n",
       "      <td>True</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-09-24</td>\n",
       "      <td>2020-09-24</td>\n",
       "      <td>NaT</td>\n",
       "      <td>4.000</td>\n",
       "      <td>2020-09-24</td>\n",
       "      <td>...</td>\n",
       "      <td>35.780607</td>\n",
       "      <td>29.304298</td>\n",
       "      <td>40.576298</td>\n",
       "      <td>54.533372</td>\n",
       "      <td>63.964688</td>\n",
       "      <td>35.470234</td>\n",
       "      <td>-5.113852</td>\n",
       "      <td>6.176091</td>\n",
       "      <td>64971XQM3</td>\n",
       "      <td>4792.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828008</th>\n",
       "      <td>64971XUJ5</td>\n",
       "      <td>411.7</td>\n",
       "      <td>True</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-09-24</td>\n",
       "      <td>2020-09-24</td>\n",
       "      <td>NaT</td>\n",
       "      <td>4.000</td>\n",
       "      <td>2020-09-24</td>\n",
       "      <td>...</td>\n",
       "      <td>35.780607</td>\n",
       "      <td>29.304298</td>\n",
       "      <td>40.576298</td>\n",
       "      <td>54.533372</td>\n",
       "      <td>64.926631</td>\n",
       "      <td>35.470234</td>\n",
       "      <td>-5.113852</td>\n",
       "      <td>6.176091</td>\n",
       "      <td>64971XQM3</td>\n",
       "      <td>4811.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828009</th>\n",
       "      <td>64971XUJ5</td>\n",
       "      <td>410.4</td>\n",
       "      <td>True</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-09-24</td>\n",
       "      <td>2020-09-24</td>\n",
       "      <td>NaT</td>\n",
       "      <td>4.000</td>\n",
       "      <td>2020-09-24</td>\n",
       "      <td>...</td>\n",
       "      <td>37.740463</td>\n",
       "      <td>30.544202</td>\n",
       "      <td>32.239721</td>\n",
       "      <td>56.367419</td>\n",
       "      <td>65.207307</td>\n",
       "      <td>35.070234</td>\n",
       "      <td>-5.113852</td>\n",
       "      <td>4.176091</td>\n",
       "      <td>64971XQM3</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828011</th>\n",
       "      <td>59259YBY4</td>\n",
       "      <td>580.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-01-13</td>\n",
       "      <td>2010-01-13</td>\n",
       "      <td>2033-11-15</td>\n",
       "      <td>6.668</td>\n",
       "      <td>2010-01-13</td>\n",
       "      <td>...</td>\n",
       "      <td>226.966434</td>\n",
       "      <td>206.830143</td>\n",
       "      <td>177.690426</td>\n",
       "      <td>149.230145</td>\n",
       "      <td>151.401379</td>\n",
       "      <td>258.086320</td>\n",
       "      <td>-47.209140</td>\n",
       "      <td>5.602060</td>\n",
       "      <td>59259YBZ1</td>\n",
       "      <td>24642.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828010</th>\n",
       "      <td>59259YBY4</td>\n",
       "      <td>580.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-01-13</td>\n",
       "      <td>2010-01-13</td>\n",
       "      <td>2033-11-15</td>\n",
       "      <td>6.668</td>\n",
       "      <td>2010-01-13</td>\n",
       "      <td>...</td>\n",
       "      <td>226.966434</td>\n",
       "      <td>206.830143</td>\n",
       "      <td>177.690426</td>\n",
       "      <td>149.230145</td>\n",
       "      <td>150.918059</td>\n",
       "      <td>258.086320</td>\n",
       "      <td>-47.209140</td>\n",
       "      <td>5.602060</td>\n",
       "      <td>59259YBZ1</td>\n",
       "      <td>24642.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>828012 rows Ã— 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            cusip  yield  is_callable refund_date  refund_price accrual_date  \\\n",
       "0       115085AY8  440.0         True         NaT           NaN   2021-12-21   \n",
       "1       74442PXL4  460.0         True         NaT           NaN   2021-12-01   \n",
       "2       544445YC7  425.0         True         NaT           NaN   2022-02-15   \n",
       "3       677523EM0  259.0        False         NaT           NaN   2022-12-21   \n",
       "4       240523ZF0  263.0        False         NaT           NaN   2022-12-14   \n",
       "...           ...    ...          ...         ...           ...          ...   \n",
       "828007  64971XUJ5  410.0         True         NaT           NaN   2020-09-24   \n",
       "828008  64971XUJ5  411.7         True         NaT           NaN   2020-09-24   \n",
       "828009  64971XUJ5  410.4         True         NaT           NaN   2020-09-24   \n",
       "828011  59259YBY4  580.0        False         NaT           NaN   2010-01-13   \n",
       "828010  59259YBY4  580.0        False         NaT           NaN   2010-01-13   \n",
       "\n",
       "       dated_date next_sink_date  coupon delivery_date  ...  \\\n",
       "0      2021-12-21     2048-09-01   4.000    2021-12-21  ...   \n",
       "1      2021-12-01     2045-07-01   3.000    2021-12-01  ...   \n",
       "2      2022-02-15            NaT   4.000    2022-02-15  ...   \n",
       "3      2022-12-21            NaT   5.000    2022-12-21  ...   \n",
       "4      2022-12-14            NaT   5.000    2022-12-14  ...   \n",
       "...           ...            ...     ...           ...  ...   \n",
       "828007 2020-09-24            NaT   4.000    2020-09-24  ...   \n",
       "828008 2020-09-24            NaT   4.000    2020-09-24  ...   \n",
       "828009 2020-09-24            NaT   4.000    2020-09-24  ...   \n",
       "828011 2010-01-13     2033-11-15   6.668    2010-01-13  ...   \n",
       "828010 2010-01-13     2033-11-15   6.668    2010-01-13  ...   \n",
       "\n",
       "       ex_cusip_masked_series_average_10 ex_cusip_masked_series_average_15  \\\n",
       "0                               0.000000                          0.000000   \n",
       "1                               0.000000                          0.000000   \n",
       "2                               0.000000                          0.000000   \n",
       "3                               0.000000                          0.000000   \n",
       "4                               0.000000                          0.000000   \n",
       "...                                  ...                               ...   \n",
       "828007                         35.780607                         29.304298   \n",
       "828008                         35.780607                         29.304298   \n",
       "828009                         37.740463                         30.544202   \n",
       "828011                        226.966434                        206.830143   \n",
       "828010                        226.966434                        206.830143   \n",
       "\n",
       "       ex_cusip_masked_series_average_20 ex_cusip_masked_series_average_30  \\\n",
       "0                               0.000000                          0.000000   \n",
       "1                               0.000000                          0.000000   \n",
       "2                               0.000000                          0.000000   \n",
       "3                               0.000000                          0.000000   \n",
       "4                               0.000000                          0.000000   \n",
       "...                                  ...                               ...   \n",
       "828007                         40.576298                         54.533372   \n",
       "828008                         40.576298                         54.533372   \n",
       "828009                         32.239721                         56.367419   \n",
       "828011                        177.690426                        149.230145   \n",
       "828010                        177.690426                        149.230145   \n",
       "\n",
       "        ex_cusip_masked_series_average_50  related_last_new_ys  \\\n",
       "0                                0.000000             0.000000   \n",
       "1                                0.000000             0.000000   \n",
       "2                                0.000000             0.000000   \n",
       "3                                0.000000             0.000000   \n",
       "4                                0.000000             0.000000   \n",
       "...                                   ...                  ...   \n",
       "828007                          63.964688            35.470234   \n",
       "828008                          64.926631            35.470234   \n",
       "828009                          65.207307            35.070234   \n",
       "828011                         151.401379           258.086320   \n",
       "828010                         150.918059           258.086320   \n",
       "\n",
       "        related_last_ficc_treasury_spread  related_last_quantity  \\\n",
       "0                                0.000000               0.000000   \n",
       "1                                0.000000               0.000000   \n",
       "2                                0.000000               0.000000   \n",
       "3                                0.000000               0.000000   \n",
       "4                                0.000000               0.000000   \n",
       "...                                   ...                    ...   \n",
       "828007                          -5.113852               6.176091   \n",
       "828008                          -5.113852               6.176091   \n",
       "828009                          -5.113852               4.176091   \n",
       "828011                         -47.209140               5.602060   \n",
       "828010                         -47.209140               5.602060   \n",
       "\n",
       "        related_last_cusip  related_last_seconds_ago  \n",
       "0                                                0.0  \n",
       "1                                                0.0  \n",
       "2                                                0.0  \n",
       "3                                                0.0  \n",
       "4                                                0.0  \n",
       "...                    ...                       ...  \n",
       "828007           64971XQM3                    4792.0  \n",
       "828008           64971XQM3                    4811.0  \n",
       "828009           64971XQM3                      64.0  \n",
       "828011           59259YBZ1                   24642.0  \n",
       "828010           59259YBZ1                   24642.0  \n",
       "\n",
       "[828012 rows x 194 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling experiments\n",
    "For each of the experiments, we start with a small subset of data to make sure the basic sanity checks are passing before using the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the data into train and test sets at a 0.85 train/test ratio\n",
      "Train set size: 703810\t\tTest set size: 124202\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No inputs\n",
    "To gauge how difficult the problem is, we compare first the `new_ys` of predicting 0 for every trade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 0 for every trade. MAE (entire dataset): 63.390951203940986\n",
      "Predicting 0 for every trade. MAE (test data only): 69.20474205335951\n"
     ]
    }
   ],
   "source": [
    "print(f'Predicting 0 for every trade. MAE (entire dataset): {np.mean(np.abs(data[\"new_ys\"]))}')\n",
    "print(f'Predicting 0 for every trade. MAE (test data only): {np.mean(np.abs(test_data[\"new_ys\"]))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline model (no machine learning) is one where we just predict the single value that minimizes the MAE of the training set, i.e., the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (entire dataset): 51.64785030517865\n",
      "MAE (test data only): 54.32577966835946\n"
     ]
    }
   ],
   "source": [
    "median_new_ys = train_data['new_ys'].median()\n",
    "error = data['new_ys'] - median_new_ys\n",
    "print(f'MAE (entire dataset): {np.mean(np.abs(error))}')\n",
    "error = test_data['new_ys'] - median_new_ys\n",
    "print(f'MAE (test data only): {np.mean(np.abs(error))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another baseline model (no machine learning) is to compare to the single most recent past trade for the same CUSIP. This information is in the `trade_history` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (entire dataset): 21.977732004825107\n",
      "MAE (test data only): 22.379508049202773\n"
     ]
    }
   ],
   "source": [
    "error = data['new_ys'] - data['last_yield_spread']\n",
    "print(f'MAE (entire dataset): {np.mean(np.abs(error))}')\n",
    "error = test_data['new_ys'] - test_data['last_yield_spread']\n",
    "print(f'MAE (test data only): {np.mean(np.abs(error))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related trades info (as the only input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using related trade features from matching categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_related_features = train_data[related_trade_features].astype('float64')\n",
    "test_data_related_features = test_data[related_trade_features].astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the `new_ys` value from the related trade to that of the output `new_ys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (entire dataset): 54.041592119969906\n",
      "MAE (test data only): 55.43313751932275\n"
     ]
    }
   ],
   "source": [
    "error = data['new_ys'] - data['related_last_new_ys']\n",
    "print(f'MAE (entire dataset): {np.mean(np.abs(error))}')\n",
    "error = test_data['new_ys'] - test_data['related_last_new_ys']\n",
    "print(f'MAE (test data only): {np.mean(np.abs(error))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Definition of \"related\" (categories to match on) | MAE on entire dataset | MAE on test data only |\n",
    "| ---------------------------------------------- | --------------------- | --------------------- |\n",
    "| `issuer` | 57.357 | 61.010 |\n",
    "| `issuer`, `trade_type` | 58.417 | 60.131 |\n",
    "| `issuer`, `maturity_year` | 57.825 | 59.211 |\n",
    "| `issuer`, `maturity_year_by_2` | 55.479 | 56.997 |\n",
    "| `issuer`, `maturity_year_by_5` | 54.042 | 55.433 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run a linear regression model that has the related trade `new_ys` as input, and must output `new_ys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_data['related_last_new_ys'].to_numpy().reshape(-1, 1), train_data['new_ys'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "linear_model = LinearRegression().fit(train_data['related_last_new_ys'].to_numpy().reshape(-1, 1), train_data['new_ys'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = test_data['new_ys'].to_numpy() - linear_model.predict(test_data['related_last_new_ys'].to_numpy().reshape(-1, 1))\n",
    "print(f'MAE: {np.mean(np.abs(error))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run a linear regression model that has the related trade information as input, and must output `new_ys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "linear_model = LinearRegression().fit(train_data_related_features.to_numpy(), train_data['new_ys'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = test_data['new_ys'].to_numpy() - linear_model.predict(test_data_related_features.to_numpy())\n",
    "print(f'MAE: {np.mean(np.abs(error))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run a LightGBM model that has the related trade information as input, and must output `new_ys`. All parameters for the LightGBM model are described here: https://lightgbm.readthedocs.io/en/latest/Parameters.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lgb_model = LGBMRegressor(num_iterations=300, max_depth=12, num_leaves=300, objective='mae', verbosity=-1)\n",
    "lgb_model.fit(train_data_related_features, train_data['new_ys'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "error = test_data['new_ys'] - lgb_model.predict(test_data_related_features)\n",
    "print(f'MAE: {np.mean(np.abs(error))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference data and related trades info\n",
    "We now add the reference data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now convert this to the feedforward NN used in the model in production, with the additional inputs of the related trade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference data, trade history, and related trades info\n",
    "We now add in the trade history processing from the recurrent model used in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the accuracy of our final model to that of the one in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching on `issuer` and `days_to_maturity` within 6 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "six_months = 365 // 2 + 1\n",
    "data['days_to_maturity_minus_six_months'] = data['days_to_maturity'] - six_months\n",
    "data['days_to_maturity_plus_six_months'] = data['days_to_maturity'] + six_months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching on `rating`, `incorporated_state_code`, and `trade_type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# check if pickle file exists in `../../../../ficc/ml_models/sequence_predictors/data/` ending in `_with_related_trades.pkl` so we do not have to re-run this query\n",
    "data = append_recent_trade_data(data, \n",
    "                                num_recent_trades=1, \n",
    "                                appended_features_names_and_functions=related_trade_features_functions_and_default_values, \n",
    "                                feature_prefix=related_trade_prefix, \n",
    "                                categories=['rating', 'incorporated_state_code', 'trade_type'], \n",
    "                                return_df=True, \n",
    "                                multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the file and removing rows with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('../data/january_data_with_related_trades.pkl')\n",
    "# data = pd.read_pickle('../data/january_data_with_related_trades.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True, subset=PREDICTORS + related_trade_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_columns(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using K-means to determine the definition of *related*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_for_k_means(data):\n",
    "    data_for_k_means = data\n",
    "    data_for_k_means[BINARY] = data_for_k_means[BINARY].astype(int)    # convert the BINARY values to 0 and 1\n",
    "    one_hot_encoded = preprocessing.OneHotEncoder(handle_unknown='ignore').fit_transform(data_for_k_means[CATEGORICAL_FEATURES].values).A    # use one-hot encoding for the categorical features; the `.A` transforms the sparse matrix into a normal matrix: https://stackoverflow.com/questions/38405047/how-to-convert-a-scipy-row-matrix-into-a-numpy-array\n",
    "    data_for_k_means = np.concatenate((data_for_k_means[BINARY + NON_CAT_FEATURES].values, one_hot_encoded), axis=1)    # concatenate the BINARY and NON_CAT_FEATURES with the one-hot encoded CATEGORICAL_FEATURES: https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html\n",
    "    return data_for_k_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create k-means classifiers for many choices of `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "k_to_k_means_classifiers_filename = '../data/k_to_k_means_classifier.pkl'\n",
    "if os.path.isfile(k_to_k_means_classifiers_filename):\n",
    "    with open('../data/k_to_k_means_classifier.pkl', 'rb') as file:\n",
    "        k_choices, k_means_classifiers = zip(*pickle.load(file))\n",
    "else:\n",
    "    n = len(train_data)\n",
    "    k_choices = [int(k) for k in [2, 5, np.log(n), 2 * np.log(n), np.sqrt(n / 4), np.sqrt(n / 2), np.sqrt(n), np.sqrt(n * 2)]]\n",
    "    train_data_for_k_means = create_data_for_k_means(train_data)\n",
    "    \n",
    "    with mp.Pool() as pool_object:    # using template from https://docs.python.org/3/library/multiprocessing.html\n",
    "        k_means_classifiers = pool_object.map(lambda k: cluster.KMeans(n_clusters=k).fit(train_data_for_k_means), k_choices)\n",
    "        \n",
    "    with open(k_to_k_means_classifiers_filename, 'wb') as file:\n",
    "        pickle.dump(list(zip(k_choices, k_means_classifiers)), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the most recent trade in the cluster as the related trade. Then, find the MAE when using each k-means classifier for a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "maes = []\n",
    "data_for_k_means = create_data_for_k_means(data)\n",
    "for idx, k_means_classifier in tqdm(enumerate(k_means_classifiers)):\n",
    "    clusters = k_means_classifier.predict(data_for_k_means)\n",
    "    data['cluster'] = clusters\n",
    "    data = get_data_with_related_trade_info(data, 'cluster', append_single_recent_trade_data_using_shift_caller)\n",
    "\n",
    "    train_data, test_data = train_test_split(data)\n",
    "    linear_model = LinearRegression().fit(train_data['related_last_new_ys'].to_numpy().reshape(-1, 1), train_data['new_ys'].to_numpy())\n",
    "    error = test_data['new_ys'].to_numpy() - linear_model.predict(test_data['related_last_new_ys'].to_numpy().reshape(-1, 1))\n",
    "    mae = np.mean(np.abs(error))\n",
    "    print(f'k: {k_choices[idx]}, mae: {mae}')\n",
    "    maes.append(mae)\n",
    "\n",
    "    data = data.drop(columns=['cluster'] + related_trade_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k_choices, maes, 'bo-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_with_min_mae = np.argmin(maes)\n",
    "k, k_means_classifier, mae = k_choices[idx_with_min_mae], k_means_classifiers[idx_with_min_mae], maes[idx_with_min_mae]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add related trade features from best `k_means` classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = k_means_classifier.predict(data_for_k_means)\n",
    "data['cluster'] = clusters\n",
    "data = get_data_with_related_trade_info(data, 'cluster', append_single_recent_trade_data_using_shift_caller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
