{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is run with the `ficc_python` package on branch `mitas_increase_sequence_length`. The `example.py` file in the package shows the query that is run to get the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "from ficc.utils.auxiliary_variables import VERY_LARGE_NUMBER, \\\n",
    "                                           IDENTIFIERS, \\\n",
    "                                           CATEGORICAL_FEATURES, \\\n",
    "                                           NON_CAT_FEATURES, \\\n",
    "                                           BINARY, \\\n",
    "                                           TRADE_HISTORY, \\\n",
    "                                           NUM_OF_DAYS_IN_YEAR\n",
    "from ficc.utils.auxiliary_functions import flatten, \\\n",
    "                                           list_to_index_dict\n",
    "from ficc.utils.diff_in_days import diff_in_days_two_dates\n",
    "from ficc.utils.trade_dict_to_list import FEATURES_IN_HISTORY, \\\n",
    "                                          FEATURES_TO_INDEX_IN_HISTORY, \\\n",
    "                                          CATEGORICAL_FEATURES_IN_HISTORY, \\\n",
    "                                          quantity_diff\n",
    "from ficc.utils.trade_dict_to_list_mappings import TRADE_TYPE_MAPPING, \\\n",
    "                                                   TRADE_TYPE_CROSS_PRODUCT_MAPPING, \\\n",
    "                                                   RATING_TO_INT_MAPPING\n",
    "from ficc.utils.related_trade import append_recent_trade_data, get_appended_feature_name\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "\n",
    "from trade_history_model_mitas.data_prep import get_past_trade_columns, \\\n",
    "                                                feature_group_as_single_feature, \\\n",
    "                                                limit_history_to_k_trades, \\\n",
    "                                                combine_two_histories_sorted_by_seconds_ago, \\\n",
    "                                                remove_feature_from_trade_history, \\\n",
    "                                                convert_trade_type_encoding_to_actual, \\\n",
    "                                                add_reference_data_to_trade_history, \\\n",
    "                                                embed_with_arrays, \\\n",
    "                                                add_single_trade_from_history_as_reference_features, \\\n",
    "                                                is_sorted\n",
    "from trade_history_model_mitas.models import MultipleRecurrentL1Loss, \\\n",
    "                                             NNL1LossEmbeddingsWithMultipleRecurrence\n",
    "\n",
    "from yield_spread_model_mitas.data_prep import FEATURES_AND_NAN_REPLACEMENT_VALUES, \\\n",
    "                                               ADDITIONAL_CATEGORICAL_FEATURES, \\\n",
    "                                               get_datestring_from_filename, \\\n",
    "                                               remove_rows_with_feature_value, \\\n",
    "                                               replace_rating_with_standalone_rating, \\\n",
    "                                               add_past_trades_info, \\\n",
    "                                               reverse_order_of_trade_history, \\\n",
    "                                               check_additional_features, \\\n",
    "                                               replace_nan_for_features, \\\n",
    "                                               encode_and_get_encoders, \\\n",
    "                                               encode_with_encoders\n",
    "from yield_spread_model_mitas.models import single_feature_model, \\\n",
    "                                            RecurrentL1Loss, \\\n",
    "                                            NNL1LossEmbeddings, \\\n",
    "                                            NNL1LossEmbeddingsWithRecurrence\n",
    "from yield_spread_model_mitas.train import get_train_test_data_trade_datetime, \\\n",
    "                                           get_train_test_data_index, \\\n",
    "                                           is_gpu_available, \\\n",
    "                                           is_mps_available, \\\n",
    "                                           train\n",
    "from yield_spread_model_mitas.tree_models import train_lightgbm_model, \\\n",
    "                                                 get_all_losses_for_single_dataset\n",
    "\n",
    "from rating_model_mitas.data_prep import read_processed_file_pickle, \\\n",
    "                                         remove_fields_with_single_unique_value, \\\n",
    "                                         remove_rows_with_nan_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use zero padding instead of nonzero padding since this gives the greatest performance for the NN models (experiments are at the bottom of this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default value of 0 is chosen for settlement_date_to_calc_date because we exclude bonds that have a calc date that is fewer than 400 days into the future, and so a true value of settlement date to calc date will never be close to 0\n",
    "DEFAULT_VALUES_NONZERO_PADDING = {'quantity_diff': np.log10(VERY_LARGE_NUMBER),    # model should learn that a quantity diff that is very large, i.e., current quantity is much smaller than quantity being compared to, means that the trade is less meaningful to use for pricing since the trades are very different. Could also be -np.log10(VERY_LARGE_NUMBER) for the same reason\n",
    "                                  'seconds_ago': np.log10(VERY_LARGE_NUMBER)}    # model should learn that if the trade being compared to is far back in the past, then it is less meaningful to pricing the current trade\n",
    "DEFAULT_VALUES_NONZERO_PADDING = defaultdict(int, DEFAULT_VALUES_NONZERO_PADDING)    # constructing a defaultdict from a dict: https://stackoverflow.com/questions/7539115/how-to-construct-a-defaultdict-from-a-dictionary\n",
    "\n",
    "DEFAULT_VALUES_ZERO_PADDING = defaultdict(int)\n",
    "\n",
    "DEFAULT_VALUES = DEFAULT_VALUES_ZERO_PADDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRADES_IN_TRADE_HISTORY = 32    # maximum number of past trades in the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = ['yield_spread']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PROCESSING_FEATURES = ['trade_datetime',    # used to split the data into training and test sets\n",
    "                            'settlement_date',    # used (in conjunction with calc_date) to create the settlement_date_to_calc_date feature in past trades\n",
    "                            'calc_date',    # used (in conjunction with settlement_date) to create the settlement_date_to_calc_date feature in past trades\n",
    "                            'calc_day_cat',    # added in the past trades\n",
    "                            'days_to_calc_date',    # new feature created in this notebook to be used by related trades\n",
    "                            # 'coupon_type'    # used to group related trades; currently commented out since there is only a single value of 8 present in the data\n",
    "                           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START: Reading from processed file at ../../../../ficc/ml_models/sequence_predictors/data/processed_data_ficc_ycl_long_history_2022-10-08-00-00.pkl\n",
      "END: Reading from processed file at ../../../../ficc/ml_models/sequence_predictors/data/processed_data_ficc_ycl_long_history_2022-10-08-00-00.pkl\n",
      "CPU times: user 7.84 s, sys: 8.11 s, total: 16 s\n",
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "processed_file_pickle = '../../../../ficc/ml_models/sequence_predictors/data/processed_data_ficc_ycl_long_history_2022-10-08-00-00.pkl'\n",
    "processed_file_pickle_datestring = get_datestring_from_filename(processed_file_pickle)\n",
    "trade_data = read_processed_file_pickle(processed_file_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep all trades before October 8, to standardize with Charles and Developer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_data = trade_data[trade_data.trade_datetime < datetime(2022, 10, 8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply exclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trades: 3360105\n"
     ]
    }
   ],
   "source": [
    "print(f'Total number of trades: {len(trade_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(df[\"purpose_sub_class\"] != 6) & (df[\"purpose_sub_class\"] != 20) & (df[\"purpose_sub_class\"] != 22) & (df[\"purpose_sub_class\"] != 44) & (df[\"purpose_sub_class\"] != 57) & (df[\"purpose_sub_class\"] != 90)\n",
      "11842 rows had purpose_sub_class in [6, 20, 22, 44, 57, 90] and were removed\n",
      "(df[\"called_redemption_type\"] != 18) & (df[\"called_redemption_type\"] != 19)\n",
      "11044 rows had called_redemption_type in [18, 19] and were removed\n",
      "CPU times: user 6.97 s, sys: 5.5 s, total: 12.5 s\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trade_data = trade_data[(trade_data.days_to_call == 0) | (trade_data.days_to_call > np.log10(400))]\n",
    "trade_data = trade_data[(trade_data.days_to_refund == 0) | (trade_data.days_to_refund > np.log10(400))]\n",
    "trade_data = trade_data[trade_data.days_to_maturity < np.log10(30000)]\n",
    "trade_data = trade_data[trade_data.sinking == False]\n",
    "trade_data = trade_data[trade_data.incorporated_state_code != 'VI']\n",
    "trade_data = trade_data[trade_data.incorporated_state_code != 'GU']\n",
    "# trade_data = trade_data[(trade_data.coupon_type == 8)]\n",
    "# trade_data = trade_data[trade_data.is_called == False]\n",
    "\n",
    "# restructured bonds and high chance of default bonds are removed\n",
    "trade_data = remove_rows_with_feature_value(trade_data, 'purpose_sub_class', [6, 20, 22, 44, 57, 90])\n",
    "# pre-refunded bonds and partially refunded bonds are removed\n",
    "trade_data = remove_rows_with_feature_value(trade_data, 'called_redemption_type', [18, 19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_data = replace_rating_with_standalone_rating(trade_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `treasury_spread` to the `NON_CAT_FEATURES`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_CAT_FEATURES.append('ficc_treasury_spread')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `days_to_calc_date` to `trade_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_data['days_to_calc_date'] = np.log10(1 + (trade_data['calc_date'] - trade_data['settlement_date']).dt.days)\n",
    "assert not trade_data['days_to_calc_date'].isnull().values.any(), f'`days_to_calc_date` is null for the following RTRS control numbers: {trade_data[trade_data[\"days_to_calc_date\"].isnull().values, \"rtrs_control_number\"].values}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADDITIONAL_CATEGORICAL_FEATURES = check_additional_features(trade_data, ADDITIONAL_CATEGORICAL_FEATURES)\n",
    "\n",
    "trade_data, _ = replace_nan_for_features(trade_data, FEATURES_AND_NAN_REPLACEMENT_VALUES, verbose=True)\n",
    "trade_data = remove_fields_with_single_unique_value(trade_data, BINARY + CATEGORICAL_FEATURES + ADDITIONAL_CATEGORICAL_FEATURES + NON_CAT_FEATURES)\n",
    "\n",
    "all_features_set = set(trade_data.columns)\n",
    "BINARY = list(set(BINARY) & all_features_set)\n",
    "CATEGORICAL_FEATURES = list((set(CATEGORICAL_FEATURES) | set(ADDITIONAL_CATEGORICAL_FEATURES)) & all_features_set)\n",
    "NON_CAT_FEATURES = list(set(NON_CAT_FEATURES) & all_features_set)\n",
    "PREDICTORS = BINARY + CATEGORICAL_FEATURES + NON_CAT_FEATURES\n",
    "\n",
    "trade_data = trade_data[IDENTIFIERS + \n",
    "                        PREDICTORS + \n",
    "                        DATA_PROCESSING_FEATURES + \n",
    "                        TRADE_HISTORY + \n",
    "                        TARGET]\n",
    "\n",
    "trade_data = remove_rows_with_nan_value(trade_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Identifiers: {sorted(IDENTIFIERS)}')\n",
    "print(f'Predictors: {sorted(PREDICTORS)}')\n",
    "print(f'Binary features: {sorted(BINARY)}')\n",
    "print(f'Categorical features: {sorted(CATEGORICAL_FEATURES)}')\n",
    "print(f'Numerical features: {sorted(NON_CAT_FEATURES)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTORS_WITHOUT_LAST_TRADE_FEATURES = [predictor for predictor in PREDICTORS if not predictor.startswith('last')]\n",
    "print(f'The following features are in PREDICTORS but not in PREDICTORS_WITHOUT_LAST_TRADE_FEATURES: {set(PREDICTORS) - set(PREDICTORS_WITHOUT_LAST_TRADE_FEATURES)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the dataframe is sorted in descending order by `trade_datetime`\n",
    "assert is_sorted(trade_data['trade_datetime'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldest_trade_datetime = trade_data['trade_datetime'].iloc[-1]\n",
    "newest_trade_datetime = trade_data['trade_datetime'].iloc[0]\n",
    "\n",
    "print(f'Oldest trade datetime: {oldest_trade_datetime}.\\\n",
    "    Newest trade datetime: {newest_trade_datetime}.\\\n",
    "    Gap: {newest_trade_datetime - oldest_trade_datetime}')\n",
    "print(f'Total number of trades: {len(trade_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset with only the reference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_TO_SPLIT = datetime(2022, 9, 15)    # September 15 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = get_train_test_data_trade_datetime(trade_data, DATE_TO_SPLIT)\n",
    "print(f'Number of trades for training: {len(train_data)}.\\\n",
    "    Number of trades for testing: {len(test_data)}')\n",
    "assert len(train_data) != 0 and len(test_data) != 0, 'Either train or test data is empty. Consider checking how the train test split is being performed.'\n",
    "train_data_with_trade_history = train_data.drop(columns=DATA_PROCESSING_FEATURES + IDENTIFIERS)\n",
    "test_data_with_trade_history = test_data.drop(columns=DATA_PROCESSING_FEATURES + IDENTIFIERS)\n",
    "train_data_only_reference = train_data_with_trade_history.drop(columns=TRADE_HISTORY)\n",
    "test_data_only_reference = test_data_with_trade_history.drop(columns=TRADE_HISTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten the trade history. The flattened data is used in the LightGBM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trade_data_flattened_trade_history, \\\n",
    "    additional_binary_features_from_past_trades, \\\n",
    "    additional_noncat_features_from_past_trades, \\\n",
    "    past_trade_feature_groups = add_past_trades_info(trade_data, NUM_TRADES_IN_TRADE_HISTORY - 1, FEATURES_TO_INDEX_IN_HISTORY)\n",
    "past_trade_feature_groups_flattened = flatten(past_trade_feature_groups)\n",
    "print(f'Each of the past trades are in the following feature groups: {past_trade_feature_groups}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_data_history_and_reference_features = trade_data_flattened_trade_history[past_trade_feature_groups_flattened + PREDICTORS_WITHOUT_LAST_TRADE_FEATURES + DATA_PROCESSING_FEATURES + TARGET]\n",
    "train_data_history_and_reference_features, test_data_history_and_reference_features = get_train_test_data_trade_datetime(trade_data_history_and_reference_features, DATE_TO_SPLIT)\n",
    "assert len(train_data_history_and_reference_features) != 0 and len(test_data_history_and_reference_features) != 0, 'Either train or test data is empty. Consider checking how the train test split is being performed.'\n",
    "\n",
    "train_data_history_and_reference_features = train_data_history_and_reference_features.drop(columns=DATA_PROCESSING_FEATURES)\n",
    "train_data_only_history = train_data_history_and_reference_features.drop(columns=PREDICTORS_WITHOUT_LAST_TRADE_FEATURES)\n",
    "\n",
    "test_data_history_and_reference_features = test_data_history_and_reference_features.drop(columns=DATA_PROCESSING_FEATURES)\n",
    "test_data_only_history = test_data_history_and_reference_features.drop(columns=PREDICTORS_WITHOUT_LAST_TRADE_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode the trade type from a 2-dimensional binary list to its original value for trades in the history. For example, a trade type of `[0, 1]` would be decoded to `S`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRADE_TYPE_NEW_COLUMN = 'trade_type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAME_CUSIP_PREFIX = 'last_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_data_history_and_reference_features_actual_trade_type, old_trade_type_columns, _ = convert_trade_type_encoding_to_actual(trade_data_history_and_reference_features, \n",
    "                                                                                                                               NUM_TRADES_IN_TRADE_HISTORY, \n",
    "                                                                                                                               TRADE_TYPE_NEW_COLUMN, \n",
    "                                                                                                                               SAME_CUSIP_PREFIX)\n",
    "del trade_data_history_and_reference_features\n",
    "gc.collect()\n",
    "\n",
    "trade_data_history_and_reference_features_actual_trade_type = trade_data_history_and_reference_features_actual_trade_type.drop(columns=old_trade_type_columns)\n",
    "train_data_history_and_reference_features_actual_trade_type, \\\n",
    "    test_data_history_and_reference_features_actual_trade_type = get_train_test_data_trade_datetime(trade_data_history_and_reference_features_actual_trade_type, DATE_TO_SPLIT)\n",
    "del trade_data_history_and_reference_features_actual_trade_type\n",
    "gc.collect()\n",
    "assert len(train_data_history_and_reference_features_actual_trade_type) != 0 and len(test_data_history_and_reference_features_actual_trade_type) != 0, 'Either train or test data is empty. Consider checking how the train test split is being performed.'\n",
    "\n",
    "train_data_history_and_reference_features_actual_trade_type = train_data_history_and_reference_features_actual_trade_type.drop(columns=DATA_PROCESSING_FEATURES)\n",
    "train_data_only_history_actual_trade_type = train_data_history_and_reference_features_actual_trade_type.drop(columns=PREDICTORS_WITHOUT_LAST_TRADE_FEATURES)\n",
    "\n",
    "test_data_history_and_reference_features_actual_trade_type = test_data_history_and_reference_features_actual_trade_type.drop(columns=DATA_PROCESSING_FEATURES)\n",
    "test_data_only_history_actual_trade_type = test_data_history_and_reference_features_actual_trade_type.drop(columns=PREDICTORS_WITHOUT_LAST_TRADE_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the above procedure removed just one feature for each past trade (`trade_type1` and `trade_type2` combine to just `trade_type`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(test_data_only_history_actual_trade_type.columns) == len(test_data_only_history.columns) - NUM_TRADES_IN_TRADE_HISTORY, f'Before converting, the dataframe had {len(test_data_only_history.columns)} columns, and after converting, the dataframe has {len(test_data_only_history_actual_trade_type.columns)} columns'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last Yield Spread\n",
    "Weakest baseline where output it just the yield spread of the most previous trade in the same CUSIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_feature_model(trade_data, 'last_yield_spread')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_feature_model(train_data_only_history, 'last_yield_spread')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_feature_model(test_data_only_history, 'last_yield_spread')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM (only reference data)\n",
    "LightGBM baseline with just the reference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model, lgb_losses = train_lightgbm_model(train_data_only_reference, \n",
    "                                             test_data_only_reference, \n",
    "                                             CATEGORICAL_FEATURES, \n",
    "                                             wandb_project='mitas_trade_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(lgb_model, figsize=(20, 10), importance_type='gain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward NN (only reference data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_data_filename = lambda name: f'data/{name}.pkl'    # used to create a filename to save the data files\n",
    "if not os.path.isdir('data/'):\n",
    "    os.mkdir('data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform encoding of categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_data_only_reference_encoded, label_encoders = encode_and_get_encoders(train_data_only_reference, BINARY, CATEGORICAL_FEATURES)\n",
    "\n",
    "label_encoders_filepath = make_data_filename('label_encoders')\n",
    "with open(label_encoders_filepath, 'wb') as pickle_handle: pickle.dump(label_encoders, pickle_handle, protocol=4)    # protocol 4 allows for use in the VM; use template from https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict-or-any-other-python-object\n",
    "encode_with_label_encoders = lambda df, features_to_exclude=[]: encode_with_encoders(df, label_encoders, features_to_exclude)\n",
    "\n",
    "test_data_only_reference_encoded = encode_with_label_encoders(test_data_only_reference)\n",
    "train_data_with_trade_history_encoded = encode_with_label_encoders(train_data_with_trade_history)\n",
    "test_data_with_trade_history_encoded = encode_with_label_encoders(test_data_with_trade_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up NN training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_filename = lambda name: f'pt/{name}.pt'    # used to create a filename to save the PyTorch model parameters\n",
    "if not os.path.isdir('pt/'):\n",
    "    os.mkdir('pt/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000\n",
    "NUM_WORKERS = 8 if is_gpu_available() or is_mps_available() else 0\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "SEED = 1\n",
    "seed_everything(SEED, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the values giving the highest accuracy (from a very limited hyperparameter search) on the reference data from `yield_spread_model_mitas/yield_spread_model.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HIDDEN_LAYERS = 3\n",
    "NUM_NODES_HIDDEN_LAYER = 600\n",
    "EMBEDDINGS_POWER = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_name = f'embeddings_power={EMBEDDINGS_POWER}_{NUM_HIDDEN_LAYERS}_hidden_layers_{NUM_NODES_HIDDEN_LAYER}_nodes_per_layer_{NUM_EPOCHS}_epochs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train(NNL1LossEmbeddings(BATCH_SIZE, \n",
    "                         NUM_WORKERS, \n",
    "                         train_data_only_reference_encoded, \n",
    "                         test_data_only_reference_encoded, \n",
    "                         label_encoders, \n",
    "                         CATEGORICAL_FEATURES, \n",
    "                         NUM_NODES_HIDDEN_LAYER, \n",
    "                         NUM_HIDDEN_LAYERS, \n",
    "                         power=EMBEDDINGS_POWER), \n",
    "      NUM_EPOCHS, \n",
    "      model_filename=make_filename(nn_name), \n",
    "      save=False, \n",
    "      print_losses_before_training=False,    # setting this to True may cause the kernel to crash\n",
    "      print_losses_after_training=False,    # setting this to True may cause the kernel to crash\n",
    "      wandb_logging_name=nn_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM\n",
    "Train LightGBM models that vary the number of trades in the history for the same CUSIP.\n",
    "## Only trade history (no reference features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lightgbm_model_only_history(num_past_trades):\n",
    "    past_trade_columns, all_categorical_features_in_trade_history = get_past_trade_columns(num_past_trades, \n",
    "                                                                                           FEATURES_IN_HISTORY, \n",
    "                                                                                           SAME_CUSIP_PREFIX, \n",
    "                                                                                           trade_type_actual=True, \n",
    "                                                                                           trade_type_column=TRADE_TYPE_NEW_COLUMN, \n",
    "                                                                                           categorical_features_per_trade=CATEGORICAL_FEATURES_IN_HISTORY)\n",
    "    return train_lightgbm_model(train_data_only_history_actual_trade_type[past_trade_columns + TARGET], \n",
    "                                test_data_only_history_actual_trade_type[past_trade_columns + TARGET], \n",
    "                                all_categorical_features_in_trade_history, \n",
    "                                wandb_project='mitas_trade_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_past_trades_in_history_candidates = list(range(1, NUM_TRADES_IN_TRADE_HISTORY + 1))    # maximum number of past trades is NUM_TRADES_IN_TRADE_HISTORY (32), since that is the number of past trades in the data pipeline\n",
    "train_l1_losses, test_l1_losses = [], []\n",
    "for num_past_trades in num_past_trades_in_history_candidates:\n",
    "    lgb_model, lgb_losses = train_lightgbm_model_only_history(num_past_trades)\n",
    "    if num_past_trades == 16: lgb_model_16, lgb_model_16_losses = lgb_model, lgb_losses\n",
    "    train_l1_losses.append(lgb_losses['Train'][0])    # index 0 indicates l1 loss\n",
    "    test_l1_losses.append(lgb_losses['Test'][0])    # index 0 indicates l1 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 16 <= NUM_TRADES_IN_TRADE_HISTORY:\n",
    "    if 'lgb_model_16' not in locals(): lgb_model_16, lgb_model_16_losses = train_lightgbm_model_only_history(16)    # if model not created, then train the lightgbm model\n",
    "    lgb.plot_importance(lgb_model_16, figsize=(20, 25), importance_type='gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train error for 5 trades: {train_l1_losses[4]}')\n",
    "print(f'Minimum value: {min(train_l1_losses)} for number of trades: {num_past_trades_in_history_candidates[np.argmin(train_l1_losses)]}')\n",
    "plt.ylabel('Train L1 losses')\n",
    "plt.xlabel('Number of trades in history')\n",
    "plt.plot(num_past_trades_in_history_candidates, train_l1_losses, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test error for 5 trades: {test_l1_losses[4]}')\n",
    "print(f'Minimum value: {min(test_l1_losses)} for number of trades: {num_past_trades_in_history_candidates[np.argmin(test_l1_losses)]}')\n",
    "plt.ylabel('Test L1 losses')\n",
    "plt.xlabel('Number of trades in history')\n",
    "plt.plot(num_past_trades_in_history_candidates, test_l1_losses, 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference features and trade history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lightgbm_model_history_and_reference(num_past_trades):\n",
    "    past_trade_columns, all_categorical_features_in_trade_history = get_past_trade_columns(num_past_trades, \n",
    "                                                                                           FEATURES_IN_HISTORY, \n",
    "                                                                                           SAME_CUSIP_PREFIX, \n",
    "                                                                                           trade_type_actual=True, \n",
    "                                                                                           trade_type_column=TRADE_TYPE_NEW_COLUMN, \n",
    "                                                                                           categorical_features_per_trade=CATEGORICAL_FEATURES_IN_HISTORY)\n",
    "    return train_lightgbm_model(train_data_history_and_reference_features_actual_trade_type[PREDICTORS_WITHOUT_LAST_TRADE_FEATURES + past_trade_columns + TARGET], \n",
    "                                test_data_history_and_reference_features_actual_trade_type[PREDICTORS_WITHOUT_LAST_TRADE_FEATURES + past_trade_columns + TARGET], \n",
    "                                CATEGORICAL_FEATURES + all_categorical_features_in_trade_history, \n",
    "                                wandb_project='mitas_trade_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_past_trades_in_history_candidates = list(range(1, NUM_TRADES_IN_TRADE_HISTORY + 1))    # maximum number of past trades is NUM_TRADES_IN_TRADE_HISTORY (32), since that is the number of past trades in the data pipeline\n",
    "train_l1_losses, test_l1_losses = [], []\n",
    "for num_past_trades in num_past_trades_in_history_candidates:\n",
    "    lgb_model, lgb_losses = train_lightgbm_model_history_and_reference(num_past_trades)\n",
    "    if num_past_trades == 16: lgb_model_16, lgb_model_16_losses = lgb_model, lgb_losses\n",
    "    train_l1_losses.append(lgb_losses['Train'][0])    # index 0 indicates l1 loss\n",
    "    test_l1_losses.append(lgb_losses['Test'][0])    # index 0 indicates l1 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 16 <= NUM_TRADES_IN_TRADE_HISTORY:\n",
    "    if 'lgb_model_16' not in locals(): lgb_model_16, lgb_model_16_losses = train_lightgbm_model_history_and_reference(16)    # if model not created, then train the lightgbm model\n",
    "    print(f'Train error: {lgb_model_16_losses[\"Train\"][0]}\\tTest error: {lgb_model_16_losses[\"Test\"][0]}')\n",
    "    lgb.plot_importance(lgb_model_16, figsize=(20, 25), importance_type='gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train error for 5 trades: {train_l1_losses[4]}')\n",
    "print(f'Minimum value: {min(train_l1_losses)} for number of trades: {num_past_trades_in_history_candidates[np.argmin(train_l1_losses)]}')\n",
    "plt.ylabel('Train L1 losses')\n",
    "plt.xlabel('Number of trades in history')\n",
    "plt.plot(num_past_trades_in_history_candidates, train_l1_losses, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test error for 5 trades: {test_l1_losses[4]}')\n",
    "print(f'Minimum value: {min(test_l1_losses)} for number of trades: {num_past_trades_in_history_candidates[np.argmin(test_l1_losses)]}')\n",
    "plt.ylabel('Test L1 losses')\n",
    "plt.xlabel('Number of trades in history')\n",
    "plt.plot(num_past_trades_in_history_candidates, test_l1_losses, 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our current design looks at just the 5 previous trades, but this plot indicates that having more trades in the history increases predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add histories of related trades\n",
    "For each trade, we find the `NUM_TRADES_IN_RELATED_TRADE_HISTORY` most recent related trades (up until the `trade_datetime` of the current trade) that are from different CUSIPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRADES_IN_RELATED_TRADE_HISTORY = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use certain reference data columns to data in order to augment the same CUSIP history. These will be added when selecting the features to use in the LightGBM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_REFERENCE_FEATURES_TO_ADD = ['rating', 'incorporated_state_code', 'purpose_sub_class']    # choosing a few features from the most important features for the LightGBM model on just reference data\n",
    "CATEGORICAL_REFERENCE_FEATURES_TO_ADD = list(set(CATEGORICAL_REFERENCE_FEATURES_TO_ADD) & set(trade_data.columns))    # make sure that all CATEGORICAL_REFERENCE_FEATURES_TO_ADD are in the trade data as columns\n",
    "print(f'Including the following reference features for each related trade and the target trade: {CATEGORICAL_REFERENCE_FEATURES_TO_ADD}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add this same reference data to related trade history. However, to make this code faster, we first encode the reference data, and then decode after creating the past trade history. This creates a speedup because `append_recent_trade_data(...)` is significantly faster when working with numerical data as opposed to objects, due to how numpy handles `dtype='O'` versus `dtype='np.float_'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODE_REFERENCE_FEATURES = False    # boolean variable that determines whether trade history will contain categorical features that must be encoded before adding these features to the trade history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in CATEGORICAL_REFERENCE_FEATURES_TO_ADD:\n",
    "    if feature not in FEATURES_TO_INDEX_IN_HISTORY: FEATURES_TO_INDEX_IN_HISTORY[feature] = len(FEATURES_TO_INDEX_IN_HISTORY)\n",
    "    ENCODE_REFERENCE_FEATURES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_trade_feature_prefix = 'related_last_'\n",
    "get_neighbor_feature = lambda feature: lambda curr, neighbor: neighbor[feature]\n",
    "RELATED_TRADE_FEATURE_FUNCTIONS = {'yield_spread': get_neighbor_feature('yield_spread'), \n",
    "                                   'treasury_spread': get_neighbor_feature('ficc_treasury_spread'), \n",
    "                                   'quantity': get_neighbor_feature('quantity'), \n",
    "                                   'quantity_diff': lambda curr, neighbor: quantity_diff(10 ** neighbor['quantity'] - 10 ** curr['quantity']), \n",
    "                                   'trade_type1': lambda curr, neighbor: TRADE_TYPE_MAPPING[neighbor['trade_type']][0], \n",
    "                                   'trade_type2': lambda curr, neighbor: TRADE_TYPE_MAPPING[neighbor['trade_type']][1], \n",
    "                                   'seconds_ago': lambda curr, neighbor: np.log10(1 + (curr['trade_datetime'] - neighbor['trade_datetime']).total_seconds()), \n",
    "                                   'settlement_date_to_calc_date': lambda curr, neighbor: np.log10(1 + diff_in_days_two_dates(neighbor['calc_date'], neighbor['settlement_date'], convention='exact')), \n",
    "                                   'calc_day_cat': get_neighbor_feature('calc_day_cat'), \n",
    "                                   'trade_type_past_latest': lambda curr, neighbor: TRADE_TYPE_CROSS_PRODUCT_MAPPING[neighbor['trade_type'] + curr['trade_type']], \n",
    "                                  #  'rating_diff': lambda curr, neighbor: RATING_TO_INT_MAPPING[curr['rating']] - RATING_TO_INT_MAPPING[neighbor['rating']]\n",
    "                                   }\n",
    "\n",
    "related_trades_features_wo_reference_features_groups = [[get_appended_feature_name(idx, feature, related_trade_feature_prefix) for feature in RELATED_TRADE_FEATURE_FUNCTIONS] \n",
    "                                                                 for idx in range(NUM_TRADES_IN_RELATED_TRADE_HISTORY)]    # insertion order of the dictionary is preserved for Python v3.7+\n",
    "related_trades_features_wo_reference_features = flatten(related_trades_features_wo_reference_features_groups)\n",
    "\n",
    "additional_related_trade_functions = {'same_day': lambda curr, neighbor: int(neighbor['trade_datetime'].date() == curr['trade_datetime'].date()),    # used to track additional information about the related trades; compare date only instead of entire datetime: https://stackoverflow.com/questions/3743222/how-do-i-convert-a-datetime-to-date\n",
    "                                      'days_to_calc_date': get_neighbor_feature('days_to_calc_date')}\n",
    "\n",
    "reference_features_to_add_functions = {feature: get_neighbor_feature(feature) for feature in CATEGORICAL_REFERENCE_FEATURES_TO_ADD}\n",
    "RELATED_TRADE_FEATURE_FUNCTIONS = RELATED_TRADE_FEATURE_FUNCTIONS | additional_related_trade_functions | reference_features_to_add_functions    # combine two dictionaries together for Python v3.9+: https://stackoverflow.com/questions/38987/how-do-i-merge-two-dictionaries-in-a-single-expression\n",
    "\n",
    "print(f'Related trades will have the following features: {RELATED_TRADE_FEATURE_FUNCTIONS.keys()}')\n",
    "\n",
    "related_trades_features_groups = [[get_appended_feature_name(idx, feature, related_trade_feature_prefix) for feature in RELATED_TRADE_FEATURE_FUNCTIONS] \n",
    "                                   for idx in range(NUM_TRADES_IN_RELATED_TRADE_HISTORY)]    # insertion order of the dictionary is preserved for Python v3.7+\n",
    "related_trades_features = flatten(related_trades_features_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary format. key: name of the feature; value: two-item tuple where the first item is a function of the current trade and related trade, and the second item is the default value to be filled in if that value does not exist\n",
    "RELATED_TRADE_FEATURE_FUNCTIONS_AND_DEFAULT_VALUES = {key: (function, DEFAULT_VALUES[key]) for key, function in RELATED_TRADE_FEATURE_FUNCTIONS.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See which features are the in the same CUSIP trade history and the related trade history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert FEATURES_IN_HISTORY == [key for key in RELATED_TRADE_FEATURE_FUNCTIONS if key not in additional_related_trade_functions and key not in reference_features_to_add_functions]    # insertion order of the dictionary is preserved for Python v3.7+ so this will check if the ordering of the keys are the same\n",
    "print(f'Each trade in the same CUSIP trade history has the following features: {FEATURES_IN_HISTORY}')\n",
    "print(f'Each trade in the related trade history has the following features: {RELATED_TRADE_FEATURE_FUNCTIONS.keys()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create \"quantized features\" which groups together certain values of the features when used to make related trades. For example, `RATING_WITHOUT_PLUS_MINUS` removes the + and - from ratings, and so a bond with rating A+ will be related to a bond with rating A or A-.\n",
    "\n",
    "Purpose class was added as a quantized feature based on a call with Desmond Dahill from Tegus on 09/27/2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epsilon = 1 / VERY_LARGE_NUMBER\n",
    "\n",
    "RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED = 'rating_without_+-_b_nr_combined'\n",
    "trade_data_flattened_trade_history[RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED] = trade_data_flattened_trade_history['rating'].transform(lambda rating: str.rstrip(rating, '+-'))    # remove + and - from right side of string\n",
    "# group BBB, BB, B, and NR together since each have a very small number of trades\n",
    "b_ratings = trade_data_flattened_trade_history[RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED].isin(['B', 'BB', 'BBB', 'NR'])\n",
    "trade_data_flattened_trade_history.loc[b_ratings, RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED] = 'B'\n",
    "print(f'Created {RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED} feature')\n",
    "\n",
    "DAYS_TO_MATURITY_CATEGORICAL = 'days_to_maturity_categorical'\n",
    "num_of_days_bins_maturity = [np.log10(days) for days in [epsilon, NUM_OF_DAYS_IN_YEAR * 2, NUM_OF_DAYS_IN_YEAR * 5, NUM_OF_DAYS_IN_YEAR * 10, VERY_LARGE_NUMBER]]    # 2 years, 5 years, 10 years; arbitrarily chosen\n",
    "trade_data_flattened_trade_history[DAYS_TO_MATURITY_CATEGORICAL] = pd.cut(trade_data_flattened_trade_history['days_to_maturity'], num_of_days_bins_maturity).astype('string')\n",
    "print(f'Created {DAYS_TO_MATURITY_CATEGORICAL} feature')\n",
    "\n",
    "DAYS_TO_CALL_CATEGORICAL = 'days_to_call_categorical'\n",
    "num_of_days_bins_call = [np.log10(days) for days in [epsilon, NUM_OF_DAYS_IN_YEAR * 2, NUM_OF_DAYS_IN_YEAR * 5, VERY_LARGE_NUMBER]]    # 2 years, 5 years; arbitrarily chosen\n",
    "trade_data_flattened_trade_history[DAYS_TO_CALL_CATEGORICAL] = pd.cut(trade_data_flattened_trade_history['days_to_call'], num_of_days_bins_call).astype('string')\n",
    "print(f'Created {DAYS_TO_CALL_CATEGORICAL} feature')\n",
    "\n",
    "COUPON_CATEGORICAL = 'coupon_categorical'\n",
    "coupon_bins = [0, 3, 4, 4.5, 5.0 + epsilon, VERY_LARGE_NUMBER]   # 0 - 2.99, 3 - 3.99, 4 - 4.49, 4.5 - 5; from discussion with a team member\n",
    "trade_data_flattened_trade_history[COUPON_CATEGORICAL] = pd.cut(trade_data_flattened_trade_history['coupon'], coupon_bins, right=False).astype('string')\n",
    "print(f'Created {COUPON_CATEGORICAL} feature')\n",
    "\n",
    "COUPON_CATEGORICAL_SUDHAR = 'coupon_categorical_sudhar'\n",
    "coupon_bins = [0, 3, 4, 4.5, 5, 5.25, 5.5, 6, VERY_LARGE_NUMBER]    # from Sudhar's paper: Kolm, Purushothaman. 2021. Systematic Pricing and Trading of Municipal Bonds\n",
    "trade_data_flattened_trade_history[COUPON_CATEGORICAL_SUDHAR] = pd.cut(trade_data_flattened_trade_history['coupon'], coupon_bins, right=False).astype('string')\n",
    "print(f'Created {COUPON_CATEGORICAL_SUDHAR} feature')\n",
    "\n",
    "# COUPON_TOP_VALUES = 'coupon_top_values'\n",
    "# trade_data_flattened_trade_history[COUPON_TOP_VALUES] = trade_data_flattened_trade_history['coupon']\n",
    "# top4_coupon_values = trade_data_flattened_trade_history['coupon'].value_counts().head(4).index.tolist()    # select the top 4 coupon values based on frequency in the data, which are: 5.0, 4.0, 3.0, 2.0 comprising about 90% of the data\n",
    "# trade_data_flattened_trade_history.loc[~trade_data_flattened_trade_history['coupon'].isin(top4_coupon_values), COUPON_TOP_VALUES] = -1    # arbitrary numerical value that is invalid as a coupon value\n",
    "# print(f'Created {COUPON_TOP_VALUES} feature')\n",
    "\n",
    "PURPOSE_CLASS_TOP_VALUES = 'purpose_class_top_values'\n",
    "trade_data_flattened_trade_history[PURPOSE_CLASS_TOP_VALUES] = trade_data_flattened_trade_history['purpose_class']\n",
    "top6_purpose_class_values = trade_data_flattened_trade_history['purpose_class'].value_counts().head(6).index.tolist()    # select the top 6 coupon values based on frequency in the data, which are: 37 (school district), 51 (various purpose), 50 (utility), 46 (tax revenue), 9 (education), 48 (transportation) comprising about 80% of the data\n",
    "trade_data_flattened_trade_history.loc[~trade_data_flattened_trade_history['purpose_class'].isin(top6_purpose_class_values), PURPOSE_CLASS_TOP_VALUES] = -1    # arbitrary numerical value that is invalid as a purpose_class value\n",
    "print(f'Created {PURPOSE_CLASS_TOP_VALUES} feature')\n",
    "\n",
    "MUNI_SECURITY_TYPE_TOP_VALUES = 'muni_security_type_top_values'\n",
    "trade_data_flattened_trade_history[MUNI_SECURITY_TYPE_TOP_VALUES] = trade_data_flattened_trade_history['muni_security_type']\n",
    "top6_muni_security_type_values = trade_data_flattened_trade_history['muni_security_type'].value_counts().head(2).index.tolist()    # select the top 2 coupon values based on frequency in the data, which are: 8 (revenue), 5 (unlimited g.o.) comprising about 80% of the data\n",
    "trade_data_flattened_trade_history.loc[~trade_data_flattened_trade_history['muni_security_type'].isin(top6_muni_security_type_values), MUNI_SECURITY_TYPE_TOP_VALUES] = -1    # arbitrary numerical value that is invalid as a purpose_class value\n",
    "print(f'Created {MUNI_SECURITY_TYPE_TOP_VALUES} feature')\n",
    "\n",
    "TRADE_DATETIME_DAY = 'trade_datetime_day'\n",
    "trade_data_flattened_trade_history[TRADE_DATETIME_DAY] = trade_data_flattened_trade_history['trade_datetime'].transform(lambda datetime: datetime.date()).astype('string')    # remove timestamp from datetime\n",
    "print(f'Created {TRADE_DATETIME_DAY} feature')\n",
    "\n",
    "QUANTITY_CATEGORICAL = 'quantity_categorical'\n",
    "quantity_bins = [0, 5, 6, 7, VERY_LARGE_NUMBER]    # 0 - 100k, 100k - 1m, 1m - 10m, 10m+\n",
    "trade_data_flattened_trade_history[QUANTITY_CATEGORICAL] = pd.cut(trade_data_flattened_trade_history['quantity'], quantity_bins).astype('string')\n",
    "print(f'Created {QUANTITY_CATEGORICAL} feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_features = [RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, \n",
    "                      DAYS_TO_MATURITY_CATEGORICAL, \n",
    "                      DAYS_TO_CALL_CATEGORICAL, \n",
    "                      COUPON_CATEGORICAL, \n",
    "                      COUPON_CATEGORICAL_SUDHAR, \n",
    "                      PURPOSE_CLASS_TOP_VALUES, \n",
    "                      MUNI_SECURITY_TYPE_TOP_VALUES, \n",
    "                    #   COUPON_TOP_VALUES, \n",
    "                      TRADE_DATETIME_DAY, \n",
    "                      QUANTITY_CATEGORICAL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that each category (for each quantized feature) has a reasonable number of trades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in quantized_features:\n",
    "    trade_data_flattened_trade_history[feature].value_counts().plot(kind='bar', title=feature, figsize=(20, 10))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantity_greater_than_100k = lambda row: row['quantity'] >= np.log10(1e5)\n",
    "quantity_greater_than_1m = lambda row: row['quantity'] >= np.log10(1e6)\n",
    "trade_type_is_interdealer = lambda row: row['trade_type'] == 'D'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This link has the below definitions and results: https://docs.google.com/document/d/1rQeB3lM_iEyv9q-rseQPmb8n8nv1ay5UtVdSNH0K0QU/edit?usp=sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key: name of criteria, value: (categories to match, filtering conditions)\n",
    "related_trades_criterion = {# 'sudhar1': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL], []), \n",
    "                            # 'sudhar1_100k': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL], [quantity_greater_than_100k]), \n",
    "                            # 'sudhar1_1m': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL], [quantity_greater_than_1m]), \n",
    "                            'sudhar2': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL, 'trade_type'], []), \n",
    "                            # 'sudhar2_100k': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL, 'trade_type'], [quantity_greater_than_100k]), \n",
    "                            # 'sudhar2_1m': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL, 'trade_type'], [quantity_greater_than_1m]), \n",
    "                            # 'sudhar3': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL], [trade_type_is_interdealer]), \n",
    "                            # 'sudhar3_100k': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL], [quantity_greater_than_100k, trade_type_is_interdealer]), \n",
    "                            # 'sudhar3_1m': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL], [quantity_greater_than_1m, trade_type_is_interdealer]), \n",
    "                            # 'sudhar4': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL, MUNI_SECURITY_TYPE_TOP_VALUES], []), \n",
    "                            # 'sudhar4_100k': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL, MUNI_SECURITY_TYPE_TOP_VALUES], [quantity_greater_than_100k]), \n",
    "                            # 'sudhar4_1m': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL, MUNI_SECURITY_TYPE_TOP_VALUES], [quantity_greater_than_1m]), \n",
    "                            # 'sudhar5': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL, TRADE_DATETIME_DAY], []), \n",
    "                            # 'sudhar5_100k': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL, TRADE_DATETIME_DAY], [quantity_greater_than_100k]), \n",
    "                            # 'sudhar5_1m': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL, TRADE_DATETIME_DAY], [quantity_greater_than_1m]), \n",
    "                            # 'sudhar6': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, COUPON_CATEGORICAL, TRADE_DATETIME_DAY], []), \n",
    "                            # 'sudhar6_100k': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, COUPON_CATEGORICAL, TRADE_DATETIME_DAY], [quantity_greater_than_100k]), \n",
    "                            # 'sudhar6_1m': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, COUPON_CATEGORICAL, TRADE_DATETIME_DAY], [quantity_greater_than_1m]), \n",
    "                            # 'mitas1': ([TRADE_DATETIME_DAY, 'trade_type'], []),\n",
    "                            # 'mitas1_100k': ([TRADE_DATETIME_DAY, 'trade_type'], [quantity_greater_than_100k]), \n",
    "                            # 'mitas1_1m': ([TRADE_DATETIME_DAY, 'trade_type'], [quantity_greater_than_1m]), \n",
    "                            # 'desmond': (['incorporated_state_code', RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, DAYS_TO_CALL_CATEGORICAL, COUPON_CATEGORICAL, PURPOSE_CLASS_TOP_VALUES], []), \n",
    "                            # 'desmond_100k': (['incorporated_state_code', RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, DAYS_TO_CALL_CATEGORICAL, COUPON_CATEGORICAL, PURPOSE_CLASS_TOP_VALUES], [quantity_greater_than_100k]), \n",
    "                            # 'desmond_1m': (['incorporated_state_code', RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, DAYS_TO_CALL_CATEGORICAL, COUPON_CATEGORICAL, PURPOSE_CLASS_TOP_VALUES], [quantity_greater_than_1m]), \n",
    "                            # 'yellow': (['trade_type', DAYS_TO_MATURITY_CATEGORICAL, QUANTITY_CATEGORICAL, DAYS_TO_CALL_CATEGORICAL], []), \n",
    "                            # 'yellow_100k': (['trade_type', DAYS_TO_MATURITY_CATEGORICAL, QUANTITY_CATEGORICAL, DAYS_TO_CALL_CATEGORICAL], [quantity_greater_than_100k]), \n",
    "                            # 'yellow_1m': (['trade_type', DAYS_TO_MATURITY_CATEGORICAL, QUANTITY_CATEGORICAL, DAYS_TO_CALL_CATEGORICAL], [quantity_greater_than_1m]), \n",
    "                            # 'yellow_lite': (['trade_type', DAYS_TO_MATURITY_CATEGORICAL], []), \n",
    "                            # 'yellow_lite_100k': (['trade_type', DAYS_TO_MATURITY_CATEGORICAL], [quantity_greater_than_100k]), \n",
    "                            # 'yellow_lite_1m': (['trade_type', DAYS_TO_MATURITY_CATEGORICAL], [quantity_greater_than_1m]), \n",
    "                            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key: name of criteria, value: (categories to match, filtering conditions)\n",
    "# combine two dictionaries together for Python v3.9+: https://stackoverflow.com/questions/38987/how-do-i-merge-two-dictionaries-in-a-single-expression\n",
    "related_trades_criterion = related_trades_criterion | \\\n",
    "                           {# 'NONE': ([], []), \n",
    "                            # 'trade_type': (['trade_type'], []), \n",
    "                            # 'incorporated_state_code': (['incorporated_state_code'], []), \n",
    "                            # 'days_to_maturity_categorical': ([DAYS_TO_MATURITY_CATEGORICAL], []), \n",
    "                            # 'quantity_categorical': ([QUANTITY_CATEGORICAL], []), \n",
    "                            # 'coupon_categorical': ([COUPON_CATEGORICAL], []), \n",
    "                            # 'trade_datetime_day': ([TRADE_DATETIME_DAY], []), \n",
    "                            # 'rating_without_plus_minus_B_NR_combined': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED], []), \n",
    "                            # 'days_to_call': ([DAYS_TO_CALL_CATEGORICAL], []), \n",
    "                            # 'purpose_class_top_values': ([PURPOSE_CLASS_TOP_VALUES], []), \n",
    "                            # 'muni_security_type_top_values': ([MUNI_SECURITY_TYPE_TOP_VALUES], []), \n",
    "                            # '100k': ([], [quantity_greater_than_100k]), \n",
    "                            # '1m': ([], [quantity_greater_than_1m]), \n",
    "                            # 'dd': ([], [trade_type_is_interdealer]), \n",
    "                            # 'rating': (['rating'], []), \n",
    "                            # 'purpose_class': (['purpose_class'], []), \n",
    "                            # 'coupon_categorical_sudhar': ([COUPON_CATEGORICAL_SUDHAR], [])\n",
    "                            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add related trades to the trade data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_encoded = encode_with_label_encoders(trade_data_flattened_trade_history, features_to_exclude=['trade_type']) if ENCODE_REFERENCE_FEATURES else trade_data_flattened_trade_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_data_flattened_trade_history_and_related_trades = dict()\n",
    "# trade_data_flattened_trade_history = None    # uncomment this line when running LightGBM experiments for data files already created in order to reduce memory overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for name, (categories_to_match, filtering_conditions) in tqdm(related_trades_criterion.items()):\n",
    "    filename = f'trade_data_flattened_trade_history_and_related_trades_{name}'\n",
    "    filepath = make_data_filename(filename)\n",
    "    if os.path.exists(filepath):    # check if a file exists https://www.pythontutorial.net/python-basics/python-check-if-file-exists/\n",
    "        print(f'Loading dataset for {name} from pickle file {filepath}')\n",
    "        trade_data_flattened_trade_history_and_related_trades[name] = pd.read_pickle(filepath)    # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html\n",
    "    elif name not in trade_data_flattened_trade_history_and_related_trades:\n",
    "        print(f'Creating dataset for {name} and saving it to {filepath}')\n",
    "        trade_data_flattened_trade_history_and_related_trades[name] = append_recent_trade_data(trade_data_flattened_trade_history, \n",
    "                                                                                               NUM_TRADES_IN_RELATED_TRADE_HISTORY, \n",
    "                                                                                               RELATED_TRADE_FEATURE_FUNCTIONS_AND_DEFAULT_VALUES, \n",
    "                                                                                               feature_prefix=related_trade_feature_prefix, \n",
    "                                                                                               categories=categories_to_match, \n",
    "                                                                                               filtering_conditions=filtering_conditions, \n",
    "                                                                                               return_df=True, \n",
    "                                                                                               multiprocessing=True, \n",
    "                                                                                               df_for_related_trades=df_encoded).drop(columns=quantized_features)    # drop the quantized features from the final dataframe\n",
    "        trade_data_flattened_trade_history_and_related_trades[name].to_pickle(filepath, protocol=4)    # protocol 4 allows for use in the VM: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_data_flattened_trade_history = trade_data_flattened_trade_history.drop(columns=quantized_features)    # drop the quantized features from the final dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the each group has a reasonable amount of trades (otherwise finding related trades will be too difficult for certain trades). Note that if a group has a count of 1, then that trade has no previous related trades according to this definition of *related*. Further, note that if a group has a count of 2, then only one of those trades has a single past related trade, and the other one doesn't, where the one with no previous related trade is the oldest trade in this group. We should loosen or tighten the definition of *related* in order to make sure almost all trades have at least one previous related trades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_TO_DETECT_NO_PAST_TRADES = 'seconds_ago'    # arbitrarily chosen, but needs to be a feature that does not naturally have occurrences of its default value\n",
    "for name, df in trade_data_flattened_trade_history_and_related_trades.items():\n",
    "    print(f'{name}')\n",
    "    for past_trade_idx in (0, 1, 15, 31):    # range(2):\n",
    "        if past_trade_idx < NUM_TRADES_IN_RELATED_TRADE_HISTORY:\n",
    "            feature_name = get_appended_feature_name(past_trade_idx, FEATURE_TO_DETECT_NO_PAST_TRADES, related_trade_feature_prefix)\n",
    "            num_trades = (df[feature_name] == DEFAULT_VALUES[FEATURE_TO_DETECT_NO_PAST_TRADES]).sum()\n",
    "            print(f'Number of trades with fewer than {past_trade_idx + 1} past related trades: {num_trades}. Percentage of total trades: {round(num_trades / len(trade_data) * 100, 3)} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode the added reference features, if they are encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if ENCODE_REFERENCE_FEATURES:\n",
    "    print('Decoding the reference features.')\n",
    "    for df in tqdm(trade_data_flattened_trade_history_and_related_trades.values()):\n",
    "        for feature in CATEGORICAL_REFERENCE_FEATURES_TO_ADD:\n",
    "            encoder = label_encoders[feature]\n",
    "            for past_trade_idx in range(NUM_TRADES_IN_RELATED_TRADE_HISTORY):\n",
    "                feature_name = get_appended_feature_name(past_trade_idx, feature, related_trade_feature_prefix)\n",
    "                df[feature_name] = encoder.inverse_transform(df[feature_name].to_numpy(dtype=int))    # inverse transform the encoded categorical feature column; must set to dtype=int since label encoder encodes to integers\n",
    "                df[feature_name] = df[feature_name].astype('category')    # change dtype to `categorical` to use in LightGBM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that reference features have dtype categorical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in tqdm(trade_data_flattened_trade_history_and_related_trades.values()):\n",
    "    for feature in CATEGORICAL_REFERENCE_FEATURES_TO_ADD:\n",
    "        if df[feature].dtype.name != 'category': df[feature] = df[feature].astype('category')    # check dtype of a column: https://stackoverflow.com/questions/26924904/check-if-dataframe-column-is-categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a related trades criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing a value for number of trades in the trade history for the same CUSIP that is large enough to capture the predictive power of increasing the number of past trades for the same CUSIP, while also being small enough to make experiments fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRADES_IN_TRADE_HISTORY_OPT = min(NUM_TRADES_IN_TRADE_HISTORY, 16)\n",
    "print(f'NUM_TRADES_IN_TRADE_HISTORY_OPT: {NUM_TRADES_IN_TRADE_HISTORY_OPT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing a value for number of trades in the related trade history that is large enough to capture the predictive power of increasing the number of past related trades, while also being small enough to make experiments fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRADES_IN_RELATED_TRADE_HISTORY_OPT = min(NUM_TRADES_IN_RELATED_TRADE_HISTORY, 32)\n",
    "print(f'NUM_TRADES_IN_RELATED_TRADE_HISTORY_OPT: {NUM_TRADES_IN_RELATED_TRADE_HISTORY_OPT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_trades_columns_opt, all_categorical_features_in_trade_history = get_past_trade_columns(NUM_TRADES_IN_TRADE_HISTORY_OPT, \n",
    "                                                                                            FEATURES_IN_HISTORY, \n",
    "                                                                                            SAME_CUSIP_PREFIX, \n",
    "                                                                                            trade_type_actual=True, \n",
    "                                                                                            trade_type_column=TRADE_TYPE_NEW_COLUMN, \n",
    "                                                                                            categorical_features_per_trade=CATEGORICAL_FEATURES_IN_HISTORY)\n",
    "\n",
    "past_related_trades_columns_opt, all_categorical_features_in_trade_history_related = get_past_trade_columns(NUM_TRADES_IN_RELATED_TRADE_HISTORY_OPT, \n",
    "                                                                                                            FEATURES_IN_HISTORY, \n",
    "                                                                                                            related_trade_feature_prefix, \n",
    "                                                                                                            trade_type_actual=True, \n",
    "                                                                                                            trade_type_column=TRADE_TYPE_NEW_COLUMN, \n",
    "                                                                                                            categorical_features_per_trade=CATEGORICAL_FEATURES_IN_HISTORY + CATEGORICAL_REFERENCE_FEATURES_TO_ADD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_trades_criterion_losses = dict()\n",
    "related_trades_criterion_losses_filepath = make_data_filename('related_trades_criterion_losses')\n",
    "if os.path.exists(related_trades_criterion_losses_filepath):\n",
    "    print(f'Loading losses from {related_trades_criterion_losses_filepath}')\n",
    "    with open(related_trades_criterion_losses_filepath, 'rb') as pickle_handle: related_trades_criterion_losses = pickle.load(pickle_handle)    # use template from https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict-or-any-other-python-object\n",
    "    print(f'Already have loss results for: {list(related_trades_criterion_losses.keys())}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create column sets for different purposes.\n",
    "\n",
    "Difference between `related_trades_features` and `past_related_trades_columns_opt` is that the later has `related_last_trade_type` instead of `related_last_trade_type1` and `related_last_trade_type2`.\n",
    "\n",
    "Difference between `past_trade_feature_groups_flattened` and `past_trades_columns_opt` is that the former has all the `<num>last_trade_type1` and `<num>last_trade_type2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_trade_features = list(set(PREDICTORS_WITHOUT_LAST_TRADE_FEATURES + CATEGORICAL_REFERENCE_FEATURES_TO_ADD)) + TARGET\n",
    "\n",
    "columns_to_select_to_create_dataframe = target_trade_features + past_trade_feature_groups_flattened + related_trades_features + DATA_PROCESSING_FEATURES\n",
    "assert len(columns_to_select_to_create_dataframe) == len(set(columns_to_select_to_create_dataframe))    # checks that there are no intersection between the groups of features\n",
    "columns_to_select_for_lightgbm_model = target_trade_features + past_trades_columns_opt + past_related_trades_columns_opt\n",
    "assert len(columns_to_select_for_lightgbm_model) == len(set(columns_to_select_for_lightgbm_model))    # checks that there are no intersection between the groups of features\n",
    "\n",
    "target_trade_categorical_features = list(set(CATEGORICAL_FEATURES + CATEGORICAL_REFERENCE_FEATURES_TO_ADD))\n",
    "\n",
    "categorical_features_for_lightgbm_model = target_trade_categorical_features + all_categorical_features_in_trade_history + all_categorical_features_in_trade_history_related\n",
    "assert len(categorical_features_for_lightgbm_model) == len(set(categorical_features_for_lightgbm_model))    # checks that there are no intersection between the groups of features\n",
    "\n",
    "print(f'Features used for LightGBM model: {columns_to_select_for_lightgbm_model}')\n",
    "print(f'Categorical features used for LightGBM model: {categorical_features_for_lightgbm_model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in tqdm(trade_data_flattened_trade_history_and_related_trades.items()):\n",
    "    if name not in related_trades_criterion_losses:\n",
    "        # convert trade_type1 and trade_type2 to trade_type with S, P, D for same CUSIP trades\n",
    "        trade_data_predictors_history_related_trades_actual_trade_type, _, _ = convert_trade_type_encoding_to_actual(df[columns_to_select_to_create_dataframe], \n",
    "                                                                                                                     NUM_TRADES_IN_TRADE_HISTORY, \n",
    "                                                                                                                     TRADE_TYPE_NEW_COLUMN, \n",
    "                                                                                                                     SAME_CUSIP_PREFIX)\n",
    "        # convert trade_type1 and trade_type2 to trade_type with S, P, D for related trades\n",
    "        trade_data_predictors_history_related_trades_actual_trade_type, _, _ = convert_trade_type_encoding_to_actual(trade_data_predictors_history_related_trades_actual_trade_type, \n",
    "                                                                                                                     NUM_TRADES_IN_RELATED_TRADE_HISTORY, \n",
    "                                                                                                                     TRADE_TYPE_NEW_COLUMN, \n",
    "                                                                                                                     related_trade_feature_prefix)\n",
    "        \n",
    "        train_data_predictors_history_related_trades_actual_trade_type, \\\n",
    "            test_data_predictors_history_related_trades_actual_trade_type = get_train_test_data_trade_datetime(trade_data_predictors_history_related_trades_actual_trade_type, DATE_TO_SPLIT)\n",
    "        del trade_data_predictors_history_related_trades_actual_trade_type\n",
    "        gc.collect()\n",
    "        assert len(train_data_predictors_history_related_trades_actual_trade_type) != 0 and len(test_data_predictors_history_related_trades_actual_trade_type) != 0, 'Either train or test data is empty. Consider checking how the train test split is being performed.'\n",
    "\n",
    "        print(f'Training the LightGBM model for {name}')\n",
    "        _, lgb_losses = train_lightgbm_model(train_data_predictors_history_related_trades_actual_trade_type[columns_to_select_for_lightgbm_model], \n",
    "                                             test_data_predictors_history_related_trades_actual_trade_type[columns_to_select_for_lightgbm_model], \n",
    "                                             categorical_features_for_lightgbm_model, \n",
    "                                             wandb_project='mitas_trade_history')\n",
    "        related_trades_criterion_losses[name] = lgb_losses['Train'][0], lgb_losses['Test'][0]\n",
    "        with open(related_trades_criterion_losses_filepath, 'wb') as pickle_handle: pickle.dump(related_trades_criterion_losses, pickle_handle, protocol=4)    # use template from https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict-or-any-other-python-object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the best definition for related trades, where *best* refers to the definition with the lowest test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, (train_loss, test_loss) in related_trades_criterion_losses.items():\n",
    "    print(f'{name}\\t\\tTrain error: {train_loss}\\tTest error: {test_loss}')\n",
    "related_trades_criterion_ascending_order_of_test_loss = sorted(related_trades_criterion_losses, key=lambda name: related_trades_criterion_losses.get(name)[1])    # sort by minimum test error (which is represented by index 1)\n",
    "related_trades_criterion_opt = related_trades_criterion_ascending_order_of_test_loss[0]    # optimal name is the one with the minimum test error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we will only have one dataset; the one with the appended trades coming from `related_trades_criterion_opt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_data_flattened_trade_history_and_related_trades = trade_data_flattened_trade_history_and_related_trades[related_trades_criterion_opt]\n",
    "trade_data_predictors_history_related_trades = trade_data_flattened_trade_history_and_related_trades[columns_to_select_to_create_dataframe]\n",
    "train_data_predictors_history_related_trades, test_data_predictors_history_related_trades = get_train_test_data_trade_datetime(trade_data_predictors_history_related_trades, DATE_TO_SPLIT)\n",
    "train_data_predictors_history_related_trades = train_data_predictors_history_related_trades.drop(columns=DATA_PROCESSING_FEATURES)\n",
    "test_data_predictors_history_related_trades = test_data_predictors_history_related_trades.drop(columns=DATA_PROCESSING_FEATURES)\n",
    "\n",
    "trade_data_predictors_history_related_trades_actual_trade_type, old_trade_type_columns, _ = convert_trade_type_encoding_to_actual(trade_data_predictors_history_related_trades, \n",
    "                                                                                                                                  NUM_TRADES_IN_TRADE_HISTORY, \n",
    "                                                                                                                                  TRADE_TYPE_NEW_COLUMN, \n",
    "                                                                                                                                  SAME_CUSIP_PREFIX)\n",
    "trade_data_predictors_history_related_trades_actual_trade_type, old_trade_type_columns_related, _ = convert_trade_type_encoding_to_actual(trade_data_predictors_history_related_trades_actual_trade_type, \n",
    "                                                                                                                                          NUM_TRADES_IN_RELATED_TRADE_HISTORY, \n",
    "                                                                                                                                          TRADE_TYPE_NEW_COLUMN, \n",
    "                                                                                                                                          related_trade_feature_prefix)\n",
    "\n",
    "trade_data_predictors_history_related_trades_actual_trade_type = trade_data_predictors_history_related_trades_actual_trade_type.drop(columns=old_trade_type_columns + old_trade_type_columns_related)\n",
    "train_data_predictors_history_related_trades_actual_trade_type, \\\n",
    "    test_data_predictors_history_related_trades_actual_trade_type = get_train_test_data_trade_datetime(trade_data_predictors_history_related_trades_actual_trade_type, DATE_TO_SPLIT)\n",
    "\n",
    "train_data_predictors_history_related_trades_actual_trade_type = train_data_predictors_history_related_trades_actual_trade_type.drop(columns=DATA_PROCESSING_FEATURES)\n",
    "test_data_predictors_history_related_trades_actual_trade_type = test_data_predictors_history_related_trades_actual_trade_type.drop(columns=DATA_PROCESSING_FEATURES)\n",
    "\n",
    "trade_data_predictors_history_related_trades = trade_data_predictors_history_related_trades.drop(columns=DATA_PROCESSING_FEATURES)\n",
    "trade_data_predictors_history_related_trades_actual_trade_type = trade_data_predictors_history_related_trades_actual_trade_type.drop(columns=DATA_PROCESSING_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining which trades benefit most from a single past related trade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the LightGBM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_lightgbm = train_data_predictors_history_related_trades_actual_trade_type[columns_to_select_for_lightgbm_model]\n",
    "test_data_lightgbm = test_data_predictors_history_related_trades_actual_trade_type[columns_to_select_for_lightgbm_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model, lgb_losses = train_lightgbm_model(train_data_lightbgm, \n",
    "                                             test_data_lightgbm, \n",
    "                                             categorical_features_for_lightgbm_model, \n",
    "                                             wandb_project='mitas_trade_history')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the LightGBM model without the related trades information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_lightgbm_wo_related_trades = test_data_lightgbm.drop(columns=past_related_trades_columns_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model_wo_related_trades, lgb_wo_related_trades_losses = train_lightgbm_model(train_data_lightgbm.drop(columns=past_related_trades_columns_opt), \n",
    "                                                                                 test_data_lightgbm_wo_related_trades, \n",
    "                                                                                 categorical_features_for_lightgbm_model, \n",
    "                                                                                 wandb_project='mitas_trade_history')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create function to test different conditions on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_with_and_wo_related_trade_info(condition):\n",
    "    print(f'Number of trades in test data with condition: {condition.sum()} ({round(condition.sum() / len(test_data_lightgbm) * 100, 3)} %)')\n",
    "    losses = get_all_losses_for_single_dataset(lgb_model, test_data_lightgbm[condition], verbose=False)    # 7 days * 24 hours / day * 60 mins / hour * 60 sec / min\n",
    "    print(f'MAE when using related trade info: {losses[0]}')\n",
    "    losses_wo_related_trades = get_all_losses_for_single_dataset(lgb_model_wo_related_trades, test_data_lightgbm_wo_related_trades[condition], verbose=False)    # 7 days * 24 hours / day * 60 mins / hour * 60 sec / min\n",
    "    print(f'MAE when not using related trade info: {losses_wo_related_trades[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get accuracy on portion of test dataset where the most recent same CUSIP trade was more than one week ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_week_ago = test_data_lightgbm['last_seconds_ago'] > 7 * 24 * 60 * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_model_with_and_wo_related_trade_info(one_week_ago)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_model_with_and_wo_related_trade_info(~one_week_ago)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get accuracy on portion of test dataset where the rating of the trade is lower than BBB+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_rating = ~test_data_lightgbm['rating'].isin(['AAA', 'AA+', 'AA', 'AA-', 'A+', 'A', 'A-', 'BBB+'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_model_with_and_wo_related_trade_info(low_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_model_with_and_wo_related_trade_info(~low_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying the number of past related trades that the model uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lightgbm_model_history_and_reference_and_related_trades(num_past_related_trades):\n",
    "    past_related_trades_columns, all_categorical_features_in_trade_history_related = get_past_trade_columns(num_past_related_trades, \n",
    "                                                                                                            FEATURES_IN_HISTORY, \n",
    "                                                                                                            related_trade_feature_prefix, \n",
    "                                                                                                            trade_type_actual=True, \n",
    "                                                                                                            trade_type_column=TRADE_TYPE_NEW_COLUMN, \n",
    "                                                                                                            categorical_features_per_trade=CATEGORICAL_FEATURES_IN_HISTORY + CATEGORICAL_REFERENCE_FEATURES_TO_ADD)\n",
    "    columns_to_select = target_trade_features + past_trades_columns_opt + past_related_trades_columns\n",
    "    assert len(columns_to_select) == len(set(target_trade_features + past_trades_columns_opt + past_related_trades_columns))    # checks that there are no intersection between the groups of features\n",
    "    train_data_predictors_history_related_trades_actual_trade_type = \n",
    "    test_data_predictors_history_related_trades_actual_trade_type = \n",
    "    return train_lightgbm_model(train_data_predictors_history_related_trades_actual_trade_type[columns_to_select], \n",
    "                                test_data_predictors_history_related_trades_actual_trade_type[columns_to_select], \n",
    "                                target_trade_categorical_features + all_categorical_features_in_trade_history + all_categorical_features_in_trade_history_related, \n",
    "                                wandb_project='mitas_trade_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_past_related_trades_in_history_candidates = list(range(NUM_TRADES_IN_RELATED_TRADE_HISTORY + 1))\n",
    "train_l1_losses_related, test_l1_losses_related = [], []\n",
    "related_lgb_models = []\n",
    "for num_past_trades in num_past_related_trades_in_history_candidates:\n",
    "    lgb_model, lgb_losses = train_lightgbm_model_history_and_reference_and_related_trades(num_past_trades)\n",
    "    if num_past_trades == 32: lgb_model_opt_32, lgb_model_opt_32_losses = lgb_model, lgb_losses\n",
    "    related_lgb_models.append(lgb_model)\n",
    "    train_l1_losses_related.append(lgb_losses['Train'][0])    # index 0 indicates l1 loss\n",
    "    test_l1_losses_related.append(lgb_losses['Test'][0])    # index 0 indicates l1 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 32 <= NUM_TRADES_IN_RELATED_TRADE_HISTORY:\n",
    "    if 'lgb_model_opt_32' not in locals(): lgb_model_opt_32, lgb_model_opt_32_losses = train_lightgbm_model_history_and_reference_and_related_trades(32)    # if model not created, then train the lightgbm model\n",
    "    print(f'Train error: {lgb_model_opt_32_losses[\"Train\"][0]}\\tTest error: {lgb_model_opt_32_losses[\"Test\"][0]}')\n",
    "    lgb.plot_importance(lgb_model_opt_32, figsize=(20, 25), importance_type='gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train error for 0 trades: {train_l1_losses_related[0]}')\n",
    "print(f'Minimum value: {min(train_l1_losses_related)} for number of trades: {num_past_related_trades_in_history_candidates[np.argmin(train_l1_losses_related)]}')\n",
    "plt.ylabel('Train L1 losses')\n",
    "plt.xlabel('Number of related trades in history')\n",
    "plt.plot(num_past_related_trades_in_history_candidates, train_l1_losses_related, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test error for 0 trades: {test_l1_losses_related[0]}')\n",
    "print(f'Minimum value: {min(test_l1_losses_related)} for number of trades: {num_past_related_trades_in_history_candidates[np.argmin(test_l1_losses_related)]}')\n",
    "plt.ylabel('Test L1 losses')\n",
    "plt.xlabel('Number of related trades in history')\n",
    "plt.plot(num_past_related_trades_in_history_candidates, test_l1_losses_related, 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the universe into two categories: (1) those with the most recent past related trade occurring on the same day, and (2) those with no past related trades occurring on the same day. \n",
    "\n",
    "**Hypothesis**: target trades where the earliest past related trade occurs on the same day have the lowest test error with just a single past related trade, whereas target trades where the earliest past related trade occurs on the previous (or further back) day have decreasing test error as the number of past related trades are increased.\n",
    "\n",
    "**Conclusion**: the hypothesis could not be verified since there are so few trades in the second group; worth revisiting this experiment at another time if the definition of *related* changes to be something more restrictive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_last_related_trade_same_day = np.where(trade_data_flattened_trade_history_and_related_trades[related_trade_feature_prefix + 'same_day'] == 1.0)[0]    # get True values from pd.Series: https://stackoverflow.com/questions/52173161/getting-a-list-of-indices-where-pandas-boolean-series-is-true\n",
    "indices_last_related_trade_same_day = trade_data_flattened_trade_history_and_related_trades.iloc[indices_last_related_trade_same_day].index\n",
    "indices_last_related_trade_diff_day = np.where(trade_data_flattened_trade_history_and_related_trades[related_trade_feature_prefix + 'same_day'] == 0.0)[0]    # get True values from pd.Series: https://stackoverflow.com/questions/52173161/getting-a-list-of-indices-where-pandas-boolean-series-is-true\n",
    "indices_last_related_trade_diff_day = trade_data_flattened_trade_history_and_related_trades.iloc[indices_last_related_trade_diff_day].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = test_data_predictors_history_related_trades_actual_trade_type.index\n",
    "test_data_predictors_history_related_trades_actual_trade_type_last_related_trade_same_day = test_data_predictors_history_related_trades_actual_trade_type.loc[test_indices.intersection(indices_last_related_trade_same_day)]    # index intersection: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.intersection.html\n",
    "test_data_predictors_history_related_trades_actual_trade_type_last_related_trade_diff_day = test_data_predictors_history_related_trades_actual_trade_type.loc[test_indices.intersection(indices_last_related_trade_diff_day)]    # index intersection: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.intersection.html\n",
    "print(f'Number of trades in test dataset. Same day: {len(test_data_predictors_history_related_trades_actual_trade_type_last_related_trade_same_day)}\\tDifferent day: {len(test_data_predictors_history_related_trades_actual_trade_type_last_related_trade_diff_day)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_related_trade_losses(dataset):\n",
    "    losses = []\n",
    "    for num_past_trades, model in tqdm(enumerate(related_lgb_models)):\n",
    "        past_related_trades_columns, _ = get_past_trade_columns(num_past_trades, \n",
    "                                                                FEATURES_IN_HISTORY, \n",
    "                                                                related_trade_feature_prefix, \n",
    "                                                                trade_type_actual=True, \n",
    "                                                                trade_type_column=TRADE_TYPE_NEW_COLUMN)\n",
    "        columns_to_select = target_trade_features + past_trades_columns_opt + past_related_trades_columns\n",
    "        losses.append(get_all_losses_for_single_dataset(model, dataset[columns_to_select], verbose=False)[0])\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_l1_losses_last_related_trade_same_day = get_last_related_trade_losses(test_data_predictors_history_related_trades_actual_trade_type_last_related_trade_same_day)\n",
    "test_l1_losses_last_related_trade_diff_day = get_last_related_trade_losses(test_data_predictors_history_related_trades_actual_trade_type_last_related_trade_diff_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test error for 0 trades: {test_l1_losses_last_related_trade_same_day[0]}')\n",
    "print(f'Minimum value: {min(test_l1_losses_last_related_trade_same_day)} for number of trades: {num_past_related_trades_in_history_candidates[np.argmin(test_l1_losses_last_related_trade_same_day)]}')\n",
    "plt.ylabel('Test L1 losses')\n",
    "plt.xlabel('Number of related trades in history')\n",
    "plt.plot(num_past_related_trades_in_history_candidates, test_l1_losses_last_related_trade_same_day, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test error for 0 trades: {test_l1_losses_last_related_trade_diff_day[0]}')\n",
    "print(f'Minimum value: {min(test_l1_losses_last_related_trade_diff_day)} for number of trades: {num_past_related_trades_in_history_candidates[np.argmin(test_l1_losses_last_related_trade_diff_day)]}')\n",
    "plt.ylabel('Test L1 losses')\n",
    "plt.xlabel('Number of related trades in history')\n",
    "plt.plot(num_past_related_trades_in_history_candidates, test_l1_losses_last_related_trade_diff_day, 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding reference data in the related past trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_only_reference_encoded = encode_with_label_encoders(train_data_only_reference)\n",
    "test_data_only_reference_encoded = encode_with_label_encoders(test_data_only_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_name = f'embeddings_{NUM_EPOCHS}_epochs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_arrays = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_arrays_filepath = make_data_filename('embeddings_arrays')\n",
    "if os.path.exists(embeddings_arrays_filepath):    # check if a file exists https://www.pythontutorial.net/python-basics/python-check-if-file-exists/\n",
    "    print(f'Loading embeddings_arrays from {embeddings_arrays_filepath}')\n",
    "    with open(embeddings_arrays_filepath, 'rb') as pickle_handle: embeddings_arrays = pickle.load(pickle_handle)    # use template from https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict-or-any-other-python-object\n",
    "else:\n",
    "    for feature in CATEGORICAL_REFERENCE_FEATURES_TO_ADD:\n",
    "        if feature not in embeddings_arrays:\n",
    "            print(f'Creating embeddings for feature: {feature}')\n",
    "            model, _ = train(NNL1LossEmbeddings(BATCH_SIZE, \n",
    "                                                NUM_WORKERS, \n",
    "                                                train_data_only_reference_encoded[[feature] + TARGET],    # just rating and labels\n",
    "                                                test_data_only_reference_encoded[[feature] + TARGET],    # just rating and labels\n",
    "                                                label_encoders, \n",
    "                                                [feature], \n",
    "                                                power=EMBEDDINGS_POWER), \n",
    "                             NUM_EPOCHS, \n",
    "                             model_filename=make_filename(f'{embeddings_name}_{feature}'), \n",
    "                             save=False, \n",
    "                             print_losses_before_training=False,    # setting this to True may cause the kernel to crash\n",
    "                             print_losses_after_training=False,    # setting this to True may cause the kernel to crash\n",
    "                             wandb_logging_name=embeddings_name, \n",
    "                             wandb_project='mitas_trade_history')\n",
    "            embedding = list(model.embeddings)[0]    # get the embedding from the model; since there is only one feature, we select it\n",
    "            embeddings_arrays[feature] = embedding.weight.detach().numpy()    # embedding is a matrix where each row corresponds to a different possible value; convert the tensor to numpy: https://stackoverflow.com/questions/49768306/pytorch-tensor-to-numpy-array\n",
    "    with open(embeddings_arrays_filepath, 'wb') as pickle_handle: pickle.dump(embeddings_arrays, pickle_handle, protocol=4)    # protocol 4 allows for use in the VM; use template from https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict-or-any-other-python-object\n",
    "print(f'Keys in embeddings_arrays: {list(embeddings_arrays.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECURRENT_ARCHITECTURE = 'lstm'\n",
    "NUM_RECURRENT_LAYERS = 3\n",
    "RECURRENT_HIDDEN_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create column name for trade history of related trades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRADE_HISTORY_RELATED = ['trade_history_related']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_name = f'{RECURRENT_ARCHITECTURE}_{NUM_RECURRENT_LAYERS}_layers_{RECURRENT_HIDDEN_SIZE}_hidden_size_{NUM_EPOCHS}_epochs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine trades from the same CUSIP trade history with the related CUSIP trade history. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMBINED_TRADE_HISTORY = ['combined_trade_history']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is used to combine the `TRADE_HISTORY` and `TRADE_HISTORY_RELATED` into a single sequence of trades sorted by trade_datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_two_histories_sorted_by_seconds_ago_caller = lambda data: combine_two_histories_sorted_by_seconds_ago(data, TRADE_HISTORY + TRADE_HISTORY_RELATED, COMBINED_TRADE_HISTORY[0], FEATURES_TO_INDEX_IN_HISTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below functions are training functions which serve as *callers* for (1) when the data has no related trades, (2) when the model uses the same RNN to process both the same cusip past trades and the related past trades as a single sequence, and (3) when the model uses different RNN's to process the same cusip past trades and the related past trades, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_related_trades_caller = lambda train_data_both_histories, \\\n",
    "                                  test_data_both_histories: train(RecurrentL1Loss(BATCH_SIZE, \n",
    "                                                                                  NUM_WORKERS, \n",
    "                                                                                  train_data_both_histories[TRADE_HISTORY + TARGET],    # just trade history from same cusip and labels\n",
    "                                                                                  test_data_both_histories[TRADE_HISTORY + TARGET],    # just trade history from same cusip and labels\n",
    "                                                                                  NUM_RECURRENT_LAYERS, \n",
    "                                                                                  RECURRENT_HIDDEN_SIZE, \n",
    "                                                                                  RECURRENT_ARCHITECTURE), \n",
    "                                                                  NUM_EPOCHS, \n",
    "                                                                  model_filename=make_filename(rnn_name), \n",
    "                                                                  save=False, \n",
    "                                                                  print_losses_before_training=False,    # setting this to True may cause the kernel to crash\n",
    "                                                                  print_losses_after_training=False,    # setting this to True may cause the kernel to crash\n",
    "                                                                  wandb_logging_name=rnn_name, \n",
    "                                                                  wandb_project='mitas_trade_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_rnn_caller = lambda train_data_both_histories, \\\n",
    "                         test_data_both_histories, \\\n",
    "                         print_losses_after_training=False: train(RecurrentL1Loss(BATCH_SIZE, \n",
    "                                                                                  NUM_WORKERS, \n",
    "                                                                                  combine_two_histories_sorted_by_seconds_ago_caller(train_data_both_histories)[COMBINED_TRADE_HISTORY + TARGET],    # just combined trade history, and labels\n",
    "                                                                                  combine_two_histories_sorted_by_seconds_ago_caller(test_data_both_histories)[COMBINED_TRADE_HISTORY + TARGET],    # just combined trade history, and labels\n",
    "                                                                                  NUM_RECURRENT_LAYERS, \n",
    "                                                                                  RECURRENT_HIDDEN_SIZE, \n",
    "                                                                                  RECURRENT_ARCHITECTURE, \n",
    "                                                                                  COMBINED_TRADE_HISTORY), \n",
    "                                                                  NUM_EPOCHS, \n",
    "                                                                  model_filename=make_filename(rnn_name), \n",
    "                                                                  save=False, \n",
    "                                                                  print_losses_before_training=False,    # setting this to True may cause the kernel to crash\n",
    "                                                                  print_losses_after_training=print_losses_after_training,    # setting this to True may cause the kernel to crash\n",
    "                                                                  wandb_logging_name=rnn_name, \n",
    "                                                                  wandb_project='mitas_trade_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_rnn_caller = lambda train_data_both_histories, \\\n",
    "                              test_data_both_histories: train(MultipleRecurrentL1Loss(BATCH_SIZE, \n",
    "                                                                                      NUM_WORKERS, \n",
    "                                                                                      train_data_both_histories[TRADE_HISTORY + TRADE_HISTORY_RELATED + TARGET], \n",
    "                                                                                      test_data_both_histories[TRADE_HISTORY + TRADE_HISTORY_RELATED + TARGET], \n",
    "                                                                                      NUM_RECURRENT_LAYERS, \n",
    "                                                                                      RECURRENT_HIDDEN_SIZE, \n",
    "                                                                                      RECURRENT_ARCHITECTURE, \n",
    "                                                                                      TRADE_HISTORY + TRADE_HISTORY_RELATED), \n",
    "                                                              NUM_EPOCHS, \n",
    "                                                              model_filename=make_filename('multiple_' + rnn_name), \n",
    "                                                              save=False, \n",
    "                                                              print_losses_before_training=False,    # setting this to True may cause the kernel to crash\n",
    "                                                              print_losses_after_training=False,    # setting this to True may cause the kernel to crash\n",
    "                                                              wandb_logging_name=rnn_name, \n",
    "                                                              wandb_project='mitas_trade_history')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing reference data in trade history\n",
    "It is not obvious how we add the reference data to the related history, since the data must be numerical in order to use it in the RNN. More thoughts: https://docs.google.com/document/d/12TqR2Axt1u0J4qECpGcgZZRiY1L6Q0z2jGkKm_gRAoU/edit?usp=sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "no_reference_data_filepath = make_data_filename('no_reference_data')\n",
    "if os.path.exists(no_reference_data_filepath):    # check if a file exists https://www.pythontutorial.net/python-basics/python-check-if-file-exists/\n",
    "    print(f'Loading dataset from pickle file {no_reference_data_filepath}')\n",
    "    no_reference_data = pd.read_pickle(no_reference_data_filepath)    # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html\n",
    "else:\n",
    "    no_reference_data = pd.DataFrame(index=trade_data.index)    # preserving the original index https://stackoverflow.com/questions/18176933/create-an-empty-data-frame-with-index-from-another-data-frame\n",
    "    no_reference_data[TRADE_HISTORY[0]] = trade_data[TRADE_HISTORY[0]]\n",
    "    no_reference_data[TRADE_HISTORY_RELATED[0]] = feature_group_as_single_feature(trade_data_flattened_trade_history_and_related_trades, related_trades_features_wo_reference_features, NUM_TRADES_IN_RELATED_TRADE_HISTORY)\n",
    "no_reference_data.to_pickle(no_reference_data_filepath, protocol=4)    # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference data represented as encoded features. First, the features are encoded. Second, the reference data is added to same CUSIP trade history. Obviously, this creates a lot of duplicate features, but needs to be done in order to interleave the same CUSIP trade history with the related trades trade history. Finally, the trade history columns are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "encoded_reference_data_filepath = make_data_filename('encoded_reference_data')\n",
    "trade_data_encoded_filepath = make_data_filename('trade_data_encoded')\n",
    "trade_data_flattened_trade_history_and_related_trades_encoded_filepath = make_data_filename('trade_data_flattened_trade_history_and_related_trades_encoded')\n",
    "if os.path.exists(encoded_reference_data_filepath):    # check if a file exists https://www.pythontutorial.net/python-basics/python-check-if-file-exists/\n",
    "    print(f'Loading dataset from pickle file {encoded_reference_data_filepath}')\n",
    "    encoded_reference_data = pd.read_pickle(encoded_reference_data_filepath)    # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html\n",
    "    trade_data_encoded = pd.read_pickle(trade_data_encoded_filepath)    # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html\n",
    "    trade_data_flattened_trade_history_and_related_trades_encoded = pd.read_pickle(trade_data_flattened_trade_history_and_related_trades_encoded_filepath)    # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html\n",
    "else:\n",
    "    trade_data_encoded = encode_with_label_encoders(trade_data)\n",
    "\n",
    "    encoded_reference_data = pd.DataFrame(index=trade_data.index)    # preserving the original index https://stackoverflow.com/questions/18176933/create-an-empty-data-frame-with-index-from-another-data-frame\n",
    "    encoded_reference_data[TRADE_HISTORY[0]] = add_reference_data_to_trade_history(trade_data_encoded, CATEGORICAL_REFERENCE_FEATURES_TO_ADD, TRADE_HISTORY)\n",
    "\n",
    "    print('Encoding the reference features')\n",
    "    trade_data_flattened_trade_history_and_related_trades_encoded = trade_data_flattened_trade_history_and_related_trades.copy()\n",
    "    for feature in CATEGORICAL_REFERENCE_FEATURES_TO_ADD:\n",
    "        encoder = label_encoders[feature]\n",
    "        for past_trade_idx in range(NUM_TRADES_IN_RELATED_TRADE_HISTORY):\n",
    "            feature_name = get_appended_feature_name(past_trade_idx, feature, related_trade_feature_prefix)\n",
    "            trade_data_flattened_trade_history_and_related_trades_encoded[feature_name] = encoder.transform(trade_data_flattened_trade_history_and_related_trades[feature_name])    # transform the categorical feature column to its encoding\n",
    "\n",
    "    encoded_reference_data[TRADE_HISTORY_RELATED[0]] = feature_group_as_single_feature(trade_data_flattened_trade_history_and_related_trades_encoded, related_trades_features, NUM_TRADES_IN_RELATED_TRADE_HISTORY)\n",
    "    \n",
    "trade_data_encoded.to_pickle(trade_data_encoded_filepath, protocol=4)    # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html\n",
    "trade_data_flattened_trade_history_and_related_trades_encoded.to_pickle(trade_data_flattened_trade_history_and_related_trades_encoded_filepath, protocol=4)    # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html\n",
    "encoded_reference_data.to_pickle(encoded_reference_data_filepath, protocol=4)    # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference data represented as one-hot encoded features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_data_one_hot_encoded, one_hot_encoders = encode_and_get_encoders(trade_data, BINARY, CATEGORICAL_FEATURES, 'one_hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded_reference_data_filepath = make_data_filename('one_hot_encoded_reference_data')\n",
    "if os.path.exists(one_hot_encoded_reference_data_filepath):    # check if a file exists https://www.pythontutorial.net/python-basics/python-check-if-file-exists/\n",
    "    print(f'Loading dataset from pickle file {one_hot_encoded_reference_data_filepath}')\n",
    "    one_hot_encoded_reference_data = pd.read_pickle(one_hot_encoded_reference_data_filepath)    # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html\n",
    "else:\n",
    "    one_hot_encoded_reference_data = pd.DataFrame(index=trade_data.index)    # preserving the original index https://stackoverflow.com/questions/18176933/create-an-empty-data-frame-with-index-from-another-data-frame\n",
    "    one_hot_encoded_reference_data[TRADE_HISTORY[0]] = add_reference_data_to_trade_history(trade_data_one_hot_encoded, CATEGORICAL_REFERENCE_FEATURES_TO_ADD, TRADE_HISTORY)\n",
    "\n",
    "    print('One hot encoding the reference features')\n",
    "    trade_data_flattened_trade_history_and_related_trades_one_hot_encoded = trade_data_flattened_trade_history_and_related_trades.copy()\n",
    "    for feature in CATEGORICAL_REFERENCE_FEATURES_TO_ADD:\n",
    "        encoder = one_hot_encoders[feature]\n",
    "        for past_trade_idx in range(NUM_TRADES_IN_RELATED_TRADE_HISTORY):\n",
    "            feature_name = get_appended_feature_name(past_trade_idx, feature, related_trade_feature_prefix)\n",
    "            transformed_values = encoder.transform(trade_data_flattened_trade_history_and_related_trades_one_hot_encoded[feature_name].to_numpy(dtype=str).reshape(-1, 1))    # need to reshape to avoid this error: `ValueError: Expected 2D array, got 1D array instead...Reshape your data either using array.reshape(-1, 1) if your data has a single feature`; casting to `str` to avoid TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
    "            trade_data_flattened_trade_history_and_related_trades_one_hot_encoded[feature_name] = list(transformed_values)    # transform the categorical feature column to its encoding\n",
    "\n",
    "    one_hot_encoded_reference_data[TRADE_HISTORY_RELATED[0]] = feature_group_as_single_feature(trade_data_flattened_trade_history_and_related_trades_one_hot_encoded, related_trades_features, NUM_TRADES_IN_RELATED_TRADE_HISTORY, flatten_each_row=True, multiprocessing=True)\n",
    "one_hot_encoded_reference_data.to_pickle(one_hot_encoded_reference_data_filepath, protocol=4)    # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference data represented as embedded features. Note: embedding occurs on encoded feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_with_embeddings_arrays = lambda df: embed_with_arrays(df, embeddings_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_reference_data_filepath = make_data_filename('embedded_reference_data')\n",
    "if os.path.exists(embedded_reference_data_filepath):    # check if a file exists https://www.pythontutorial.net/python-basics/python-check-if-file-exists/\n",
    "    print(f'Loading dataset from pickle file {embedded_reference_data_filepath}')\n",
    "    embedded_reference_data = pd.read_pickle(embedded_reference_data_filepath)    # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html\n",
    "else:\n",
    "    trade_data_embedded = embed_with_embeddings_arrays(trade_data_encoded)    # embedding occurs on the encoded feature values\n",
    "\n",
    "    embedded_reference_data = pd.DataFrame(index=trade_data.index)    # preserving the original index https://stackoverflow.com/questions/18176933/create-an-empty-data-frame-with-index-from-another-data-frame\n",
    "    embedded_reference_data[TRADE_HISTORY[0]] = add_reference_data_to_trade_history(trade_data_embedded, CATEGORICAL_REFERENCE_FEATURES_TO_ADD, TRADE_HISTORY, True)\n",
    "\n",
    "    print('Embedding the encoded reference features')\n",
    "    trade_data_flattened_trade_history_and_related_trades_embedded = trade_data_flattened_trade_history_and_related_trades_encoded.copy()\n",
    "    for feature in CATEGORICAL_REFERENCE_FEATURES_TO_ADD:\n",
    "        embeddings_array = embeddings_arrays[feature]\n",
    "        for past_trade_idx in range(NUM_TRADES_IN_RELATED_TRADE_HISTORY):\n",
    "            feature_name = get_appended_feature_name(past_trade_idx, feature, related_trade_feature_prefix)\n",
    "            trade_data_flattened_trade_history_and_related_trades_embedded[feature_name] = trade_data_flattened_trade_history_and_related_trades_encoded[feature_name].map(list_to_index_dict(embeddings_array))    # `.map(...)` is the fastest way to do this: https://stackoverflow.com/questions/20250771/remap-values-in-pandas-column-with-a-dict-preserve-nans\n",
    "\n",
    "    embedded_reference_data[TRADE_HISTORY_RELATED[0]] = feature_group_as_single_feature(trade_data_flattened_trade_history_and_related_trades_embedded, related_trades_features, NUM_TRADES_IN_RELATED_TRADE_HISTORY, flatten_each_row=True, multiprocessing=True)\n",
    "embedded_reference_data.to_pickle(embedded_reference_data_filepath, protocol=4)    # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the best reference data representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_history_data = {'no_reference_data': no_reference_data, \n",
    "                      'encoded_reference_data': encoded_reference_data, \n",
    "                    #   'one_hot_encoded_reference_data': one_hot_encoded_reference_data, \n",
    "                      'embedded_reference_data': embedded_reference_data\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_data_representation_losses = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing a small number of past trades to make experiments fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_history_8_same_cusip_16_related_caller = lambda trade_data: limit_history_to_k_trades(trade_data, {TRADE_HISTORY[0]: 8, TRADE_HISTORY_RELATED[0]: 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "reference_data_representation_losses_filepath = make_data_filename('reference_data_representation_losses')\n",
    "if os.path.exists(reference_data_representation_losses_filepath):\n",
    "    print(f'Loading losses form {reference_data_representation_losses_filepath}')\n",
    "    with open(reference_data_representation_losses_filepath, 'rb') as pickle_handle: reference_data_representation_losses = pickle.load(pickle_handle)    # use template from https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict-or-any-other-python-object\n",
    "else:\n",
    "    limit_history_8_same_cusip_16_related_caller = lambda trade_data: limit_history_to_k_trades(trade_data, {TRADE_HISTORY[0]: 8, TRADE_HISTORY_RELATED[0]: 16})\n",
    "    train_indices, test_indices = train_data_only_reference.index, test_data_only_reference.index\n",
    "\n",
    "    for name, trade_data_both_histories in trade_history_data.items():\n",
    "        if name not in reference_data_representation_losses:\n",
    "            trade_data_both_histories[TARGET[0]] = trade_data[TARGET[0]]\n",
    "            train_data_both_histories, test_data_both_histories = get_train_test_data_index(trade_data_both_histories, train_indices, test_indices)\n",
    "            assert len(train_data_both_histories) != 0 and len(test_data_both_histories) != 0, 'Either train or test data is empty. Consider checking how the train test split is being performed.'\n",
    "            train_data_both_histories = reverse_order_of_trade_history(train_data_both_histories, TRADE_HISTORY + TRADE_HISTORY_RELATED)\n",
    "            test_data_both_histories = reverse_order_of_trade_history(test_data_both_histories, TRADE_HISTORY + TRADE_HISTORY_RELATED)\n",
    "            _, (_, _, (test_l1_loss, _)) = same_rnn_caller(limit_history_8_same_cusip_16_related_caller(train_data_both_histories), \n",
    "                                                        limit_history_8_same_cusip_16_related_caller(test_data_both_histories), \n",
    "                                                        True)\n",
    "            reference_data_representation_losses[name] = test_l1_loss\n",
    "        with open(reference_data_representation_losses_filepath, 'wb') as pickle_handle: pickle.dump(reference_data_representation_losses, pickle_handle, protocol=4)    # use template from https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict-or-any-other-python-object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, test_loss in reference_data_representation_losses.items():\n",
    "    print(f'{name}\\t\\tTest error: {test_loss}')\n",
    "reference_data_representation_losses_ascending_order = sorted(reference_data_representation_losses, key=lambda name: related_trades_criterion_losses.get(name))    # sort by minimum test error\n",
    "reference_data_representation_opt = reference_data_representation_losses_ascending_order[0]    # optimal name is the one with the minimum test error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions** \n",
    "\n",
    "The following parameter choices were used for the LSTM: NUM_RECURRENT_LAYERS=3, RECURRENT_HIDDEN_SIZE=64\n",
    "\n",
    "|  | No reference data | Reference data encoded | Reference data embedded |\n",
    "| --- | --- | --- | --- |\n",
    "| Train loss | 7.520 | 7.527 | 7.511 |\n",
    "| Test loss | 8.157 | 8.135 | 8.158 |\n",
    "\n",
    "- There is almost no difference between the three settings\n",
    "- The lack of difference is surprising, and may suggest that (1) better feature engineering is required, such as differences between the target trade and the related trade, and (2) overly complex representations of the reference data may be unnecessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the trade data dataframe to keep only the data from the best representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_data_representation_opt = 'encoded_reference_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_data[TRADE_HISTORY + TRADE_HISTORY_RELATED] = trade_history_data[reference_data_representation_opt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that trade history columns have only numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for trade_history_column in TRADE_HISTORY + TRADE_HISTORY_RELATED:\n",
    "    trade_history_column_dtype = np.stack(trade_data[trade_history_column].to_numpy()).dtype    # `np.stack(...)` converts the numpy array from a numpy array of numpy arrays to a single 3d numpy array\n",
    "    assert np.issubdtype(trade_history_column_dtype, np.number), f'trade history column dtype: {trade_history_column_dtype}'    # asserts that the dtype of the trade history array is a numerical type: https://stackoverflow.com/questions/29518923/numpy-asarray-how-to-check-up-that-its-result-dtype-is-numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse the trade history to have it in the correct order to be passed into the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_reference_and_both_histories, test_data_reference_and_both_histories = get_train_test_data_trade_datetime(trade_data, DATE_TO_SPLIT)\n",
    "assert len(train_data_reference_and_both_histories) != 0 and len(test_data_reference_and_both_histories) != 0, 'Either train or test data is empty. Consider checking how the train test split is being performed.'\n",
    "train_data_reference_and_both_histories = reverse_order_of_trade_history(train_data_reference_and_both_histories, TRADE_HISTORY + TRADE_HISTORY_RELATED)\n",
    "test_data_reference_and_both_histories = reverse_order_of_trade_history(test_data_reference_and_both_histories, TRADE_HISTORY + TRADE_HISTORY_RELATED)\n",
    "train_data_reference_and_both_histories = train_data_reference_and_both_histories.drop(columns=DATA_PROCESSING_FEATURES + IDENTIFIERS)\n",
    "test_data_reference_and_both_histories = test_data_reference_and_both_histories.drop(columns=DATA_PROCESSING_FEATURES + IDENTIFIERS)\n",
    "train_data_both_histories = train_data_reference_and_both_histories[TRADE_HISTORY + TRADE_HISTORY_RELATED + TARGET]\n",
    "test_data_both_histories = test_data_reference_and_both_histories[TRADE_HISTORY + TRADE_HISTORY_RELATED + TARGET]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 previous trades same CUSIP (baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without `settlement_date_to_calc_date` and `quantity_diff` features in `trade_history`\n",
    "This is the original baseline that we are working to beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_both_histories_samecusip5 = limit_history_to_k_trades(train_data_both_histories, {TRADE_HISTORY[0]: 5})\n",
    "test_data_both_histories_samecusip5 = limit_history_to_k_trades(test_data_both_histories, {TRADE_HISTORY[0]: 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_new_features_from_trade_history_caller = lambda data, trade_history_columns: remove_feature_from_trade_history(data, trade_history_columns, ['settlement_date_to_calc_date', 'quantity_diff'], FEATURES_TO_INDEX_IN_HISTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "no_related_trades_caller(remove_new_features_from_trade_history_caller(train_data_both_histories_samecusip5, TRADE_HISTORY), \n",
    "                         remove_new_features_from_trade_history_caller(test_data_both_histories_samecusip5, TRADE_HISTORY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With `settlement_date_to_calc_date` and `quantity_diff` features in `trade_history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "no_related_trades_caller(train_data_both_histories_samecusip5, test_data_both_histories_samecusip5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16 previous trades same CUSIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_both_histories_samecusip16 = limit_history_to_k_trades(train_data_both_histories, {TRADE_HISTORY[0]: 16})\n",
    "test_data_both_histories_samecusip16 = limit_history_to_k_trades(test_data_both_histories, {TRADE_HISTORY[0]: 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No related trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "no_related_trades_caller(train_data_both_histories_samecusip16, test_data_both_histories_samecusip16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 32 related trades (same RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_both_histories_samecusip16_related32 = limit_history_to_k_trades(train_data_both_histories_samecusip16, {TRADE_HISTORY_RELATED[0]: 32})\n",
    "test_data_both_histories_samecusip16_related32 = limit_history_to_k_trades(test_data_both_histories_samecusip16, {TRADE_HISTORY_RELATED[0]: 32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "same_rnn_caller(train_data_both_histories_samecusip16_related32, test_data_both_histories_samecusip16_related32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 32 related trades (different RNN's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "different_rnn_caller(train_data_both_histories_samecusip16_related32, test_data_both_histories_samecusip16_related32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 64 related trades (same RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# assumes that the number of related trades is 64 by default\n",
    "same_rnn_caller(train_data_both_histories_samecusip16, test_data_both_histories_samecusip16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 64 related trades (different RNN's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# assumes that the number of related trades is 64 by default\n",
    "different_rnn_caller(train_data_both_histories_samecusip16, test_data_both_histories_samecusip16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 32 previous trades same CUSIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No related trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# assumes that the number of same cusip past trades is 32 by default\n",
    "no_related_trades_caller(train_data_both_histories, test_data_both_histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 32 related trades (same RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_both_histories_related32 = limit_history_to_k_trades(train_data_both_histories, {TRADE_HISTORY_RELATED[0]: 32})\n",
    "test_data_both_histories_related32 = limit_history_to_k_trades(test_data_both_histories, {TRADE_HISTORY_RELATED[0]: 32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# assumes that the number of same cusip past trades is 32 by default\n",
    "same_rnn_caller(train_data_both_histories_related32, test_data_both_histories_related32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 32 related trades (different RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# assumes that the number of same cusip past trades is 32 by default\n",
    "different_rnn_caller(train_data_both_histories_related32, test_data_both_histories_related32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 64 related trades (same RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# assumes that the number of same cusip past trades is 32 by default and the number of related trades is 64 by default\n",
    "same_rnn_caller(train_data_both_histories, test_data_both_histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 64 related trades (different RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# assumes that the number of same cusip past trades is 32 by default and the number of related trades is 64 by default\n",
    "different_rnn_caller(train_data_both_histories, test_data_both_histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below table shows the results of different experiments above, where each cell has two values: (1) training loss, (2) test loss.\n",
    "\n",
    "|  | No related trades | 32 related trades (same RNN) | 64 related trades (same RNN) | 32 related trades (different RNN) | 64 related trades (different RNN) |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| **5 same CUSIP trades** | 7.78582, 8.76154 | TODO | TODO | TODO | TODO |\n",
    "| **16 same CUSIP trades** | 7.30065, 8.65277 | 7.44454, 8.49455 | 7.62937, 8.48275 | 7.13232, 8.65048 | 7.3365, 8.64571 |\n",
    "| **32 same CUSIP trades** | 7.18217, 8.55768 | 6.92592, 8.40626 | 7.4078, 8.47732 | 7.14426, 8.56969 | 6.77395, 8.57151 |\n",
    "\n",
    "**Conclusions**\n",
    "- increasing the number of same CUSIP trades increases accuracy (on average about 0.1 bps improvement)\n",
    "- same RNN performs better than different RNN (on average about 0.15 bps improvement); the higher test error, but lower training error implies that the different RNN architecture is overfitting\n",
    "- difference between 32 related trades and 64 related trades is negligible (all differences within 0.07 bps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on, we use `NUM_TRADES_IN_TRADE_HISTORY_OPT` and `NUM_TRADES_IN_RELATED_TRADE_HISTORY_OPT` to see the impact of having longer histories, but also to make experiments fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_history_to_opt_trades_caller = lambda data: limit_history_to_k_trades(data, {TRADE_HISTORY[0]: NUM_TRADES_IN_TRADE_HISTORY_OPT, TRADE_HISTORY_RELATED[0]: NUM_TRADES_IN_RELATED_TRADE_HISTORY})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different padding schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_file_nonzero_padding_pickle = '../../../../ficc/ml_models/sequence_predictors/data/processed_data_ficc_ycl_long_history_nonzero_padding_2021-12-31-23-59.pkl'\n",
    "processed_file_nonzero_padding_pickle_datestring = get_datestring_from_filename(processed_file_nonzero_padding_pickle)\n",
    "assert processed_file_pickle_datestring == processed_file_nonzero_padding_pickle_datestring, 'Different files are being used'\n",
    "trade_data_nonzero_padding = read_processed_file_pickle(processed_file_nonzero_padding_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply exclusions (identical to first few cells of notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_data_nonzero_padding = trade_data_nonzero_padding[(trade_data_nonzero_padding.days_to_call == 0) | (trade_data_nonzero_padding.days_to_call > np.log10(400))]\n",
    "trade_data_nonzero_padding = trade_data_nonzero_padding[(trade_data_nonzero_padding.days_to_refund == 0) | (trade_data_nonzero_padding.days_to_refund > np.log10(400))]\n",
    "trade_data_nonzero_padding = trade_data_nonzero_padding[trade_data_nonzero_padding.days_to_maturity < np.log10(30000)]\n",
    "trade_data_nonzero_padding = trade_data_nonzero_padding[trade_data_nonzero_padding.sinking == False]\n",
    "trade_data_nonzero_padding = trade_data_nonzero_padding[trade_data_nonzero_padding.incorporated_state_code != 'VI']\n",
    "trade_data_nonzero_padding = trade_data_nonzero_padding[trade_data_nonzero_padding.incorporated_state_code != 'GU']\n",
    "trade_data_nonzero_padding = trade_data_nonzero_padding[(trade_data_nonzero_padding.coupon_type == 8)]\n",
    "trade_data_nonzero_padding = trade_data_nonzero_padding[trade_data_nonzero_padding.is_called == False]\n",
    "\n",
    "# restructured bonds and high chance of default bonds are removed\n",
    "trade_data_nonzero_padding = remove_rows_with_feature_value(trade_data_nonzero_padding, 'purpose_sub_class', [6, 20, 21, 22, 44, 57, 90, 106])\n",
    "# pre-refunded bonds and partially refunded bonds are removed\n",
    "trade_data_nonzero_padding = remove_rows_with_feature_value(trade_data_nonzero_padding, 'called_redemption_type', [18, 19])\n",
    "\n",
    "\n",
    "trade_data_nonzero_padding = replace_rating_with_standalone_rating(trade_data_nonzero_padding)\n",
    "\n",
    "\n",
    "ADDITIONAL_CATEGORICAL_FEATURES = check_additional_features(trade_data_nonzero_padding, ADDITIONAL_CATEGORICAL_FEATURES)\n",
    "\n",
    "all_features_set = set(trade_data_nonzero_padding.columns)\n",
    "BINARY = list(set(BINARY).intersection(all_features_set))\n",
    "CATEGORICAL_FEATURES = list((set(CATEGORICAL_FEATURES) | set(ADDITIONAL_CATEGORICAL_FEATURES)).intersection(all_features_set))\n",
    "NON_CAT_FEATURES = list(set(NON_CAT_FEATURES).intersection(all_features_set))\n",
    "\n",
    "trade_data_nonzero_padding = trade_data_nonzero_padding[IDENTIFIERS + \n",
    "                        BINARY + \n",
    "                        CATEGORICAL_FEATURES + \n",
    "                        NON_CAT_FEATURES + \n",
    "                        DATA_PROCESSING_FEATURES + \n",
    "                        TRADE_HISTORY + \n",
    "                        TARGET]\n",
    "\n",
    "trade_data_nonzero_padding, _ = replace_nan_for_features(trade_data_nonzero_padding, FEATURES_AND_NAN_REPLACEMENT_VALUES, verbose=True)\n",
    "trade_data_nonzero_padding = remove_fields_with_single_unique_value(trade_data_nonzero_padding)\n",
    "\n",
    "all_features_set = set(trade_data_nonzero_padding.columns)\n",
    "BINARY = list(set(BINARY).intersection(all_features_set))\n",
    "CATEGORICAL_FEATURES = list(set(CATEGORICAL_FEATURES).intersection(all_features_set))\n",
    "NON_CAT_FEATURES = list(set(NON_CAT_FEATURES).intersection(all_features_set))\n",
    "PREDICTORS = BINARY + CATEGORICAL_FEATURES + NON_CAT_FEATURES\n",
    "\n",
    "trade_data_nonzero_padding = remove_rows_with_nan_value(trade_data_nonzero_padding)\n",
    "\n",
    "\n",
    "# sort by trade_datetime since order can be changed when reading pickle file into m1 since it loads by chunks\n",
    "trade_data_nonzero_padding = trade_data_nonzero_padding.sort_values(by='trade_datetime', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append related trades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED = 'rating_without_+-_b_nr_combined'\n",
    "trade_data_nonzero_padding[RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED] = trade_data_nonzero_padding['rating'].transform(lambda rating: str.rstrip(rating, '+-'))    # remove + and - from right side of string\n",
    "# group BBB, BB, B, and NR together since each have a very small number of trades\n",
    "b_ratings = trade_data_nonzero_padding[RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED].isin(['B', 'BB', 'BBB', 'NR'])\n",
    "trade_data_nonzero_padding.loc[b_ratings, RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED] = 'B'\n",
    "print(f'Created {RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED} feature')\n",
    "\n",
    "DAYS_TO_MATURITY_CATEGORICAL = 'days_to_maturity_categorical'\n",
    "trade_data_nonzero_padding[DAYS_TO_MATURITY_CATEGORICAL] = pd.cut(trade_data_nonzero_padding['days_to_maturity'], num_of_days_bins_maturity).astype('string')\n",
    "print(f'Created {DAYS_TO_MATURITY_CATEGORICAL} feature')\n",
    "\n",
    "DAYS_TO_CALL_CATEGORICAL = 'days_to_call_categorical'\n",
    "trade_data_nonzero_padding[DAYS_TO_CALL_CATEGORICAL] = pd.cut(trade_data_nonzero_padding['days_to_call'], num_of_days_bins_call).astype('string')\n",
    "print(f'Created {DAYS_TO_CALL_CATEGORICAL} feature')\n",
    "\n",
    "COUPON_CATEGORICAL = 'coupon_categorical'\n",
    "coupon_bins = [0, 3, 4, 4.5, 5.0 + epsilon, VERY_LARGE_NUMBER]   # 0 - 2.99, 3 - 3.99, 4 - 4.49, 4.5 - 5; from discussion with a team member\n",
    "trade_data_nonzero_padding[COUPON_CATEGORICAL] = pd.cut(trade_data_nonzero_padding['coupon'], coupon_bins, right=False).astype('string')\n",
    "print(f'Created {COUPON_CATEGORICAL} feature')\n",
    "\n",
    "COUPON_CATEGORICAL_SUDHAR = 'coupon_categorical_sudhar'\n",
    "coupon_bins = [0, 3, 4, 4.5, 5, 5.25, 5.5, 6, VERY_LARGE_NUMBER]    # from Sudhar's paper: Kolm, Purushothaman. 2021. Systematic Pricing and Trading of Municipal Bonds\n",
    "trade_data_nonzero_padding[COUPON_CATEGORICAL_SUDHAR] = pd.cut(trade_data_nonzero_padding['coupon'], coupon_bins, right=False).astype('string')\n",
    "print(f'Created {COUPON_CATEGORICAL_SUDHAR} feature')\n",
    "\n",
    "# COUPON_TOP_VALUES = 'coupon_top_values'\n",
    "# trade_data_nonzero_padding[COUPON_TOP_VALUES] = trade_data_nonzero_padding['coupon']\n",
    "# top4_coupon_values = trade_data_nonzero_padding['coupon'].value_counts().head(4).index.tolist()    # select the top 4 coupon values based on frequency in the data, which are: 5.0, 4.0, 3.0, 2.0 comprising about 90% of the data\n",
    "# trade_data_nonzero_padding.loc[~trade_data_nonzero_padding['coupon'].isin(top4_coupon_values), COUPON_TOP_VALUES] = -1    # arbitrary numerical value that is invalid as a coupon value\n",
    "# print(f'Created {COUPON_TOP_VALUES} feature')\n",
    "\n",
    "PURPOSE_CLASS_TOP_VALUES = 'purpose_class_top_values'\n",
    "trade_data_nonzero_padding[PURPOSE_CLASS_TOP_VALUES] = trade_data_nonzero_padding['purpose_class']\n",
    "trade_data_nonzero_padding.loc[~trade_data_nonzero_padding['purpose_class'].isin(top6_purpose_class_values), PURPOSE_CLASS_TOP_VALUES] = -1    # arbitrary numerical value that is invalid as a purpose_class value\n",
    "print(f'Created {PURPOSE_CLASS_TOP_VALUES} feature')\n",
    "\n",
    "MUNI_SECURITY_TYPE_TOP_VALUES = 'muni_security_type_top_values'\n",
    "trade_data_nonzero_padding[MUNI_SECURITY_TYPE_TOP_VALUES] = trade_data_nonzero_padding['muni_security_type']\n",
    "trade_data_nonzero_padding.loc[~trade_data_nonzero_padding['muni_security_type'].isin(top6_muni_security_type_values), MUNI_SECURITY_TYPE_TOP_VALUES] = -1    # arbitrary numerical value that is invalid as a purpose_class value\n",
    "print(f'Created {MUNI_SECURITY_TYPE_TOP_VALUES} feature')\n",
    "\n",
    "TRADE_DATETIME_DAY = 'trade_datetime_day'\n",
    "trade_data_nonzero_padding[TRADE_DATETIME_DAY] = trade_data_nonzero_padding['trade_datetime'].transform(lambda datetime: datetime.date()).astype('string')    # remove timestamp from datetime\n",
    "print(f'Created {TRADE_DATETIME_DAY} feature')\n",
    "\n",
    "QUANTITY_CATEGORICAL = 'quantity_categorical'\n",
    "trade_data_nonzero_padding[QUANTITY_CATEGORICAL] = pd.cut(trade_data_nonzero_padding['quantity'], quantity_bins).astype('string')\n",
    "print(f'Created {QUANTITY_CATEGORICAL} feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format for this dictionary: key is the name of the feature; value is a tuple where the first item is a function of the current trade and related trade, and the second item is the default value to be filled in if that value does not exist\n",
    "RELATED_TRADE_FEATURE_FUNCTIONS_AND_NONZERO_PADDING_DEFAULT_VALUES = {key: (function, DEFAULT_VALUES_NONZERO_PADDING[key]) for key, function in RELATED_TRADE_FEATURE_FUNCTIONS.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "categories_to_match_opt, filtering_conditions_opt = related_trades_criterion[related_trades_criterion_opt]\n",
    "df_encoded = encode_with_label_encoders(trade_data_flattened_trade_history, features_to_exclude=['trade_type']) if ENCODE_REFERENCE_FEATURES else trade_data_flattened_trade_history\n",
    "filepath = make_data_filename('trade_data_nonzero_padding_flattened_trade_history_and_related_trades')\n",
    "if os.path.exists(filepath):    # check if a file exists https://www.pythontutorial.net/python-basics/python-check-if-file-exists/\n",
    "    print(f'Loading dataset from pickle file {filepath}')\n",
    "    trade_data_nonzero_padding_related_trades = pd.read_pickle(filepath)    # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html\n",
    "else:\n",
    "    print(f'Creating dataset and saving it to {filepath}')\n",
    "    trade_data_nonzero_padding_related_trades = append_recent_trade_data(trade_data_nonzero_padding, \n",
    "                                                                         NUM_TRADES_IN_RELATED_TRADE_HISTORY, \n",
    "                                                                         RELATED_TRADE_FEATURE_FUNCTIONS_AND_NONZERO_PADDING_DEFAULT_VALUES, \n",
    "                                                                         feature_prefix=related_trade_feature_prefix, \n",
    "                                                                         categories=categories_to_match_opt, \n",
    "                                                                         filtering_conditions=filtering_conditions_opt, \n",
    "                                                                         return_df=True, \n",
    "                                                                         multiprocessing=True, \n",
    "                                                                         df_for_related_trades=df_encoded).drop(columns=quantized_features)    # drop the quantized features from the final dataframe\n",
    "    trade_data_nonzero_padding_related_trades.to_pickle(filepath, protocol=4)    # protocol 4 allows for use in the VM: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html\n",
    "trade_data_nonzero_padding = trade_data_nonzero_padding.drop(columns=quantized_features)    # drop the quantized features from the final dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep dataframe for training same RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_data_nonzero_padding[TRADE_HISTORY[0]] = add_reference_data_to_trade_history(encode_with_label_encoders(trade_data_nonzero_padding), \n",
    "                                                                                   CATEGORICAL_REFERENCE_FEATURES_TO_ADD, \n",
    "                                                                                   TRADE_HISTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_data_nonzero_padding[TRADE_HISTORY_RELATED[0]] = feature_group_as_single_feature(trade_data_nonzero_padding_related_trades, \n",
    "                                                                                       related_trades_features, \n",
    "                                                                                       NUM_TRADES_IN_RELATED_TRADE_HISTORY)\n",
    "\n",
    "train_data_nonzero_padding_reference_and_both_histories, \\\n",
    "    test_data_nonzero_padding_reference_and_both_histories = get_train_test_data_trade_datetime(trade_data_nonzero_padding, DATE_TO_SPLIT)\n",
    "assert len(train_data_nonzero_padding_reference_and_both_histories) != 0 and len(test_data_nonzero_padding_reference_and_both_histories) != 0, 'Either train or test data is empty. Consider checking how the train test split is being performed.'\n",
    "train_data_nonzero_padding_reference_and_both_histories = reverse_order_of_trade_history(train_data_nonzero_padding_reference_and_both_histories, \n",
    "                                                                                         TRADE_HISTORY + TRADE_HISTORY_RELATED)\n",
    "test_data_nonzero_padding_reference_and_both_histories = reverse_order_of_trade_history(test_data_nonzero_padding_reference_and_both_histories, \n",
    "                                                                                        TRADE_HISTORY + TRADE_HISTORY_RELATED)\n",
    "train_data_nonzero_padding_reference_and_both_histories = train_data_nonzero_padding_reference_and_both_histories.drop(columns=DATA_PROCESSING_FEATURES + IDENTIFIERS)\n",
    "test_data_nonzero_padding_reference_and_both_histories = test_data_nonzero_padding_reference_and_both_histories.drop(columns=DATA_PROCESSING_FEATURES + IDENTIFIERS)\n",
    "train_data_nonzero_padding_both_histories = train_data_nonzero_padding_reference_and_both_histories[TRADE_HISTORY + TRADE_HISTORY_RELATED + TARGET]\n",
    "test_data_nonzero_padding_both_histories = test_data_nonzero_padding_reference_and_both_histories[TRADE_HISTORY + TRADE_HISTORY_RELATED + TARGET]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train same RNN model over different seeds to reduce the effect of randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SEEDS = 5\n",
    "losses_dict_filepath = make_data_filename('padding_losses_dict')\n",
    "losses_dict = {'zero_padding': [], 'nonzero_padding': []}\n",
    "\n",
    "if os.path.exists(losses_dict_filepath):\n",
    "    print(f'Loading losses form {losses_dict_filepath}')\n",
    "    with open(losses_dict_filepath, 'rb') as pickle_handle: losses_dict = pickle.load(pickle_handle)    # use template from https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict-or-any-other-python-object\n",
    "\n",
    "test_l1_losses, test_l1_losses_nonzero_padding = losses_dict['zero_padding'], losses_dict['nonzero_padding']\n",
    "assert len(test_l1_losses) == len(test_l1_losses_nonzero_padding)\n",
    "\n",
    "if len(test_l1_losses) < NUM_SEEDS :\n",
    "    train_data_both_histories_8_16 = limit_history_8_same_cusip_16_related_caller(train_data_both_histories)\n",
    "    test_data_both_histories_8_16 = limit_history_8_same_cusip_16_related_caller(test_data_both_histories)\n",
    "    train_data_nonzero_padding_both_histories_8_16 = limit_history_8_same_cusip_16_related_caller(train_data_nonzero_padding_both_histories)\n",
    "    test_data_nonzero_padding_both_histories_8_16 = limit_history_8_same_cusip_16_related_caller(test_data_nonzero_padding_both_histories)\n",
    "    for seed in range(len(test_l1_losses), NUM_SEEDS):\n",
    "        seed_everything(seed, workers=True)\n",
    "\n",
    "        _, test_l1_loss = same_rnn_caller(train_data_both_histories_8_16, test_data_both_histories_8_16)\n",
    "        test_l1_losses.append(test_l1_loss.item())    # get value from a single element tensor: https://stackoverflow.com/questions/57727372/how-do-i-get-the-value-of-a-tensor-in-pytorch\n",
    "\n",
    "        _, test_l1_loss = same_rnn_caller(train_data_nonzero_padding_both_histories_8_16, test_data_nonzero_padding_both_histories_8_16)\n",
    "        test_l1_losses_nonzero_padding.append(test_l1_loss.item())    # get value from a single element tensor: https://stackoverflow.com/questions/57727372/how-do-i-get-the-value-of-a-tensor-in-pytorch\n",
    "        with open(losses_dict_filepath, 'wb') as pickle_handle: pickle.dump(losses_dict, pickle_handle, protocol=4)    # use template from https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict-or-any-other-python-object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Average loss: {sum(test_l1_losses) / NUM_SEEDS}\\t\\tAll losses: {test_l1_losses}')\n",
    "print(f'Average loss (zero padding): {sum(test_l1_losses_nonzero_padding) / NUM_SEEDS}\\t\\tAll losses: {test_l1_losses_nonzero_padding}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset seed back to default `SEED` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions** \n",
    "\n",
    "To make experiments faster, the following parameter choices were used for the LSTM: NUM_RECURRENT_LAYERS=2, RECURRENT_HIDDEN_SIZE=16\n",
    "\n",
    "|  | Seed 0 | Seed 1 | Seed 2 | Seed 3 | Seed 4 | Average |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| Zero padding | 8.63817024230957 | 8.734763145446777 | 8.767729759216309 | 8.626927375793457 | 8.581864356994629 | 8.669890975952148 |\n",
    "| Nonzero padding | 8.457047462463379 | 8.560364723205566 | 8.584809303283691 | 8.534771919250488 | 8.473348617553711 | 8.522068405151368 |\n",
    "\n",
    "- Nonzero padding outperforms zero padding by 0.15 basis points on average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Network for Reference Data with RNN's for Trade History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different models while incorporating the reference data:\n",
    "- Model (a): feedforward network with just reference data\n",
    "- Model (b): feedforward network with reference data and RNN on same cusip trade history\n",
    "- Model (c): feedforward network with reference data and RNN on same cusip trade history and reference data for last related trade\n",
    "- Model (d): feedforward network with reference data and reference data for last related trade and interleaved RNN on same cusip and related trade histories\n",
    "- Model (e): feedforward network with reference data and reference data for last related trade and different RNN on same cusip and related trade histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_data_reference_and_both_histories = limit_history_to_opt_trades_caller(train_data_reference_and_both_histories)\n",
    "test_data_reference_and_both_histories = limit_history_to_opt_trades_caller(test_data_reference_and_both_histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_data_reference_and_both_histories_encoded = encode_with_label_encoders(train_data_reference_and_both_histories)\n",
    "test_data_reference_and_both_histories_encoded = encode_with_label_encoders(test_data_reference_and_both_histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedforward_losses = dict()\n",
    "feedforward_losses_filepath = make_data_filename('feedforward_losses')\n",
    "if os.path.exists(feedforward_losses_filepath):    # check if a file exists https://www.pythontutorial.net/python-basics/python-check-if-file-exists/\n",
    "    print(f'Loading results so far from pickle file {feedforward_losses_filepath}')\n",
    "    with open(feedforward_losses_filepath, 'rb') as pickle_handle: feedforward_losses = pickle.load(pickle_handle)    # use template from https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict-or-any-other-python-object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_store_model_loss(model, experiment_name):\n",
    "    model_name = experiment_name + '_' + nn_name\n",
    "    _, test_loss = train(model, \n",
    "                         NUM_EPOCHS, \n",
    "                         model_filename=make_filename(model_name), \n",
    "                         save=True, \n",
    "                         print_losses_before_training=False,    # setting this to True may cause the kernel to crash\n",
    "                         print_losses_after_training=False,    # setting this to True may cause the kernel to crash\n",
    "                         wandb_logging_name=model_name)\n",
    "    feedforward_losses[experiment_name] = test_loss\n",
    "    with open(feedforward_losses_filepath, 'wb') as pickle_handle: pickle.dump(feedforward_losses, pickle_handle, protocol=4)    # protocol 4 allows for use in the VM; use template from https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict-or-any-other-python-object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "experiment_name = 'reference_data'\n",
    "if experiment_name not in feedforward_losses:\n",
    "    model = NNL1LossEmbeddings(BATCH_SIZE, \n",
    "                               NUM_WORKERS, \n",
    "                               train_data_only_reference_encoded, \n",
    "                               test_data_only_reference_encoded, \n",
    "                               label_encoders, \n",
    "                               CATEGORICAL_FEATURES, \n",
    "                               NUM_NODES_HIDDEN_LAYER, \n",
    "                               NUM_HIDDEN_LAYERS, \n",
    "                               power=EMBEDDINGS_POWER)\n",
    "    train_and_store_model_loss(model, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "experiment_name = 'reference_data_same_cusip_rnn'\n",
    "if experiment_name not in feedforward_losses:\n",
    "    model = NNL1LossEmbeddingsWithRecurrence(BATCH_SIZE, \n",
    "                                             NUM_WORKERS, \n",
    "                                             train_data_with_trade_history_encoded, \n",
    "                                             test_data_with_trade_history_encoded, \n",
    "                                             label_encoders, \n",
    "                                             CATEGORICAL_FEATURES, \n",
    "                                             NUM_NODES_HIDDEN_LAYER, \n",
    "                                             NUM_HIDDEN_LAYERS, \n",
    "                                             NUM_RECURRENT_LAYERS, \n",
    "                                             RECURRENT_HIDDEN_SIZE, \n",
    "                                             recurrent_architecture=RECURRENT_ARCHITECTURE, \n",
    "                                             power=EMBEDDINGS_POWER)\n",
    "    train_and_store_model_loss(model, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a single past related trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_data_reference_both_histories_single_related_trade_encoded_filepath = make_data_filename('train_data_reference_both_histories_single_related_trade_encoded')\n",
    "test_data_reference_both_histories_single_related_trade_encoded_filepath = make_data_filename('test_data_reference_both_histories_single_related_trade_encoded')\n",
    "train_data_reference_same_cusip_history_single_related_trade_encoded_filepath = make_data_filename('train_data_reference_same_cusip_history_single_related_trade_encoded')\n",
    "test_data_reference_same_cusip_history_single_related_trade_encoded_filepath = make_data_filename('test_data_reference_same_cusip_history_single_related_trade_encoded')\n",
    "if os.path.exists(test_data_reference_same_cusip_history_single_related_trade_encoded_filepath):\n",
    "    print(f'Loading dataset from pickle file {test_data_reference_same_cusip_history_single_related_trade_encoded_filepath}')\n",
    "    train_data_reference_both_histories_single_related_trade_encoded = pd.read_pickle(train_data_reference_both_histories_single_related_trade_encoded_filepath)\n",
    "    test_data_reference_both_histories_single_related_trade_encoded = pd.read_pickle(test_data_reference_both_histories_single_related_trade_encoded_filepath)\n",
    "    train_data_reference_same_cusip_history_single_related_trade_encoded = pd.read_pickle(train_data_reference_same_cusip_history_single_related_trade_encoded_filepath)\n",
    "    test_data_reference_same_cusip_history_single_related_trade_encoded = pd.read_pickle(test_data_reference_same_cusip_history_single_related_trade_encoded_filepath)\n",
    "else:\n",
    "    train_data_reference_both_histories_single_related_trade_encoded = add_single_trade_from_history_as_reference_features(train_data_reference_and_both_histories_encoded, \n",
    "                                                                                                                           TRADE_HISTORY_RELATED, \n",
    "                                                                                                                           FEATURES_IN_HISTORY, \n",
    "                                                                                                                           prefix=related_trade_feature_prefix, \n",
    "                                                                                                                           datetime_ascending=True)\n",
    "    train_data_reference_same_cusip_history_single_related_trade_encoded = train_data_reference_both_histories_single_related_trade_encoded.drop(columns=TRADE_HISTORY_RELATED[0])\n",
    "    test_data_reference_both_histories_single_related_trade_encoded = add_single_trade_from_history_as_reference_features(test_data_reference_and_both_histories_encoded, \n",
    "                                                                                                                          TRADE_HISTORY_RELATED, \n",
    "                                                                                                                          FEATURES_IN_HISTORY, \n",
    "                                                                                                                          prefix=related_trade_feature_prefix, \n",
    "                                                                                                                          datetime_ascending=True)\n",
    "    test_data_reference_same_cusip_history_single_related_trade_encoded = test_data_reference_both_histories_single_related_trade_encoded.drop(columns=TRADE_HISTORY_RELATED[0])\n",
    "\n",
    "    train_data_reference_both_histories_single_related_trade_encoded.to_pickle(train_data_reference_both_histories_single_related_trade_encoded_filepath, protocol=4)    # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html\n",
    "    test_data_reference_both_histories_single_related_trade_encoded.to_pickle(test_data_reference_both_histories_single_related_trade_encoded_filepath, protocol=4)    # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html\n",
    "    train_data_reference_same_cusip_history_single_related_trade_encoded.to_pickle(train_data_reference_same_cusip_history_single_related_trade_encoded_filepath, protocol=4)    # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html\n",
    "    test_data_reference_same_cusip_history_single_related_trade_encoded.to_pickle(test_data_reference_same_cusip_history_single_related_trade_encoded_filepath, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoders_for_single_related_trade = dict()\n",
    "categorical_features_for_single_related_trade = []\n",
    "for new_categorical_feature, original_categorical_feature in zip(get_past_trade_columns(1, CATEGORICAL_REFERENCE_FEATURES_TO_ADD, related_trade_feature_prefix)[0], CATEGORICAL_REFERENCE_FEATURES_TO_ADD):    # select index 0 to get just the column names\n",
    "    label_encoders_for_single_related_trade[new_categorical_feature] = label_encoders[original_categorical_feature]\n",
    "    categorical_features_for_single_related_trade.append(new_categorical_feature)\n",
    "label_encoders_and_label_encoders_for_single_related_trade = label_encoders | label_encoders_for_single_related_trade    # combine two dictionaries together for Python v3.9+: https://stackoverflow.com/questions/38987/how-do-i-merge-two-dictionaries-in-a-single-expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "experiment_name = 'reference_data_same_cusip_rnn_single_related_trade'\n",
    "if experiment_name not in feedforward_losses:\n",
    "    model = NNL1LossEmbeddingsWithRecurrence(BATCH_SIZE, \n",
    "                                             NUM_WORKERS, \n",
    "                                             train_data_reference_same_cusip_history_single_related_trade_encoded, \n",
    "                                             test_data_reference_same_cusip_history_single_related_trade_encoded, \n",
    "                                             label_encoders_and_label_encoders_for_single_related_trade, \n",
    "                                             CATEGORICAL_FEATURES + categorical_features_for_single_related_trade, \n",
    "                                             NUM_NODES_HIDDEN_LAYER, \n",
    "                                             NUM_HIDDEN_LAYERS, \n",
    "                                             NUM_RECURRENT_LAYERS, \n",
    "                                             RECURRENT_HIDDEN_SIZE, \n",
    "                                             recurrent_architecture=RECURRENT_ARCHITECTURE, \n",
    "                                             power=EMBEDDINGS_POWER)\n",
    "    train_and_store_model_loss(model, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "experiment_name = 'reference_data_single_related_trade_both_histories_interleaved_rnn'\n",
    "if experiment_name not in feedforward_losses:\n",
    "    model = NNL1LossEmbeddingsWithRecurrence(BATCH_SIZE, \n",
    "                                             NUM_WORKERS, \n",
    "                                             combine_two_histories_sorted_by_seconds_ago_caller(train_data_reference_both_histories_single_related_trade_encoded).drop(columns=TRADE_HISTORY + TRADE_HISTORY_RELATED), \n",
    "                                             combine_two_histories_sorted_by_seconds_ago_caller(test_data_reference_both_histories_single_related_trade_encoded).drop(columns=TRADE_HISTORY + TRADE_HISTORY_RELATED), \n",
    "                                             label_encoders_and_label_encoders_for_single_related_trade, \n",
    "                                             CATEGORICAL_FEATURES + categorical_features_for_single_related_trade, \n",
    "                                             NUM_NODES_HIDDEN_LAYER, \n",
    "                                             NUM_HIDDEN_LAYERS, \n",
    "                                             NUM_RECURRENT_LAYERS, \n",
    "                                             RECURRENT_HIDDEN_SIZE, \n",
    "                                             recurrent_architecture=RECURRENT_ARCHITECTURE, \n",
    "                                             trade_history_column=COMBINED_TRADE_HISTORY, \n",
    "                                             power=EMBEDDINGS_POWER)\n",
    "    train_and_store_model_loss(model, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "experiment_name = 'reference_data_single_related_trade_both_histories_different_rnn'\n",
    "if experiment_name not in feedforward_losses:\n",
    "    model = NNL1LossEmbeddingsWithMultipleRecurrence(BATCH_SIZE, \n",
    "                                                     NUM_WORKERS, \n",
    "                                                     train_data_reference_both_histories_single_related_trade_encoded, \n",
    "                                                     test_data_reference_both_histories_single_related_trade_encoded, \n",
    "                                                     label_encoders_and_label_encoders_for_single_related_trade, \n",
    "                                                     CATEGORICAL_FEATURES + categorical_features_for_single_related_trade, \n",
    "                                                     NUM_NODES_HIDDEN_LAYER, \n",
    "                                                     NUM_HIDDEN_LAYERS, \n",
    "                                                     NUM_RECURRENT_LAYERS, \n",
    "                                                     RECURRENT_HIDDEN_SIZE, \n",
    "                                                     recurrent_architecture=RECURRENT_ARCHITECTURE, \n",
    "                                                     trade_history_columns=TRADE_HISTORY + TRADE_HISTORY_RELATED, \n",
    "                                                     power=EMBEDDINGS_POWER)\n",
    "    train_and_store_model_loss(model, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, test_loss in feedforward_losses.items():\n",
    "    print(f'{name}\\t\\tTest error: {test_loss}')\n",
    "feedforward_losses_ascending_order = sorted(feedforward_losses, key=lambda name: feedforward_losses.get(name))    # sort by minimum test error\n",
    "opt = feedforward_losses_ascending_order[0]    # optimal name is the one with the minimum test error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions** \n",
    "\n",
    "The following parameter choices were used for the LSTM: NUM_RECURRENT_LAYERS=3, RECURRENT_HIDDEN_SIZE=64\n",
    "\n",
    "|  | Model (a) | Model (b) | Model (c) | Model (d) | Model (e) |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| Train loss | 7.469 | 6.055 | 5.550 | 6.100 | 5.217 |\n",
    "| Test loss | 8.791 | 7.576 | 7.650 | 7.838 | 7.598 |\n",
    "\n",
    "- Model (b) is giving the best test accuracy followed closely by model (e); this is particularly disappointing since the greater capacity of models (c), (d), and (e) are not leading to greater test loss\n",
    "- Model (e) has the lowest train loss meaning that it has the greatest capacity (as expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:38:29) [Clang 13.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e18c70c74e919487903475d4b9ee892d16d9f5351cb9291a624c367bcb362da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
