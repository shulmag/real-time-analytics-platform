{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from ficc.utils.auxiliary_variables import VERY_LARGE_NUMBER, \\\n",
    "                                           IDENTIFIERS, \\\n",
    "                                           CATEGORICAL_FEATURES, \\\n",
    "                                           NON_CAT_FEATURES, \\\n",
    "                                           BINARY, \\\n",
    "                                           TRADE_HISTORY, \\\n",
    "                                           NUM_OF_DAYS_IN_YEAR\n",
    "from ficc.utils.auxiliary_functions import flatten\n",
    "from ficc.utils.diff_in_days import diff_in_days_two_dates\n",
    "from ficc.utils.trade_dict_to_list import FEATURES_IN_HISTORY, \\\n",
    "                                          FEATURES_TO_INDEX_IN_HISTORY, \\\n",
    "                                          CATEGORICAL_FEATURES_IN_HISTORY, \\\n",
    "                                          quantity_diff\n",
    "from ficc.utils.trade_dict_to_list_mappings import TRADE_TYPE_MAPPING, \\\n",
    "                                                   TRADE_TYPE_CROSS_PRODUCT_MAPPING, \\\n",
    "                                                   RATING_TO_INT_MAPPING\n",
    "from ficc.utils.related_trade import append_recent_trade_data, \\\n",
    "                                     get_appended_feature_name\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "\n",
    "from trade_history_model_mitas.data_prep import get_past_trade_columns, \\\n",
    "                                                convert_trade_type_encoding_to_actual, \\\n",
    "                                                is_sorted\n",
    "\n",
    "from yield_spread_model_mitas.data_prep import FEATURES_AND_NAN_REPLACEMENT_VALUES, \\\n",
    "                                               ADDITIONAL_CATEGORICAL_FEATURES, \\\n",
    "                                               get_datestring_from_filename, \\\n",
    "                                               remove_rows_with_feature_value, \\\n",
    "                                               replace_rating_with_standalone_rating, \\\n",
    "                                               add_past_trades_info, \\\n",
    "                                               check_additional_features, \\\n",
    "                                               replace_nan_for_features, \\\n",
    "                                               encode_and_get_encoders, \\\n",
    "                                               encode_with_encoders\n",
    "from yield_spread_model_mitas.models import single_feature_model\n",
    "from yield_spread_model_mitas.train import get_train_test_data_trade_datetime\n",
    "from yield_spread_model_mitas.tree_models import train_lightgbm_model, \\\n",
    "                                                 get_predictions_for_single_dataset, \\\n",
    "                                                 convert_columns_with_dtype_object_to_category\n",
    "\n",
    "from rating_model_mitas.data_prep import read_processed_file_pickle, \\\n",
    "                                         remove_fields_with_single_unique_value, \\\n",
    "                                         remove_rows_with_nan_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use zero padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_VALUES_ZERO_PADDING = defaultdict(int)\n",
    "\n",
    "DEFAULT_VALUES = DEFAULT_VALUES_ZERO_PADDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRADES_IN_TRADE_HISTORY = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = ['yield_spread']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PROCESSING_FEATURES = ['trade_datetime',    # used to split the data into training and test sets\n",
    "                            'settlement_date',    # used (in conjunction with calc_date) to create the settlement_date_to_calc_date feature in past trades\n",
    "                            'calc_date',    # used (in conjunction with settlement_date) to create the settlement_date_to_calc_date feature in past trades\n",
    "                            'calc_day_cat',    # added in the past trades\n",
    "                            # 'coupon_type'    # used to group related trades; currently commented out since there is only a single value of 8 present in the data\n",
    "                           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START: Reading from processed file at ../../../../ficc/ml_models/sequence_predictors/data/processed_data_ficc_ycl_long_history_2022-10-08-00-00.pkl\n",
      "END: Reading from processed file at ../../../../ficc/ml_models/sequence_predictors/data/processed_data_ficc_ycl_long_history_2022-10-08-00-00.pkl\n",
      "CPU times: user 7.65 s, sys: 8.48 s, total: 16.1 s\n",
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "processed_file_pickle = '../../../../ficc/ml_models/sequence_predictors/data/processed_data_ficc_ycl_long_history_2022-10-08-00-00.pkl'\n",
    "processed_file_pickle_datestring = get_datestring_from_filename(processed_file_pickle)\n",
    "trade_data = read_processed_file_pickle(processed_file_pickle)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep all trades before October 8, to standardize with Charles and Developer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_data = trade_data[trade_data.trade_datetime < datetime(2022, 10, 8)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply exclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total number of trades: {len(trade_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trade_data = trade_data[(trade_data.days_to_call == 0) | (trade_data.days_to_call > np.log10(400))]\n",
    "trade_data = trade_data[(trade_data.days_to_refund == 0) | (trade_data.days_to_refund > np.log10(400))]\n",
    "trade_data = trade_data[trade_data.days_to_maturity < np.log10(30000)]\n",
    "trade_data = trade_data[trade_data.sinking == False]\n",
    "trade_data = trade_data[trade_data.incorporated_state_code != 'VI']\n",
    "trade_data = trade_data[trade_data.incorporated_state_code != 'GU']\n",
    "# trade_data = trade_data[(trade_data.coupon_type == 8)]\n",
    "# trade_data = trade_data[trade_data.is_called == False]\n",
    "\n",
    "# restructured bonds and high chance of default bonds are removed\n",
    "trade_data = remove_rows_with_feature_value(trade_data, 'purpose_sub_class', [6, 20, 22, 44, 57, 90])\n",
    "# pre-refunded bonds and partially refunded bonds are removed\n",
    "trade_data = remove_rows_with_feature_value(trade_data, 'called_redemption_type', [18, 19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_data = replace_rating_with_standalone_rating(trade_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `treasury_spread` to the `NON_CAT_FEATURES`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_CAT_FEATURES.append('ficc_treasury_spread')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADDITIONAL_CATEGORICAL_FEATURES = check_additional_features(trade_data, ADDITIONAL_CATEGORICAL_FEATURES)\n",
    "\n",
    "trade_data, _ = replace_nan_for_features(trade_data, FEATURES_AND_NAN_REPLACEMENT_VALUES, verbose=True)\n",
    "trade_data = remove_fields_with_single_unique_value(trade_data, BINARY + CATEGORICAL_FEATURES + ADDITIONAL_CATEGORICAL_FEATURES + NON_CAT_FEATURES)\n",
    "\n",
    "all_features_set = set(trade_data.columns)\n",
    "BINARY = list(set(BINARY) & all_features_set)\n",
    "CATEGORICAL_FEATURES = list((set(CATEGORICAL_FEATURES) | set(ADDITIONAL_CATEGORICAL_FEATURES)) & all_features_set)\n",
    "NON_CAT_FEATURES = list(set(NON_CAT_FEATURES) & all_features_set)\n",
    "PREDICTORS = BINARY + CATEGORICAL_FEATURES + NON_CAT_FEATURES\n",
    "\n",
    "trade_data = trade_data[IDENTIFIERS + \n",
    "                        PREDICTORS + \n",
    "                        DATA_PROCESSING_FEATURES + \n",
    "                        TRADE_HISTORY + \n",
    "                        TARGET]\n",
    "\n",
    "trade_data = remove_rows_with_nan_value(trade_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Identifiers: {sorted(IDENTIFIERS)}')\n",
    "print(f'Predictors: {sorted(PREDICTORS)}')\n",
    "print(f'Binary features: {sorted(BINARY)}')\n",
    "print(f'Categorical features: {sorted(CATEGORICAL_FEATURES)}')\n",
    "print(f'Numerical features: {sorted(NON_CAT_FEATURES)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert `CATEGORICAL_FEATURES` with dtype object to dtype category to be used in the LightGBM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_data = convert_columns_with_dtype_object_to_category(trade_data, CATEGORICAL_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTORS_WITHOUT_LAST_TRADE_FEATURES = [predictor for predictor in PREDICTORS if not predictor.startswith('last')]\n",
    "print(f'The following features are in PREDICTORS but not in PREDICTORS_WITHOUT_LAST_TRADE_FEATURES: {set(PREDICTORS) - set(PREDICTORS_WITHOUT_LAST_TRADE_FEATURES)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the dataframe is sorted in descending order by `trade_datetime`\n",
    "assert is_sorted(trade_data['trade_datetime'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldest_trade_datetime = trade_data['trade_datetime'].iloc[-1]\n",
    "newest_trade_datetime = trade_data['trade_datetime'].iloc[0]\n",
    "\n",
    "print(f'Oldest trade datetime: {oldest_trade_datetime}.\\\n",
    "    Newest trade datetime: {newest_trade_datetime}.\\\n",
    "    Gap: {newest_trade_datetime - oldest_trade_datetime}')\n",
    "print(f'Total number of trades: {len(trade_data)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset with only the reference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_TO_SPLIT = datetime(2022, 9, 15)    # September 15 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = get_train_test_data_trade_datetime(trade_data, DATE_TO_SPLIT)\n",
    "print(f'Number of trades for training: {len(train_data)}.\\\n",
    "    Number of trades for testing: {len(test_data)}')\n",
    "assert len(train_data) != 0 and len(test_data) != 0, 'Either train or test data is empty. Consider checking how the train test split is being performed.'\n",
    "train_data_with_trade_history = train_data.drop(columns=DATA_PROCESSING_FEATURES + IDENTIFIERS)\n",
    "test_data_with_trade_history = test_data.drop(columns=DATA_PROCESSING_FEATURES + IDENTIFIERS)\n",
    "train_data_only_reference = train_data_with_trade_history.drop(columns=TRADE_HISTORY)\n",
    "test_data_only_reference = test_data_with_trade_history.drop(columns=TRADE_HISTORY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten the trade history. The flattened data is used in the LightGBM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trade_data_flattened_trade_history, \\\n",
    "    additional_binary_features_from_past_trades, \\\n",
    "    additional_noncat_features_from_past_trades, \\\n",
    "    past_trade_feature_groups = add_past_trades_info(trade_data, NUM_TRADES_IN_TRADE_HISTORY - 1, FEATURES_TO_INDEX_IN_HISTORY)\n",
    "past_trade_feature_groups_flattened = flatten(past_trade_feature_groups)\n",
    "print(f'Each of the past trades are in the following feature groups: {past_trade_feature_groups}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode the trade type from a 2-dimensional binary list to its original value for trades in the history. For example, a trade type of `[0, 1]` would be decoded to `S`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRADE_TYPE_NEW_COLUMN = 'trade_type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAME_CUSIP_PREFIX = 'last_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_data_history_and_reference_features = trade_data_flattened_trade_history[past_trade_feature_groups_flattened + PREDICTORS_WITHOUT_LAST_TRADE_FEATURES + DATA_PROCESSING_FEATURES + TARGET]\n",
    "\n",
    "trade_data_history_and_reference_features_actual_trade_type, old_trade_type_columns, _ = convert_trade_type_encoding_to_actual(trade_data_history_and_reference_features, \n",
    "                                                                                                                               NUM_TRADES_IN_TRADE_HISTORY, \n",
    "                                                                                                                               TRADE_TYPE_NEW_COLUMN, \n",
    "                                                                                                                               SAME_CUSIP_PREFIX)\n",
    "trade_data_history_and_reference_features_actual_trade_type = trade_data_history_and_reference_features_actual_trade_type.drop(columns=old_trade_type_columns)\n",
    "# check that the above procedure removed just one feature for each past trade (`trade_type1` and `trade_type2` combine to just `trade_type`)\n",
    "assert len(trade_data_history_and_reference_features_actual_trade_type.columns) == len(trade_data_history_and_reference_features.columns) - NUM_TRADES_IN_TRADE_HISTORY, f'Before converting, the dataframe had {len(trade_data_history_and_reference_features.columns)} columns, and after converting, the dataframe has {len(trade_data_history_and_reference_features_actual_trade_type.columns)} columns'\n",
    "\n",
    "del trade_data_history_and_reference_features\n",
    "gc.collect()\n",
    "\n",
    "train_data_history_and_reference_features_actual_trade_type, \\\n",
    "    test_data_history_and_reference_features_actual_trade_type = get_train_test_data_trade_datetime(trade_data_history_and_reference_features_actual_trade_type, DATE_TO_SPLIT)\n",
    "del trade_data_history_and_reference_features_actual_trade_type\n",
    "gc.collect()\n",
    "assert len(train_data_history_and_reference_features_actual_trade_type) != 0 and len(test_data_history_and_reference_features_actual_trade_type) != 0, 'Either train or test data is empty. Consider checking how the train test split is being performed.'\n",
    "\n",
    "train_data_history_and_reference_features_actual_trade_type = train_data_history_and_reference_features_actual_trade_type.drop(columns=DATA_PROCESSING_FEATURES)\n",
    "test_data_history_and_reference_features_actual_trade_type = test_data_history_and_reference_features_actual_trade_type.drop(columns=DATA_PROCESSING_FEATURES)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last Yield Spread\n",
    "Weakest baseline where output it just the yield spread of the most previous trade in the same CUSIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_feature_model(trade_data, 'last_yield_spread')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_trade_columns, all_categorical_features_in_trade_history = get_past_trade_columns(NUM_TRADES_IN_TRADE_HISTORY, \n",
    "                                                                                       FEATURES_IN_HISTORY, \n",
    "                                                                                       SAME_CUSIP_PREFIX, \n",
    "                                                                                       trade_type_actual=True, \n",
    "                                                                                       trade_type_column=TRADE_TYPE_NEW_COLUMN, \n",
    "                                                                                       categorical_features_per_trade=CATEGORICAL_FEATURES_IN_HISTORY)\n",
    "columns_to_select_for_lightgbm_model = PREDICTORS_WITHOUT_LAST_TRADE_FEATURES + past_trade_columns + TARGET\n",
    "categorical_features_for_lightgbm_model = CATEGORICAL_FEATURES + all_categorical_features_in_trade_history\n",
    "\n",
    "assert len(columns_to_select_for_lightgbm_model) == len(set(columns_to_select_for_lightgbm_model))    # checks that there are no intersection between the groups of features\n",
    "assert len(categorical_features_for_lightgbm_model) == len(set(categorical_features_for_lightgbm_model))    # checks that there are no intersection between the groups of features\n",
    "\n",
    "print(f'Features used for LightGBM model: {columns_to_select_for_lightgbm_model}')\n",
    "print(f'Categorical features used for LightGBM model: {categorical_features_for_lightgbm_model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model, lgb_losses = train_lightgbm_model(train_data_history_and_reference_features_actual_trade_type[columns_to_select_for_lightgbm_model], \n",
    "                                             test_data_history_and_reference_features_actual_trade_type[columns_to_select_for_lightgbm_model], \n",
    "                                             CATEGORICAL_FEATURES + all_categorical_features_in_trade_history, \n",
    "                                             wandb_project='mitas_trade_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(lgb_model, figsize=(20, 10), importance_type='gain')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add error of the LightGBM model into the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_errors(lgb_model, dataset: pd.core.frame.DataFrame):\n",
    "    predictions = get_predictions_for_single_dataset(lgb_model, dataset[columns_to_select_for_lightgbm_model])\n",
    "    return predictions - dataset[TARGET[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train_errors = get_errors(lgb_model, train_data_history_and_reference_features_actual_trade_type)\n",
    "train_data_history_and_reference_features_actual_trade_type['lgb_error'] = lgb_train_errors\n",
    "lgb_test_errors = get_errors(lgb_model, test_data_history_and_reference_features_actual_trade_type)\n",
    "test_data_history_and_reference_features_actual_trade_type['lgb_error'] = lgb_test_errors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `lgb_error` to `trade_data` so that it persists in `CATEGORICAL_REFERENCE_FEATURES_TO_ADD`, and `trade_data_flattened_trade_history` to use for future experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_errors = np.append(lgb_test_errors, lgb_train_errors)\n",
    "trade_data['lgb_error'] = all_errors    # test_error comes before train_error since `trade_data` is sorted in DESCENDING order of `trade_datetime`\n",
    "trade_data_flattened_trade_history['lgb_error'] = all_errors    # test_error comes before train_error since `trade_data` is sorted in DESCENDING order of `trade_datetime`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check: make sure the distribution of `lgb_error` is centered tightly at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(all_errors, bins='auto')\n",
    "print(f'Largest positive error: {np.max(all_errors)}')\n",
    "print(f'Largest negative error: {np.min(all_errors)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add histories of related trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_data_filename = lambda name: f'data/{name}.pkl'    # used to create a filename to save the PyTorch model parameters\n",
    "if not os.path.isdir('data/'):\n",
    "    os.mkdir('data/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform encoding of categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_data_only_reference_encoded, label_encoders = encode_and_get_encoders(train_data_only_reference, BINARY, CATEGORICAL_FEATURES)\n",
    "\n",
    "label_encoders_filepath = make_data_filename('label_encoders')\n",
    "with open(label_encoders_filepath, 'wb') as pickle_handle: pickle.dump(label_encoders, pickle_handle, protocol=4)    # protocol 4 allows for use in the VM; use template from https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict-or-any-other-python-object\n",
    "encode_with_label_encoders = lambda df, features_to_exclude=[]: encode_with_encoders(df, label_encoders, features_to_exclude)\n",
    "\n",
    "test_data_only_reference_encoded = encode_with_label_encoders(test_data_only_reference)\n",
    "train_data_with_trade_history_encoded = encode_with_label_encoders(train_data_with_trade_history)\n",
    "test_data_with_trade_history_encoded = encode_with_label_encoders(test_data_with_trade_history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each trade, we find the `NUM_TRADES_IN_RELATED_TRADE_HISTORY` most recent related trades (up until the `trade_datetime` of the current trade) that are from different CUSIPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRADES_IN_RELATED_TRADE_HISTORY = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use certain reference data columns to data in order to augment the same CUSIP history. These will be added when selecting the features to use in the LightGBM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_REFERENCE_FEATURES_TO_ADD = ['rating', 'incorporated_state_code', 'purpose_sub_class']    # choosing a few features from the most important features for the LightGBM model on just reference data along with the error of the LightGBM model\n",
    "CATEGORICAL_REFERENCE_FEATURES_TO_ADD = list(set(CATEGORICAL_REFERENCE_FEATURES_TO_ADD) & set(trade_data.columns))    # make sure that all CATEGORICAL_REFERENCE_FEATURES_TO_ADD are in the trade data as columns\n",
    "print(f'Including the following reference features for each related trade and the target trade: {CATEGORICAL_REFERENCE_FEATURES_TO_ADD}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add this same reference data to related trade history. However, to make this code faster, we first encode the reference data, and then decode after creating the past trade history. This creates a speedup because `append_recent_trade_data(...)` is significantly faster when working with numerical data as opposed to objects, due to how numpy handles `dtype='O'` versus `dtype='np.float_'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODE_REFERENCE_FEATURES = False    # boolean variable that determines whether trade history will contain categorical features that must be encoded before adding these features to the trade history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in CATEGORICAL_REFERENCE_FEATURES_TO_ADD:\n",
    "    if feature not in FEATURES_TO_INDEX_IN_HISTORY: FEATURES_TO_INDEX_IN_HISTORY[feature] = len(FEATURES_TO_INDEX_IN_HISTORY)\n",
    "    ENCODE_REFERENCE_FEATURES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_trade_feature_prefix = 'related_last_'\n",
    "get_neighbor_feature = lambda feature: lambda curr, neighbor: neighbor[feature]\n",
    "RELATED_TRADE_FEATURE_FUNCTIONS = {'yield_spread': get_neighbor_feature('yield_spread'), \n",
    "                                   'treasury_spread': get_neighbor_feature('ficc_treasury_spread'), \n",
    "                                   'quantity': get_neighbor_feature('quantity'), \n",
    "                                   'quantity_diff': lambda curr, neighbor: quantity_diff(10 ** neighbor['quantity'] - 10 ** curr['quantity']), \n",
    "                                   'trade_type1': lambda curr, neighbor: TRADE_TYPE_MAPPING[neighbor['trade_type']][0], \n",
    "                                   'trade_type2': lambda curr, neighbor: TRADE_TYPE_MAPPING[neighbor['trade_type']][1], \n",
    "                                   'seconds_ago': lambda curr, neighbor: np.log10(1 + (curr['trade_datetime'] - neighbor['trade_datetime']).total_seconds()), \n",
    "                                   'settlement_date_to_calc_date': lambda curr, neighbor: np.log10(1 + diff_in_days_two_dates(neighbor['calc_date'], neighbor['settlement_date'], convention='exact')), \n",
    "                                   'calc_day_cat': get_neighbor_feature('calc_day_cat'), \n",
    "                                   'trade_type_past_latest': lambda curr, neighbor: TRADE_TYPE_CROSS_PRODUCT_MAPPING[neighbor['trade_type'] + curr['trade_type']], \n",
    "                                  #  'rating_diff': lambda curr, neighbor: RATING_TO_INT_MAPPING[curr['rating']] - RATING_TO_INT_MAPPING[neighbor['rating']]\n",
    "                                   }\n",
    "\n",
    "additional_related_trade_functions = {'same_day': lambda curr, neighbor: int(neighbor['trade_datetime'].date() == curr['trade_datetime'].date()),    # used to track additional information about the related trades; compare date only instead of entire datetime: https://stackoverflow.com/questions/3743222/how-do-i-convert-a-datetime-to-date\n",
    "                                      'lgb_error': get_neighbor_feature('lgb_error')}\n",
    "\n",
    "reference_features_to_add_functions = {feature: get_neighbor_feature(feature) for feature in CATEGORICAL_REFERENCE_FEATURES_TO_ADD}\n",
    "RELATED_TRADE_FEATURE_FUNCTIONS = RELATED_TRADE_FEATURE_FUNCTIONS | additional_related_trade_functions | reference_features_to_add_functions    # combine two dictionaries together for Python v3.9+: https://stackoverflow.com/questions/38987/how-do-i-merge-two-dictionaries-in-a-single-expression\n",
    "\n",
    "print(f'Related trades will have the following features: {RELATED_TRADE_FEATURE_FUNCTIONS.keys()}')\n",
    "\n",
    "related_trades_features_groups = [[get_appended_feature_name(idx, feature, related_trade_feature_prefix) for feature in RELATED_TRADE_FEATURE_FUNCTIONS] \n",
    "                                   for idx in range(NUM_TRADES_IN_RELATED_TRADE_HISTORY)]    # insertion order of the dictionary is preserved for Python v3.7+\n",
    "related_trades_features = flatten(related_trades_features_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary format. key: name of the feature; value: two-item tuple where the first item is a function of the current trade and related trade, and the second item is the default value to be filled in if that value does not exist\n",
    "RELATED_TRADE_FEATURE_FUNCTIONS_AND_DEFAULT_VALUES = {key: (function, DEFAULT_VALUES[key]) for key, function in RELATED_TRADE_FEATURE_FUNCTIONS.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See which features are the in the same CUSIP trade history and the related trade history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert FEATURES_IN_HISTORY == [key for key in RELATED_TRADE_FEATURE_FUNCTIONS if key not in additional_related_trade_functions and key not in reference_features_to_add_functions]    # insertion order of the dictionary is preserved for Python v3.7+ so this will check if the ordering of the keys are the same\n",
    "print(f'Each trade in the same CUSIP trade history has the following features: {FEATURES_IN_HISTORY}')\n",
    "print(f'Each trade in the related trade history has the following features: {RELATED_TRADE_FEATURE_FUNCTIONS.keys()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create \"quantized features\" which groups together certain values of the features when used to make related trades. For example, `RATING_WITHOUT_PLUS_MINUS` removes the + and - from ratings, and so a bond with rating A+ will be related to a bond with rating A or A-.\n",
    "\n",
    "Purpose class was added as a quantized feature based on a call with Desmond Dahill from Tegus on 09/27/2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epsilon = 1 / VERY_LARGE_NUMBER\n",
    "\n",
    "RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED = 'rating_without_+-_b_nr_combined'\n",
    "trade_data_flattened_trade_history[RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED] = trade_data_flattened_trade_history['rating'].transform(lambda rating: str.rstrip(rating, '+-'))    # remove + and - from right side of string\n",
    "# group BBB, BB, B, and NR together since each have a very small number of trades\n",
    "b_ratings = trade_data_flattened_trade_history[RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED].isin(['B', 'BB', 'BBB', 'NR'])\n",
    "trade_data_flattened_trade_history.loc[b_ratings, RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED] = 'B'\n",
    "print(f'Created {RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED} feature')\n",
    "\n",
    "DAYS_TO_MATURITY_CATEGORICAL = 'days_to_maturity_categorical'\n",
    "num_of_days_bins_maturity = [np.log10(days) for days in [epsilon, NUM_OF_DAYS_IN_YEAR * 2, NUM_OF_DAYS_IN_YEAR * 5, NUM_OF_DAYS_IN_YEAR * 10, VERY_LARGE_NUMBER]]    # 2 years, 5 years, 10 years; arbitrarily chosen\n",
    "trade_data_flattened_trade_history[DAYS_TO_MATURITY_CATEGORICAL] = pd.cut(trade_data_flattened_trade_history['days_to_maturity'], num_of_days_bins_maturity).astype('string')\n",
    "print(f'Created {DAYS_TO_MATURITY_CATEGORICAL} feature')\n",
    "\n",
    "DAYS_TO_CALL_CATEGORICAL = 'days_to_call_categorical'\n",
    "num_of_days_bins_call = [np.log10(days) for days in [epsilon, NUM_OF_DAYS_IN_YEAR * 2, NUM_OF_DAYS_IN_YEAR * 5, VERY_LARGE_NUMBER]]    # 2 years, 5 years; arbitrarily chosen\n",
    "trade_data_flattened_trade_history[DAYS_TO_CALL_CATEGORICAL] = pd.cut(trade_data_flattened_trade_history['days_to_call'], num_of_days_bins_call).astype('string')\n",
    "print(f'Created {DAYS_TO_CALL_CATEGORICAL} feature')\n",
    "\n",
    "COUPON_CATEGORICAL = 'coupon_categorical'\n",
    "coupon_bins = [0, 3, 4, 4.5, 5.0 + epsilon, VERY_LARGE_NUMBER]   # 0 - 2.99, 3 - 3.99, 4 - 4.49, 4.5 - 5; from discussion with a team member\n",
    "trade_data_flattened_trade_history[COUPON_CATEGORICAL] = pd.cut(trade_data_flattened_trade_history['coupon'], coupon_bins, right=False).astype('string')\n",
    "print(f'Created {COUPON_CATEGORICAL} feature')\n",
    "\n",
    "COUPON_CATEGORICAL_SUDHAR = 'coupon_categorical_sudhar'\n",
    "coupon_bins = [0, 3, 4, 4.5, 5, 5.25, 5.5, 6, VERY_LARGE_NUMBER]    # from Sudhar's paper: Kolm, Purushothaman. 2021. Systematic Pricing and Trading of Municipal Bonds\n",
    "trade_data_flattened_trade_history[COUPON_CATEGORICAL_SUDHAR] = pd.cut(trade_data_flattened_trade_history['coupon'], coupon_bins, right=False).astype('string')\n",
    "print(f'Created {COUPON_CATEGORICAL_SUDHAR} feature')\n",
    "\n",
    "# COUPON_TOP_VALUES = 'coupon_top_values'\n",
    "# trade_data_flattened_trade_history[COUPON_TOP_VALUES] = trade_data_flattened_trade_history['coupon']\n",
    "# top4_coupon_values = trade_data_flattened_trade_history['coupon'].value_counts().head(4).index.tolist()    # select the top 4 coupon values based on frequency in the data, which are: 5.0, 4.0, 3.0, 2.0 comprising about 90% of the data\n",
    "# trade_data_flattened_trade_history.loc[~trade_data_flattened_trade_history['coupon'].isin(top4_coupon_values), COUPON_TOP_VALUES] = -1    # arbitrary numerical value that is invalid as a coupon value\n",
    "# print(f'Created {COUPON_TOP_VALUES} feature')\n",
    "\n",
    "PURPOSE_CLASS_TOP_VALUES = 'purpose_class_top_values'\n",
    "trade_data_flattened_trade_history[PURPOSE_CLASS_TOP_VALUES] = trade_data_flattened_trade_history['purpose_class']\n",
    "top6_purpose_class_values = trade_data_flattened_trade_history['purpose_class'].value_counts().head(6).index.tolist()    # select the top 6 coupon values based on frequency in the data, which are: 37 (school district), 51 (various purpose), 50 (utility), 46 (tax revenue), 9 (education), 48 (transportation) comprising about 80% of the data\n",
    "trade_data_flattened_trade_history.loc[~trade_data_flattened_trade_history['purpose_class'].isin(top6_purpose_class_values), PURPOSE_CLASS_TOP_VALUES] = -1    # arbitrary numerical value that is invalid as a purpose_class value\n",
    "print(f'Created {PURPOSE_CLASS_TOP_VALUES} feature')\n",
    "\n",
    "MUNI_SECURITY_TYPE_TOP_VALUES = 'muni_security_type_top_values'\n",
    "trade_data_flattened_trade_history[MUNI_SECURITY_TYPE_TOP_VALUES] = trade_data_flattened_trade_history['muni_security_type']\n",
    "top6_muni_security_type_values = trade_data_flattened_trade_history['muni_security_type'].value_counts().head(2).index.tolist()    # select the top 2 coupon values based on frequency in the data, which are: 8 (revenue), 5 (unlimited g.o.) comprising about 80% of the data\n",
    "trade_data_flattened_trade_history.loc[~trade_data_flattened_trade_history['muni_security_type'].isin(top6_muni_security_type_values), MUNI_SECURITY_TYPE_TOP_VALUES] = -1    # arbitrary numerical value that is invalid as a purpose_class value\n",
    "print(f'Created {MUNI_SECURITY_TYPE_TOP_VALUES} feature')\n",
    "\n",
    "TRADE_DATETIME_DAY = 'trade_datetime_day'\n",
    "trade_data_flattened_trade_history[TRADE_DATETIME_DAY] = trade_data_flattened_trade_history['trade_datetime'].transform(lambda datetime: datetime.date()).astype('string')    # remove timestamp from datetime\n",
    "print(f'Created {TRADE_DATETIME_DAY} feature')\n",
    "\n",
    "QUANTITY_CATEGORICAL = 'quantity_categorical'\n",
    "quantity_bins = [0, 5, 6, 7, VERY_LARGE_NUMBER]    # 0 - 100k, 100k - 1m, 1m - 10m, 10m+\n",
    "trade_data_flattened_trade_history[QUANTITY_CATEGORICAL] = pd.cut(trade_data_flattened_trade_history['quantity'], quantity_bins).astype('string')\n",
    "print(f'Created {QUANTITY_CATEGORICAL} feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_features = [RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, \n",
    "                      DAYS_TO_MATURITY_CATEGORICAL, \n",
    "                      DAYS_TO_CALL_CATEGORICAL, \n",
    "                      COUPON_CATEGORICAL, \n",
    "                      COUPON_CATEGORICAL_SUDHAR, \n",
    "                      PURPOSE_CLASS_TOP_VALUES, \n",
    "                      MUNI_SECURITY_TYPE_TOP_VALUES, \n",
    "                    #   COUPON_TOP_VALUES, \n",
    "                      TRADE_DATETIME_DAY, \n",
    "                      QUANTITY_CATEGORICAL]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that each category (for each quantized feature) has a reasonable number of trades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in quantized_features:\n",
    "    trade_data_flattened_trade_history[feature].value_counts().plot(kind='bar', title=feature, figsize=(20, 10))\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantity_greater_than_100k = lambda row: row['quantity'] >= np.log10(1e5)\n",
    "quantity_greater_than_1m = lambda row: row['quantity'] >= np.log10(1e6)\n",
    "trade_type_is_interdealer = lambda row: row['trade_type'] == 'D'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This link has the below definitions and results: https://docs.google.com/document/d/1rQeB3lM_iEyv9q-rseQPmb8n8nv1ay5UtVdSNH0K0QU/edit?usp=sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key: name of criteria, value: (categories to match, filtering conditions)\n",
    "related_trades_criterion = {# 'sudhar1': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL], []), \n",
    "                            # 'sudhar1_100k': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL], [quantity_greater_than_100k]), \n",
    "                            # 'sudhar1_1m': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL], [quantity_greater_than_1m]), \n",
    "                            # 'sudhar2': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL, 'trade_type'], []), \n",
    "                            # 'sudhar2_100k': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL, 'trade_type'], [quantity_greater_than_100k]), \n",
    "                            # 'sudhar2_1m': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL, 'trade_type'], [quantity_greater_than_1m]), \n",
    "                            # 'sudhar3': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL], [trade_type_is_interdealer]), \n",
    "                            # 'sudhar3_100k': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL], [quantity_greater_than_100k, trade_type_is_interdealer]), \n",
    "                            # 'sudhar3_1m': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL], [quantity_greater_than_1m, trade_type_is_interdealer]), \n",
    "                            # 'sudhar4': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL, MUNI_SECURITY_TYPE_TOP_VALUES], []), \n",
    "                            # 'sudhar4_100k': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL, MUNI_SECURITY_TYPE_TOP_VALUES], [quantity_greater_than_100k]), \n",
    "                            # 'sudhar4_1m': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL, MUNI_SECURITY_TYPE_TOP_VALUES], [quantity_greater_than_1m]), \n",
    "                            # 'sudhar5': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL, TRADE_DATETIME_DAY], []), \n",
    "                            # 'sudhar5_100k': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL, TRADE_DATETIME_DAY], [quantity_greater_than_100k]), \n",
    "                            # 'sudhar5_1m': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, COUPON_CATEGORICAL, TRADE_DATETIME_DAY], [quantity_greater_than_1m]), \n",
    "                            # 'sudhar6': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, COUPON_CATEGORICAL, TRADE_DATETIME_DAY], []), \n",
    "                            # 'sudhar6_100k': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, COUPON_CATEGORICAL, TRADE_DATETIME_DAY], [quantity_greater_than_100k]), \n",
    "                            # 'sudhar6_1m': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, COUPON_CATEGORICAL, TRADE_DATETIME_DAY], [quantity_greater_than_1m]), \n",
    "                            # 'mitas1': ([TRADE_DATETIME_DAY, 'trade_type'], []),\n",
    "                            # 'mitas1_100k': ([TRADE_DATETIME_DAY, 'trade_type'], [quantity_greater_than_100k]), \n",
    "                            # 'mitas1_1m': ([TRADE_DATETIME_DAY, 'trade_type'], [quantity_greater_than_1m]), \n",
    "                            # 'desmond': (['incorporated_state_code', RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, DAYS_TO_CALL_CATEGORICAL, COUPON_CATEGORICAL, PURPOSE_CLASS_TOP_VALUES], []), \n",
    "                            # 'desmond_100k': (['incorporated_state_code', RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, DAYS_TO_CALL_CATEGORICAL, COUPON_CATEGORICAL, PURPOSE_CLASS_TOP_VALUES], [quantity_greater_than_100k]), \n",
    "                            # 'desmond_1m': (['incorporated_state_code', RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED, DAYS_TO_MATURITY_CATEGORICAL, DAYS_TO_CALL_CATEGORICAL, COUPON_CATEGORICAL, PURPOSE_CLASS_TOP_VALUES], [quantity_greater_than_1m]), \n",
    "                            # 'yellow': (['trade_type', DAYS_TO_MATURITY_CATEGORICAL, QUANTITY_CATEGORICAL, DAYS_TO_CALL_CATEGORICAL], []), \n",
    "                            # 'yellow_100k': (['trade_type', DAYS_TO_MATURITY_CATEGORICAL, QUANTITY_CATEGORICAL, DAYS_TO_CALL_CATEGORICAL], [quantity_greater_than_100k]), \n",
    "                            # 'yellow_1m': (['trade_type', DAYS_TO_MATURITY_CATEGORICAL, QUANTITY_CATEGORICAL, DAYS_TO_CALL_CATEGORICAL], [quantity_greater_than_1m]), \n",
    "                            # 'yellow_lite': (['trade_type', DAYS_TO_MATURITY_CATEGORICAL], []), \n",
    "                            # 'yellow_lite_100k': (['trade_type', DAYS_TO_MATURITY_CATEGORICAL], [quantity_greater_than_100k]), \n",
    "                            # 'yellow_lite_1m': (['trade_type', DAYS_TO_MATURITY_CATEGORICAL], [quantity_greater_than_1m]), \n",
    "                            }\n",
    "\n",
    "# combine two dictionaries together for Python v3.9+: https://stackoverflow.com/questions/38987/how-do-i-merge-two-dictionaries-in-a-single-expression\n",
    "related_trades_criterion = related_trades_criterion | \\\n",
    "                           {'NONE': ([], []), \n",
    "                            'trade_type': (['trade_type'], []), \n",
    "                            'incorporated_state_code': (['incorporated_state_code'], []), \n",
    "                            'days_to_maturity_categorical': ([DAYS_TO_MATURITY_CATEGORICAL], []), \n",
    "                            'quantity_categorical': ([QUANTITY_CATEGORICAL], []), \n",
    "                            'coupon_categorical': ([COUPON_CATEGORICAL], []), \n",
    "                            'trade_datetime_day': ([TRADE_DATETIME_DAY], []), \n",
    "                            'rating_without_plus_minus_B_NR_combined': ([RATING_WITHOUT_PLUS_MINUS_B_NR_COMBINED], []), \n",
    "                            'days_to_call': ([DAYS_TO_CALL_CATEGORICAL], []), \n",
    "                            'purpose_class_top_values': ([PURPOSE_CLASS_TOP_VALUES], []), \n",
    "                            'muni_security_type_top_values': ([MUNI_SECURITY_TYPE_TOP_VALUES], []), \n",
    "                            '100k': ([], [quantity_greater_than_100k]), \n",
    "                            '1m': ([], [quantity_greater_than_1m]), \n",
    "                            'dd': ([], [trade_type_is_interdealer]), \n",
    "                            'rating': (['rating'], []), \n",
    "                            'purpose_class': (['purpose_class'], []), \n",
    "                            'coupon_categorical_sudhar': ([COUPON_CATEGORICAL_SUDHAR], [])\n",
    "                            }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add related trades to the trade data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_encoded = encode_with_label_encoders(trade_data_flattened_trade_history, features_to_exclude=['trade_type']) if ENCODE_REFERENCE_FEATURES else trade_data_flattened_trade_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_data_flattened_trade_history_and_related_trades = dict()\n",
    "# trade_data_flattened_trade_history = None    # uncomment this line when running LightGBM experiments for data files already created in order to reduce memory overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for name, (categories_to_match, filtering_conditions) in tqdm(related_trades_criterion.items()):\n",
    "    filename = f'trade_data_flattened_trade_history_and_related_trades_{name}_lgb_error'\n",
    "    filepath = make_data_filename(filename)\n",
    "    if os.path.exists(filepath):    # check if a file exists https://www.pythontutorial.net/python-basics/python-check-if-file-exists/\n",
    "        print(f'Loading dataset for {name} from pickle file {filepath}')\n",
    "        trade_data_flattened_trade_history_and_related_trades[name] = pd.read_pickle(filepath)    # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html\n",
    "    elif name not in trade_data_flattened_trade_history_and_related_trades:\n",
    "        print(f'Creating dataset for {name} and saving it to {filepath}')\n",
    "        trade_data_flattened_trade_history_and_related_trades[name] = append_recent_trade_data(trade_data_flattened_trade_history, \n",
    "                                                                                               NUM_TRADES_IN_RELATED_TRADE_HISTORY, \n",
    "                                                                                               RELATED_TRADE_FEATURE_FUNCTIONS_AND_DEFAULT_VALUES, \n",
    "                                                                                               feature_prefix=related_trade_feature_prefix, \n",
    "                                                                                               categories=categories_to_match, \n",
    "                                                                                               filtering_conditions=filtering_conditions, \n",
    "                                                                                               return_df=True, \n",
    "                                                                                               multiprocessing=True, \n",
    "                                                                                               df_for_related_trades=df_encoded).drop(columns=quantized_features)    # drop the quantized features from the final dataframe\n",
    "        trade_data_flattened_trade_history_and_related_trades[name].to_pickle(filepath, protocol=4)    # protocol 4 allows for use in the VM: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_data_flattened_trade_history = trade_data_flattened_trade_history.drop(columns=quantized_features)    # drop the quantized features from the final dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the each group has a reasonable amount of trades (otherwise finding related trades will be too difficult for certain trades). Note that if a group has a count of 1, then that trade has no previous related trades according to this definition of *related*. Further, note that if a group has a count of 2, then only one of those trades has a single past related trade, and the other one doesn't, where the one with no previous related trade is the oldest trade in this group. We should loosen or tighten the definition of *related* in order to make sure almost all trades have at least one previous related trades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_TO_DETECT_NO_PAST_TRADES = 'seconds_ago'    # arbitrarily chosen, but needs to be a feature that does not naturally have occurrences of its default value\n",
    "for name, df in trade_data_flattened_trade_history_and_related_trades.items():\n",
    "    print(f'{name}')\n",
    "    for past_trade_idx in (0, 1, 15, 31):    # range(2):\n",
    "        if past_trade_idx < NUM_TRADES_IN_RELATED_TRADE_HISTORY:\n",
    "            feature_name = get_appended_feature_name(past_trade_idx, FEATURE_TO_DETECT_NO_PAST_TRADES, related_trade_feature_prefix)\n",
    "            num_trades = (df[feature_name] == DEFAULT_VALUES[FEATURE_TO_DETECT_NO_PAST_TRADES]).sum()\n",
    "            print(f'Number of trades with fewer than {past_trade_idx + 1} past related trades: {num_trades}. Percentage of total trades: {round(num_trades / len(trade_data) * 100, 3)} %')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode the added reference features, if they are encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if ENCODE_REFERENCE_FEATURES:\n",
    "    print('Decoding the reference features.')\n",
    "    for df in tqdm(trade_data_flattened_trade_history_and_related_trades.values()):\n",
    "        for feature in CATEGORICAL_REFERENCE_FEATURES_TO_ADD:\n",
    "            encoder = label_encoders[feature]\n",
    "            for past_trade_idx in range(NUM_TRADES_IN_RELATED_TRADE_HISTORY):\n",
    "                feature_name = get_appended_feature_name(past_trade_idx, feature, related_trade_feature_prefix)\n",
    "                df[feature_name] = encoder.inverse_transform(df[feature_name].to_numpy(dtype=int))    # inverse transform the encoded categorical feature column; must set to dtype=int since label encoder encodes to integers\n",
    "                df[feature_name] = df[feature_name].astype('category')    # change dtype to `categorical` to use in LightGBM model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that reference features have dtype categorical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in tqdm(trade_data_flattened_trade_history_and_related_trades.values()):\n",
    "    for feature in CATEGORICAL_REFERENCE_FEATURES_TO_ADD:\n",
    "        if df[feature].dtype.name != 'category': df[feature] = df[feature].astype('category')    # check dtype of a column: https://stackoverflow.com/questions/26924904/check-if-dataframe-column-is-categorical"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a related trades criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_related_trades_columns_opt, all_categorical_features_in_trade_history_related = get_past_trade_columns(NUM_TRADES_IN_RELATED_TRADE_HISTORY, \n",
    "                                                                                                            RELATED_TRADE_FEATURE_FUNCTIONS.keys(), \n",
    "                                                                                                            related_trade_feature_prefix, \n",
    "                                                                                                            trade_type_actual=True, \n",
    "                                                                                                            trade_type_column=TRADE_TYPE_NEW_COLUMN, \n",
    "                                                                                                            categorical_features_per_trade=CATEGORICAL_FEATURES_IN_HISTORY + CATEGORICAL_REFERENCE_FEATURES_TO_ADD)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create column sets for different purposes.\n",
    "\n",
    "Difference between `related_trades_features` and `past_related_trades_columns_opt` is that the later has `related_last_trade_type` instead of `related_last_trade_type1` and `related_last_trade_type2`.\n",
    "\n",
    "Difference between `past_trade_feature_groups_flattened` and `past_trades_columns_opt` is that the former has all the `<num>last_trade_type1` and `<num>last_trade_type2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_trade_features = list(set(PREDICTORS_WITHOUT_LAST_TRADE_FEATURES + CATEGORICAL_REFERENCE_FEATURES_TO_ADD)) + TARGET\n",
    "\n",
    "columns_to_select_to_create_dataframe = target_trade_features + past_trade_feature_groups_flattened + related_trades_features + DATA_PROCESSING_FEATURES\n",
    "assert len(columns_to_select_to_create_dataframe) == len(set(columns_to_select_to_create_dataframe))    # checks that there are no intersection between the groups of features\n",
    "columns_to_select_for_lightgbm_model = target_trade_features + past_trade_columns + past_related_trades_columns_opt\n",
    "assert len(columns_to_select_for_lightgbm_model) == len(set(columns_to_select_for_lightgbm_model))    # checks that there are no intersection between the groups of features\n",
    "\n",
    "target_trade_categorical_features = list(set(CATEGORICAL_FEATURES + CATEGORICAL_REFERENCE_FEATURES_TO_ADD))\n",
    "\n",
    "categorical_features_for_lightgbm_model = target_trade_categorical_features + all_categorical_features_in_trade_history + all_categorical_features_in_trade_history_related\n",
    "assert len(categorical_features_for_lightgbm_model) == len(set(categorical_features_for_lightgbm_model))    # checks that there are no intersection between the groups of features\n",
    "\n",
    "print(f'Features used for LightGBM model: {sorted(columns_to_select_for_lightgbm_model)}')\n",
    "print(f'Categorical features used for LightGBM model: {sorted(categorical_features_for_lightgbm_model)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load info from previously run experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_trades_criterion_losses = dict()\n",
    "related_trades_criterion_losses_filepath = make_data_filename('related_trades_criterion_losses')\n",
    "if os.path.exists(related_trades_criterion_losses_filepath):\n",
    "    print(f'Loading losses from {related_trades_criterion_losses_filepath}')\n",
    "    with open(related_trades_criterion_losses_filepath, 'rb') as pickle_handle: related_trades_criterion_losses = pickle.load(pickle_handle)    # use template from https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict-or-any-other-python-object\n",
    "    print(f'Already have loss results for: {list(related_trades_criterion_losses.keys())}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create runner for experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lgb_error_for_each_related_trade_criterion(trade_data_dict, results_dict, results_dict_pickle_filepath, features, categorical_features):\n",
    "    for name, df in tqdm(trade_data_dict.items()):\n",
    "        if name not in results_dict:\n",
    "            # convert trade_type1 and trade_type2 to trade_type with S, P, D for same CUSIP trades\n",
    "            trade_data_predictors_history_related_trades_actual_trade_type, _, _ = convert_trade_type_encoding_to_actual(df[columns_to_select_to_create_dataframe], \n",
    "                                                                                                                         NUM_TRADES_IN_TRADE_HISTORY, \n",
    "                                                                                                                         TRADE_TYPE_NEW_COLUMN, \n",
    "                                                                                                                         SAME_CUSIP_PREFIX)\n",
    "            # convert trade_type1 and trade_type2 to trade_type with S, P, D for related trades\n",
    "            trade_data_predictors_history_related_trades_actual_trade_type, _, _ = convert_trade_type_encoding_to_actual(trade_data_predictors_history_related_trades_actual_trade_type, \n",
    "                                                                                                                         NUM_TRADES_IN_RELATED_TRADE_HISTORY, \n",
    "                                                                                                                         TRADE_TYPE_NEW_COLUMN, \n",
    "                                                                                                                         related_trade_feature_prefix)\n",
    "\n",
    "            # split data into train and test set\n",
    "            train_data_predictors_history_related_trades_actual_trade_type, \\\n",
    "                test_data_predictors_history_related_trades_actual_trade_type = get_train_test_data_trade_datetime(trade_data_predictors_history_related_trades_actual_trade_type, DATE_TO_SPLIT)\n",
    "            del trade_data_predictors_history_related_trades_actual_trade_type\n",
    "            gc.collect()\n",
    "            assert len(train_data_predictors_history_related_trades_actual_trade_type) != 0 and len(test_data_predictors_history_related_trades_actual_trade_type) != 0, 'Either train or test data is empty. Consider checking how the train test split is being performed.'\n",
    "\n",
    "            print(f'Training the LightGBM model for {name}')\n",
    "            _, lgb_losses = train_lightgbm_model(train_data_predictors_history_related_trades_actual_trade_type[features], \n",
    "                                                 test_data_predictors_history_related_trades_actual_trade_type[features], \n",
    "                                                 categorical_features, \n",
    "                                                 wandb_project='mitas_trade_history')\n",
    "\n",
    "            del train_data_predictors_history_related_trades_actual_trade_type\n",
    "            del test_data_predictors_history_related_trades_actual_trade_type\n",
    "            gc.collect()\n",
    "\n",
    "            results_dict[name] = lgb_losses['Train'][0], lgb_losses['Test'][0]\n",
    "            with open(results_dict_pickle_filepath, 'wb') as pickle_handle: pickle.dump(results_dict, pickle_handle, protocol=4)    # use template from https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict-or-any-other-python-object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lgb_error_for_each_related_trade_criterion(trade_data_flattened_trade_history_and_related_trades, \n",
    "                                               related_trades_criterion_losses, \n",
    "                                               related_trades_criterion_losses_filepath, \n",
    "                                               columns_to_select_for_lightgbm_model, \n",
    "                                               categorical_features_for_lightgbm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, (train_loss, test_loss) in related_trades_criterion_losses.items():\n",
    "    print(f'{name}\\t\\tTrain error: {train_loss}\\tTest error: {test_loss}')\n",
    "related_trades_criterion_ascending_order_of_test_loss = sorted(related_trades_criterion_losses, key=lambda name: related_trades_criterion_losses.get(name)[1])    # sort by minimum test error (which is represented by index 1)\n",
    "related_trades_criterion_opt = related_trades_criterion_ascending_order_of_test_loss[0]    # optimal name is the one with the minimum test error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run identical experiments without using the `lgb_error` in the related trades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_trades_wo_lgb_error_criterion_losses = dict()\n",
    "related_trades_wo_lgb_error_criterion_losses_filepath = make_data_filename('related_trades_wo_lgb_error_criterion_losses')\n",
    "if os.path.exists(related_trades_wo_lgb_error_criterion_losses_filepath):\n",
    "    print(f'Loading losses from {related_trades_wo_lgb_error_criterion_losses_filepath}')\n",
    "    with open(related_trades_wo_lgb_error_criterion_losses_filepath, 'rb') as pickle_handle: related_trades_wo_lgb_error_criterion_losses = pickle.load(pickle_handle)    # use template from https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict-or-any-other-python-object\n",
    "    print(f'Already have loss results for: {list(related_trades_wo_lgb_error_criterion_losses.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_select_for_lightgbm_model_wo_lgb_error = [column for column in columns_to_select_for_lightgbm_model if not column.endswith('lgb_error')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lgb_error_for_each_related_trade_criterion(trade_data_flattened_trade_history_and_related_trades, \n",
    "                                               related_trades_wo_lgb_error_criterion_losses, \n",
    "                                               related_trades_wo_lgb_error_criterion_losses_filepath, \n",
    "                                               columns_to_select_for_lightgbm_model_wo_lgb_error, \n",
    "                                               categorical_features_for_lightgbm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in tqdm(trade_data_flattened_trade_history_and_related_trades.items()):\n",
    "    if name not in related_trades_wo_lgb_error_criterion_losses:\n",
    "        # convert trade_type1 and trade_type2 to trade_type with S, P, D for same CUSIP trades\n",
    "        trade_data_predictors_history_related_trades_actual_trade_type, _, _ = convert_trade_type_encoding_to_actual(df[columns_to_select_to_create_dataframe], \n",
    "                                                                                                                     NUM_TRADES_IN_TRADE_HISTORY, \n",
    "                                                                                                                     TRADE_TYPE_NEW_COLUMN, \n",
    "                                                                                                                     SAME_CUSIP_PREFIX)\n",
    "        # convert trade_type1 and trade_type2 to trade_type with S, P, D for related trades\n",
    "        trade_data_predictors_history_related_trades_actual_trade_type, _, _ = convert_trade_type_encoding_to_actual(trade_data_predictors_history_related_trades_actual_trade_type, \n",
    "                                                                                                                     NUM_TRADES_IN_RELATED_TRADE_HISTORY, \n",
    "                                                                                                                     TRADE_TYPE_NEW_COLUMN, \n",
    "                                                                                                                     related_trade_feature_prefix)\n",
    "\n",
    "        # split data into train and test set\n",
    "        train_data_predictors_history_related_trades_actual_trade_type, \\\n",
    "            test_data_predictors_history_related_trades_actual_trade_type = get_train_test_data_trade_datetime(trade_data_predictors_history_related_trades_actual_trade_type, DATE_TO_SPLIT)\n",
    "        del trade_data_predictors_history_related_trades_actual_trade_type\n",
    "        gc.collect()\n",
    "        assert len(train_data_predictors_history_related_trades_actual_trade_type) != 0 and len(test_data_predictors_history_related_trades_actual_trade_type) != 0, 'Either train or test data is empty. Consider checking how the train test split is being performed.'\n",
    "\n",
    "        print(f'Training the LightGBM model for {name}')\n",
    "        _, lgb_losses = train_lightgbm_model(train_data_predictors_history_related_trades_actual_trade_type[columns_to_select_for_lightgbm_model_without_lgb_error], \n",
    "                                             test_data_predictors_history_related_trades_actual_trade_type[columns_to_select_for_lightgbm_model_without_lgb_error], \n",
    "                                             categorical_features_for_lightgbm_model, \n",
    "                                             wandb_project='mitas_trade_history')\n",
    "\n",
    "        del train_data_predictors_history_related_trades_actual_trade_type\n",
    "        del test_data_predictors_history_related_trades_actual_trade_type\n",
    "        gc.collect()\n",
    "\n",
    "        related_trades_criterion_losses[name] = lgb_losses['Train'][0], lgb_losses['Test'][0]\n",
    "        with open(related_trades_criterion_losses_filepath, 'wb') as pickle_handle: pickle.dump(related_trades_criterion_losses, pickle_handle, protocol=4)    # use template from https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict-or-any-other-python-object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, (train_loss, test_loss) in related_trades_wo_lgb_error_criterion_losses.items():\n",
    "    print(f'{name}\\t\\tTrain error: {train_loss}\\tTest error: {test_loss}')\n",
    "related_trades_wo_lgb_error_criterion_ascending_order_of_test_loss = sorted(related_trades_wo_lgb_error_criterion_losses, key=lambda name: related_trades_wo_lgb_error_criterion_losses.get(name)[1])    # sort by minimum test error (which is represented by index 1)\n",
    "related_trades_wo_lgb_error_criterion_opt = related_trades_wo_lgb_error_criterion_ascending_order_of_test_loss[0]    # optimal name is the one with the minimum test error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we will only have one dataset; the one with the appended trades coming from `related_trades_criterion_opt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_data_flattened_trade_history_and_related_trades = trade_data_flattened_trade_history_and_related_trades[related_trades_criterion_opt]\n",
    "trade_data_predictors_history_related_trades = trade_data_flattened_trade_history_and_related_trades[columns_to_select_to_create_dataframe]\n",
    "train_data_predictors_history_related_trades, test_data_predictors_history_related_trades = get_train_test_data_trade_datetime(trade_data_predictors_history_related_trades, DATE_TO_SPLIT)\n",
    "train_data_predictors_history_related_trades = train_data_predictors_history_related_trades.drop(columns=DATA_PROCESSING_FEATURES)\n",
    "test_data_predictors_history_related_trades = test_data_predictors_history_related_trades.drop(columns=DATA_PROCESSING_FEATURES)\n",
    "\n",
    "trade_data_predictors_history_related_trades_actual_trade_type, old_trade_type_columns, _ = convert_trade_type_encoding_to_actual(trade_data_predictors_history_related_trades, \n",
    "                                                                                                                                  NUM_TRADES_IN_TRADE_HISTORY, \n",
    "                                                                                                                                  TRADE_TYPE_NEW_COLUMN, \n",
    "                                                                                                                                  SAME_CUSIP_PREFIX)\n",
    "trade_data_predictors_history_related_trades_actual_trade_type, old_trade_type_columns_related, _ = convert_trade_type_encoding_to_actual(trade_data_predictors_history_related_trades_actual_trade_type, \n",
    "                                                                                                                                          NUM_TRADES_IN_RELATED_TRADE_HISTORY, \n",
    "                                                                                                                                          TRADE_TYPE_NEW_COLUMN, \n",
    "                                                                                                                                          related_trade_feature_prefix)\n",
    "trade_data_predictors_history_related_trades_actual_trade_type = trade_data_predictors_history_related_trades_actual_trade_type.drop(columns=old_trade_type_columns + old_trade_type_columns_related)\n",
    "\n",
    "train_data_predictors_history_related_trades_actual_trade_type, \\\n",
    "    test_data_predictors_history_related_trades_actual_trade_type = get_train_test_data_trade_datetime(trade_data_predictors_history_related_trades_actual_trade_type, DATE_TO_SPLIT)\n",
    "\n",
    "train_data_predictors_history_related_trades_actual_trade_type = train_data_predictors_history_related_trades_actual_trade_type.drop(columns=DATA_PROCESSING_FEATURES)\n",
    "test_data_predictors_history_related_trades_actual_trade_type = test_data_predictors_history_related_trades_actual_trade_type.drop(columns=DATA_PROCESSING_FEATURES)\n",
    "\n",
    "trade_data_predictors_history_related_trades = trade_data_predictors_history_related_trades.drop(columns=DATA_PROCESSING_FEATURES)\n",
    "trade_data_predictors_history_related_trades_actual_trade_type = trade_data_predictors_history_related_trades_actual_trade_type.drop(columns=DATA_PROCESSING_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model, lgb_losses = train_lightgbm_model(train_data_predictors_history_related_trades_actual_trade_type[columns_to_select_for_lightgbm_model], \n",
    "                                             test_data_predictors_history_related_trades_actual_trade_type[columns_to_select_for_lightgbm_model], \n",
    "                                             categorical_features_for_lightgbm_model, \n",
    "                                             wandb_project='mitas_trade_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(lgb_model, figsize=(20, 10), importance_type='gain')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the LightGBM model without the `lgb_error` feature in the related trades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model_wo_lgb_error, lgb_losses_wo_lgb_error = train_lightgbm_model(train_data_predictors_history_related_trades_actual_trade_type[columns_to_select_for_lightgbm_model_wo_lgb_error], \n",
    "                                                                       test_data_predictors_history_related_trades_actual_trade_type[columns_to_select_for_lightgbm_model_wo_lgb_error], \n",
    "                                                                       categorical_features_for_lightgbm_model, \n",
    "                                                                       wandb_project='mitas_trade_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(lgb_model_wo_lgb_error, figsize=(20, 10), importance_type='gain')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The related trades feature `lgb_error` does not provide any gain. See the results in the spreadsheet: https://docs.google.com/spreadsheets/d/15Z97BDO6g1VEw-4iQ-zTA7_tp3thsGg9XJrby3gziKQ/edit#gid=0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e18c70c74e919487903475d4b9ee892d16d9f5351cb9291a624c367bcb362da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
