{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "358ab8fd",
   "metadata": {},
   "source": [
    "## Dollar Price Model\n",
    "\n",
    "This notebook presents a model that utilizes reference and trade history data to accurately predict dollar prices. Based on the foundation of the yield spread model, this adaptation incorporates an attention layer positioned between two LSTM layers. The model's training encompasses data spanning from January 1, 2023, to April 1, 2023, while the test set is limited to the month of April 2023.\n",
    "\n",
    "\n",
    "Last Update: Comparing models trained on different labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58de4fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing pandarallel with 16.0 cores\n",
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from sklearn import preprocessing\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "\n",
    "\n",
    "from ficc.data.process_data import process_data\n",
    "from ficc.utils.auxiliary_variables import IDENTIFIERS, NON_CAT_FEATURES_DOLLAR_PRICE, BINARY_DOLLAR_PRICE, CATEGORICAL_FEATURES_DOLLAR_PRICE,PREDICTORS_DOLLAR_PRICE\n",
    "from ficc.utils.gcp_storage_functions import upload_data, download_data\n",
    "from ficc.utils.auxiliary_variables import RELATED_TRADE_BINARY_FEATURES, RELATED_TRADE_NON_CAT_FEATURES, RELATED_TRADE_CATEGORICAL_FEATURES\n",
    "from ficc.utils.auxiliary_variables import COUPON_FREQUENCY_TYPE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dbb1703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 20:25:06.056789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-23 20:25:06.068109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-23 20:25:06.068816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae670dc",
   "metadata": {},
   "source": [
    "Setting the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07c11246",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"ahmad_creds.json\"\n",
    "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440e0be5",
   "metadata": {},
   "source": [
    "Initializing BigQuery client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b1d17e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d4563b",
   "metadata": {},
   "source": [
    "Initializing GCP storage client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92b48799",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e06a8d",
   "metadata": {},
   "source": [
    "Declaring hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "767f6023",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_SPLIT = 0.85\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 1000\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "DROPOUT = 0.01\n",
    "SEQUENCE_LENGTH = 2\n",
    "NUM_FEATURES = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb809f8",
   "metadata": {},
   "source": [
    "Checking if the treasury spreads and target attention features are present in PREDICTORS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "338b5e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'target_attention_features' not in PREDICTORS_DOLLAR_PRICE:\n",
    "    PREDICTORS_DOLLAR_PRICE.append('target_attention_features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090e08df",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "Our data retrieval process involves accessing the data from a designated GCP bucket. To prepare the data, we utilize the ficc Python package, which offers comprehensive functionality and tools for data preparation. For more detailed information on the specific steps involved in data preparation, including data cleaning, feature engineering, and preprocessing, please refer to the accompanying [code](https://github.com/Ficc-ai/ficc/blob/ahmad_ml/ml_models/sequence_predictors/data_prep/data_preparation.ipynb) that provides deeper insights into our data preparation pipeline. This comprehensive approach ensures that the data utilized in our analysis is carefully curated and optimized to yield accurate and reliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2c7302a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.4 s, sys: 15.4 s, total: 1min\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem(project='eng-reactor-287421')\n",
    "with fs.open('automated_training/processed_data_dollar_price.pkl') as f:\n",
    "    data = pd.read_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf8d8a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.trade_date > '2023-05-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6c17c",
   "metadata": {},
   "source": [
    "#### Date range for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5680e3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2023-08-21 00:00:00')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.trade_date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "548f2ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2023-05-02 00:00:00')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.trade_date.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "859c0b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricting history to 2 trades\n"
     ]
    }
   ],
   "source": [
    "print(f'Restricting history to {SEQUENCE_LENGTH} trades')\n",
    "data.trade_history = data.trade_history.apply(lambda x: x[:SEQUENCE_LENGTH])\n",
    "data.target_attention_features = data.target_attention_features.apply(lambda x:x[:SEQUENCE_LENGTH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c81c3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.trade_history.iloc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3131cf94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target_attention_features.iloc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23c937dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2973754"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b98109c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(by='trade_datetime', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f524c3",
   "metadata": {},
   "source": [
    "### Creating features from trade history\n",
    "\n",
    "\n",
    "This implementation builds upon Charles's original approach. Rather than relying on the yield spread, we have opted to utilize the dollar price of previous trades. This adjustment acknowledges the importance of the historical dollar prices in our analysis and allows us to capture valuable insights for more accurate predictions. By leveraging this alternative approach, our primary objective is to not only refine the model's performance but also extend its capacity to provide more robust estimations for a larger number of CUSIPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d782d387",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttype_dict = { (0,0):'D', (0,1):'S', (1,0):'P' }\n",
    "\n",
    "dp_variants = [\"max_dp\", \"min_dp\", \"max_qty\", \"min_ago\", \"D_min_ago\", \"P_min_ago\", \"S_min_ago\"]\n",
    "dp_feats = [\"_dp\", \"_ttypes\", \"_ago\", \"_qdiff\"]\n",
    "D_prev = dict()\n",
    "P_prev = dict()\n",
    "S_prev = dict()\n",
    "\n",
    "def get_trade_history_columns():\n",
    "    '''\n",
    "    This function is used to create a list of columns\n",
    "    '''\n",
    "    YS_COLS = []\n",
    "    for prefix in dp_variants:\n",
    "        for suffix in dp_feats:\n",
    "            YS_COLS.append(prefix + suffix)\n",
    "    return YS_COLS\n",
    "\n",
    "def extract_feature_from_trade(row, name, trade):\n",
    "    dollar_price = trade[0]\n",
    "    ttypes = ttype_dict[(trade[2],trade[3])] + row.trade_type\n",
    "    seconds_ago = trade[4]\n",
    "    quantity_diff = np.log10(1 + np.abs(10**trade[1] - 10**row.quantity))\n",
    "    return [dollar_price, ttypes,  seconds_ago, quantity_diff]\n",
    "\n",
    "def trade_history_derived_features(row):\n",
    "    trade_history = row.trade_history\n",
    "    trade = trade_history[0]\n",
    "    \n",
    "    D_min_ago_t = D_prev.get(row.cusip,trade)\n",
    "    D_min_ago = 9        \n",
    "\n",
    "    P_min_ago_t = P_prev.get(row.cusip,trade)\n",
    "    P_min_ago = 9\n",
    "    \n",
    "    S_min_ago_t = S_prev.get(row.cusip,trade)\n",
    "    S_min_ago = 9\n",
    "    \n",
    "    max_dp_t = trade; max_dp = trade[0]\n",
    "    min_dp_t = trade; min_dp = trade[0]\n",
    "    max_qty_t = trade; max_qty = trade[1]\n",
    "    min_ago_t = trade; min_ago = trade[4]\n",
    "    \n",
    "    for trade in trade_history[0:]:\n",
    "        #Checking if the first trade in the history is from the same block\n",
    "        if trade[4] <= 0: \n",
    "            continue\n",
    " \n",
    "        if trade[0] > max_dp: \n",
    "            max_dp_t = trade\n",
    "            max_dp = trade[0]\n",
    "        elif trade[0] < min_dp: \n",
    "            min_dp_t = trade; \n",
    "            min_dp = trade[0]\n",
    "\n",
    "        if trade[1] > max_qty: \n",
    "            max_qty_t = trade \n",
    "            max_qty = trade[1]\n",
    "        if trade[4] < min_ago: \n",
    "            min_ago_t = trade; \n",
    "            min_ago = trade[4]\n",
    "            \n",
    "        side = ttype_dict[(trade[2],trade[3])]\n",
    "        if side == \"D\":\n",
    "            if trade[4] < D_min_ago: \n",
    "                D_min_ago_t = trade\n",
    "                D_min_ago = trade[4]\n",
    "                D_prev[row.cusip] = trade\n",
    "        elif side == \"P\":\n",
    "            if trade[4] < P_min_ago: \n",
    "                P_min_ago_t = trade\n",
    "                P_min_ago = trade[4]\n",
    "                P_prev[row.cusip] = trade\n",
    "        elif side == \"S\":\n",
    "            if trade[4] < S_min_ago: \n",
    "                S_min_ago_t = trade\n",
    "                S_min_ago = trade[4]\n",
    "                S_prev[row.cusip] = trade\n",
    "        else: \n",
    "            print(\"invalid side\", trade)\n",
    "    \n",
    "    trade_history_dict = {\"max_dp\":max_dp_t,\n",
    "                          \"min_dp\":min_dp_t,\n",
    "                          \"max_qty\":max_qty_t,\n",
    "                          \"min_ago\":min_ago_t,\n",
    "                          \"D_min_ago\":D_min_ago_t,\n",
    "                          \"P_min_ago\":P_min_ago_t,\n",
    "                          \"S_min_ago\":S_min_ago_t}\n",
    "\n",
    "    return_list = []\n",
    "    for variant in dp_variants:\n",
    "        feature_list = extract_feature_from_trade(row,variant,trade_history_dict[variant])\n",
    "        return_list += feature_list\n",
    "    \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8ce568c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.5 s, sys: 8.31 s, total: 55.8 s\n",
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "temp = data[['cusip','trade_history','quantity','trade_type']].parallel_apply(trade_history_derived_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5519788",
   "metadata": {},
   "outputs": [],
   "source": [
    "YS_COLS = get_trade_history_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fb65b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[YS_COLS] = pd.DataFrame(temp.tolist(), index=data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bab8345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values('trade_datetime',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630f457d",
   "metadata": {},
   "source": [
    "The purpose of this feature is to conduct a thorough check for any NaN values within the trade history. It's important to note that while this feature serves as a quality control measure, it does not contribute to the training of the model itself. **Its sole purpose is to ensure the integrity and reliability of the data used in the model training process, guaranteeing that any missing values or inconsistencies are identified and appropriately addressed prior to model training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13523b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2973754"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8e732eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.2 s, sys: 3.18 s, total: 16.4 s\n",
      "Wall time: 18.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['trade_history_sum'] = data.trade_history.parallel_apply(lambda x: np.sum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9eeceb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['trade_history_sum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65ee4a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2973754"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d69d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the purpose of plotting, not used in training\n",
    "data.purpose_sub_class.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed96b6f7",
   "metadata": {},
   "source": [
    "Selecting a subset of features for training. PREDICTORS are the features that we are going to use to train the model. More information about the feature set can be found [here](https://github.com/Ficc-ai/ficc_python/blob/d455bd30eca18f26a2535523530facad516dd90f/ficc/utils/auxiliary_variables.py#L120). We also select a set of additonal features, which are not used in training. These features are used to uderstand the results from the model.\n",
    "\n",
    "\n",
    "To address the issue of missing calc date and yield for certain CUSIPs, we have made the decision to remove the features that are constructed using the yield. This ensures that our data remains consistent and accurate, even for those CUSIPs where calc date and yield information is unavailable. By removing these features selectively, we can maintain the integrity of our data while still providing valuable insights for the majority of CUSIPs.\n",
    "\n",
    "In order to enhance the model's accuracy in estimating prices for current trades, we propose incorporating the last dollar price as a feature during the training process. Based on our experience, we have observed that features derived from the last trade play a crucial role in accurate price estimation. By including the last dollar price as a feature, we can leverage this valuable information to improve the performance and predictive capabilities of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37a71950",
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliary_features = ['dollar_price',\n",
    "                     'trade_date',\n",
    "                     'trade_datetime', \n",
    "                     'purpose_sub_class', \n",
    "                     'called_redemption_type', \n",
    "                     'trade_history_sum',\n",
    "                     'days_to_refund',\n",
    "                     'last_rtrs_control_number',\n",
    "                     'is_called',\n",
    "                     'next_call_date',\n",
    "                     'maturity_date',\n",
    "                     'refund_date',\n",
    "                     'yield',\n",
    "                     'interest_payment_frequency',\n",
    "                     'next_coupon_payment_date',\n",
    "                     'previous_coupon_payment_date',\n",
    "                     'is_callable',\n",
    "                     'first_coupon_date',\n",
    "                     'settlement_date',\n",
    "                     'accrual_date',\n",
    "                     'last_period_accrues_from_date',\n",
    "                     'par_call_date',\n",
    "                     'par_call_price',\n",
    "                     'next_call_price',\n",
    "                     'refund_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bfbe065",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = data[IDENTIFIERS + PREDICTORS_DOLLAR_PRICE + auxiliary_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f6fd0a",
   "metadata": {},
   "source": [
    "Checking for missing data and NaN values. Removing callable at cav bonds, as they are extremly difficult to predict using the dollar price model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "094bf076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2973754"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fadc1162",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.issue_amount = processed_data.issue_amount.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da24fd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.dropna(inplace=True , subset=PREDICTORS_DOLLAR_PRICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a3fff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = processed_data[processed_data.callable_at_cav == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d990c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2962355"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1901e2a1",
   "metadata": {},
   "source": [
    "#### Fitting encoders to the categorical features. These encoders are then used to encode the categorical features of the train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d668ef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_feature_values = {'purpose_class' : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
    "                                                 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46,\n",
    "                                                 47, 48, 49, 50, 51, 52, 53],\n",
    "                              'rating' : ['A', 'A+', 'A-', 'AA', 'AA+', 'AA-', 'AAA', 'B', 'B+', 'B-', 'BB', 'BB+', 'BB-',\n",
    "                                         'BBB', 'BBB+', 'BBB-', 'CC', 'CCC', 'CCC+', 'CCC-' , 'D', 'NR', 'MR'],\n",
    "                              'trade_type' : ['D', 'S', 'P'],\n",
    "                              'incorporated_state_code' : ['AK', 'AL', 'AR', 'AS', 'AZ', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA', 'GU',\n",
    "                                                         'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME', 'MI', 'MN',\n",
    "                                                         'MO', 'MP', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM', 'NV', 'NY', 'OH',\n",
    "                                                         'OK', 'OR', 'PA', 'PR', 'RI', 'SC', 'SD', 'TN', 'TX', 'US', 'UT', 'VA', 'VI',\n",
    "                                                         'VT', 'WA', 'WI', 'WV', 'WY'] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66d92939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating\n",
      "incorporated_state_code\n",
      "trade_type\n",
      "purpose_class\n",
      "max_dp_ttypes\n",
      "min_dp_ttypes\n",
      "max_qty_ttypes\n",
      "min_ago_ttypes\n",
      "D_min_ago_ttypes\n",
      "P_min_ago_ttypes\n",
      "S_min_ago_ttypes\n"
     ]
    }
   ],
   "source": [
    "encoders = {}\n",
    "fmax = {}\n",
    "for f in CATEGORICAL_FEATURES_DOLLAR_PRICE:\n",
    "    print(f)\n",
    "    if f in ['rating', 'incorporated_state_code', 'trade_type', 'purpose_class']:\n",
    "        fprep = preprocessing.LabelEncoder().fit(categorical_feature_values[f])\n",
    "    else:\n",
    "        fprep = preprocessing.LabelEncoder().fit(data[f].drop_duplicates())\n",
    "    fmax[f] = np.max(fprep.transform(fprep.classes_))\n",
    "    encoders[f] = fprep\n",
    "    \n",
    "with open('encoders_dollar_price.pkl','wb') as file:\n",
    "    pickle.dump(encoders,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35893438",
   "metadata": {},
   "source": [
    "#### Splitting the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9daf580",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe = processed_data[(processed_data.trade_date < '07-01-2023')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40be739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataframe = processed_data[(processed_data.trade_date >= '07-01-2023') & (processed_data.trade_date <= '07-31-2023') ]\n",
    "test_dataframe.interest_payment_frequency = test_dataframe.interest_payment_frequency.map(COUPON_FREQUENCY_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "104c4ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1735010"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9940216d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2023-06-30 00:00:00')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataframe.trade_date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "014a4bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "712700"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "29cf32a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2023-07-03 00:00:00')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataframe.trade_date.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86081f1",
   "metadata": {},
   "source": [
    "Converting data into format suitable for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1e5fd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input(df):\n",
    "    global encoders\n",
    "    datalist = []\n",
    "    datalist.append(np.stack(df['trade_history'].to_numpy()))\n",
    "    datalist.append(np.stack(df['target_attention_features'].to_numpy()))\n",
    "\n",
    "    noncat_and_binary = []\n",
    "    for f in NON_CAT_FEATURES_DOLLAR_PRICE + BINARY_DOLLAR_PRICE:\n",
    "        noncat_and_binary.append(np.expand_dims(df[f].to_numpy().astype('float32'), axis=1))\n",
    "    datalist.append(np.concatenate(noncat_and_binary, axis=-1))\n",
    "    \n",
    "    for f in CATEGORICAL_FEATURES_DOLLAR_PRICE:\n",
    "        encoded = encoders[f].transform(df[f])\n",
    "        datalist.append(encoded.astype('float32'))\n",
    "    \n",
    "    return datalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2b65617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.87 s, sys: 402 ms, total: 10.3 s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train = create_input(train_dataframe)\n",
    "y_train = train_dataframe.dollar_price\n",
    "y_train_diff  = train_dataframe.dollar_price - train_dataframe.last_dollar_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7af0cf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.01 s, sys: 21.5 ms, total: 4.03 s\n",
      "Wall time: 4.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_test = create_input(test_dataframe)\n",
    "y_test = test_dataframe.dollar_price\n",
    "y_test_diff = test_dataframe.dollar_price - test_dataframe.last_dollar_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "295ac362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1735010, 2, 5)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "270a622a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(712700, 47)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce6aa16",
   "metadata": {},
   "source": [
    "# Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c03994",
   "metadata": {},
   "source": [
    "#### Adapting Normalization layers to the non categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f16ae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0] = x_train[0].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a5916de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 20:30:24.538307: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-23 20:30:24.541029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-23 20:30:24.541889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-23 20:30:24.542524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-23 20:30:25.027081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-23 20:30:25.027864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-23 20:30:25.028494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-23 20:30:25.029112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13817 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Normalization layer for the trade history\n",
    "trade_history_normalizer = Normalization(name='Trade_history_normalizer')\n",
    "trade_history_normalizer.adapt(x_train[0],batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "43293ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization layer for the non-categorical and binary features\n",
    "noncat_binary_normalizer = Normalization(name='Numerical_binary_normalizer')\n",
    "noncat_binary_normalizer.adapt(x_train[2], batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083c851e",
   "metadata": {},
   "source": [
    "#### Setting the seed for intialization of the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "39e964e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855af067",
   "metadata": {},
   "source": [
    "#### Implementation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b28ffa",
   "metadata": {},
   "source": [
    "We define an attention mechanism to weigh the importance of the target trade with respect to the output of the second LSTM layer. The attention mechanism first calculates the dot product between the LSTM output and the target trade using the Dot layer. The axes=\\[2, 2\\] means that we want to perform a dot product between the last axis of lstm ouput and the last axis of target trade sequence. The result is passed through a softmax activation function using the Activation layer to obtain the attention weights. The attention weights indicate the relative importance of each lstm hidden state with the target trade. Finally, the attention weights are used to calculate a context vector as a weighted sum of the traget trade using another Dot layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "527dc2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "55ccb036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dollar_price_model():\n",
    "    inputs = []\n",
    "    layer = []\n",
    "\n",
    "    ############## INPUT BLOCK ###################\n",
    "    trade_history_input = layers.Input(name=\"trade_history_input\", \n",
    "                                       shape=(SEQUENCE_LENGTH,NUM_FEATURES), \n",
    "                                       dtype = tf.float32) \n",
    "\n",
    "    target_attention_input = layers.Input(name=\"target_attention_input\", \n",
    "                                       shape=(1, 3), \n",
    "                                       dtype = tf.float32) \n",
    "\n",
    "\n",
    "    inputs.append(trade_history_input)\n",
    "    inputs.append(target_attention_input)\n",
    "\n",
    "    inputs.append(layers.Input(\n",
    "        name=\"NON_CAT_AND_BINARY_FEATURES\",\n",
    "        shape=(len(NON_CAT_FEATURES_DOLLAR_PRICE + BINARY_DOLLAR_PRICE),)\n",
    "    ))\n",
    "\n",
    "\n",
    "    layer.append(noncat_binary_normalizer(inputs[2]))\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "    ############## TRADE HISTORY MODEL #################\n",
    "\n",
    "    lstm_layer = layers.Bidirectional(layers.LSTM(50, \n",
    "                             activation='tanh',\n",
    "                             input_shape=(SEQUENCE_LENGTH,NUM_FEATURES),\n",
    "                             return_sequences = True,\n",
    "                             name='LSTM'))\n",
    "\n",
    "    lstm_layer_2 = layers.Bidirectional(layers.LSTM(100, \n",
    "                                                    activation='tanh',\n",
    "                                                    input_shape=(SEQUENCE_LENGTH,50),\n",
    "                                                    return_sequences = True,\n",
    "                                                    name='LSTM_2'))\n",
    "\n",
    "\n",
    "\n",
    "    features = lstm_layer(trade_history_normalizer(inputs[0]))\n",
    "    features = lstm_layer_2(features)\n",
    "\n",
    "\n",
    "    attention_sequence = layers.Dense(200, activation='relu', name='attention_dense')(target_attention_input)\n",
    "    attention = layers.Dot(axes=[2, 2])([features, attention_sequence])\n",
    "    attention = layers.Activation('softmax')(attention)\n",
    "\n",
    "    context_vector = layers.Dot(axes=[1, 1])([features, attention])\n",
    "    context_vector = layers.Flatten(name='context_vector_flatten')(context_vector)\n",
    "\n",
    "\n",
    "    trade_history_output = layers.Dense(100, \n",
    "                                        activation='relu')(context_vector)\n",
    "\n",
    "    ####################################################\n",
    "\n",
    "    ############## REFERENCE DATA MODEL ################\n",
    "    global encoders\n",
    "    for f in CATEGORICAL_FEATURES_DOLLAR_PRICE:\n",
    "        fin = layers.Input(shape=(1,), name = f)\n",
    "        inputs.append(fin)\n",
    "        embedded = layers.Flatten(name = f + \"_flat\")(layers.Embedding(input_dim = fmax[f]+1,\n",
    "                                                                       output_dim = max(30,int(np.sqrt(fmax[f]))),\n",
    "                                                                       input_length= 1,\n",
    "                                                                       name = f + \"_embed\")(fin))\n",
    "        layer.append(embedded)\n",
    "\n",
    "\n",
    "    reference_hidden = layers.Dense(400,\n",
    "                                    activation='relu',\n",
    "                                    name='reference_hidden_1')(layers.concatenate(layer, axis=-1))\n",
    "\n",
    "    reference_hidden = layers.BatchNormalization()(reference_hidden)\n",
    "    reference_hidden = layers.Dropout(DROPOUT)(reference_hidden)\n",
    "\n",
    "    reference_hidden2 = layers.Dense(200,activation='relu',name='reference_hidden_2')(reference_hidden)\n",
    "    reference_hidden2 = layers.BatchNormalization()(reference_hidden2)\n",
    "    reference_hidden2 = layers.Dropout(DROPOUT)(reference_hidden2)\n",
    "\n",
    "    reference_output = layers.Dense(100,activation='tanh',name='reference_hidden_3')(reference_hidden2)\n",
    "\n",
    "    ####################################################\n",
    "\n",
    "    feed_forward_input = layers.concatenate([reference_output, trade_history_output])\n",
    "\n",
    "    hidden = layers.Dense(300,activation='relu')(feed_forward_input)\n",
    "    hidden = layers.BatchNormalization()(hidden)\n",
    "    hidden = layers.Dropout(DROPOUT)(hidden)\n",
    "\n",
    "    hidden2 = layers.Dense(100,activation='relu')(hidden)\n",
    "    hidden2 = layers.BatchNormalization()(hidden2)\n",
    "    hidden2 = layers.Dropout(DROPOUT)(hidden2)\n",
    "\n",
    "    final = layers.Dense(1)(hidden2)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=final)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1c69ed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)\n",
    "\n",
    "time_callback = TimeHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ff295340",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                               patience=10,\n",
    "                                               verbose=0,\n",
    "                                               mode=\"auto\",\n",
    "                                               restore_best_weights=True),\n",
    "                 time_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a047d0b",
   "metadata": {},
   "source": [
    "Training the model using the difference in dollar prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eaacdb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 20:36:39.543466: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-08-23 20:37:04.553673: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-08-23 20:45:27.294634: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE current run : 0.491\n",
      "Training iteration : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 20:45:37.795961: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-08-23 20:46:01.314125: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-08-23 20:54:35.296357: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE current run : 0.488\n",
      "Training iteration : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 20:54:45.666487: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-08-23 20:55:09.453567: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-08-23 21:04:04.974816: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE current run : 0.485\n",
      "Training iteration : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 21:04:15.379944: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-08-23 21:04:39.102117: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-08-23 21:14:43.229473: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE current run : 0.489\n",
      "Training iteration : 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 21:14:58.321202: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-08-23 21:15:22.439723: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-08-23 21:21:40.007314: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE current run : 0.495\n"
     ]
    }
   ],
   "source": [
    "error_list_diff = []\n",
    "for i in range(5):\n",
    "    print(f\"Training iteration : {i+1}\")\n",
    "    model = generate_dollar_price_model()\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "          loss=keras.losses.MeanAbsoluteError())\n",
    "    model.fit(x_train, \n",
    "              y_train_diff, \n",
    "              epochs=100, \n",
    "              batch_size=BATCH_SIZE, \n",
    "              verbose=0, \n",
    "              validation_split=0.1, \n",
    "              callbacks=fit_callbacks,\n",
    "              use_multiprocessing=True,\n",
    "              workers=8)\n",
    "    \n",
    "    test_dataframe['predicted_difference'] = model.predict(x_test, batch_size=BATCH_SIZE)\n",
    "    test_dataframe['predicted_dollar_price'] = test_dataframe['last_dollar_price'] + test_dataframe['predicted_difference']\n",
    "    current_error = round(np.mean(np.abs(test_dataframe.dollar_price - test_dataframe.predicted_dollar_price)), 3)\n",
    "    print(f\"MAE current run : {current_error}\")\n",
    "    error_list.append(current_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b4c719e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean error over 5 runs:0.49\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean error over 5 runs:{np.round(np.mean(error_list_diff),3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b861d0e4",
   "metadata": {},
   "source": [
    "Training the model using dollar price as a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c31d1ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 21:21:50.029305: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-08-23 21:22:13.228840: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-08-23 21:34:53.066730: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE current run : 0.557\n",
      "Training iteration : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 21:35:03.133088: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-08-23 21:35:27.033319: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-08-23 21:47:59.332420: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE current run : 0.549\n",
      "Training iteration : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 21:48:09.409079: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-08-23 21:48:33.182589: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-08-23 22:02:16.337877: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE current run : 0.542\n",
      "Training iteration : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 22:02:26.536402: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-08-23 22:02:50.259870: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-08-23 22:15:01.078083: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE current run : 0.566\n",
      "Training iteration : 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 22:15:16.649160: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-08-23 22:15:40.730276: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-08-23 22:31:10.503560: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE current run : 0.563\n"
     ]
    }
   ],
   "source": [
    "error_list = []\n",
    "for i in range(5):\n",
    "    print(f\"Training iteration : {i+1}\")\n",
    "    model = generate_dollar_price_model()\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "          loss=keras.losses.MeanAbsoluteError())\n",
    "    model.fit(x_train, \n",
    "              y_train, \n",
    "              epochs=100, \n",
    "              batch_size=BATCH_SIZE, \n",
    "              verbose=0, \n",
    "              validation_split=0.1, \n",
    "              callbacks=fit_callbacks,\n",
    "              use_multiprocessing=True,\n",
    "              workers=8)\n",
    "    \n",
    "    test_dataframe['predicted_dollar_price'] = model.predict(x_test, batch_size=BATCH_SIZE)\n",
    "    current_error = round(np.mean(np.abs(test_dataframe.dollar_price - test_dataframe.predicted_dollar_price)), 3)\n",
    "    print(f\"MAE current run : {current_error}\")\n",
    "    error_list.append(current_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "73794e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean error over 5 runs:0.555\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean error over 5 runs:{np.round(np.mean(error_list),3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf2de4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-cpu.2-6.m78",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-6:m78"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
