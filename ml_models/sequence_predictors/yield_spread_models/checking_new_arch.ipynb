{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "358ab8fd",
   "metadata": {},
   "source": [
    "## Yield Spread model\n",
    "\n",
    "This notebook implements a model to predict new yield spreads from reference and trade history data. The model uses an attention layer between the two LSTM layers. The model is trained on data from 1 January 2023 to 1 March 2023. The test set is the month of March 2023. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58de4fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing pandarallel with 16.0 cores\n",
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from sklearn import preprocessing\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "\n",
    "\n",
    "from ficc.data.process_data import process_data\n",
    "from ficc.utils.auxiliary_variables import PREDICTORS, NON_CAT_FEATURES, BINARY, CATEGORICAL_FEATURES, IDENTIFIERS, PURPOSE_CLASS_DICT\n",
    "from ficc.utils.gcp_storage_functions import upload_data, download_data\n",
    "from ficc.utils.auxiliary_variables import RELATED_TRADE_BINARY_FEATURES, RELATED_TRADE_NON_CAT_FEATURES, RELATED_TRADE_CATEGORICAL_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dbb1703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 22:47:00.748586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-02 22:47:00.760035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-02 22:47:00.760676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae670dc",
   "metadata": {},
   "source": [
    "Setting the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07c11246",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"../ahmad_creds.json\"\n",
    "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440e0be5",
   "metadata": {},
   "source": [
    "Initializing BigQuery client and GCP storage client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b1d17e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client()\n",
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e06a8d",
   "metadata": {},
   "source": [
    "Declaring hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "767f6023",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_SPLIT = 0.85\n",
    "LEARNING_RATE = 0.0007\n",
    "BATCH_SIZE = 10000\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "DROPOUT = 0.10\n",
    "SEQUENCE_LENGTH = 5\n",
    "NUM_FEATURES = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb809f8",
   "metadata": {},
   "source": [
    "Checking if the treasury spreads and target attention features are present in PREDICTORS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "338b5e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ficc_treasury_spread' not in PREDICTORS:\n",
    "    PREDICTORS.append('ficc_treasury_spread')\n",
    "    NON_CAT_FEATURES.append('ficc_treasury_spread')\n",
    "if 'target_attention_features' not in PREDICTORS:\n",
    "    PREDICTORS.append('target_attention_features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090e08df",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "We grab the data from a GCP bucket. The data is prepared using the ficc python package. More insight on how the data is prepared can be found [here](https://github.com/Ficc-ai/ficc/blob/ahmad_ml/ml_models/sequence_predictors/data_prep/data_preparation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2c7302a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 24s, sys: 19.4 s, total: 1min 43s\n",
      "Wall time: 2min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem(project='eng-reactor-287421')\n",
    "with fs.open('automated_training/processed_data_new.pkl') as f:\n",
    "# with fs.open('ahmad_data/processed_data_dollar_price_2023-09-12-21:27.pkl') as f:\n",
    "    data = pd.read_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1a4e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "307481c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = temp_data[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6428cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data.trade_date >= '2023-07-01') & (data.trade_date <= '2023-07-31')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6c17c",
   "metadata": {},
   "source": [
    "#### Date range for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5680e3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2023-07-31 00:00:00')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.trade_date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "548f2ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2023-07-03 00:00:00')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.trade_date.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "859c0b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricting history to 5 trades\n"
     ]
    }
   ],
   "source": [
    "print(f'Restricting history to {SEQUENCE_LENGTH} trades')\n",
    "data.trade_history = data.trade_history.apply(lambda x: x[:SEQUENCE_LENGTH])\n",
    "data.target_attention_features = data.target_attention_features.apply(lambda x:x[:SEQUENCE_LENGTH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c81c3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 6)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.trade_history.iloc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3131cf94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target_attention_features.iloc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23c937dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values('trade_datetime', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df935f50",
   "metadata": {},
   "source": [
    "We don't give a predictions if yield is greater than 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d59af044",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['yield'] < 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "020cf2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "706764"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f524c3",
   "metadata": {},
   "source": [
    "### Creating features from trade history\n",
    "\n",
    "This implementation is an adaption of Charles's implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a533c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttype_dict = { (0,0):'D', (0,1):'S', (1,0):'P' }\n",
    "\n",
    "ys_variants = [\"max_ys\", \"min_ys\", \"max_qty\", \"min_ago\", \"D_min_ago\", \"P_min_ago\", \"S_min_ago\"]\n",
    "ys_feats = [\"_ys\", \"_ttypes\", \"_ago\", \"_qdiff\"]\n",
    "D_prev = dict()\n",
    "P_prev = dict()\n",
    "S_prev = dict()\n",
    "\n",
    "def get_trade_history_columns():\n",
    "    '''\n",
    "    This function is used to create a list of columns\n",
    "    '''\n",
    "    YS_COLS = []\n",
    "    for prefix in ys_variants:\n",
    "        for suffix in ys_feats:\n",
    "            YS_COLS.append(prefix + suffix)\n",
    "    return YS_COLS\n",
    "\n",
    "def extract_feature_from_trade(row, name, trade):\n",
    "    yield_spread = trade[0]\n",
    "    ttypes = ttype_dict[(trade[3],trade[4])] + row.trade_type\n",
    "    seconds_ago = trade[5]\n",
    "    quantity_diff = np.log10(1 + np.abs(10**trade[2] - 10**row.quantity))\n",
    "    return [yield_spread, ttypes,  seconds_ago, quantity_diff]\n",
    "\n",
    "def trade_history_derived_features(row):\n",
    "    trade_history = row.trade_history\n",
    "    trade = trade_history[0]\n",
    "    \n",
    "    D_min_ago_t = D_prev.get(row.cusip,trade)\n",
    "    D_min_ago = 9        \n",
    "\n",
    "    P_min_ago_t = P_prev.get(row.cusip,trade)\n",
    "    P_min_ago = 9\n",
    "    \n",
    "    S_min_ago_t = S_prev.get(row.cusip,trade)\n",
    "    S_min_ago = 9\n",
    "    \n",
    "    max_ys_t = trade; max_ys = trade[0]\n",
    "    min_ys_t = trade; min_ys = trade[0]\n",
    "    max_qty_t = trade; max_qty = trade[2]\n",
    "    min_ago_t = trade; min_ago = trade[5]\n",
    "    \n",
    "    for trade in trade_history[0:]:\n",
    "        #Checking if the first trade in the history is from the same block\n",
    "        if trade[5] == 0: \n",
    "            continue\n",
    " \n",
    "        if trade[0] > max_ys: \n",
    "            max_ys_t = trade\n",
    "            max_ys = trade[0]\n",
    "        elif trade[0] < min_ys: \n",
    "            min_ys_t = trade; \n",
    "            min_ys = trade[0]\n",
    "\n",
    "        if trade[2] > max_qty: \n",
    "            max_qty_t = trade \n",
    "            max_qty = trade[2]\n",
    "        if trade[5] < min_ago: \n",
    "            min_ago_t = trade; \n",
    "            min_ago = trade[5]\n",
    "            \n",
    "        side = ttype_dict[(trade[3],trade[4])]\n",
    "        if side == \"D\":\n",
    "            if trade[5] < D_min_ago: \n",
    "                D_min_ago_t = trade; D_min_ago = trade[5]\n",
    "                D_prev[row.cusip] = trade\n",
    "        elif side == \"P\":\n",
    "            if trade[5] < P_min_ago: \n",
    "                P_min_ago_t = trade; P_min_ago = trade[5]\n",
    "                P_prev[row.cusip] = trade\n",
    "        elif side == \"S\":\n",
    "            if trade[5] < S_min_ago: \n",
    "                S_min_ago_t = trade; S_min_ago = trade[5]\n",
    "                S_prev[row.cusip] = trade\n",
    "        else: \n",
    "            print(\"invalid side\", trade)\n",
    "    \n",
    "    trade_history_dict = {\"max_ys\":max_ys_t,\n",
    "                          \"min_ys\":min_ys_t,\n",
    "                          \"max_qty\":max_qty_t,\n",
    "                          \"min_ago\":min_ago_t,\n",
    "                          \"D_min_ago\":D_min_ago_t,\n",
    "                          \"P_min_ago\":P_min_ago_t,\n",
    "                          \"S_min_ago\":S_min_ago_t}\n",
    "\n",
    "    return_list = []\n",
    "    for variant in ys_variants:\n",
    "        feature_list = extract_feature_from_trade(row,variant,trade_history_dict[variant])\n",
    "        return_list += feature_list\n",
    "    \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8ce568c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.3 s, sys: 5.39 s, total: 18.7 s\n",
      "Wall time: 31.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "YS_COLS = get_trade_history_columns()\n",
    "temp = data[['cusip','trade_history','quantity','trade_type']].parallel_apply(trade_history_derived_features, axis=1)\n",
    "data[YS_COLS] = pd.DataFrame(temp.tolist(), index=data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ad9fd2",
   "metadata": {},
   "source": [
    "Adding trade history features to PREDICTORS list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70a7f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in YS_COLS:\n",
    "    if 'ttypes' in col and col not in PREDICTORS:\n",
    "        PREDICTORS.append(col)\n",
    "        CATEGORICAL_FEATURES.append(col)\n",
    "    elif col not in PREDICTORS:\n",
    "        NON_CAT_FEATURES.append(col)\n",
    "        PREDICTORS.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630f457d",
   "metadata": {},
   "source": [
    "This feature is used to check if there are any NaN values in the trade history. **It is not used to train the model**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13523b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706764\n",
      "706764\n",
      "CPU times: user 3.38 s, sys: 3.94 s, total: 7.31 s\n",
      "Wall time: 8.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(len(data))\n",
    "data['trade_history_sum'] = data.trade_history.parallel_apply(lambda x: np.sum(x))\n",
    "data = data.dropna(subset=['trade_history_sum'])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a258f7c4",
   "metadata": {},
   "source": [
    "For the purpose of plotting, not used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d69d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.purpose_sub_class.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbf9900",
   "metadata": {},
   "source": [
    "Creating new ys label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbdcd21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['new_ys'] = data['yield'] - data['new_ficc_ycl']\n",
    "data['diff_ys'] = data['new_ys'] - data['last_yield_spread']\n",
    "# data['new_ys'] = data['yield'] - data['new_real_time_ficc_ycl']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cd4f26",
   "metadata": {},
   "source": [
    "Adding additional features proposed by Charles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b123b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.last_trade_date = pd.to_datetime(data.last_trade_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3705614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['last_duration'] = (data.last_calc_date - data.last_trade_date).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e913e425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duration(coupon, ytw, years, dollar_price, peryear=2):\n",
    "    ytw = ytw.clip(0.001,np.inf)\n",
    "    c = (coupon/100) / peryear\n",
    "    y = (ytw/10000) / peryear\n",
    "    n = years * peryear\n",
    "    m = peryear\n",
    "    macaulay_duration = ((1+y) / (m*y)) - ( (1 + y + n*(c-y)) / ((m*c* ((1+y)**n - 1)) + m*y))\n",
    "    modified_duration = macaulay_duration / (1 + y)\n",
    "    dv01 = modified_duration * dollar_price / 10000\n",
    "    return dv01\n",
    "\n",
    "def add_additional_feature(data):\n",
    "    data['diff_ficc_ycl'] = data.new_ficc_ycl - data.last_ficc_ycl\n",
    "    data['diff_ficc_treasury_spread'] = data.last_ficc_ycl - (data.treasury_rate * 100)\n",
    "    data['dv01'] = duration(data.coupon, data.last_yield, data.last_duration, data.last_dollar_price)\n",
    "    data['approx_dpd'] =  data.dv01 * data.diff_ficc_ycl\n",
    "    data['overage'] =  (data.last_dollar_price + data.approx_dpd - data.next_call_price)\n",
    "    #data['de_minimis_gap'] = data.last_dollar_price - data.de_minimis_threshold\n",
    "    return data\n",
    "\n",
    "data = add_additional_feature(data)\n",
    "additional_features = ['diff_ficc_ycl','diff_ficc_treasury_spread','dv01','approx_dpd','overage']#,'de_minimis_gap']\n",
    "# for i in additional_features:\n",
    "#     if i not in NON_CAT_FEATURES:\n",
    "#         NON_CAT_FEATURES.append(i)\n",
    "#         PREDICTORS.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed96b6f7",
   "metadata": {},
   "source": [
    "Selecting a subset of features for training. PREDICTORS are the features that we are going to use to train the model. More information about the feature set can be found [here](https://github.com/Ficc-ai/ficc_python/blob/d455bd30eca18f26a2535523530facad516dd90f/ficc/utils/auxiliary_variables.py#L120). We also select a set of additonal features, which are not used in training. These features are used to uderstand the results from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37a71950",
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliary_features = ['dollar_price',\n",
    "                     'calc_date', \n",
    "                     'trade_date',\n",
    "                     'trade_datetime', \n",
    "                     'purpose_sub_class', \n",
    "                     'called_redemption_type', \n",
    "                     'calc_day_cat',\n",
    "                     'yield',\n",
    "                     'ficc_ycl',\n",
    "                     'new_ys',\n",
    "                     'trade_history_sum',\n",
    "                     'new_ficc_ycl',\n",
    "                     'days_to_refund',\n",
    "                     'last_dollar_price',\n",
    "                     'last_rtrs_control_number',\n",
    "                     'is_called',\n",
    "                     'federal_tax_status','diff_ys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bfbe065",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = data[IDENTIFIERS + PREDICTORS + auxiliary_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f6fd0a",
   "metadata": {},
   "source": [
    "Checking for missing data and NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "094bf076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706764\n",
      "706764\n"
     ]
    }
   ],
   "source": [
    "print(len(processed_data))\n",
    "processed_data.issue_amount = processed_data.issue_amount.replace([np.inf, -np.inf], np.nan)\n",
    "processed_data.dropna(inplace=True, subset=PREDICTORS)\n",
    "print(len(processed_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b93bcb",
   "metadata": {},
   "source": [
    "Here is a list of exclusions that we will be experimenting with. The model is trained with these exclusions.\n",
    "<ul>\n",
    "<li>Callable less than a year in the future\n",
    "<li>Maturity less than a year in the future and more than 30 years in the future\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10fc6a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "706764"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18e88493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_data = processed_data[(processed_data.days_to_call == 0) | (processed_data.days_to_call > np.log10(400))]\n",
    "# processed_data = processed_data[(processed_data.days_to_refund == 0) | (processed_data.days_to_refund > np.log10(400))]\n",
    "# processed_data = processed_data[(processed_data.days_to_maturity == 0) | (processed_data.days_to_maturity > np.log10(400))]\n",
    "# processed_data = processed_data[processed_data.days_to_maturity < np.log10(30000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0674277c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "706764"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1901e2a1",
   "metadata": {},
   "source": [
    "#### Fitting encoders to the categorical features. These encoders are then used to encode the categorical features of the train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d668ef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_feature_values = {'purpose_class' : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
    "                                                 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46,\n",
    "                                                 47, 48, 49, 50, 51, 52, 53],\n",
    "                              'rating' : ['A', 'A+', 'A-', 'AA', 'AA+', 'AA-', 'AAA', 'B', 'B+', 'B-', 'BB', 'BB+', 'BB-',\n",
    "                                         'BBB', 'BBB+', 'BBB-', 'CC', 'CCC', 'CCC+', 'CCC-' , 'D', 'NR', 'MR'],\n",
    "                              'trade_type' : ['D', 'S', 'P'],\n",
    "                              'incorporated_state_code' : ['AK', 'AL', 'AR', 'AS', 'AZ', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA', 'GU',\n",
    "                                                         'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME', 'MI', 'MN',\n",
    "                                                         'MO', 'MP', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM', 'NV', 'NY', 'OH',\n",
    "                                                         'OK', 'OR', 'PA', 'PR', 'RI', 'SC', 'SD', 'TN', 'TX', 'US', 'UT', 'VA', 'VI',\n",
    "                                                         'VT', 'WA', 'WI', 'WV', 'WY'] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66d92939",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = {}\n",
    "fmax = {}\n",
    "for f in CATEGORICAL_FEATURES:\n",
    "    if f in ['rating', 'incorporated_state_code', 'trade_type', 'purpose_class']:\n",
    "        fprep = preprocessing.LabelEncoder().fit(categorical_feature_values[f])\n",
    "    else:\n",
    "        fprep = preprocessing.LabelEncoder().fit(data[f].drop_duplicates())\n",
    "    fmax[f] = np.max(fprep.transform(fprep.classes_))\n",
    "    encoders[f] = fprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "985080c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.sort_values('trade_datetime',ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0957caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURES.remove('trade_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f3bb97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_CAT_FEATURES.remove('quantity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "616abfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(NON_CAT_FEATURES + BINARY) == 47"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86081f1",
   "metadata": {},
   "source": [
    "##### Converting data into format suitable for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1e5fd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input(df):\n",
    "    global encoders\n",
    "    datalist = []\n",
    "    datalist.append(np.stack(df['trade_history'].to_numpy()))\n",
    "    datalist.append(np.stack(df['target_attention_features'].to_numpy()))\n",
    "\n",
    "    noncat_and_binary = []\n",
    "    for f in NON_CAT_FEATURES + BINARY:\n",
    "        noncat_and_binary.append(np.expand_dims(df[f].to_numpy().astype('float64'), axis=1))\n",
    "    datalist.append(np.concatenate(noncat_and_binary, axis=-1))\n",
    "    \n",
    "    for f in CATEGORICAL_FEATURES:\n",
    "        encoded = encoders[f].transform(df[f])\n",
    "        datalist.append(encoded.astype('float32'))\n",
    "    \n",
    "    #adding trade type and quantity separately\n",
    "    datalist.append(encoders['trade_type'].transform(df['trade_type']).astype('float32'))\n",
    "    datalist.append(df.quantity.to_numpy().astype('float64'))\n",
    "    return datalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2b65617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.1 s, sys: 201 ms, total: 4.3 s\n",
      "Wall time: 4.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x = create_input(data)\n",
    "y = data.new_ys\n",
    "#y_train = train_dataframe.diff_ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b07435d",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ec39ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 22:50:39.510585: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-02 22:50:39.513942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-02 22:50:39.514742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-02 22:50:39.515444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-02 22:50:39.989738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-02 22:50:39.990477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-02 22:50:39.991076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-02 22:50:39.991641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13817 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('saved_model_new_architecture_2023-09-29-22-28')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4be50cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 22:50:53.367953: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14488961024 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-10-02 22:50:54.863123: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8200\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(x, batch_size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9655756",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['predicted_ys'] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "14fd79c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_results(data):\n",
    "    data['delta'] = np.abs(data.predicted_ys - data.new_ys)\n",
    "\n",
    "    investment_grade = ['AAA','AA+','AA','AA-','A+','A','A-','BBB+','BBB','BBB-']\n",
    "\n",
    "    total_mae, total_count = np.mean(data.delta), data.shape[0] \n",
    "\n",
    "    dd_mae, dd_count = np.mean(data['delta'][data.trade_type == 'D']), data[data.trade_type == 'D'].shape[0]\n",
    "    dp_mae, dp_count = np.mean(data['delta'][data.trade_type == 'P']), data[data.trade_type == 'P'].shape[0]\n",
    "    ds_mae, ds_count = np.mean(data['delta'][data.trade_type == 'S']), data[data.trade_type == 'S'].shape[0]\n",
    "\n",
    "    AAA_mae, AAA_count = np.mean(data['delta'][data.rating == 'AAA']), data[data.rating == 'AAA'].shape[0]\n",
    "    investment_grade_mae, investment_grade_count = np.mean(data['delta'][data.rating.isin(investment_grade)]), data[data.rating.isin(investment_grade)].shape[0]\n",
    "    hundred_k_mae, hundred_k_count = np.mean(data['delta'][data.par_traded >= 1e5]), data[data.par_traded >= 1e5].shape[0]\n",
    "\n",
    "    result_df = pd.DataFrame(data=[[total_mae,total_count],\n",
    "                                   [dd_mae,dd_count],\n",
    "                                   [dp_mae,dp_count],\n",
    "                                   [ds_mae,dp_count], \n",
    "                                   [AAA_mae, AAA_count], \n",
    "                                   [investment_grade_mae,investment_grade_count],\n",
    "                                   [hundred_k_mae,hundred_k_count]],\n",
    "                            columns=['Mean absolute Error','Trade count'],\n",
    "                            index = ['Entire set','Dealer-Dealer','Dealer-Purchase','Dealer-Sell','AAA','Investment Grade','Trade size > 100k'])\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d3bd14c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.88 s, sys: 269 ms, total: 2.15 s\n",
      "Wall time: 2.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_df = segment_results(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "169de210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_results_email_table(result_df, last_trade_date):\n",
    "    receiver_email = [\"eng@ficc.ai\"]\n",
    "    sender_email = \"notifications@ficc.ai\"\n",
    "    \n",
    "    msg = MIMEMultipart()\n",
    "    msg['Subject'] = f\"Mae for model trained till {last_trade_date}\"\n",
    "    msg['From'] = sender_email\n",
    "\n",
    "    html_table = result_df.to_html(index=True)\n",
    "    body = MIMEText(html_table, 'html')\n",
    "    msg.attach(body)\n",
    "\n",
    "    smtp_server = \"smtp.gmail.com\"\n",
    "    port = 587\n",
    "\n",
    "    with smtplib.SMTP(smtp_server,port) as server:\n",
    "        try:\n",
    "            server.starttls()\n",
    "            server.login(sender_email, 'ztwbwrzdqsucetbg')\n",
    "            for receiver in receiver_email:\n",
    "                server.sendmail(sender_email, receiver, msg.as_string())\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        finally:\n",
    "            server.quit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6dd8ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "send_results_email_table(result_df,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b18da637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib, ssl\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a7acbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-cpu.2-6.m78",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-6:m78"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
