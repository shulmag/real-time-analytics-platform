{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "358ab8fd",
   "metadata": {},
   "source": [
    "## Yield Spread model\n",
    "Adapted by Developer on 07/19/2022\n",
    "\n",
    "This notebook implements a model to predict yield spreads from reference and trade history data. The model uses an attention layer between the two LSTM layers. Compared to the original model, this one takes calc date as a categorical input, and the log10 durations to each possible calc-date, in days, as numerical features\n",
    "\n",
    "Last modification: Experimenting with different target trade features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58de4fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from sklearn import preprocessing\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "\n",
    "from ficc.data.process_data import process_data\n",
    "from ficc.utils.auxiliary_variables import PREDICTORS, NON_CAT_FEATURES, BINARY, CATEGORICAL_FEATURES, IDENTIFIERS, PURPOSE_CLASS_DICT\n",
    "from ficc.utils.gcp_storage_functions import upload_data, download_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f009a53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae670dc",
   "metadata": {},
   "source": [
    "Setting the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07c11246",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"eng-reactor-287421-112eb767e1b3.json\"\n",
    "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440e0be5",
   "metadata": {},
   "source": [
    "Initializing BigQuery client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b1d17e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d4563b",
   "metadata": {},
   "source": [
    "Initializing GCP storage client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92b48799",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e06a8d",
   "metadata": {},
   "source": [
    "Declaring hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "767f6023",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_SPLIT = 0.85\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 1000\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "DROPOUT = 0.01\n",
    "SEQUENCE_LENGTH = 5\n",
    "NUM_FEATURES = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0613e7e",
   "metadata": {},
   "source": [
    "#### Initializing Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10a42b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(project=\"yield_spread_model\", entity=\"ficc-ai\", name=\"window_train_till_may_2022_test_may_2022\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac0153d",
   "metadata": {},
   "source": [
    "#### Query to fetch data\n",
    "\n",
    "We create the training data from the trades which occurred between August 2021 and June 2022. All three trade directions, namely dealer-dealer (D), dealer-sells (S), and dealer-purchases (P) are included. We are limiting the training to bonds whose yield is a positive. The maturity description code is restricted to 2, to remove all muni derivatives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2610ae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_QUERY = ''' \n",
    "SELECT\n",
    "  * \n",
    "FROM\n",
    "  `eng-reactor-287421.auxiliary_views.materialized_trade_history`\n",
    "WHERE\n",
    "  yield IS NOT NULL\n",
    "  AND yield > 0\n",
    "  AND par_traded >= 10000\n",
    "  AND trade_date >= '2021-10-01'\n",
    "  AND maturity_description_code = 2\n",
    "  AND coupon_type in (8, 4, 10)\n",
    "  AND capital_type <> 10\n",
    "  AND default_exists <> TRUE\n",
    "  AND sale_type <> 4\n",
    "  AND sec_regulation IS NULL\n",
    "  AND most_recent_default_event IS NULL\n",
    "  AND default_indicator IS FALSE\n",
    "  --AND DATETIME_DIFF(trade_datetime,recent[SAFE_OFFSET(0)].trade_datetime,SECOND) < 1000000 -- 12 days to the most recent trade\n",
    "  AND msrb_valid_to_date > current_date -- condition to remove cancelled trades\n",
    "ORDER BY\n",
    "  trade_datetime DESC ''' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090e08df",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "The trade history is an array which contains the yield spread, trade type, trade size, and the number of seconds ago the trade occured. \n",
    "\n",
    "We grab the data from a GCP bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2c7302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem(project='eng-reactor-287421')\n",
    "with fs.open('ficc_training_data_latest/processed_data.pkl') as f:\n",
    "    data = pd.read_pickle(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0975ff3",
   "metadata": {},
   "source": [
    "Here is a list of what we are currently excluding: \n",
    "<ul>\n",
    "    <li>Variable coupon</li>\n",
    "    <li>Derivatives</li>\n",
    "    <li>Zero coupon bonds</li>\n",
    "    <li>Term bonds </li>\n",
    "    <li>Territories (VI, GU, PR)</li>\n",
    "    <li>Called bonds</li>\n",
    "    <li>Crossover refunding and partially pre-refunded</li>\n",
    "    <li>Maturity less than a year in the future and more than 30 years in the future</li>\n",
    "    <li>Callable less than a year in the future </li>\n",
    "    <li>Restructured debt</li>\n",
    "    <li>Defaulted securities</li>\n",
    "    <li>Private placement/subject to SEC regulation 144A</li>\n",
    "    <li>Purpose Types:</li>\n",
    "    <ul>\n",
    "        <li>Assisted_living</li>\n",
    "        <li>Continuing Care Retirement Center</li>\n",
    "        <li>Correctional facilities</li>\n",
    "        <li>Harbor/chanel</li>\n",
    "        <li>Mall</li>\n",
    "        <li>Real Estate</li>\n",
    "    </ul>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10c0435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data.days_to_call == 0) | (data.days_to_call > np.log10(400))]\n",
    "data = data[(data.days_to_refund == 0) | (data.days_to_refund > np.log10(400))]\n",
    "data = data[data.days_to_maturity < np.log10(30000)]\n",
    "data = data[data.sinking == False]\n",
    "data = data[data.incorporated_state_code != 'VI']\n",
    "data = data[data.incorporated_state_code != 'GU']\n",
    "data = data[(data.coupon_type == 8)]\n",
    "data = data[data.is_called == False]\n",
    "data = data[~data.purpose_sub_class.isin([6, 20, 22, 44, 57, 90])]\n",
    "data = data[~data.called_redemption_type.isin([18, 19])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93968b58",
   "metadata": {},
   "source": [
    "### Adding target trade features to calculate attention\n",
    "\n",
    "As a first step, we only utilize the size of the trade and the directions as features to calculate the attention. Going forward we will be adding more features like the state code, coupon, interest payment frequency, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b94c781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_mapping = {'D':[0,0], 'S':[0,1], 'P':[1,0]}\n",
    "def target_trade_processing_for_attention(row):\n",
    "    target_trade_features = []\n",
    "    target_trade_features.append(row['quantity'])\n",
    "    target_trade_features = target_trade_features + trade_mapping[row['trade_type']]\n",
    "    #target_trade_features.append(row['coupon'])\n",
    "    return np.tile(target_trade_features, (5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4754e1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.7 s, sys: 4.65 s, total: 24.3 s\n",
      "Wall time: 41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['target_attention_features'] = data.parallel_apply(target_trade_processing_for_attention, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a49df93f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.87506104, 0.        , 0.        ],\n",
       "       [4.87506104, 0.        , 0.        ],\n",
       "       [4.87506104, 0.        , 0.        ],\n",
       "       [4.87506104, 0.        , 0.        ],\n",
       "       [4.87506104, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0]['target_attention_features']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a258f7c4",
   "metadata": {},
   "source": [
    "For the purpose of plotting, not used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d69d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.purpose_sub_class.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc038ee0",
   "metadata": {},
   "source": [
    "#### Replacing the ratings with the stand alone ratings. This is done to exclude enhancements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "850a59fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.sp_stand_alone.isna(), 'sp_stand_alone'] = 'NR'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8a810f",
   "metadata": {},
   "source": [
    "For some versions of pandas you cannot intorduce new categories in a colum with dtype categorical. Thus changing it to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b5ff64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rating = data.rating.astype('str')\n",
    "data.sp_stand_alone = data.sp_stand_alone.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9ae26ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[(data.sp_stand_alone != 'NR'),'rating'] = data[(data.sp_stand_alone != 'NR')]['sp_stand_alone'].loc[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fb4facc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>sp_stand_alone</th>\n",
       "      <th>sp_long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>A+</td>\n",
       "      <td>A+</td>\n",
       "      <td>AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>A+</td>\n",
       "      <td>A+</td>\n",
       "      <td>AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>A+</td>\n",
       "      <td>A+</td>\n",
       "      <td>AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>A+</td>\n",
       "      <td>A+</td>\n",
       "      <td>AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>A+</td>\n",
       "      <td>A+</td>\n",
       "      <td>AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5010851</th>\n",
       "      <td>A+</td>\n",
       "      <td>A+</td>\n",
       "      <td>AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5010863</th>\n",
       "      <td>A+</td>\n",
       "      <td>A+</td>\n",
       "      <td>AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5010889</th>\n",
       "      <td>BBB+</td>\n",
       "      <td>BBB+</td>\n",
       "      <td>AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5010890</th>\n",
       "      <td>BBB+</td>\n",
       "      <td>BBB+</td>\n",
       "      <td>AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5010929</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>AA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        rating sp_stand_alone sp_long\n",
       "20          A+             A+      AA\n",
       "21          A+             A+      AA\n",
       "38          A+             A+      AA\n",
       "39          A+             A+      AA\n",
       "40          A+             A+      AA\n",
       "...        ...            ...     ...\n",
       "5010851     A+             A+      AA\n",
       "5010863     A+             A+      AA\n",
       "5010889   BBB+           BBB+      AA\n",
       "5010890   BBB+           BBB+      AA\n",
       "5010929      A              A      AA\n",
       "\n",
       "[200100 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[(data.sp_stand_alone != 'NR')][['rating','sp_stand_alone','sp_long']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2b9e75",
   "metadata": {},
   "source": [
    "### Add duration to all possible calc dates, as inputs to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "771b0e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for date_type in ['next_call', 'par_call', 'maturity', 'refund']:\n",
    "    data.loc[:, date_type + '_duration'] = (data[date_type + '_date'] - data.trade_date).dt.days\n",
    "    data.loc[:, date_type + '_duration'].fillna(-1.0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e7d21b",
   "metadata": {},
   "source": [
    "Selecting a subset of features that will be used to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2476c327",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'target_attention_features' not in PREDICTORS:\n",
    "    PREDICTORS.append('target_attention_features')\n",
    "if 'calc_day_cat' not in CATEGORICAL_FEATURES:\n",
    "    CATEGORICAL_FEATURES.append('calc_day_cat')\n",
    "# for date_type in ['next_call', 'par_call', 'maturity', 'refund']:\n",
    "#     if date_type + '_duration' not in PREDICTORS:\n",
    "#         PREDICTORS.append(date_type + '_duration')\n",
    "#     if date_type + '_duration' not in NON_CAT_FEATURES:\n",
    "#         NON_CAT_FEATURES.append(date_type + '_duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfbe065",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = data[IDENTIFIERS + PREDICTORS + ['dollar_price','calc_date', 'trade_date','trade_datetime', 'purpose_sub_class', 'called_redemption_type', 'muni_issue_type','yield','ficc_ycl']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8b7333",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e02d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(processed_data.rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1901e2a1",
   "metadata": {},
   "source": [
    "#### Fitting encoders to the categorical features. These encoders are then used to encode the categorical features of the train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d92939",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = {}\n",
    "fmax = {}\n",
    "for f in CATEGORICAL_FEATURES:\n",
    "    print(f)\n",
    "    fprep = preprocessing.LabelEncoder().fit(processed_data[f].drop_duplicates())\n",
    "    fmax[f] = np.max(fprep.transform(fprep.classes_))\n",
    "    encoders[f] = fprep\n",
    "    \n",
    "with open('encoders.pkl','wb') as file:\n",
    "    pickle.dump(encoders,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191fda84",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_data(storage_client, 'ficc_training_data_latest', f\"encoders.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5420226",
   "metadata": {},
   "source": [
    "#### Splitting the data into train and test sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05eefb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9daf580",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe = processed_data[(processed_data.trade_date < '07-01-2022')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataframe = processed_data[(processed_data.trade_date >= '07-01-2022')] # & (processed_data.trade_date <= '12-31-2021')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d2a0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014a4bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86081f1",
   "metadata": {},
   "source": [
    "Converting data into format suitable for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e5fd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input(df):\n",
    "    global encoders\n",
    "    datalist = []\n",
    "    datalist.append(np.stack(df['trade_history'].to_numpy()))\n",
    "    datalist.append(np.stack(df['target_attention_features'].to_numpy()))\n",
    "\n",
    "    noncat_and_binary = []\n",
    "    for f in NON_CAT_FEATURES + BINARY:\n",
    "        noncat_and_binary.append(np.expand_dims(df[f].to_numpy().astype('float32'), axis=1))\n",
    "    datalist.append(np.concatenate(noncat_and_binary, axis=-1))\n",
    "\n",
    "    for f in CATEGORICAL_FEATURES:\n",
    "        encoded = encoders[f].transform(df[f])\n",
    "        datalist.append(encoded.astype('float32'))\n",
    "        \n",
    "    return datalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b65617",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x_train = create_input(train_dataframe)\n",
    "y_train = train_dataframe.yield_spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af0cf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x_test = create_input(test_dataframe)\n",
    "y_test = test_dataframe.yield_spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295ac362",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270a622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c03994",
   "metadata": {},
   "source": [
    "#### Adapting Normalization layers to the non categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5916de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization layer for the trade history\n",
    "trade_history_normalizer = Normalization(name='Trade_history_normalizer')\n",
    "trade_history_normalizer.adapt(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43293ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization layer for the non-categorical and binary features\n",
    "noncat_binary_normalizer = Normalization(name='Numerical_binary_normalizer')\n",
    "noncat_binary_normalizer.adapt(x_train[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083c851e",
   "metadata": {},
   "source": [
    "#### Setting the seed for intialization of the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e964e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ffc279",
   "metadata": {},
   "source": [
    "#### Attention layer\n",
    "This is an implementation of a layer that calculates scaled dot product attention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a762aeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, depth):\n",
    "        super(CustomAttention, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.wq = layers.Dense(depth, name='weights_query') \n",
    "        self.wk = layers.Dense(depth, name='weights_key')\n",
    "        self.wv = layers.Dense(depth, name='weights_value')\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, v, k):\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "        scaling = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(scaling)\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=1) \n",
    "        output = tf.matmul(attention_weights, v)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def call(self, q, v, k):\n",
    "        \n",
    "        q = self.wq(q)\n",
    "        v = self.wv(v)\n",
    "        k = self.wk(k)\n",
    "\n",
    "        output = self.scaled_dot_product_attention(q, v, k)\n",
    "        \n",
    "        return output    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855af067",
   "metadata": {},
   "source": [
    "#### Implementation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ccb036",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "layer = []\n",
    "\n",
    "############## INPUT BLOCK ###################\n",
    "trade_history_input = layers.Input(name=\"trade_history_input\", \n",
    "                                   shape=(SEQUENCE_LENGTH,NUM_FEATURES), \n",
    "                                   dtype = tf.float32) \n",
    "\n",
    "target_attention_input = layers.Input(name=\"target_attention_input\", \n",
    "                                   shape=(SEQUENCE_LENGTH, 3), \n",
    "                                   dtype = tf.float32) \n",
    "\n",
    "\n",
    "inputs.append(trade_history_input)\n",
    "inputs.append(target_attention_input)\n",
    "\n",
    "inputs.append(layers.Input(\n",
    "    name=\"NON_CAT_AND_BINARY_FEATURES\",\n",
    "    shape=(len(NON_CAT_FEATURES + BINARY),)\n",
    "))\n",
    "\n",
    "\n",
    "layer.append(noncat_binary_normalizer(inputs[2]))\n",
    "####################################################\n",
    "\n",
    "\n",
    "############## TRADE HISTORY MODEL #################\n",
    "\n",
    "# Adding the time2vec encoding to the input to transformer\n",
    "lstm_layer = layers.LSTM(50, \n",
    "                         activation='tanh',\n",
    "                         input_shape=(SEQUENCE_LENGTH,NUM_FEATURES),\n",
    "                         return_sequences = True,\n",
    "                         name='LSTM')\n",
    "\n",
    "# lstm_attention_layer = layers.Attention(use_scale=True, name='attention_layer_1')\n",
    "lstm_attention_layer = CustomAttention(50)\n",
    "\n",
    "lstm_layer_2 = layers.LSTM(100, \n",
    "                           activation='tanh',\n",
    "                           input_shape=(SEQUENCE_LENGTH,50),\n",
    "                           return_sequences = False,\n",
    "                           name='LSTM_2')\n",
    "\n",
    "\n",
    "features = lstm_layer(trade_history_normalizer(inputs[0]))\n",
    "# features = lstm_attention_layer([features, features])\n",
    "features = lstm_attention_layer(features, features, inputs[1])\n",
    "features = layers.BatchNormalization()(features)\n",
    "# features = layers.Dropout(DROPOUT)(features)\n",
    "\n",
    "features = lstm_layer_2(features)\n",
    "features = layers.BatchNormalization()(features)\n",
    "# features = layers.Dropout(DROPOUT)(features)\n",
    "\n",
    "trade_history_output = layers.Dense(100, \n",
    "                                    activation='relu')(features)\n",
    "\n",
    "####################################################\n",
    "\n",
    "############## REFERENCE DATA MODEL ################\n",
    "global encoders\n",
    "for f in CATEGORICAL_FEATURES:\n",
    "    fin = layers.Input(shape=(1,), name = f)\n",
    "    inputs.append(fin)\n",
    "    embedded = layers.Flatten(name = f + \"_flat\")( layers.Embedding(input_dim = fmax[f]+1,\n",
    "                                                                    output_dim = max(30,int(np.sqrt(fmax[f]))),\n",
    "                                                                    input_length= 1,\n",
    "                                                                    name = f + \"_embed\")(fin))\n",
    "    layer.append(embedded)\n",
    "\n",
    "    \n",
    "reference_hidden = layers.Dense(400,\n",
    "                                activation='relu',\n",
    "                                name='reference_hidden_1')(layers.concatenate(layer, axis=-1))\n",
    "\n",
    "reference_hidden = layers.BatchNormalization()(reference_hidden)\n",
    "reference_hidden = layers.Dropout(DROPOUT)(reference_hidden)\n",
    "\n",
    "reference_hidden2 = layers.Dense(200,activation='relu',name='reference_hidden_2')(reference_hidden)\n",
    "reference_hidden2 = layers.BatchNormalization()(reference_hidden2)\n",
    "reference_hidden2 = layers.Dropout(DROPOUT)(reference_hidden2)\n",
    "\n",
    "reference_output = layers.Dense(100,activation='tanh',name='reference_hidden_3')(reference_hidden2)\n",
    "\n",
    "####################################################\n",
    "\n",
    "feed_forward_input = layers.concatenate([reference_output, trade_history_output])\n",
    "\n",
    "hidden = layers.Dense(300,activation='relu')(feed_forward_input)\n",
    "hidden = layers.BatchNormalization()(hidden)\n",
    "hidden = layers.Dropout(DROPOUT)(hidden)\n",
    "\n",
    "hidden2 = layers.Dense(100,activation='tanh')(hidden)\n",
    "hidden2 = layers.BatchNormalization()(hidden2)\n",
    "hidden2 = layers.Dropout(DROPOUT)(hidden2)\n",
    "\n",
    "final = layers.Dense(1)(hidden2)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7228b195",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a879474",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    show_shapes=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"LR\",\n",
    "    expand_nested=False,\n",
    "    dpi=96,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef3f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)\n",
    "\n",
    "time_callback = TimeHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ccdeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_callbacks = [\n",
    "    #WandbCallback(save_model=False),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=10,\n",
    "        verbose=0,\n",
    "        mode=\"auto\",\n",
    "        restore_best_weights=True,\n",
    "    ),\n",
    "    time_callback\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8252f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "          loss=keras.losses.MeanAbsoluteError(),\n",
    "          metrics=[keras.metrics.MeanAbsoluteError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54876f0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "history = model.fit(x_train, \n",
    "                    y_train, \n",
    "                    epochs=100, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    verbose=1, \n",
    "                    validation_split=0.1, \n",
    "                    callbacks=fit_callbacks,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb21f0a9",
   "metadata": {},
   "source": [
    "#### Plotting train vs validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7715f867",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(history.history['val_loss'])),history.history['val_loss'], label='val_loss')\n",
    "plt.plot(range(len(history.history['loss'])),history.history['loss'], label='loss')\n",
    "plt.title('Validation loss and training Loss per epoch')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7ddf58",
   "metadata": {},
   "source": [
    "#### Gigaflops for one epoch of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e370b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_time = np.mean(time_callback.times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e91d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(x_train[0])\n",
    "p = model.count_params()\n",
    "avg_time = np.mean(time_callback.times)\n",
    "gflops = ((n*p*2*3)/avg_time)/10**9\n",
    "\n",
    "print(gflops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab75917",
   "metadata": {},
   "source": [
    "### Test accuracy on the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb19c56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3588c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, mae = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(f\"Test loss: {round(mae, 3)}\")\n",
    "# wandb.log({\"Test MAE\": mae})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7484a43e",
   "metadata": {},
   "source": [
    "### Accuracy on a daily basis\n",
    "\n",
    "Measuring the daily accuracy for large dealer dealer trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4519357",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in [d for d in pd.date_range(start=\"06/01/2022\",end=\"07/31/2022\",freq='D')]:\n",
    "    next_day = test_dataframe[(test_dataframe.trade_date == d) &\n",
    "                              (test_dataframe.trade_type == 'D') &\n",
    "                              (test_dataframe.quantity >= np.log10(500000)) & \n",
    "                              (test_dataframe.coupon == 5) ].copy()\n",
    "    if len(next_day) == 0:\n",
    "        continue\n",
    "    next_day_test = create_input(next_day)  \n",
    "    next_day_preds = model.predict(next_day_test)\n",
    "    error = next_day.yield_spread - next_day_preds.reshape(-1)\n",
    "    MAE = np.mean(np.abs(error))\n",
    "    print(f\"Date :{d.date()} MAE:{MAE}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e5f0a0",
   "metadata": {},
   "source": [
    "### Test accuracy on large dealer-dealer trades\n",
    "We define large as any trade which is above $500,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb9c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_mid = test_dataframe[(test_dataframe.trade_type == 'D') & (test_dataframe.quantity >= np.log10(500000)) & (test_dataframe.coupon == 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e728eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(true_mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e6e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_mid.value_counts('rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8658ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_mid = true_mid[(true_mid.rating == 'AAA') | (true_mid.rating == 'MR')]# & (true_mid.muni_issue_type != 14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cffee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_mid = true_mid[true_mid.trade_date > '12-15-2021']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d139b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(true_mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd8689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x_true_mid = create_input(true_mid)\n",
    "y_true_mid = true_mid.yield_spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82da545",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, mae = model.evaluate(x_true_mid, y_true_mid, verbose=1)\n",
    "print(f\"Test MAE: {round(mae, 3)}\")\n",
    "# wandb.log({\"Dealer Dealer true mid Test MAE\": mae})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8041f74a",
   "metadata": {},
   "source": [
    "### Alternative evaluation set\n",
    "Experiment with other conditions. From the error analysis notebook, it was observed that the number of accrued days that have passed since the beginning of interest being accrued is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5798bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_test = test_dataframe[(test_dataframe['coupon'] == 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966069c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(temp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59426341",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x_temp_test = create_input(temp_test)\n",
    "y_temp_test = temp_test.yield_spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68ef76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, mae = model.evaluate(x_temp_test, y_temp_test, verbose=1)\n",
    "print(f\"Test MAE: {round(mae, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9711a506",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42ce3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e5c452",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = False\n",
    "\n",
    "if save_model:\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "    file_timestamp = datetime.now().strftime('%Y-%m-%d-%H-%M')\n",
    "    print(f\"file time stamp : {file_timestamp}\")\n",
    "\n",
    "    print(\"Saving encoders and uploading encoders\")\n",
    "    with open(f\"encoders_{file_timestamp}.pkl\",'wb') as file:\n",
    "        pickle.dump(encoders,file)    \n",
    "    upload_data(storage_client, 'ficc_encoders', f\"encoders_{file_timestamp}.pkl\")\n",
    "\n",
    "    print(\"Saving and uploading model\")\n",
    "    model.save(f\"saved_model_calc_date_conditioned_june_{file_timestamp}\")\n",
    "    import shutil\n",
    "    shutil.make_archive(f\"model_calc_date_conditioned_jul\", 'zip', f\"saved_model_calc_date_conditioned_june_{file_timestamp}\")\n",
    "    upload_data(storage_client, 'ficc_training_data_latest', f\"model_calc_date_conditioned_jul.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b2fe93",
   "metadata": {},
   "source": [
    "# Evaluate the ability for a calc-date conditioned model to determine the final price (TODO)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-cpu.2-6.m78",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-6:m78"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ficc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2266f9d190a7b880f073f634cb88d3a7b9a2324a600bb63b91ff505e59b5d8d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
