{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model\n",
    "\n",
    "\n",
    "Base line model that runs a linear regression on the yiled spreas sequence to predict the yield spread in the next timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_SPLIT = 0.85\n",
    "SEQUENCE_LENGTH = 32\n",
    "STEP_SIZE = 1\n",
    "DAYS_AGO_PROCESSING = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query for BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_QUERY = \"\"\" SELECT\n",
    "  --Combination of rtrs_control_number and publish_datetime uniquely identifies the trade messages\n",
    "  --and therefore we are groupping transaction messages based on these two fields\n",
    "  --Note: there are few records that have same rtrs_control_number and publish_datetime, but different sequence_number and transaction_type that need to be explored later (e.g. 2020120307600500, 2021020407278700, 2020111902310500, 2021020201831300)\n",
    "  latest.rtrs_control_number,\n",
    "  latest.publish_datetime,\n",
    "  --Previous transactions\n",
    "  ARRAY_AGG(STRUCT (past.rtrs_control_number,\n",
    "      past.cusip,\n",
    "      past.trade_datetime,\n",
    "      past.publish_datetime,\n",
    "      past.msrb_valid_from_date,\n",
    "      past.msrb_valid_to_date,\n",
    "      past.yield_spread,\n",
    "      past.yield,\n",
    "      past.dollar_price,\n",
    "      past.par_traded,\n",
    "      past.trade_type,\n",
    "      DATE_DIFF(latest.trade_date,past.trade_date, day) AS days_ago )\n",
    "  ORDER BY\n",
    "    past.trade_datetime ASC ) AS recent\n",
    "FROM\n",
    "  eng-reactor-287421.MSRB.msrb_transforms latest\n",
    "LEFT JOIN\n",
    "  eng-reactor-287421.MSRB.msrb_transforms past\n",
    "ON\n",
    "  latest.cusip = past.cusip\n",
    "  AND latest.trade_datetime BETWEEN past.msrb_valid_from_date\n",
    "  AND past.msrb_valid_to_date\n",
    "WHERE\n",
    "  --filter to show the most recent message for each trade\n",
    "  latest.MSRB_INST_ORDR_DESC = 1\n",
    "  --This date can be updated show all trades that has accored during the desired dates\n",
    "  AND past.trade_date >= '2021-01-01'\n",
    "  AND past.trade_date < '2021-04-01'\n",
    "GROUP BY\n",
    "  latest.rtrs_control_number,\n",
    "  latest.publish_datetime\n",
    "LIMIT\n",
    "  10000\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataLoader class grabs the data from BigQuery and returns the dataset as a [tensorflow dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset). The query creates an array of all trades for every CUSIP. The query drops the trades that have been canceled. We calculate the yield spreads by taking the diffrence of the bond's yield and the yield of the [s&p muni bond index](https://www.spglobal.com/spdji/en/indices/fixed-income/sp-municipal-bond-index/#overview). To test the implementation I have limited the number of rows to 200, this can easily be changed by removing the limit in the DATA_QUERY.\n",
    "\n",
    "The main driver method of the class is the processData function, which has been implemented as a class method. I decided to implement it as a class method as it can be easily called in other files without creating an instance for the class\n",
    "\n",
    "The dataset is split into training and testing, with 85% of the data being used to train the model. The parameter that decides the ratio of train test split are defined in the cells above. We create a sequence of 5 trades and feed that into the model. The sequence parameter is defined as a hyper-parameter and can be easily changed. The sequence contains the yield spreads, prices, par traded value, the type of trade, and the number of days ago the trade was executed. Additional features can be added with a few minor tweaks to the source code.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    '''\n",
    "    Class to load the data from big query \n",
    "    and process it to create train and test data\n",
    "    '''\n",
    "    def __init__(self,query,client):\n",
    "        self.query = query\n",
    "        self.trade_dataframe = None\n",
    "        self.client = client\n",
    "        \n",
    "    @staticmethod\n",
    "    def createSequence(x):\n",
    "        '''\n",
    "        Creates sequence of historical trades\n",
    "        x : list\n",
    "        '''\n",
    "        chunks = [x[base:base+SEQUENCE_LENGTH] for base in range(0,len(x), STEP_SIZE) if len(x[base:base+SEQUENCE_LENGTH]) == SEQUENCE_LENGTH]\n",
    "        return chunks\n",
    "\n",
    "    @staticmethod\n",
    "    def tradeDictToList(trade_dict: dict) -> list:\n",
    "        '''\n",
    "        This function converts the recent trades dictionary\n",
    "        to a list\n",
    "\n",
    "        parameters:\n",
    "        trade_dict : dict\n",
    "        '''\n",
    "        trade_list = []\n",
    "        \n",
    "        if DAYS_AGO_PROCESSING is None:\n",
    "            trade_list.append(trade_dict['days_ago'])\n",
    "        elif DAYS_AGO_PROCESSING.upper() == 'LOG':\n",
    "            trade_list.append(np.log(1 + trade_dict['days_ago']))\n",
    "        elif DAYS_AGO_PROCESSING.upper() == 'SQRT':\n",
    "            trade_list.append(np.sqrt(trade_dict['days_ago']))\n",
    "        else:\n",
    "            raise NotImplementedError(\"The provided processing type does not match any implemented\")\n",
    "            \n",
    "        trade_list.append(trade_dict['yield_spread'] * 100)\n",
    "        \n",
    "        \n",
    "        # A few blunt normalizations will experiment with others as well\n",
    "        # Multiplying the yield spreads by 100 to convert to basis points\n",
    "        return trade_list \n",
    "        \n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def tradeListToArray(trade_history):\n",
    "        '''\n",
    "        parameters:\n",
    "        trade_history - list\n",
    "        \n",
    "        The floating values from BigQuery come as\n",
    "        decimal data type. We convert it to a 32 bit\n",
    "        float\n",
    "        '''\n",
    "        if len(trade_history) == 0:\n",
    "            return np.array([])\n",
    "    \n",
    "        trades_list = [DataLoader.tradeDictToList(entry) for entry in trade_history]\n",
    "        return np.stack(trades_list)\n",
    "    \n",
    "    @staticmethod\n",
    "    def getFeatures(x,ind) -> str:\n",
    "        '''\n",
    "        Returns the feature sequence\n",
    "        parameters:\n",
    "        x - list\n",
    "        ind - int\n",
    "        '''\n",
    "        return ','.join(map(str, [i[ind] for i in x]))  \n",
    "    \n",
    "    @staticmethod\n",
    "    def create_features(x):\n",
    "        return x[:SEQUENCE_LENGTH - 1]\n",
    "\n",
    "    @staticmethod\n",
    "    def create_target(x):\n",
    "        return x[SEQUENCE_LENGTH-1][1]\n",
    "\n",
    "    def fetchData(self):         \n",
    "        if os.path.isfile('base.pkl'):\n",
    "            self.trade_dataframe = pd.read_pickle('base.pkl')\n",
    "        else:\n",
    "            self.trade_dataframe = self.client.query(self.query).result().to_dataframe()\n",
    "            \n",
    "        self.trade_dataframe['trade_history'] = self.trade_dataframe.recent.apply(self.tradeListToArray)\n",
    "        self.trade_dataframe.drop(columns=['recent'],inplace=True)\n",
    "    \n",
    "    # Class functions do not need an instance of the calss to be called.\n",
    "    # They are mehtods associated with the class and not the instance\n",
    "    # and can be called by the class directly\n",
    "    @classmethod\n",
    "    def processData(cls,query,client):\n",
    "        '''\n",
    "        Class method to process training and test data\n",
    "        This function queries the \n",
    "        '''\n",
    "        instance = cls(query,client)\n",
    "        instance.fetchData()\n",
    "            \n",
    "        instance.trade_dataframe.trade_history = instance.trade_dataframe.trade_history.apply(instance.createSequence) \n",
    "        instance.trade_dataframe = instance.trade_dataframe[['rtrs_control_number','trade_history']].explode(\"trade_history\",ignore_index=True)\n",
    "        instance.trade_dataframe = instance.trade_dataframe.dropna()\n",
    "        \n",
    "        instance.trade_dataframe['features'] = instance.trade_dataframe['trade_history'].apply(instance.create_features)\n",
    "        instance.trade_dataframe['target'] = instance.trade_dataframe['trade_history'].apply(instance.create_target)\n",
    "        instance.trade_dataframe = instance.trade_dataframe.drop(columns=['trade_history'])\n",
    "        \n",
    "        if len(instance.trade_dataframe.iloc[0].features.shape) > 1:\n",
    "            instance.trade_dataframe.features = instance.trade_dataframe.features.apply(lambda x: x.flatten())\n",
    "        \n",
    "        random_selection = np.random.rand(len(instance.trade_dataframe.index)) <= TRAIN_TEST_SPLIT\n",
    "        train_data = instance.trade_dataframe[random_selection]\n",
    "        test_data = instance.trade_dataframe[~random_selection]\n",
    "        return train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe, test_dataframe = DataLoader.processData(DATA_QUERY,bq_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe = train_dataframe.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.stack(train_dataframe.features.to_numpy())\n",
    "target = train_dataframe.target.to_numpy()\n",
    "\n",
    "test_data = np.stack(test_dataframe.features.to_numpy())\n",
    "test_target =  test_dataframe.target.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression(fit_intercept=False).fit(train_data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_target= reg.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.193020739081778"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(test_target,predicted_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.47387869,  0.02912572,  0.84481038, -0.03643999,  0.59516149,\n",
       "        0.17631237, -1.08789602,  0.01165806, -0.4696235 , -0.09141885,\n",
       "        1.23573229, -0.01088993, -0.63246414,  0.08061162, -1.30069041,\n",
       "       -0.09451323,  0.92780613, -0.02184098,  0.44507619,  0.11487903,\n",
       "       -0.13335717, -0.07788781,  0.69930775,  0.07081154, -1.06834229,\n",
       "       -0.14576499,  0.28378074,  0.08758424,  0.63579619,  0.00759879,\n",
       "       -0.42647782,  0.02603858, -0.19117437, -0.05901626,  0.03572699,\n",
       "       -0.11834497, -0.01381655,  0.20872997,  0.36427898,  0.00189678,\n",
       "       -0.63728922, -0.03708282, -0.19395008,  0.11366395,  0.43969185,\n",
       "        0.11015604, -0.18291194,  0.01576893,  0.02793643,  0.00323986,\n",
       "        0.13084619,  0.06293257,  0.45619042, -0.02209613, -0.04776599,\n",
       "        0.06647938,  1.08867538,  0.1594163 , -0.84623941,  0.03080845,\n",
       "       -0.40889909,  0.29119417])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
