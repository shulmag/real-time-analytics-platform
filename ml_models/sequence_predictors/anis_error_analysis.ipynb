{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ficc.utils.auxiliary_variables import *\n",
    "from ficc.data.queries import *\n",
    "from ficc.models import get_model_instance\n",
    "import ficc.utils.attr as attr\n",
    "from ficc.utils.eval import eval_model\n",
    "from ficc.utils.gcp_storage_functions import *\n",
    "from ficc.utils.plotting import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "import gcsfs\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seed for layer initializer. We want the layers to be initialized with the same values in all the experiments to remove randomness from the results\n",
    "SEED = 10\n",
    "\n",
    "pl.utilities.seed.seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the credentials for GCP\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"eng-reactor-287421-112eb767e1b3.json\"\n",
    "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "\n",
    "# Initializing the big query client\n",
    "bq_client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The batch size and learning rate have an impact on the smoothness of convergence of the model.\\\n",
    "# Larger the batch size the smoother the convergence. For a larger batch size we need a higher learning rate and vice-versa\n",
    "TRAIN_TEST_SPLIT = 0.85\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 10000\n",
    "NUM_EPOCHS = 250\n",
    "\n",
    "DROPOUT = 0.37909650481643176\n",
    "SEQUENCE_LENGTH = 5\n",
    "NUM_FEATURES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = gcsfs.GCSFileSystem(project='eng-reactor-287421')\n",
    "with fs.open('ficc_training_data_latest/processed_data.pkl') as f:\n",
    "    df = pd.read_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.purpose_class.fillna(1, inplace=True)\n",
    "\n",
    "df = df[~df.purpose_sub_class.isin([6, 20, 21, 22, 44, 57, 90, 106])]\n",
    "df = df[~df.called_redemption_type.isin([18, 19])]\n",
    "\n",
    "# Add additional features\n",
    "processed_data = df[IDENTIFIERS + PREDICTORS + ['trade_datetime', 'next_call_date', 'par_call_date', 'maturity_date', 'refund_date']]\n",
    "\n",
    "# A few features such as the initial issue amount cannot be filled with their logical counterparts as their values are not known and hence are dropped.\n",
    "processed_data = processed_data.dropna()\n",
    "\n",
    "# Uniform normalization\n",
    "for f in NON_CAT_FEATURES + BINARY:\n",
    "    processed_data[f] = preprocessing.scale(processed_data[f].to_numpy().astype('float32'))\n",
    "\n",
    "# Fitting encoders to the categorical features. These encoders are then used to encode the categorical features of the train and test set\n",
    "fmax = {}\n",
    "with fs.open('ficc_training_data_latest/encoders.pkl') as f:\n",
    "    encoders = pickle.load(f)\n",
    "for f in CATEGORICAL_FEATURES:\n",
    "    fprep = encoders[f]\n",
    "    fmax[f] = np.max(fprep.transform(fprep.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The create input function encodes the categorical features. It then combines the trade history, categorical, non-categorical, and binary features to return a NumPy array containing the data to be fed into the model.\n",
    "def create_input(df):\n",
    "    sdf = df[IDENTIFIERS + PREDICTORS]\n",
    "\n",
    "    global encoders\n",
    "    datalist = []\n",
    "    datalist.append(torch.tensor(np.stack(sdf['trade_history'])).float())\n",
    "\n",
    "    noncat_and_binary = []\n",
    "    for f in NON_CAT_FEATURES + BINARY:\n",
    "        noncat_and_binary.append(np.expand_dims(\n",
    "            sdf[f].to_numpy().astype('float32'), axis=1))\n",
    "    datalist.append(torch.tensor(np.concatenate(noncat_and_binary, axis=-1)))\n",
    "\n",
    "    for f in CATEGORICAL_FEATURES:\n",
    "        encoded = encoders[f].transform(sdf[f])\n",
    "        datalist.append(torch.tensor(encoded).long())\n",
    "\n",
    "    return datalist\n",
    "\n",
    "def create_label(df):\n",
    "    sdf = df[IDENTIFIERS + PREDICTORS]\n",
    "    return torch.tensor(sdf.yield_spread.to_numpy()).float()\n",
    "\n",
    "# Splitting the date into train and test set\n",
    "train_index = int(len(processed_data) * (1.0 - TRAIN_TEST_SPLIT))\n",
    "test_dataframe = processed_data[:train_index]\n",
    "train_dataframe = processed_data[train_index:]\n",
    "\n",
    "# Split the training data in to train and validation set\n",
    "split_point = int(len(train_dataframe) * (1.0 - 0.9))\n",
    "val_dataframe = train_dataframe[:split_point]\n",
    "train_dataframe = train_dataframe[split_point:]\n",
    "\n",
    "x_train = create_input(train_dataframe)\n",
    "y_train = create_label(train_dataframe)\n",
    "train_dataset = TensorDataset(*x_train, y_train)\n",
    "\n",
    "x_val = create_input(val_dataframe)\n",
    "y_val = create_label(val_dataframe)\n",
    "val_dataset = TensorDataset(*x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'num_trade_history_features': NUM_FEATURES,\n",
    "    'non_categorical_size': NON_CAT_FEATURES + BINARY,\n",
    "    'category_sizes': fmax,\n",
    "    'lstm_sizes': [50, 100],\n",
    "    'embed_sizes': 15,\n",
    "    'tabular_sizes': [400, 200, 100],\n",
    "    'tabular_resblocks': 1,\n",
    "    'final_sizes': [300, 100],\n",
    "    'final_resblocks': 0,\n",
    "    'dropout': 0.3758110031582248,\n",
    "    'learning_schedule': 'constant', \n",
    "    'learning_rate': 0.00017360566254027907, \n",
    "    'weight_decay': 0.00039784787398219684\n",
    "}\n",
    "\n",
    "model = get_model_instance(\n",
    "    \"lstm_yield_spread_model_pytorch\",\n",
    "    **model_params)\n",
    "\n",
    "# Reload the checkpoint of the best model, to this point\n",
    "model = model.load_from_checkpoint(\n",
    "    checkpoint_path=\"best_ys_model.ckpt\",\n",
    "    **model_params\n",
    ")\n",
    "\n",
    "eval_model(model, test_dataframe, create_input, create_label, wandb = None)\n",
    "\n",
    "x_test = create_input(test_dataframe)\n",
    "y_test = create_label(test_dataframe)\n",
    "predictions = model(*x_test)\n",
    "\n",
    "test_dataframe.loc[:, 'ficc_spread'] = predictions.detach().numpy()\n",
    "test_dataframe.loc[:, 'abs_delta_yield_spread'] = (test_dataframe.yield_spread - test_dataframe.ficc_spread).abs()\n",
    "\n",
    "test_dataframe.sort_values(by='abs_delta_yield_spread', ascending=False, inplace=True)\n",
    "print(test_dataframe.loc[:, 'abs_delta_yield_spread'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_yield_spread_analysis(test_dataframe):\n",
    "    from IPython.display import display, Markdown\n",
    "\n",
    "    cusip_set = set()\n",
    "    for _, row in test_dataframe.iterrows():\n",
    "        cusip_dataframe = test_dataframe[test_dataframe.cusip == row['cusip']]\n",
    "\n",
    "        if (len(cusip_dataframe) > 1) and (row['cusip'] not in cusip_set):\n",
    "            display(\n",
    "                Markdown(\n",
    "                    f\"## CUSIP: {row['cusip']} - RTRS Control Numbers: {cusip_dataframe.iloc[0]['rtrs_control_number']} and {cusip_dataframe.iloc[-1]['rtrs_control_number']}\\n\" +\n",
    "                    f\"### Worst Error: {cusip_dataframe.iloc[0, :].abs_delta_yield_spread}\\n\" +\n",
    "                    f\"### Best Error: {cusip_dataframe.iloc[-1, :].abs_delta_yield_spread}\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            x_test = create_input(cusip_dataframe)\n",
    "            y_test = create_label(cusip_dataframe)\n",
    "\n",
    "            plot_cusip(cusip_dataframe, 'yield_spread')\n",
    "\n",
    "            attributes = attr.compute_integrated_gradient_attributions(model, x_test)\n",
    "            error_attributes = attr.compute_integrated_gradient_ysm_error_attributions(model, x_test, y_test)\n",
    "\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(30, 10))\n",
    "            fig.suptitle('Yield-Spread Trade History Attributions')\n",
    "            attr.visualize_trade_history_attribution(attributes[0][0], subtitle=\"Worst\", fig_ax=(fig, ax[0]))\n",
    "            attr.visualize_trade_history_attribution(attributes[0][-1], subtitle=\"Best\", fig_ax=(fig, ax[1]))\n",
    "\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(30, 10))\n",
    "            fig.suptitle('Yield-Spread Squared Trade History ERROR Attributions')\n",
    "            attr.visualize_trade_history_attribution(error_attributes[0][0], subtitle=\"Worst\", fig_ax=(fig, ax[0]))\n",
    "            attr.visualize_trade_history_attribution(error_attributes[0][-1], subtitle=\"Best\", fig_ax=(fig, ax[1]))\n",
    "\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(30, 5))\n",
    "            fig.suptitle('Yield-Spread Numerical Feature Attributions')\n",
    "            attr.visualize_trade_numerical_and_binary_attribution(attributes[1][0], NON_CAT_FEATURES, BINARY, subtitle=\"Worst\", fig_ax=(fig, ax[0]))\n",
    "            attr.visualize_trade_numerical_and_binary_attribution(attributes[1][-1], NON_CAT_FEATURES, BINARY, subtitle=\"Best\", fig_ax=(fig, ax[1]))\n",
    "\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(30, 5))\n",
    "            fig.suptitle('Yield-Spread Squared Numerical Feature ERROR Attributions')\n",
    "            attr.visualize_trade_numerical_and_binary_attribution(error_attributes[1][0], NON_CAT_FEATURES, BINARY, subtitle=\"Worst\", fig_ax=(fig, ax[0]))\n",
    "            attr.visualize_trade_numerical_and_binary_attribution(error_attributes[1][-1], NON_CAT_FEATURES, BINARY, subtitle=\"Best\", fig_ax=(fig, ax[1]))\n",
    "\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(20, 3))\n",
    "            ax.set_axis_off()\n",
    "            values = x_test[1][(0, -1), :].cpu().numpy()\n",
    "            tbl = ax.table(cellText=values.T, rowLabels=NON_CAT_FEATURES + BINARY, colLabels=[\"Worst\", \"Best\"], loc='center')\n",
    "            for i in range(len(NON_CAT_FEATURES + BINARY) + 1):\n",
    "                if i % 2:\n",
    "                    for j in range(2):\n",
    "                        tbl[(i, j)].set_facecolor(\"#dddddd\")\n",
    "\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(30, 3))\n",
    "            fig.suptitle('Yield-Spread Categorical Feature Attributions')\n",
    "            attr.visualize_categorical_attribution(torch.cat([a.unsqueeze(1) for a in attributes[2:]], dim=-1)[0], CATEGORICAL_FEATURES, subtitle=\"Worst\", fig_ax=(fig, ax[0]), values=[x[0].cpu().numpy() for x in x_test[2:]])\n",
    "            attr.visualize_categorical_attribution(torch.cat([a.unsqueeze(1) for a in attributes[2:]], dim=-1)[-1], CATEGORICAL_FEATURES, subtitle=\"Best\", fig_ax=(fig, ax[1]), values=[x[-1].cpu().numpy() for x in x_test[2:]])\n",
    "\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(30, 3))\n",
    "            fig.suptitle('Yield-Spread Squared Categorical Feature ERROR Attributions')\n",
    "            attr.visualize_categorical_attribution(torch.cat([a.unsqueeze(1) for a in error_attributes[2:]], dim=-1)[0], CATEGORICAL_FEATURES, subtitle=\"Worst\", fig_ax=(fig, ax[0]), values=[x[0].cpu().numpy() for x in x_test[2:]])\n",
    "            attr.visualize_categorical_attribution(torch.cat([a.unsqueeze(1) for a in error_attributes[2:]], dim=-1)[-1], CATEGORICAL_FEATURES, subtitle=\"Best\", fig_ax=(fig, ax[1]), values=[x[-1].cpu().numpy() for x in x_test[2:]])\n",
    "\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(20, 3))\n",
    "            ax.set_axis_off()\n",
    "            values = [[x[i].cpu().numpy() for i in (0, -1)] for x in x_test[2:]]\n",
    "            tbl = ax.table(cellText=values, rowLabels=CATEGORICAL_FEATURES, colLabels=[\"Worst\", \"Best\"], loc='center')\n",
    "            for i in range(len(NON_CAT_FEATURES + BINARY) + 1):\n",
    "                if i % 2:\n",
    "                    for j in range(2):\n",
    "                        tbl[(i, j)].set_facecolor(\"#dddddd\")\n",
    "\n",
    "            cusip_set.add(row['cusip'])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        yield\n",
    "\n",
    "yield_spread_analysis_generator = yield_yield_spread_analysis(test_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display the 10 cusips with the worst yield-spread predictions\n",
    "## Includes analysis of the cusip's best vs. worst predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(yield_spread_analysis_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(yield_spread_analysis_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(yield_spread_analysis_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(yield_spread_analysis_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(yield_spread_analysis_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(yield_spread_analysis_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(yield_spread_analysis_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(yield_spread_analysis_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(yield_spread_analysis_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(yield_spread_analysis_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The create input function encodes the categorical features. It then combines the trade history, categorical, non-categorical, and binary features to return a NumPy array containing the data to be fed into the model.\n",
    "def create_input(df):\n",
    "    sdf = df[IDENTIFIERS + PREDICTORS]\n",
    "\n",
    "    global encoders\n",
    "    datalist = []\n",
    "    datalist.append(torch.tensor(np.stack(sdf['trade_history'])).float())\n",
    "\n",
    "    noncat_and_binary = []\n",
    "    for f in NON_CAT_FEATURES + BINARY:\n",
    "        noncat_and_binary.append(np.expand_dims(\n",
    "            sdf[f].to_numpy().astype('float32'), axis=1))\n",
    "    datalist.append(torch.tensor(np.concatenate(noncat_and_binary, axis=-1)))\n",
    "\n",
    "    for f in CATEGORICAL_FEATURES:\n",
    "        encoded = encoders[f].transform(sdf[f])\n",
    "        datalist.append(torch.tensor(encoded).long())\n",
    "\n",
    "    return datalist\n",
    "\n",
    "def create_ys_label(df):\n",
    "    sdf = df[IDENTIFIERS + PREDICTORS]\n",
    "    return torch.tensor(sdf.yield_spread.to_numpy()).float()\n",
    "\n",
    "def create_cdc_label(df):\n",
    "    sdf = df[IDENTIFIERS + PREDICTORS]\n",
    "    return torch.tensor(sdf.calc_day_cat.to_numpy()).int()\n",
    "\n",
    "def create_ys_cdc_label(df):\n",
    "    sdf = df[IDENTIFIERS + PREDICTORS]\n",
    "    return create_ys_label(sdf), create_cdc_label(df)\n",
    "\n",
    "# Splitting the date into train and test set\n",
    "train_index = int(len(processed_data) * (1.0 - TRAIN_TEST_SPLIT))\n",
    "test_dataframe = processed_data[:train_index]\n",
    "train_dataframe = processed_data[train_index:]\n",
    "\n",
    "# Split the training data in to train and validation set\n",
    "split_point = int(len(train_dataframe) * (1.0 - 0.9))\n",
    "val_dataframe = train_dataframe[:split_point]\n",
    "train_dataframe = train_dataframe[split_point:]\n",
    "\n",
    "x_train = create_input(train_dataframe)\n",
    "y_train = create_cdc_label(train_dataframe)\n",
    "train_dataset = TensorDataset(*x_train, y_train)\n",
    "\n",
    "x_val = create_input(val_dataframe)\n",
    "y_val = create_cdc_label(val_dataframe)\n",
    "val_dataset = TensorDataset(*x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'num_trade_history_features': NUM_FEATURES,\n",
    "    'non_categorical_size': NON_CAT_FEATURES + BINARY,\n",
    "    'category_sizes': fmax,\n",
    "    'lstm_sizes': [50, 100],\n",
    "    'embed_sizes': 15,\n",
    "    'tabular_sizes': [400, 200, 100],\n",
    "    'tabular_resblocks': 1,\n",
    "    'final_sizes': [300, 100],\n",
    "    'final_resblocks': 0,\n",
    "    'dropout': 0.3758110031582248,\n",
    "    'learning_schedule': 'constant', \n",
    "    'learning_rate': 0.00017360566254027907, \n",
    "    'weight_decay': 0.00039784787398219684\n",
    "}\n",
    "\n",
    "model = get_model_instance(\n",
    "    \"lstm_calc_date_model_pytorch\",\n",
    "    **model_params)\n",
    "\n",
    "# Reload the checkpoint of the best model, to this point\n",
    "model = model.load_from_checkpoint(\n",
    "    checkpoint_path=\"best_cd_model.ckpt\",\n",
    "    **model_params\n",
    ")\n",
    "\n",
    "x_test = create_input(test_dataframe)\n",
    "y_test = create_cdc_label(test_dataframe)\n",
    "predictions = model(*x_test)\n",
    "\n",
    "test_dataframe.loc[:, 'calc_date_prediction'] = predictions.detach().numpy().argmax(axis=1)\n",
    "test_dataframe.loc[:, 'calc_date_match'] = (test_dataframe.calc_day_cat - test_dataframe.calc_date_prediction).abs()\n",
    "test_dataframe.sort_values(by='calc_date_match', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_calc_date_analysis(test_dataframe):\n",
    "    from IPython.display import display, Markdown\n",
    "\n",
    "    calc_date_cat_dict = {0:'next_call_date',\n",
    "    1:'par_call_date',\n",
    "    2:'maturity_date',\n",
    "    3:'refund_date'}\n",
    "\n",
    "    cusip_set = set()\n",
    "    for _, row in test_dataframe.iterrows():\n",
    "        cusip_dataframe = test_dataframe[test_dataframe.cusip == row['cusip']]\n",
    "\n",
    "        if (cusip_dataframe.calc_date_prediction == cusip_dataframe.calc_day_cat).any() and (cusip_dataframe.calc_date_prediction != cusip_dataframe.calc_day_cat).any() and (len(cusip_dataframe) > 1) and (row['cusip'] not in cusip_set):\n",
    "            display(Markdown(\n",
    "                f\"## CUSIP: {row['cusip']} - RTRS Control Numbers: {cusip_dataframe.iloc[0]['rtrs_control_number']} and {cusip_dataframe.iloc[-1]['rtrs_control_number']}\\n\" +\n",
    "                f\"### Actual Calculation Date (failed): {cusip_dataframe.iloc[0]['calc_day_cat']} - {cusip_dataframe.iloc[0][calc_date_cat_dict[cusip_dataframe.iloc[-1]['calc_day_cat']]]}\\n\" +\n",
    "                f\"### Predicted Calculation Date (failed): {cusip_dataframe.iloc[0]['calc_date_prediction']} - {cusip_dataframe.iloc[0][calc_date_cat_dict[cusip_dataframe.iloc[-1]['calc_date_prediction']]]}\\n\" +\n",
    "                f\"### Actual Calculation Date (succeeded): {cusip_dataframe.iloc[-1]['calc_day_cat']} - {cusip_dataframe.iloc[-1][calc_date_cat_dict[cusip_dataframe.iloc[-1]['calc_day_cat']]]}\\n\" +\n",
    "                f\"### Predicted Calculation Date (succeeded): {cusip_dataframe.iloc[-1]['calc_date_prediction']} - {cusip_dataframe.iloc[-1][calc_date_cat_dict[cusip_dataframe.iloc[-1]['calc_date_prediction']]]}\"\n",
    "            ))\n",
    "\n",
    "            x_test = create_input(cusip_dataframe)\n",
    "            y_test = create_cdc_label(cusip_dataframe)\n",
    "\n",
    "            # plot_cusip(cusip_dataframe, 'calc_date')\n",
    "\n",
    "            error_attributes = attr.compute_integrated_gradient_calc_date_error_attributions(model, x_test, y_test)\n",
    "\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(30, 10))\n",
    "            fig.suptitle('Calc-Date Trade History ERROR Attributions')\n",
    "            attr.visualize_trade_history_attribution(error_attributes[0][0], subtitle=\"Worst\", fig_ax=(fig, ax[0]))\n",
    "            attr.visualize_trade_history_attribution(error_attributes[0][-1], subtitle=\"Best\", fig_ax=(fig, ax[1]))\n",
    "\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(30, 5))\n",
    "            fig.suptitle('Calc-Date Numerical Feature ERROR Attributions')\n",
    "            attr.visualize_trade_numerical_and_binary_attribution(error_attributes[1][0], NON_CAT_FEATURES, BINARY, subtitle=\"Worst\", fig_ax=(fig, ax[0]))\n",
    "            attr.visualize_trade_numerical_and_binary_attribution(error_attributes[1][-1], NON_CAT_FEATURES, BINARY, subtitle=\"Best\", fig_ax=(fig, ax[1]))\n",
    "\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(20, 3))\n",
    "            ax.set_axis_off()\n",
    "            values = x_test[1][(0, -1), :].cpu().numpy()\n",
    "            tbl = ax.table(cellText=values.T, rowLabels=NON_CAT_FEATURES + BINARY, colLabels=[\"Worst\", \"Best\"], loc='center')\n",
    "            for i in range(len(NON_CAT_FEATURES + BINARY) + 1):\n",
    "                if i % 2:\n",
    "                    for j in range(2):\n",
    "                        tbl[(i, j)].set_facecolor(\"#dddddd\")\n",
    "\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(30, 3))\n",
    "            fig.suptitle('Calc-Date Categorical Feature ERROR Attributions')\n",
    "            attr.visualize_categorical_attribution(torch.cat([a.unsqueeze(1) for a in error_attributes[2:]], dim=-1)[0], CATEGORICAL_FEATURES, subtitle=\"Worst\", fig_ax=(fig, ax[0]))\n",
    "            attr.visualize_categorical_attribution(torch.cat([a.unsqueeze(1) for a in error_attributes[2:]], dim=-1)[-1], CATEGORICAL_FEATURES, subtitle=\"Best\", fig_ax=(fig, ax[1]))\n",
    "\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(20, 3))\n",
    "            ax.set_axis_off()\n",
    "            values = [[x[i].cpu().numpy() for i in (0, -1)] for x in x_test[2:]]\n",
    "            tbl = ax.table(cellText=values, rowLabels=CATEGORICAL_FEATURES, colLabels=[\"Worst\", \"Best\"], loc='center')\n",
    "            for i in range(len(NON_CAT_FEATURES + BINARY) + 1):\n",
    "                if i % 2:\n",
    "                    for j in range(2):\n",
    "                        tbl[(i, j)].set_facecolor(\"#dddddd\")\n",
    "\n",
    "            cusip_set.add(row['cusip'])\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        yield\n",
    "\n",
    "calc_date_analysis_generator = yield_calc_date_analysis(test_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 cusips with both correct and incorrect calc-date predictions\n",
    "## Includes analysis of the cusip's best vs. worst predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(calc_date_analysis_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(calc_date_analysis_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(calc_date_analysis_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(calc_date_analysis_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(calc_date_analysis_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(calc_date_analysis_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(calc_date_analysis_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(calc_date_analysis_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(calc_date_analysis_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(calc_date_analysis_generator)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2266f9d190a7b880f073f634cb88d3a7b9a2324a600bb63b91ff505e59b5d8d2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ficc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
