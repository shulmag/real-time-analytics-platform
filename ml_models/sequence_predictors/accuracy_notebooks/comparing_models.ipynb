{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "358ab8fd",
   "metadata": {},
   "source": [
    "## Comparing diff vs and \n",
    "\n",
    "This notebook compares model trained on new ys and diff ys. Both the model are trained on trades from May 2023 to June 2023. The models are then tested on trades after July 2023.\n",
    "\n",
    "The results clearly show that both diff_ys and new_ys perform equally well on the entire dataset. However, it's worth noting that the diff_ys model exhibits a slight advantage in predicting significant dealer-dealer trades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58de4fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing pandarallel with 16.0 cores\n",
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from sklearn import preprocessing\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "\n",
    "\n",
    "from ficc.data.process_data import process_data\n",
    "from ficc.utils.auxiliary_variables import PREDICTORS, NON_CAT_FEATURES, BINARY, CATEGORICAL_FEATURES, IDENTIFIERS, PURPOSE_CLASS_DICT\n",
    "from ficc.utils.gcp_storage_functions import upload_data, download_data\n",
    "from ficc.utils.auxiliary_variables import RELATED_TRADE_BINARY_FEATURES, RELATED_TRADE_NON_CAT_FEATURES, RELATED_TRADE_CATEGORICAL_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dbb1703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 22:35:47.945068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-18 22:35:47.955267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-18 22:35:47.955800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae670dc",
   "metadata": {},
   "source": [
    "Setting the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07c11246",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"ahmad_creds.json\"\n",
    "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440e0be5",
   "metadata": {},
   "source": [
    "Initializing BigQuery client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b1d17e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d4563b",
   "metadata": {},
   "source": [
    "Initializing GCP storage client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92b48799",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e06a8d",
   "metadata": {},
   "source": [
    "Declaring hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "767f6023",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_SPLIT = 0.85\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 1000\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "DROPOUT = 0.10\n",
    "SEQUENCE_LENGTH = 5\n",
    "NUM_FEATURES = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb809f8",
   "metadata": {},
   "source": [
    "Checking if the treasury spreads and target attention features are present in PREDICTORS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "338b5e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ficc_treasury_spread' not in PREDICTORS:\n",
    "    PREDICTORS.append('ficc_treasury_spread')\n",
    "    NON_CAT_FEATURES.append('ficc_treasury_spread')\n",
    "if 'target_attention_features' not in PREDICTORS:\n",
    "    PREDICTORS.append('target_attention_features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090e08df",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "We grab the data from a GCP bucket. The data is prepared using the ficc python package. More insight on how the data is prepared can be found [here](https://github.com/Ficc-ai/ficc/blob/ahmad_ml/ml_models/sequence_predictors/data_prep/data_preparation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2c7302a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.91 s, sys: 2.73 s, total: 11.6 s\n",
      "Wall time: 25.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem(project='eng-reactor-287421')\n",
    "with fs.open('ahmad_data/processed_data_2023-09-18-17:34.pkl') as f:\n",
    "    data = pd.read_pickle(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6c17c",
   "metadata": {},
   "source": [
    "#### Date range for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5680e3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2023-09-15 00:00:00')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.trade_date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "548f2ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2023-08-01 00:00:00')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.trade_date.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "859c0b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricting history to 5 trades\n"
     ]
    }
   ],
   "source": [
    "print(f'Restricting history to {SEQUENCE_LENGTH} trades')\n",
    "data.trade_history = data.trade_history.apply(lambda x: x[:SEQUENCE_LENGTH])\n",
    "data.target_attention_features = data.target_attention_features.apply(lambda x:x[:SEQUENCE_LENGTH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c81c3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.trade_history.iloc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3131cf94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target_attention_features.iloc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23c937dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values('trade_datetime', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26de206",
   "metadata": {},
   "source": [
    "#### Creating features from trade history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a533c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttype_dict = { (0,0):'D', (0,1):'S', (1,0):'P' }\n",
    "\n",
    "ys_variants = [\"max_ys\", \"min_ys\", \"max_qty\", \"min_ago\", \"D_min_ago\", \"P_min_ago\", \"S_min_ago\"]\n",
    "ys_feats = [\"_ys\", \"_ttypes\", \"_ago\", \"_qdiff\"]\n",
    "D_prev = dict()\n",
    "P_prev = dict()\n",
    "S_prev = dict()\n",
    "\n",
    "def get_trade_history_columns():\n",
    "    '''\n",
    "    This function is used to create a list of columns\n",
    "    '''\n",
    "    YS_COLS = []\n",
    "    for prefix in ys_variants:\n",
    "        for suffix in ys_feats:\n",
    "            YS_COLS.append(prefix + suffix)\n",
    "    return YS_COLS\n",
    "\n",
    "def extract_feature_from_trade(row, name, trade):\n",
    "    yield_spread = trade[0]\n",
    "    ttypes = ttype_dict[(trade[3],trade[4])] + row.trade_type\n",
    "    seconds_ago = trade[5]\n",
    "    quantity_diff = np.log10(1 + np.abs(10**trade[2] - 10**row.quantity))\n",
    "    return [yield_spread, ttypes,  seconds_ago, quantity_diff]\n",
    "\n",
    "def trade_history_derived_features(row):\n",
    "    trade_history = row.trade_history\n",
    "    trade = trade_history[0]\n",
    "    \n",
    "    D_min_ago_t = D_prev.get(row.cusip,trade)\n",
    "    D_min_ago = 9        \n",
    "\n",
    "    P_min_ago_t = P_prev.get(row.cusip,trade)\n",
    "    P_min_ago = 9\n",
    "    \n",
    "    S_min_ago_t = S_prev.get(row.cusip,trade)\n",
    "    S_min_ago = 9\n",
    "    \n",
    "    max_ys_t = trade; max_ys = trade[0]\n",
    "    min_ys_t = trade; min_ys = trade[0]\n",
    "    max_qty_t = trade; max_qty = trade[2]\n",
    "    min_ago_t = trade; min_ago = trade[5]\n",
    "    \n",
    "    for trade in trade_history[0:]:\n",
    "        #Checking if the first trade in the history is from the same block\n",
    "        if trade[5] == 0: \n",
    "            continue\n",
    " \n",
    "        if trade[0] > max_ys: \n",
    "            max_ys_t = trade\n",
    "            max_ys = trade[0]\n",
    "        elif trade[0] < min_ys: \n",
    "            min_ys_t = trade; \n",
    "            min_ys = trade[0]\n",
    "\n",
    "        if trade[2] > max_qty: \n",
    "            max_qty_t = trade \n",
    "            max_qty = trade[2]\n",
    "        if trade[5] < min_ago: \n",
    "            min_ago_t = trade; \n",
    "            min_ago = trade[5]\n",
    "            \n",
    "        side = ttype_dict[(trade[3],trade[4])]\n",
    "        if side == \"D\":\n",
    "            if trade[5] < D_min_ago: \n",
    "                D_min_ago_t = trade; D_min_ago = trade[5]\n",
    "                D_prev[row.cusip] = trade\n",
    "        elif side == \"P\":\n",
    "            if trade[5] < P_min_ago: \n",
    "                P_min_ago_t = trade; P_min_ago = trade[5]\n",
    "                P_prev[row.cusip] = trade\n",
    "        elif side == \"S\":\n",
    "            if trade[5] < S_min_ago: \n",
    "                S_min_ago_t = trade; S_min_ago = trade[5]\n",
    "                S_prev[row.cusip] = trade\n",
    "        else: \n",
    "            print(\"invalid side\", trade)\n",
    "    \n",
    "    trade_history_dict = {\"max_ys\":max_ys_t,\n",
    "                          \"min_ys\":min_ys_t,\n",
    "                          \"max_qty\":max_qty_t,\n",
    "                          \"min_ago\":min_ago_t,\n",
    "                          \"D_min_ago\":D_min_ago_t,\n",
    "                          \"P_min_ago\":P_min_ago_t,\n",
    "                          \"S_min_ago\":S_min_ago_t}\n",
    "\n",
    "    return_list = []\n",
    "    for variant in ys_variants:\n",
    "        feature_list = extract_feature_from_trade(row,variant,trade_history_dict[variant])\n",
    "        return_list += feature_list\n",
    "    \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8ce568c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.2 s, sys: 3.7 s, total: 22.9 s\n",
      "Wall time: 48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "temp = data[['cusip',\n",
    "             'trade_history',\n",
    "             'quantity',\n",
    "             'trade_type']].parallel_apply(trade_history_derived_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5519788",
   "metadata": {},
   "outputs": [],
   "source": [
    "YS_COLS = get_trade_history_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fb65b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[YS_COLS] = pd.DataFrame(temp.tolist(), index=data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ad9fd2",
   "metadata": {},
   "source": [
    "Adding trade history features to PREDICTORS list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70a7f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in YS_COLS:\n",
    "    if 'ttypes' in col and col not in PREDICTORS:\n",
    "        PREDICTORS.append(col)\n",
    "        CATEGORICAL_FEATURES.append(col)\n",
    "    elif col not in PREDICTORS:\n",
    "        NON_CAT_FEATURES.append(col)\n",
    "        PREDICTORS.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5eae161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duration(coupon, ytw, years, dollar_price, peryear=2):\n",
    "    ytw = ytw.clip(0.001,np.inf)\n",
    "    c = (coupon/100) / peryear\n",
    "    y = (ytw/10000) / peryear\n",
    "    n = years * peryear\n",
    "    m = peryear\n",
    "    macaulay_duration = ((1+y) / (m*y)) - ( (1 + y + n*(c-y)) / ((m*c* ((1+y)**n - 1)) + m*y))\n",
    "    modified_duration = macaulay_duration / (1 + y)\n",
    "    dv01 = modified_duration * dollar_price / 10000\n",
    "    return dv01\n",
    "\n",
    "def add_additional_feature(data):\n",
    "    data['diff_ficc_ycl'] = data.new_ficc_ycl - data.last_ficc_ycl\n",
    "    data['diff_ficc_treasury_spread'] = data.last_ficc_ycl - (data.treasury_rate * 100)\n",
    "    data['dv01'] = duration(data.coupon, data.last_yield, data.last_duration, data.last_dollar_price)\n",
    "    data['approx_dpd'] =  data.dv01 * data.diff_ficc_ycl\n",
    "    data['overage'] =  (data.last_dollar_price + data.approx_dpd - data.next_call_price)\n",
    "    #data['de_minimis_gap'] = data.last_dollar_price - data.de_minimis_threshold\n",
    "    return data\n",
    "\n",
    "data = add_additional_feature(data)\n",
    "additional_features = ['diff_ficc_ycl','diff_ficc_treasury_spread','dv01','approx_dpd','overage']#,'de_minimis_gap']\n",
    "for i in additional_features:\n",
    "    if i not in NON_CAT_FEATURES:\n",
    "        NON_CAT_FEATURES.append(i)\n",
    "        PREDICTORS.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630f457d",
   "metadata": {},
   "source": [
    "This feature is used to check if there are any NaN values in the trade history. **It is not used to train the model**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13523b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1318395"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8e732eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.8 s, sys: 1.68 s, total: 8.48 s\n",
      "Wall time: 9.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['trade_history_sum'] = data.trade_history.parallel_apply(lambda x: np.sum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9eeceb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['trade_history_sum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65ee4a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1318395"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a258f7c4",
   "metadata": {},
   "source": [
    "For the purpose of plotting, not used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d69d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.purpose_sub_class.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbf9900",
   "metadata": {},
   "source": [
    "Creating new ys label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbdcd21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['new_ys'] = data['yield'] - data['new_ficc_ycl']\n",
    "data['diff_ys'] = data['new_ys'] - data['last_yield_spread']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed96b6f7",
   "metadata": {},
   "source": [
    "Selecting a subset of features for training. PREDICTORS are the features that we are going to use to train the model. More information about the feature set can be found [here](https://github.com/Ficc-ai/ficc_python/blob/d455bd30eca18f26a2535523530facad516dd90f/ficc/utils/auxiliary_variables.py#L120). We also select a set of additonal features, which are not used in training. These features are used to uderstand the results from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37a71950",
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliary_features = ['dollar_price',\n",
    "                     'calc_date', \n",
    "                     'trade_date',\n",
    "                     'trade_datetime', \n",
    "                     'purpose_sub_class', \n",
    "                     'called_redemption_type', \n",
    "                     'calc_day_cat',\n",
    "                     'yield',\n",
    "                     'ficc_ycl',\n",
    "                     'new_ys',\n",
    "                     'trade_history_sum',\n",
    "                     'new_ficc_ycl',\n",
    "                     'days_to_refund',\n",
    "                     'last_dollar_price',\n",
    "                     'last_rtrs_control_number',\n",
    "                     'is_called',\n",
    "                     'federal_tax_status','par_traded']\n",
    "                      #,'maturity_description_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7bfbe065",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = data[IDENTIFIERS + PREDICTORS + auxiliary_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f6fd0a",
   "metadata": {},
   "source": [
    "Checking for missing data and NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "094bf076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1318395"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fadc1162",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.issue_amount = processed_data.issue_amount.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da24fd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.dropna(inplace=True, subset=PREDICTORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0674277c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1272886"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1901e2a1",
   "metadata": {},
   "source": [
    "#### Fitting encoders to the categorical features. These encoders are then used to encode the categorical features of the train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "66d92939",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('encoders_test.pkl','rb') as f:\n",
    "    encoders = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "985080c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.sort_values('trade_datetime',ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35893438",
   "metadata": {},
   "source": [
    "#### Splitting the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "40be739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataframe = processed_data[(processed_data.trade_date >= '08-01-2023') & (processed_data.trade_date <= '08-31-2023')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "014a4bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "864056"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "29cf32a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2023-08-01 00:00:00')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataframe.trade_date.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86081f1",
   "metadata": {},
   "source": [
    "##### Converting data into format suitable for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d1e5fd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input(df):\n",
    "    global encoders\n",
    "    datalist = []\n",
    "    datalist.append(np.stack(df['trade_history'].to_numpy()))\n",
    "    datalist.append(np.stack(df['target_attention_features'].to_numpy()))\n",
    "\n",
    "    noncat_and_binary = []\n",
    "    for f in NON_CAT_FEATURES + BINARY:\n",
    "        noncat_and_binary.append(np.expand_dims(df[f].to_numpy().astype('float32'), axis=1))\n",
    "    datalist.append(np.concatenate(noncat_and_binary, axis=-1))\n",
    "    \n",
    "    for f in CATEGORICAL_FEATURES:\n",
    "        encoded = encoders[f].transform(df[f])\n",
    "        datalist.append(encoded.astype('float32'))\n",
    "    \n",
    "    return datalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7af0cf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.72 s, sys: 63.3 ms, total: 4.79 s\n",
      "Wall time: 4.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_test = create_input(test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "270a622a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(864056, 53)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbb4b67",
   "metadata": {},
   "source": [
    "### Load ys model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0673ed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yield_spread_model = keras.models.load_model('saved_model_new_ys_2023-09-18-20-32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f9a7c797",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 22:43:48.198212: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 9431678976 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    }
   ],
   "source": [
    "test_dataframe['predicted_ys'] = yield_spread_model.predict(x_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eb3588c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE yield spread model: 14.215\n"
     ]
    }
   ],
   "source": [
    "print(f\"MAE yield spread model: {round(np.mean(np.abs(test_dataframe.new_ys - test_dataframe.predicted_ys)), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e75994ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAD yield spread model: 7.438\n"
     ]
    }
   ],
   "source": [
    "print(f\"MAD yield spread model: {round(np.median(np.abs(test_dataframe.new_ys - test_dataframe.predicted_ys)), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02962e63",
   "metadata": {},
   "source": [
    "Measuring accuracy for large dealer-dealer trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cb23ef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_mid = test_dataframe[(test_dataframe.trade_type == 'D') & (test_dataframe.par_traded > 500000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "96005b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE yield spread model: 8.056\n"
     ]
    }
   ],
   "source": [
    "print(f\"MAE yield spread model: {round(np.mean(np.abs(true_mid.new_ys - true_mid.predicted_ys)), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c2ed2b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAD yield spread model: 3.566\n"
     ]
    }
   ],
   "source": [
    "print(f\"MAD yield spread model: {round(np.median(np.abs(true_mid.new_ys - true_mid.predicted_ys)), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4697dce8",
   "metadata": {},
   "source": [
    "### Loading diff ys model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a6a106b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_ys_model = keras.models.load_model('saved_model_diff_ys_2023-09-18-22-32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "81e4fc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 22:44:12.991578: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 9431678976 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    }
   ],
   "source": [
    "test_dataframe['predicted_diff_ys'] = diff_ys_model.predict(x_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "552817f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataframe['predicted_spread_diff_ys'] = test_dataframe['last_yield_spread'] + test_dataframe['predicted_diff_ys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "62483f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE diff ys loss: 11.092\n"
     ]
    }
   ],
   "source": [
    "print(f\"MAE diff ys loss: {round(np.mean(np.abs(test_dataframe.new_ys - test_dataframe.predicted_spread_diff_ys)), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cec13bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAD diff ys model: 5.891\n"
     ]
    }
   ],
   "source": [
    "print(f\"MAD diff ys model: {round(np.median(np.abs(test_dataframe.new_ys - test_dataframe.predicted_spread_diff_ys)), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e640d21e",
   "metadata": {},
   "source": [
    "Measuring accuracy on large dealer-dealer trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b12c7b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_mid = test_dataframe[(test_dataframe.trade_type == 'D') & (test_dataframe.par_traded > 500000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a03c9c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE diff ys loss: 6.365\n"
     ]
    }
   ],
   "source": [
    "print(f\"MAE diff ys loss: {round(np.mean(np.abs(true_mid.new_ys - true_mid.predicted_spread_diff_ys)), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c69e48e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAD diff ys model: 2.516\n"
     ]
    }
   ],
   "source": [
    "print(f\"MAD diff ys model: {round(np.median(np.abs(true_mid.new_ys - true_mid.predicted_spread_diff_ys)), 3)}\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-cpu.2-6.m78",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-6:m78"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
