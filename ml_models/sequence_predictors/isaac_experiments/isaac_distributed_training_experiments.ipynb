{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9089ae53-5e6a-46ba-a9c0-a3df8c3a83e3",
   "metadata": {},
   "source": [
    "## Attempting to Implement Distributed Training to Maximize GPU Usage  \n",
    "\n",
    "\n",
    "On a single model training instance, GPU usage is limited by batch size (assuming no other bottlenecks, which should not be the case here since data processing is done beforehand and the dataset is relatively small). One idea is to virtualize the GPU into two separate instances and to use distributed learning to use those two sub-GPUs concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58de4fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 18:19:08.870037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-26 18:19:08.886165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-26 18:19:08.888044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing pandarallel with 8.0 cores\n",
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import seaborn as sns\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from sklearn import preprocessing\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle5 as pickle\n",
    "\n",
    "\n",
    "from ficc.utils.nelson_siegel_model import *\n",
    "from ficc.utils.diff_in_days import *\n",
    "from ficc.utils.auxiliary_functions import sqltodf\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "\n",
    "\n",
    "from ficc.data.process_data import process_data\n",
    "from ficc.utils.auxiliary_variables import PREDICTORS, NON_CAT_FEATURES, BINARY, CATEGORICAL_FEATURES, IDENTIFIERS, PURPOSE_CLASS_DICT, NUM_OF_DAYS_IN_YEAR\n",
    "from ficc.utils.gcp_storage_functions import upload_data, download_data\n",
    "from ficc.utils.auxiliary_variables import RELATED_TRADE_BINARY_FEATURES, RELATED_TRADE_NON_CAT_FEATURES, RELATED_TRADE_CATEGORICAL_FEATURES\n",
    "\n",
    "from ficc_keras_utils import *\n",
    "import ficc_keras_utils\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87d96a0f-05bd-4af6-bc16-9c87ddd5dc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPU, 2 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 18:19:11.385579: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-26 18:19:11.387458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-26 18:19:11.389365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-26 18:19:11.390901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-26 18:19:12.106283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-26 18:19:12.108103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-26 18:19:12.109654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-26 18:19:12.111199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-26 18:19:12.113143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7000 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "2023-04-26 18:19:12.113738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-26 18:19:12.115329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7000 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "memory = int(15360/2)\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.set_logical_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.LogicalDeviceConfiguration(memory_limit=7000),\n",
    "             tf.config.LogicalDeviceConfiguration(memory_limit=7000)\n",
    "            ]\n",
    "             )\n",
    "            \n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab23ee98-9bc1-47c0-bc91-d4a4e71747e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07c11246",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/home/jupyter/ficc/isaac_creds.json\"\n",
    "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "bq_client = bigquery.Client()\n",
    "storage_client = storage.Client()\n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.85\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 1000\n",
    "NUM_EPOCHS = 20\n",
    "ficc_keras_utils.NUM_EPOCHS = NUM_EPOCHS\n",
    "\n",
    "DROPOUT = 0.01\n",
    "SEQUENCE_LENGTH = 5\n",
    "NUM_FEATURES = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee8a90a-c634-4424-9acf-4ea0238c6a49",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "685f0c18-06c4-4c79-9e43-059a1e0ea55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File available, loading pickle\n",
      "CPU times: user 19.8 s, sys: 8.08 s, total: 27.9 s\n",
      "Wall time: 27.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "path = '../processed_file_FULL_2023-04-12-20:44.pkl'\n",
    "if os.path.isfile(path):\n",
    "    print('File available, loading pickle')\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "else:\n",
    "    print('File not available, downloading from cloud storage')\n",
    "    with open('processed_file_FULL_2023-04-12-20:44.pkl', 'wb') as f:\n",
    "        import gcsfs\n",
    "        fs = gcsfs.GCSFileSystem(project='eng-reactor-287421')\n",
    "        with fs.open('isaac_data/processed_file_FULL_2023-04-12-20:44.pkl') as f:\n",
    "            data = pd.read_pickle(f)\n",
    "            pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afabed04-a496-434e-910d-5971913abd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['new_ys'] = data['yield'] - data['new_ficc_ycl']\n",
    "data['new_ys_realtime'] = data['yield'] - data['new_real_time_ficc_ycl']\n",
    "data.dropna(subset=['new_ys', 'new_ys_realtime'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37a71950",
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliary_features = ['dollar_price',\n",
    "                      'last_calc_date',\n",
    "                     'calc_date', \n",
    "                     'trade_date',\n",
    "                      'last_trade_date',\n",
    "                     'trade_datetime', \n",
    "                     'purpose_sub_class', \n",
    "                     'called_redemption_type', \n",
    "                     'calc_day_cat',\n",
    "                     'yield',\n",
    "                     'ficc_ycl',\n",
    "                     #'same_ys',\n",
    "                     #'trade_history_sum',\n",
    "                     'new_ficc_ycl',\n",
    "                      'new_real_time_ficc_ycl',\n",
    "                     'days_to_refund',\n",
    "                      'last_dollar_price',\n",
    "                      'last_rtrs_control_number',\n",
    "                     'is_called',\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2476c327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing extraordinary_make_whole_call from PREDICTORS and BINARY\n",
      "Removing make_whole_call from PREDICTORS and BINARY\n",
      "Removing has_unexpired_lines_of_credit from PREDICTORS and BINARY\n"
     ]
    }
   ],
   "source": [
    "if 'target_attention_features' not in PREDICTORS:\n",
    "    PREDICTORS.append('target_attention_features')\n",
    "    \n",
    "if 'ficc_treasury_spread' not in PREDICTORS:\n",
    "    PREDICTORS.append('ficc_treasury_spread')\n",
    "    NON_CAT_FEATURES.append('ficc_treasury_spread')\n",
    "    \n",
    "for col in ['new_ficc_ycl', 'new_real_time_ficc_ycl']:     \n",
    "    if col not in PREDICTORS:\n",
    "        PREDICTORS.append(col)\n",
    "        NON_CAT_FEATURES.append(col)\n",
    "\n",
    "for col in ['extraordinary_make_whole_call', 'make_whole_call', 'has_unexpired_lines_of_credit']:     \n",
    "    if col not in data.columns:\n",
    "        try: \n",
    "            print(f'Removing {col} from PREDICTORS and BINARY')\n",
    "            BINARY.remove(col)\n",
    "            PREDICTORS.remove(col) \n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e97d1342-f725-45f2-8121-3ba449966606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data): \n",
    "    data['ted-rate'] = (data['t_rate_10'] - data['t_rate_2']) * 100\n",
    "    \n",
    "    # Here is a list of exclusions that we will be experimenting with. The model is trained with these exclusions. These exclusions were discussed with a team member.\n",
    "    # Callable less than a year in the future\n",
    "    # Maturity less than a year in the future and more than 30 years in the future\n",
    "    \n",
    "    data = data[(data.days_to_call == 0) | (data.days_to_call > np.log10(400))]\n",
    "    data = data[(data.days_to_refund == 0) | (data.days_to_refund > np.log10(400))]\n",
    "    data = data[(data.days_to_maturity == 0) | (data.days_to_maturity > np.log10(400))]\n",
    "    data = data[data.days_to_maturity < np.log10(30000)]\n",
    "    data['trade_history_sum'] = data.trade_history.parallel_apply(lambda x: np.sum(x))\n",
    "    data.issue_amount = data.issue_amount.replace([np.inf, -np.inf], np.nan)\n",
    "    data.dropna(inplace=True, subset=PREDICTORS+['trade_history_sum'])\n",
    "    data.purpose_sub_class.fillna(0, inplace=True)\n",
    "    \n",
    "    # data['calc_date_duration'] = data[['last_calc_date','last_trade_date']].parallel_apply(get_calc_date_duration, axis=1)\n",
    "    # data['new_ficc_ycl_fixed_shape'] = data[['trade_date', 'calc_date_duration']].parallel_apply(lambda x: calculate_ycl(x, new_yc_params), axis = 1)\n",
    "    # data['new_ficc_ycl_prev_day'] = data[['last_calc_date', 'last_trade_date' ,'calc_date_duration','trade_date']].parallel_apply(get_yield_for_last_duration, axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bfbe065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.6 s, sys: 12.3 s, total: 52.9 s\n",
      "Wall time: 58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "processed_data = process_data(data) \n",
    "# processed_data = processed_data[IDENTIFIERS + PREDICTORS + auxiliary_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66d92939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating\n",
      "incorporated_state_code\n",
      "trade_type\n",
      "purpose_class\n"
     ]
    }
   ],
   "source": [
    "encoders = {}\n",
    "fmax = {}\n",
    "for f in CATEGORICAL_FEATURES:\n",
    "    print(f)\n",
    "    fprep = preprocessing.LabelEncoder().fit(processed_data[f].drop_duplicates()) #note that there are apparently no trades with CC \n",
    "    fmax[f] = np.max(fprep.transform(fprep.classes_))\n",
    "    encoders[f] = fprep\n",
    "    \n",
    "with open('encoders.pkl','wb') as file:\n",
    "    pickle.dump(encoders,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9daf580",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe = processed_data[(processed_data.trade_date <\n",
    "                                  '2023-02-01')].sort_values(by='trade_date', ascending=True).reset_index(drop=True)\n",
    "\n",
    "test_dataframe = processed_data[(processed_data.trade_date >'2023-02-01')].sort_values(by='trade_date', ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1e5fd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input(df):\n",
    "    global encoders\n",
    "    datalist = []\n",
    "    datalist.append(np.stack(df['trade_history'].to_numpy()))\n",
    "    datalist.append(np.stack(df['target_attention_features'].to_numpy()))\n",
    "\n",
    "    noncat_and_binary = []\n",
    "    for f in NON_CAT_FEATURES + BINARY:\n",
    "        noncat_and_binary.append(np.expand_dims(df[f].to_numpy().astype('float32'), axis=1))\n",
    "    datalist.append(np.concatenate(noncat_and_binary, axis=-1))\n",
    "    \n",
    "    for f in CATEGORICAL_FEATURES:\n",
    "        encoded = encoders[f].transform(df[f])\n",
    "        datalist.append(encoded.astype('float32'))\n",
    "    \n",
    "    return datalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2b65617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.4 s, sys: 944 ms, total: 23.4 s\n",
      "Wall time: 23.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train = create_input(train_dataframe)\n",
    "x_train[0] = x_train[0][:,:,[0,2,3,4,5,6]]\n",
    "y_train = train_dataframe.new_ys\n",
    "\n",
    "x_test = create_input(test_dataframe)\n",
    "x_test[0] = x_test[0][:,:,[0,2,3,4,5,6]]\n",
    "y_test = test_dataframe.new_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27cedeaa-be7e-4700-85fe-2e628b303ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_idx = int(len(x_train[0])*0.5)\n",
    "x_train = [x[cutoff_idx:] for x in x_train]\n",
    "y_train = y_train[cutoff_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce6aa16",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cdb94f2-09a4-4c7f-8a07-b3fbac7a14d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization layer for the trade history\n",
    "trade_history_normalizer = Normalization(name='Trade_history_normalizer')\n",
    "trade_history_normalizer.adapt(x_train[0],batch_size=BATCH_SIZE)\n",
    "\n",
    "# Normalization layer for the non-categorical and binary features\n",
    "noncat_binary_normalizer = Normalization(name='Numerical_binary_normalizer')\n",
    "noncat_binary_normalizer.adapt(x_train[2], batch_size = BATCH_SIZE)\n",
    "\n",
    "tf.keras.utils.set_random_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63d9d0d8-e8ee-4f99-9788-98bd4c43ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_data(x_train, y_train, shuffle=False, shuffle_buffer=1):\n",
    "\n",
    "    train_size = int(0.8*len(x_train[0]))\n",
    "                     \n",
    "    X=()\n",
    "    for x in x_train:\n",
    "        X += (tf.data.Dataset.from_tensor_slices(x),)\n",
    "        \n",
    "\n",
    "    temp = tf.data.Dataset.zip((X))\n",
    "    del X\n",
    "    dataset = tf.data.Dataset.zip((temp,\n",
    "                        tf.data.Dataset.from_tensor_slices(y_train)))\n",
    "    del temp\n",
    "    if shuffle:\n",
    "        shuffle_buffer = int(len(x_train[0])*shuffle_buffer)\n",
    "        dataset = dataset.shuffle(shuffle_buffer)\n",
    "            \n",
    "    train_ds = dataset.take(train_size)\n",
    "    val_ds = dataset.skip(train_size)                 \n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68e6f7cd-8220-419d-8a57-7ff97a918297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.backend.clear_session()\n",
    "# gc.collect()\n",
    "\n",
    "# timestamp = datetime.now().strftime('%Y-%m-%d %H-%M')\n",
    "\n",
    "# fit_callbacks = fit_callbacks = [\n",
    "# keras.callbacks.EarlyStopping(\n",
    "#     monitor=\"val_loss\",\n",
    "#     patience=10,\n",
    "#     verbose=0,\n",
    "#     mode=\"auto\",\n",
    "#     restore_best_weights=True),\n",
    "#     CSVLoggerTimeHistory(timestamp+'_training_logs.csv', separator=\",\", append=False)]\n",
    "\n",
    "# # with tf.device('/cpu:0'):\n",
    "    \n",
    "\n",
    "\n",
    "# gpus = tf.config.list_logical_devices('GPU')\n",
    "\n",
    "# strategy = tf.distribute.MirroredStrategy(gpus)\n",
    "\n",
    "# with strategy.scope():\n",
    "#     train_ds, val_ds = create_tf_data(x_train, y_train, shuffle = True)\n",
    "#     train_ds = train_ds.batch(BATCH_SIZE).prefetch(2).cache()\n",
    "#     val_ds = val_ds.batch(BATCH_SIZE).prefetch(2).cache()\n",
    "    \n",
    "#     model_new_ys = generate_model(SEQUENCE_LENGTH=5, \n",
    "#                               NUM_FEATURES=6, \n",
    "#                               trade_history_normalizer=trade_history_normalizer\n",
    "#                              )\n",
    "\n",
    "#     fit_callbacks = fit_callbacks = [\n",
    "#     keras.callbacks.EarlyStopping(\n",
    "#         monitor=\"val_loss\",\n",
    "#         patience=10,\n",
    "#         verbose=0,\n",
    "#         mode=\"auto\",\n",
    "#         restore_best_weights=True),\n",
    "#         # time_callback,\n",
    "#         CSVLoggerTimeHistory('_'.join([model_new_ys.name,timestamp,'training_logs.csv'])\n",
    "#                              , separator=\",\", \n",
    "#                              append=False)\n",
    "#     ]\n",
    "    \n",
    "    \n",
    "#     model_new_ys.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "#           loss=keras.losses.MeanAbsoluteError(),\n",
    "#           metrics=[keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "# #     history_new_ys = model_new_ys.fit(train_ds,\n",
    "# #                                       validation_data=val_ds,\n",
    "# #                                         epochs=NUM_EPOCHS,     \n",
    "# #                                         verbose=1, \n",
    "# #                                         callbacks=fit_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8e0c7e2-a56f-4d3e-bb11-5edcd9bc18a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "gpus = tf.config.list_logical_devices('GPU')\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(gpus,\n",
    "                                         cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0f7861b-cbd0-4289-b110-58966c86a93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogicalDevice(name='/device:GPU:0', device_type='GPU'),\n",
       " LogicalDevice(name='/device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_logical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65b16aa6-4293-47c2-9726-77628eb29df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 18:54:11.023963: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_DOUBLE\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 2212912\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\026TensorSliceDataset:352\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 5\n",
      "        }\n",
      "        dim {\n",
      "          size: 6\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 18:54:33.847521: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 1120999 of 2212912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/886 [..............................] - ETA: 7:51:17 - loss: 59.8879 - mean_absolute_error: 59.8879"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 18:54:43.697307: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:405] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "886/886 [==============================] - ETA: 0s - loss: 55.3517 - mean_absolute_error: 55.3517"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 18:55:18.799378: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_DOUBLE\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 2212912\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\026TensorSliceDataset:352\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 5\n",
      "        }\n",
      "        dim {\n",
      "          size: 6\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2023-04-26 18:55:32.533271: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 1084115 of 2212912\n",
      "2023-04-26 18:55:42.533267: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 2210471 of 2212912\n",
      "2023-04-26 18:55:42.554736: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:405] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "886/886 [==============================] - 108s 86ms/step - loss: 55.3517 - mean_absolute_error: 55.3517 - val_loss: 52.0356 - val_mean_absolute_error: 52.0356\n",
      "Epoch 2/20\n",
      "886/886 [==============================] - 35s 39ms/step - loss: 51.2707 - mean_absolute_error: 51.2707 - val_loss: 47.7998 - val_mean_absolute_error: 47.7998\n",
      "Epoch 3/20\n",
      "886/886 [==============================] - 26s 30ms/step - loss: 44.8810 - mean_absolute_error: 44.8810 - val_loss: 38.4885 - val_mean_absolute_error: 38.4885\n",
      "Epoch 4/20\n",
      "886/886 [==============================] - 33s 37ms/step - loss: 36.1231 - mean_absolute_error: 36.1231 - val_loss: 33.8369 - val_mean_absolute_error: 33.8369\n",
      "Epoch 5/20\n",
      "886/886 [==============================] - 29s 33ms/step - loss: 25.9464 - mean_absolute_error: 25.9464 - val_loss: 21.6827 - val_mean_absolute_error: 21.6827\n",
      "Epoch 6/20\n",
      "886/886 [==============================] - 29s 33ms/step - loss: 16.7447 - mean_absolute_error: 16.7447 - val_loss: 13.8450 - val_mean_absolute_error: 13.8450\n",
      "Epoch 7/20\n",
      "886/886 [==============================] - 25s 29ms/step - loss: 12.0492 - mean_absolute_error: 12.0492 - val_loss: 11.1217 - val_mean_absolute_error: 11.1217\n",
      "Epoch 8/20\n",
      "886/886 [==============================] - 34s 37ms/step - loss: 11.0139 - mean_absolute_error: 11.0139 - val_loss: 10.5986 - val_mean_absolute_error: 10.5986\n",
      "Epoch 9/20\n",
      "886/886 [==============================] - 34s 39ms/step - loss: 10.7961 - mean_absolute_error: 10.7961 - val_loss: 10.3682 - val_mean_absolute_error: 10.3682\n",
      "Epoch 10/20\n",
      "886/886 [==============================] - 26s 29ms/step - loss: 10.6565 - mean_absolute_error: 10.6565 - val_loss: 10.3430 - val_mean_absolute_error: 10.3430\n",
      "Epoch 11/20\n",
      "886/886 [==============================] - 34s 39ms/step - loss: 10.5489 - mean_absolute_error: 10.5489 - val_loss: 10.2028 - val_mean_absolute_error: 10.2028\n",
      "Epoch 12/20\n",
      "886/886 [==============================] - 26s 29ms/step - loss: 10.4624 - mean_absolute_error: 10.4624 - val_loss: 10.0761 - val_mean_absolute_error: 10.0761\n",
      "Epoch 13/20\n",
      "886/886 [==============================] - 34s 38ms/step - loss: 10.3835 - mean_absolute_error: 10.3835 - val_loss: 10.0162 - val_mean_absolute_error: 10.0162\n",
      "Epoch 14/20\n",
      "886/886 [==============================] - 26s 29ms/step - loss: 10.3144 - mean_absolute_error: 10.3144 - val_loss: 9.9540 - val_mean_absolute_error: 9.9540\n",
      "Epoch 15/20\n",
      "886/886 [==============================] - 34s 38ms/step - loss: 10.2499 - mean_absolute_error: 10.2499 - val_loss: 9.8010 - val_mean_absolute_error: 9.8010\n",
      "Epoch 16/20\n",
      "886/886 [==============================] - 26s 29ms/step - loss: 10.2001 - mean_absolute_error: 10.2001 - val_loss: 9.7466 - val_mean_absolute_error: 9.7466\n",
      "Epoch 17/20\n",
      "886/886 [==============================] - 33s 38ms/step - loss: 10.1473 - mean_absolute_error: 10.1473 - val_loss: 9.7435 - val_mean_absolute_error: 9.7435\n",
      "Epoch 18/20\n",
      "886/886 [==============================] - 25s 29ms/step - loss: 10.1016 - mean_absolute_error: 10.1016 - val_loss: 9.6769 - val_mean_absolute_error: 9.6769\n",
      "Epoch 19/20\n",
      "886/886 [==============================] - 32s 37ms/step - loss: 10.0590 - mean_absolute_error: 10.0590 - val_loss: 9.6682 - val_mean_absolute_error: 9.6682\n",
      "Epoch 20/20\n",
      "886/886 [==============================] - 25s 29ms/step - loss: 10.0180 - mean_absolute_error: 10.0180 - val_loss: 9.6419 - val_mean_absolute_error: 9.6419\n",
      "Model training time was 11.26 minutes (675.57 seconds).\n",
      "Average time for each epoch was 0.56 minutes (33.78 seconds).\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    \n",
    "    # Normalization layer for the trade history\n",
    "    trade_history_normalizer = Normalization(name='Trade_history_normalizer')\n",
    "    trade_history_normalizer.adapt(x_train[0],batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Normalization layer for the non-categorical and binary features\n",
    "    noncat_binary_normalizer = Normalization(name='Numerical_binary_normalizer')\n",
    "    noncat_binary_normalizer.adapt(x_train[2], batch_size = BATCH_SIZE)\n",
    "\n",
    "    tf.keras.utils.set_random_seed(10)\n",
    "    \n",
    "    def create_tf_data(x_train, y_train, shuffle=False, shuffle_buffer=1):\n",
    "\n",
    "        train_size = int(0.8*len(x_train[0]))\n",
    "\n",
    "        X=()\n",
    "        for x in x_train:\n",
    "            X += (tf.data.Dataset.from_tensor_slices(x),)\n",
    "\n",
    "\n",
    "        temp = tf.data.Dataset.zip((X))\n",
    "        del X\n",
    "        dataset = tf.data.Dataset.zip((temp,\n",
    "                            tf.data.Dataset.from_tensor_slices(y_train)))\n",
    "        del temp\n",
    "        if shuffle:\n",
    "            shuffle_buffer = int(len(x_train[0])*shuffle_buffer)\n",
    "            dataset = dataset.shuffle(shuffle_buffer)\n",
    "\n",
    "        train_ds = dataset.take(train_size)\n",
    "        val_ds = dataset.skip(train_size)                 \n",
    "        return train_ds, val_ds\n",
    "\n",
    "    def generate_model(name = None, SEQUENCE_LENGTH = SEQUENCE_LENGTH ,NUM_FEATURES = NUM_FEATURES, trade_history_normalizer = trade_history_normalizer):\n",
    "        inputs = []\n",
    "        layer = []\n",
    "\n",
    "        ############## INPUT BLOCK ###################\n",
    "        trade_history_input = layers.Input(name=\"trade_history_input\", \n",
    "                                           shape=(SEQUENCE_LENGTH,NUM_FEATURES), \n",
    "                                           dtype = tf.float32) \n",
    "\n",
    "        target_attention_input = layers.Input(name=\"target_attention_input\", \n",
    "                                           shape=(SEQUENCE_LENGTH, 3), \n",
    "                                           dtype = tf.float32) \n",
    "\n",
    "\n",
    "        inputs.append(trade_history_input)\n",
    "        inputs.append(target_attention_input)\n",
    "\n",
    "        inputs.append(layers.Input(\n",
    "            name=\"NON_CAT_AND_BINARY_FEATURES\",\n",
    "            shape=(len(NON_CAT_FEATURES + BINARY),)\n",
    "        ))\n",
    "\n",
    "\n",
    "        layer.append(noncat_binary_normalizer(inputs[2]))\n",
    "        ####################################################\n",
    "\n",
    "\n",
    "        ############## TRADE HISTORY MODEL #################\n",
    "\n",
    "        lstm_layer = layers.LSTM(50, \n",
    "                                 activation='tanh',\n",
    "                                 input_shape=(SEQUENCE_LENGTH,NUM_FEATURES),\n",
    "                                 return_sequences = True,\n",
    "                                 name='LSTM')\n",
    "\n",
    "        lstm_attention_layer = CustomAttention(50)\n",
    "\n",
    "        lstm_layer_2 = layers.LSTM(100, \n",
    "                                   activation='tanh',\n",
    "                                   input_shape=(SEQUENCE_LENGTH,50),\n",
    "                                   return_sequences = False,\n",
    "                                   name='LSTM_2')\n",
    "\n",
    "\n",
    "        features = lstm_layer(trade_history_normalizer(inputs[0]))\n",
    "        features = lstm_attention_layer(features, features, inputs[1])\n",
    "        features = layers.BatchNormalization()(features)\n",
    "        # features = layers.Dropout(DROPOUT)(features)\n",
    "\n",
    "        features = lstm_layer_2(features)\n",
    "        features = layers.BatchNormalization()(features)\n",
    "        # features = layers.Dropout(DROPOUT)(features)\n",
    "\n",
    "        trade_history_output = layers.Dense(100, \n",
    "                                            activation='relu')(features)\n",
    "\n",
    "        ####################################################\n",
    "\n",
    "        ############## REFERENCE DATA MODEL ################\n",
    "        global encoders\n",
    "        for f in CATEGORICAL_FEATURES:\n",
    "            fin = layers.Input(shape=(1,), name = f)\n",
    "            inputs.append(fin)\n",
    "            embedded = layers.Flatten(name = f + \"_flat\")( layers.Embedding(input_dim = fmax[f]+1,\n",
    "                                                                            output_dim = max(30,int(np.sqrt(fmax[f]))),\n",
    "                                                                            input_length= 1,\n",
    "                                                                            name = f + \"_embed\")(fin))\n",
    "            layer.append(embedded)\n",
    "\n",
    "\n",
    "        reference_hidden = layers.Dense(400,\n",
    "                                        activation='relu',\n",
    "                                        name='reference_hidden_1')(layers.concatenate(layer, axis=-1))\n",
    "\n",
    "        reference_hidden = layers.BatchNormalization()(reference_hidden)\n",
    "        reference_hidden = layers.Dropout(DROPOUT)(reference_hidden)\n",
    "\n",
    "        reference_hidden2 = layers.Dense(200,activation='relu',name='reference_hidden_2')(reference_hidden)\n",
    "        reference_hidden2 = layers.BatchNormalization()(reference_hidden2)\n",
    "        reference_hidden2 = layers.Dropout(DROPOUT)(reference_hidden2)\n",
    "\n",
    "        reference_output = layers.Dense(100,activation='tanh',name='reference_hidden_3')(reference_hidden2)\n",
    "\n",
    "        ####################################################\n",
    "\n",
    "        feed_forward_input = layers.concatenate([reference_output, trade_history_output])\n",
    "\n",
    "        hidden = layers.Dense(300,activation='relu')(feed_forward_input)\n",
    "        hidden = layers.BatchNormalization()(hidden)\n",
    "        hidden = layers.Dropout(DROPOUT)(hidden)\n",
    "\n",
    "        hidden2 = layers.Dense(100,activation='tanh')(hidden)\n",
    "        hidden2 = layers.BatchNormalization()(hidden2)\n",
    "        hidden2 = layers.Dropout(DROPOUT)(hidden2)\n",
    "\n",
    "        final = layers.Dense(1)(hidden2)\n",
    "\n",
    "        if name: model = keras.Model(inputs=inputs, outputs=final, name=name)\n",
    "        else: model = keras.Model(inputs=inputs, outputs=final)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    train_ds, val_ds = create_tf_data(x_train, y_train, shuffle = True)\n",
    "    train_ds = train_ds.batch(2*BATCH_SIZE).prefetch(2).cache()\n",
    "    val_ds = val_ds.batch(2*BATCH_SIZE).prefetch(2).cache()\n",
    "    \n",
    "    model_new_ys = generate_model(SEQUENCE_LENGTH=5, \n",
    "                              NUM_FEATURES=6, \n",
    "                              trade_history_normalizer=trade_history_normalizer\n",
    "                             )\n",
    "\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H-%M')\n",
    "    fit_callbacks = fit_callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=10,\n",
    "        verbose=0,\n",
    "        mode=\"auto\",\n",
    "        restore_best_weights=True),\n",
    "        # time_callback,\n",
    "        CSVLoggerTimeHistory('_'.join([model_new_ys.name,timestamp,'training_logs.csv'])\n",
    "                             , separator=\",\", \n",
    "                             append=False)\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    model_new_ys.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "          loss=keras.losses.MeanAbsoluteError(),\n",
    "          metrics=[keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "    history_new_ys = model_new_ys.fit(train_ds,\n",
    "                                      validation_data=val_ds,\n",
    "                                        epochs=NUM_EPOCHS,     \n",
    "                                        verbose=1, \n",
    "                                        callbacks=fit_callbacks,                                        \n",
    "                                      use_multiprocessing=True,\n",
    "                                        workers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c7eea81-0c51-4ac7-bb58-4aef23343c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 19:05:58.597429: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 1084517 of 2212912\n",
      "2023-04-26 19:06:08.597450: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 1779101 of 2212912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/886 [..............................] - ETA: 7:10:36 - loss: 59.8816 - mean_absolute_error: 59.8816"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 19:06:13.677092: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:405] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "886/886 [==============================] - ETA: 0s - loss: 55.3494 - mean_absolute_error: 55.3494"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 19:06:43.228949: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 1071653 of 2212912\n",
      "2023-04-26 19:06:53.228943: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 2145358 of 2212912\n",
      "2023-04-26 19:06:53.851687: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:405] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "886/886 [==============================] - 90s 69ms/step - loss: 55.3494 - mean_absolute_error: 55.3494 - val_loss: 52.1391 - val_mean_absolute_error: 52.1391\n",
      "Epoch 2/20\n",
      "886/886 [==============================] - 13s 15ms/step - loss: 51.2680 - mean_absolute_error: 51.2680 - val_loss: 46.9597 - val_mean_absolute_error: 46.9597\n",
      "Epoch 3/20\n",
      "886/886 [==============================] - 13s 15ms/step - loss: 44.8739 - mean_absolute_error: 44.8739 - val_loss: 38.2392 - val_mean_absolute_error: 38.2392\n",
      "Epoch 4/20\n",
      "886/886 [==============================] - 13s 15ms/step - loss: 36.1072 - mean_absolute_error: 36.1072 - val_loss: 34.9983 - val_mean_absolute_error: 34.9983\n",
      "Epoch 5/20\n",
      "886/886 [==============================] - 20s 23ms/step - loss: 25.9093 - mean_absolute_error: 25.9093 - val_loss: 21.9260 - val_mean_absolute_error: 21.9260\n",
      "Epoch 6/20\n",
      "886/886 [==============================] - 13s 15ms/step - loss: 16.6674 - mean_absolute_error: 16.6674 - val_loss: 13.9651 - val_mean_absolute_error: 13.9651\n",
      "Epoch 7/20\n",
      "886/886 [==============================] - 13s 15ms/step - loss: 11.9667 - mean_absolute_error: 11.9667 - val_loss: 10.8686 - val_mean_absolute_error: 10.8686\n",
      "Epoch 8/20\n",
      "886/886 [==============================] - 13s 15ms/step - loss: 10.9310 - mean_absolute_error: 10.9310 - val_loss: 10.4619 - val_mean_absolute_error: 10.4619\n",
      "Epoch 9/20\n",
      "886/886 [==============================] - 20s 22ms/step - loss: 10.7053 - mean_absolute_error: 10.7053 - val_loss: 10.2817 - val_mean_absolute_error: 10.2817\n",
      "Epoch 10/20\n",
      "886/886 [==============================] - 13s 15ms/step - loss: 10.5719 - mean_absolute_error: 10.5719 - val_loss: 10.1639 - val_mean_absolute_error: 10.1639\n",
      "Epoch 11/20\n",
      "886/886 [==============================] - 13s 15ms/step - loss: 10.4664 - mean_absolute_error: 10.4664 - val_loss: 10.0697 - val_mean_absolute_error: 10.0697\n",
      "Epoch 12/20\n",
      "886/886 [==============================] - 13s 15ms/step - loss: 10.3807 - mean_absolute_error: 10.3807 - val_loss: 9.9952 - val_mean_absolute_error: 9.9952\n",
      "Epoch 13/20\n",
      "886/886 [==============================] - 21s 23ms/step - loss: 10.3042 - mean_absolute_error: 10.3042 - val_loss: 9.9344 - val_mean_absolute_error: 9.9344\n",
      "Epoch 14/20\n",
      "886/886 [==============================] - 14s 15ms/step - loss: 10.2342 - mean_absolute_error: 10.2342 - val_loss: 9.8889 - val_mean_absolute_error: 9.8889\n",
      "Epoch 15/20\n",
      "886/886 [==============================] - 13s 15ms/step - loss: 10.1741 - mean_absolute_error: 10.1741 - val_loss: 9.8583 - val_mean_absolute_error: 9.8583\n",
      "Epoch 16/20\n",
      "886/886 [==============================] - 13s 15ms/step - loss: 10.1207 - mean_absolute_error: 10.1207 - val_loss: 9.8064 - val_mean_absolute_error: 9.8064\n",
      "Epoch 17/20\n",
      "886/886 [==============================] - 20s 23ms/step - loss: 10.0676 - mean_absolute_error: 10.0676 - val_loss: 9.7629 - val_mean_absolute_error: 9.7629\n",
      "Epoch 18/20\n",
      "886/886 [==============================] - 13s 15ms/step - loss: 10.0179 - mean_absolute_error: 10.0179 - val_loss: 9.7144 - val_mean_absolute_error: 9.7144\n",
      "Epoch 19/20\n",
      "886/886 [==============================] - 13s 15ms/step - loss: 9.9740 - mean_absolute_error: 9.9740 - val_loss: 9.6778 - val_mean_absolute_error: 9.6778\n",
      "Epoch 20/20\n",
      "886/886 [==============================] - 14s 15ms/step - loss: 9.9318 - mean_absolute_error: 9.9318 - val_loss: 9.6262 - val_mean_absolute_error: 9.6262\n",
      "Model training time was 6.18 minutes (371.08 seconds).\n",
      "Average time for each epoch was 0.31 minutes (18.55 seconds).\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "# Normalization layer for the trade history\n",
    "trade_history_normalizer = Normalization(name='Trade_history_normalizer')\n",
    "trade_history_normalizer.adapt(x_train[0],batch_size=BATCH_SIZE)\n",
    "\n",
    "# Normalization layer for the non-categorical and binary features\n",
    "noncat_binary_normalizer = Normalization(name='Numerical_binary_normalizer')\n",
    "noncat_binary_normalizer.adapt(x_train[2], batch_size = BATCH_SIZE)\n",
    "\n",
    "tf.keras.utils.set_random_seed(10)\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    train_ds, val_ds = create_tf_data(x_train, y_train, shuffle = True)\n",
    "    train_ds = train_ds.batch(2*BATCH_SIZE).prefetch(2).cache()\n",
    "    val_ds = val_ds.batch(2*BATCH_SIZE).prefetch(2).cache()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    model_new_ys = generate_model(SEQUENCE_LENGTH=5, \n",
    "                              NUM_FEATURES=6, \n",
    "                              trade_history_normalizer=trade_history_normalizer\n",
    "                             )\n",
    "\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H-%M')\n",
    "    fit_callbacks = fit_callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=10,\n",
    "        verbose=0,\n",
    "        mode=\"auto\",\n",
    "        restore_best_weights=True),\n",
    "        # time_callback,\n",
    "        CSVLoggerTimeHistory('_'.join([model_new_ys.name,timestamp,'training_logs.csv'])\n",
    "                             , separator=\",\", \n",
    "                             append=False)\n",
    "    ]\n",
    "\n",
    "\n",
    "    model_new_ys.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "          loss=keras.losses.MeanAbsoluteError(),\n",
    "          metrics=[keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "    history_new_ys = model_new_ys.fit(train_ds,\n",
    "                                      validation_data=val_ds,\n",
    "                                        epochs=NUM_EPOCHS,     \n",
    "                                        verbose=1, \n",
    "                                        callbacks=fit_callbacks,                                        \n",
    "                                      use_multiprocessing=True,\n",
    "                                        workers=-1)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
