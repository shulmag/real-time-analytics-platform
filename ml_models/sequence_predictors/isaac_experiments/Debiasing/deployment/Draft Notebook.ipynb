{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "978b7893-4d93-4b95-84cf-553a1c56a24c",
   "metadata": {},
   "source": [
    "# Test notebook for cloud function to calculate model MAE during production:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b7be2a-9868-427a-9c20-9b153e0342b8",
   "metadata": {},
   "source": [
    "## TO DO LIST\n",
    "\n",
    "- (DONE) Test that debiasing works within the day, retroactively first (by loading all trades and processing them and then debiasing with masking) \n",
    "    - Based on 06-12, kind of. Bias decreaes but MAE does not. \n",
    "- To implement: \n",
    "    - (DONE)  timestamp checker to keep track of when it is a new day. When it is a new day: \n",
    "        - Refresh trades_df to zero \n",
    "        - If it is before some cutoff (e.g. 10 minutes before market open), don't bother running the MAE check \n",
    "    - (DONE)  time each run of the debiasing algorithm with masking \n",
    "        - no need to mask in production, all visible trades are fair game \n",
    "    - (TODO) we need to keep the entire dataframe of intraday trades in-memory, but each time we run debiasing we only need to run it on the most recent batch of trades \n",
    "        - maybe implement a simple pointer for which index in the groupby.expanding list to continue from \n",
    "            - we can only slice after we have applied groupby.expanding to ensure we have past trades in view \n",
    "    - (TODO) test if having a separate in memory dictionary to maintain rtrs control numbers helps, but do operations in a pandas dataframe for convenience \n",
    "    - (TODO) SHELVE weighting by quantity for now, but in future can just add it to the data processing when messages are parsed, if it proves useful. So far, it is only as good as simple averaging\n",
    "    - (TODO) CONFIRM CHECKPOINTING IS CORRECT THEN TEST ON FUNCTIONS FRAMEWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c2dbc-5b54-4eff-8898-ce98d1397470",
   "metadata": {},
   "source": [
    "\n",
    "## Important Notes:\n",
    "\n",
    "* **If time is before 8, dont run the entire function**\n",
    "* **MSRB Trade Messages:**\n",
    "    - M, trade modification: all fields present, replace all fields  \n",
    "    - C, trade cancellation: all fields present, but delete from dictionary (need to keep a separate copy of deleted cusips in-memory?)\n",
    "    - R, MSRB modification: all fields present, but delete from dictionary (need to keep a separate copy of deleted cusips in-memory?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31eac305-02c3-42bf-91d3-d28c239a56a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import functions_framework\n",
    "from auxiliary_functions import *\n",
    "import datetime\n",
    "import pytz\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/home/jupyter/ficc/isaac_creds.json\"\n",
    "\n",
    "# INITIALIZE WHEN CLOUD FUNCTION INSTANCE BEGINS \n",
    "\n",
    "last_updated = None\n",
    "trades_df = pd.DataFrame(columns = trades_df_cols)\n",
    "tz = pytz.timezone('US/Eastern')\n",
    "now = datetime.datetime.now(tz)\n",
    "processed_files = set()\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72a3d8a6-a94c-42dd-a5d7-a4b17a8495ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = gcsfs.GCSFileSystem(project='eng-reactor-287421')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6415801b-42b5-4554-9c17-1e2634453cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functions_framework.cloud_event    \n",
    "def main(cloud_event):\n",
    "    start = time.time()\n",
    "    now = datetime.datetime.now(tz)\n",
    "    \n",
    "    # # for eventarc trigger\n",
    "    # bucket = cloud_event.data[\"bucket\"]\n",
    "    # name = cloud_event.data[\"name\"]\n",
    "    \n",
    "    #for pub/sub\n",
    "    data = json.loads(base64.b64decode(cloud_event.data[\"message\"][\"data\"]))\n",
    "    bucket = data['bucket']\n",
    "    name = data['name']\n",
    "    try:\n",
    "        timestamp = re.search(re.compile('\\d{2}:\\d{2}'), name)[0]\n",
    "\n",
    "        #the timestamp on a file received can never be larger than the current time, because such a file does not exist \n",
    "        #it must hence be the same time or earlier, and if too long ago we exit the function\n",
    "        if (now.minute - int(timestamp[3:]) > 3) or (now.hour != int(timestamp[:2])):\n",
    "            print(f'{name} is asynchronous from time now, {now}. Exiting function.')\n",
    "            return name\n",
    "        \n",
    "    except Exception as e: \n",
    "        print(f'Error handling file {name} . Exception: {e}')\n",
    "        return\n",
    "        \n",
    "        \n",
    "    if name in processed_files:\n",
    "        return (f'{name} already processed. Exiting function.')\n",
    "        \n",
    "    else:\n",
    "        print(f'Function executing now, {now}, for {name}.')\n",
    "    \n",
    "    if not bucket or not name: \n",
    "        raise ValueError(f'Eventarc did not send a valid cloud storage bucket or file. Cloud event arguments parsed were {(bucket, name)}')\n",
    "        \n",
    "    global last_updated, trades_df\n",
    "    fs = gcsfs.GCSFileSystem(project='eng-reactor-287421')\n",
    "    \n",
    "    #TODO: simplify the if-else logic in the following blocks \n",
    "    \n",
    "    #if it is a new day or cloud function just initialized, note down the current date and reset the trade dataframe\n",
    "    if not last_updated or last_updated < now.date(): \n",
    "        if not last_updated:\n",
    "            print('No last update time, cloud function has just been initialized.')\n",
    "        else:\n",
    "            if last_updated < now.date():\n",
    "                print(f'Trades_df last updated {last_updated}, current date is {datetime.datetime.now(tz).date()}. Refreshing trades_df.')\n",
    "        reset_cloud_function_state(now)\n",
    "        print('Cloud function state reset')\n",
    "       \n",
    "    #Only run function from 1 hour before market open till 1 hour after market close\n",
    "    if (now.hour <=8 and now.minute <= 30):\n",
    "        print('Before market open, function will not run.')\n",
    "        return name\n",
    "    elif now.hour >= 22:\n",
    "        print('After market close, function will not run.')    \n",
    "        return name\n",
    "    #If trades_df is empty, it is either the start of the data or the cloud function instance was just initialized\n",
    "    #If cloud function just initialized and it is during market open hours, then we check for a checkpoint file first\n",
    "    #If that checkpoint file is not available, then we can safely start trades_df from scratch\n",
    "    else: \n",
    "        trade_log_path = f\"gs://biases/trade_log_{now.strftime('%Y-%m-%d')}.pkl\"\n",
    "        if not len(trades_df):            \n",
    "            try:\n",
    "                print('Loading checkpoint')\n",
    "                trades_df = load_from_cloud_storage(trade_log_path, fs)\n",
    "                print(f\"Successfully loaded checkpoint {trade_log_path} from cloud storage, with {len(trades_df)} rows, from {trades_df['published_datetime'].min()} to {trades_df['published_datetime'].max()}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f'Cannot load {trade_log_path} from cloud storage, resetting trades_df. Exception: {e}')\n",
    "                reset_cloud_function_state(now)\n",
    "\n",
    "    print('Beginning to process trade messages.')\n",
    "    N = len(trades_df)\n",
    "    update_intraday_cusips(*get_trade_messages(os.path.join(bucket,name), fs), trades_df)\n",
    "    print(f'Trades processed, trades_df rows changed by {len(trades_df) - N}.')\n",
    "    if len(trades_df):    \n",
    "        upload_to_cloud_storage(trade_log_path, trades_df, fs)\n",
    "        bias = simulate_weighted_average(trades_df, \n",
    "                                         weighting_col = 'error', \n",
    "                                         error_col = 'error', \n",
    "                                         groupby_cols=['trade_date'], \n",
    "                                         window_size=2000, \n",
    "                                         weighting_method='simple_average', \n",
    "                                         mask_large=0.3)\n",
    "        bias = bias_warm_start(bias, 500)\n",
    "        bias_path = f\"gs://biases/bias_{now.strftime('%Y-%m-%d')}.pkl\"\n",
    "        bias_path_demo = f\"gs://biases/bias_for_demo.pkl\"\n",
    "        upload_to_cloud_storage(bias_path, bias, fs)\n",
    "        upload_to_cloud_storage(bias_path_demo, bias, fs)\n",
    "        processed_files.add(name)\n",
    "        print(f'Biases updated with trades from {name} to {bias_path}.') \n",
    "        \n",
    "    print(f'Entire function took {time.time() - start}')\n",
    "    return 'SUCCESS'\n",
    "\n",
    "\n",
    "def reset_cloud_function_state(now):\n",
    "    '''Resets the state of the cloud function and sets last_updated to the current date'''\n",
    "    \n",
    "    global last_updated, trades_df, processed_files\n",
    "    print('Resetting cloud function state') #DEBUG\n",
    "    \n",
    "    last_updated = now.date()\n",
    "    trades_df = pd.DataFrame(columns = trades_df_cols)\n",
    "    processed_files = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e65cc096-a720-4a9a-bc4c-9d54458c40c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting trade messages\n",
      "Trade messages loaded\n",
      "Processing 64 trade messages\n"
     ]
    }
   ],
   "source": [
    "bucket = 'msrb_intraday_real_time_trade_files'\n",
    "name = 'real_time_msrb_file_2023-06-23_13:20:02.json'\n",
    "\n",
    "processed_trade_messages = get_trade_messages(os.path.join(bucket, name), fs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b4653e4b-bebd-4424-b221-162c5b084714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with fs.open(f\"gs://biases/trade_log_{now.strftime('%Y-%m-%d')}.pkl\", 'rb') as f:\n",
    "#     trades_df = pickle.load(f)\n",
    "\n",
    "# trades_df = trades_df.reset_index().rename({'index':'rtrs_control_number'}, axis=1)\n",
    "# trades_df.rtrs_control_number = trades_df.rtrs_control_number.astype(int)\n",
    "# uploadData(trades_df, 'eng-reactor-287421.debiasing.msrb_intraday_real_time_trade_predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cd76b933-6fd9-4139-a904-26f56f79be7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "update_intraday_cusips() missing 2 required positional arguments: 'cancellations' and 'trades_df'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25397/2559071889.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mupdate_intraday_cusips\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_trade_messages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: update_intraday_cusips() missing 2 required positional arguments: 'cancellations' and 'trades_df'"
     ]
    }
   ],
   "source": [
    "update_intraday_cusips(processed_trade_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cba3cfd-1466-4fe7-981f-933202176d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
