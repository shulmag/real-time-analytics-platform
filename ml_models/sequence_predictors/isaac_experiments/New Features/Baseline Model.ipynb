{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54acdc5b-7dc8-4578-a0bd-02740ba377a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Packages, Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58de4fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 18:59:21.731161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-19 18:59:21.743565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-19 18:59:21.745205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing pandarallel with 16.0 cores\n",
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "TF Version: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import gcsfs\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import seaborn as sns\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle5 as pickle\n",
    "\n",
    "\n",
    "from ficc.utils.nelson_siegel_model import *\n",
    "from ficc.utils.diff_in_days import *\n",
    "from ficc.utils.auxiliary_functions import sqltodf\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "\n",
    "\n",
    "from ficc.data.process_data import process_data\n",
    "from ficc.utils.auxiliary_variables import PREDICTORS, NON_CAT_FEATURES, BINARY, CATEGORICAL_FEATURES, IDENTIFIERS, PURPOSE_CLASS_DICT, NUM_OF_DAYS_IN_YEAR\n",
    "from ficc.utils.gcp_storage_functions import upload_data, download_data\n",
    "from ficc.utils.auxiliary_variables import RELATED_TRADE_BINARY_FEATURES, RELATED_TRADE_NON_CAT_FEATURES, RELATED_TRADE_CATEGORICAL_FEATURES\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from ficc_keras_utils import *\n",
    "import ficc_keras_utils\n",
    "from lgbm_tools import *\n",
    "from ficc_debiasing import *\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "print(f'TF Version: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07c11246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/home/jupyter/ficc/isaac_creds.json\"\n",
    "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "bq_client = bigquery.Client()\n",
    "storage_client = storage.Client()\n",
    "\n",
    "##COMMON VARIABLES\n",
    "#DATA WINDOW\n",
    "train_start = '2023-05-01'\n",
    "train_end = '2023-06-01'\n",
    "test_start = '2023-06-01'\n",
    "test_end = '2023-07-01'\n",
    "#MODEL PARAMETERS \n",
    "VALIDATION_SPLIT = ficc_keras_utils.VALIDATION_SPLIT\n",
    "LEARNING_RATE = ficc_keras_utils.LEARNING_RATE\n",
    "BATCH_SIZE = 1000 #ficc_keras_utils.BATCH_SIZE\n",
    "NUM_EPOCHS = 75 #ficc_keras_utils.NUM_EPOCHS\n",
    "DROPOUT = 0.01 #ficc_keras_utils.DROPOUT\n",
    "\n",
    "##NOTEBOOK SPECIFIC VARIABLES \n",
    "TRADE_SEQUENCE_LENGTH = 2\n",
    "NUM_FEATURES = 6\n",
    "# target_variable = 'new_ys_diff'\n",
    "# target_variable = 'new_ys' \n",
    "target_variable = 'new_ys' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ca18089-58e8-4a2e-9c50-11898bfde236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-01 2023-06-01 2023-06-01 2023-07-01\n"
     ]
    }
   ],
   "source": [
    "print(train_start ,\n",
    "train_end ,\n",
    "test_start ,\n",
    "test_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917f52d4-5834-401a-a2ac-46559494098f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Data and Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "877a208b-c665-48ad-a8da-316b29c256b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File available, loading pickle\n",
      "CPU times: user 6.7 s, sys: 2.73 s, total: 9.43 s\n",
      "Wall time: 9.43 s\n"
     ]
    }
   ],
   "source": [
    "%time processed_data = load_data_from_pickle('processed_file_FULL_2023-07-12-17:22.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e934f6f-e216-4871-aea7-598e8c16140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = processed_data[~(processed_data.trade_history.apply(lambda x: x.shape) == (5,6))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ab6562f-cb41-4dcf-91a5-17d7373d4d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data['trade_history'] = processed_data.trade_history.apply(lambda x: x[:,[0,2,3,4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a792e5f4-76fa-48db-8746-63e50f1603c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data['target_attention_features'] = processed_data['target_attention_features'].apply(lambda x: x[[0],:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c184fe-3bff-410b-9bfe-44392274d494",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Checking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e94dfab-86e9-4cc1-91eb-c017cee0d551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2023-05-01 00:00:00'), Timestamp('2023-07-11 00:00:00'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.trade_date.min(), processed_data.trade_date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be6e3073-3be5-4b5c-a7fa-c0f42f623aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yield</th>\n",
       "      <th>new_real_time_ficc_ycl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1746702.000</td>\n",
       "      <td>1746702.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>364.498</td>\n",
       "      <td>323.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>91.917</td>\n",
       "      <td>41.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.100</td>\n",
       "      <td>263.663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>307.000</td>\n",
       "      <td>297.164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>350.300</td>\n",
       "      <td>310.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>411.000</td>\n",
       "      <td>336.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9892.500</td>\n",
       "      <td>541.456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            yield  new_real_time_ficc_ycl\n",
       "count 1746702.000             1746702.000\n",
       "mean      364.498                 323.013\n",
       "std        91.917                  41.183\n",
       "min         0.100                 263.663\n",
       "25%       307.000                 297.164\n",
       "50%       350.300                 310.806\n",
       "75%       411.000                 336.136\n",
       "max      9892.500                 541.456"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data[['yield','new_real_time_ficc_ycl']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2a60e40-35a7-4769-9fd2-399bb2f0cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f5c8ed7-c8f9-433b-92db-2868d3c716f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data['new_ys_realtime'] = processed_data['yield'] - processed_data['new_real_time_ficc_ycl']\n",
    "processed_data['new_ys'] = processed_data['yield'] - processed_data['new_ficc_ycl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a46c03ad-d7d5-4492-bba2-4cf4aef63c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### EXTRA DATA PREPROCESSING #####\n",
    "ttype_dict = { (0,0):'D', (0,1):'S', (1,0):'P' }\n",
    "\n",
    "ys_variants = [\"max_ys\", \"min_ys\", \"max_qty\", \"min_ago\", \"D_min_ago\", \"P_min_ago\", \"S_min_ago\"]\n",
    "ys_feats = [\"_ys\", \"_ttypes\", \"_ago\", \"_qdiff\"]\n",
    "D_prev = dict()\n",
    "P_prev = dict()\n",
    "S_prev = dict()\n",
    "\n",
    "def get_trade_history_columns():\n",
    "    '''\n",
    "    This function is used to create a list of columns\n",
    "    '''\n",
    "    YS_COLS = []\n",
    "    for prefix in ys_variants:\n",
    "        for suffix in ys_feats:\n",
    "            YS_COLS.append(prefix + suffix)\n",
    "    return YS_COLS\n",
    "\n",
    "def extract_feature_from_trade(row, name, trade):\n",
    "    yield_spread = trade[0]\n",
    "    ttypes = ttype_dict[(trade[3],trade[4])] + row.trade_type\n",
    "    seconds_ago = trade[5]\n",
    "    quantity_diff = np.log10(1 + np.abs(10**trade[2] - 10**row.quantity))\n",
    "    return [yield_spread, ttypes,  seconds_ago, quantity_diff]\n",
    "\n",
    "def trade_history_derived_features(row):\n",
    "    trade_history = row.trade_history\n",
    "    trade = trade_history[0]\n",
    "    \n",
    "    D_min_ago_t = D_prev.get(row.cusip,trade)\n",
    "    D_min_ago = 9        \n",
    "\n",
    "    P_min_ago_t = P_prev.get(row.cusip,trade)\n",
    "    P_min_ago = 9\n",
    "    \n",
    "    S_min_ago_t = S_prev.get(row.cusip,trade)\n",
    "    S_min_ago = 9\n",
    "    \n",
    "    max_ys_t = trade; max_ys = trade[0]\n",
    "    min_ys_t = trade; min_ys = trade[0]\n",
    "    max_qty_t = trade; max_qty = trade[2]\n",
    "    min_ago_t = trade; min_ago = trade[5]\n",
    "    \n",
    "    for trade in trade_history[0:]:\n",
    "        #Checking if the first trade in the history is from the same block\n",
    "        if trade[5] == 0: \n",
    "            continue\n",
    " \n",
    "        if trade[0] > max_ys: \n",
    "            max_ys_t = trade\n",
    "            max_ys = trade[0]\n",
    "        elif trade[0] < min_ys: \n",
    "            min_ys_t = trade; \n",
    "            min_ys = trade[0]\n",
    "\n",
    "        if trade[2] > max_qty: \n",
    "            max_qty_t = trade \n",
    "            max_qty = trade[2]\n",
    "        if trade[5] < min_ago: \n",
    "            min_ago_t = trade; \n",
    "            min_ago = trade[5]\n",
    "            \n",
    "        side = ttype_dict[(trade[3],trade[4])]\n",
    "        if side == \"D\":\n",
    "            if trade[5] < D_min_ago: \n",
    "                D_min_ago_t = trade; D_min_ago = trade[5]\n",
    "                D_prev[row.cusip] = trade\n",
    "        elif side == \"P\":\n",
    "            if trade[5] < P_min_ago: \n",
    "                P_min_ago_t = trade; P_min_ago = trade[5]\n",
    "                P_prev[row.cusip] = trade\n",
    "        elif side == \"S\":\n",
    "            if trade[5] < S_min_ago: \n",
    "                S_min_ago_t = trade; S_min_ago = trade[5]\n",
    "                S_prev[row.cusip] = trade\n",
    "        else: \n",
    "            print(\"invalid side\", trade)\n",
    "    \n",
    "    trade_history_dict = {\"max_ys\":max_ys_t,\n",
    "                          \"min_ys\":min_ys_t,\n",
    "                          \"max_qty\":max_qty_t,\n",
    "                          \"min_ago\":min_ago_t,\n",
    "                          \"D_min_ago\":D_min_ago_t,\n",
    "                          \"P_min_ago\":P_min_ago_t,\n",
    "                          \"S_min_ago\":S_min_ago_t}\n",
    "\n",
    "    return_list = []\n",
    "    for variant in ys_variants:\n",
    "        feature_list = extract_feature_from_trade(row,variant,trade_history_dict[variant])\n",
    "        return_list += feature_list\n",
    "    \n",
    "    return return_list\n",
    "\n",
    "def process_data(data): \n",
    "    data['ted-rate'] = (data['t_rate_10'] - data['t_rate_2']) * 100\n",
    "    \n",
    "    data = data[(data.days_to_call == 0) | (data.days_to_call > np.log10(400))]\n",
    "    data = data[(data.days_to_refund == 0) | (data.days_to_refund > np.log10(400))]\n",
    "    data = data[(data.days_to_maturity == 0) | (data.days_to_maturity > np.log10(400))]\n",
    "    data = data[data.days_to_maturity < np.log10(30000)]\n",
    "    data['trade_history_sum'] = data.trade_history.parallel_apply(lambda x: np.sum(x))\n",
    "    data.issue_amount = data.issue_amount.replace([np.inf, -np.inf], np.nan)\n",
    "    data.dropna(inplace=True, subset=PREDICTORS+['trade_history_sum'])\n",
    "    data.purpose_sub_class.fillna(0, inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "964a86ed-8535-4195-8ff8-88f9da5e8999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.2 s, sys: 5.81 s, total: 47 s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "temp = processed_data[['cusip','trade_history','quantity','trade_type']].parallel_apply(trade_history_derived_features, axis=1)\n",
    "YS_COLS = get_trade_history_columns()\n",
    "processed_data[YS_COLS] = pd.DataFrame(temp.tolist(), index=processed_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06f9fae8-c191-49ac-af98-2f6b720b663c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 853 ms, sys: 28 ms, total: 881 ms\n",
      "Wall time: 880 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "processed_data['ttypes'] = (processed_data.last_trade_type.astype(str) + processed_data.trade_type.astype(str)).astype('category')\n",
    "processed_data['diff_size'] = (processed_data.par_traded.astype(float) - processed_data.last_size).astype(np.float32)\n",
    "processed_data['abs_last_yield_spread'] = np.abs(processed_data['last_yield_spread'])\n",
    "processed_data['abs_diff_size'] = np.abs(processed_data['diff_size'])\n",
    "processed_data['days_duration'] = (processed_data.last_calc_date - processed_data.last_settlement_date).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddf12468-0634-4ac8-8085-17777a34d796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.5 s, sys: 3.62 s, total: 16.1 s\n",
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "processed_data['trade_history_sum'] = processed_data.trade_history.parallel_apply(lambda x: np.sum(x))\n",
    "processed_data = processed_data.dropna(subset=['trade_history_sum'])\n",
    "processed_data.purpose_sub_class.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e294b38-2ba3-49ec-930e-8f8d3a5a0187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in NON_CAT_FEATURES:\n",
    "#     print(col + ':' + str(train_dataframe[col].isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26e3d97e-c3b0-4511-b6de-79717352924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['new_ficc_ycl', 'yield_spread']:\n",
    "    if col in NON_CAT_FEATURES:\n",
    "        NON_CAT_FEATURES.remove(col)\n",
    "    if col in PREDICTORS:\n",
    "        PREDICTORS.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "302404ee-098f-4e8f-9bd3-cfc57fc96974",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = process_data(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc47f4e4-9a1a-4ffa-932c-5413827d0731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating\n",
      "incorporated_state_code\n",
      "trade_type\n",
      "purpose_class\n",
      "max_ys_ttypes\n",
      "min_ys_ttypes\n",
      "max_qty_ttypes\n",
      "min_ago_ttypes\n",
      "D_min_ago_ttypes\n",
      "P_min_ago_ttypes\n",
      "S_min_ago_ttypes\n"
     ]
    }
   ],
   "source": [
    "additional_features = ['ttypes', 'diff_size', 'abs_last_yield_spread', 'abs_diff_size', 'days_duration']\n",
    "YS_COLS = ['max_ys_ys', 'max_ys_ttypes', 'max_ys_ago', 'max_ys_qdiff', 'min_ys_ys', 'min_ys_ttypes', 'min_ys_ago', 'min_ys_qdiff', 'max_qty_ys', 'max_qty_ttypes', \\\n",
    "           'max_qty_ago', 'max_qty_qdiff', 'min_ago_ys', 'min_ago_ttypes', 'min_ago_ago', 'min_ago_qdiff', 'D_min_ago_ys', 'D_min_ago_ttypes', 'D_min_ago_ago', 'D_min_ago_qdiff',\\\n",
    "           'P_min_ago_ys', 'P_min_ago_ttypes', 'P_min_ago_ago', 'P_min_ago_qdiff', 'S_min_ago_ys', 'S_min_ago_ttypes', 'S_min_ago_ago', 'S_min_ago_qdiff']\n",
    "    \n",
    "for col in YS_COLS:\n",
    "    if 'ttypes' in col and col not in PREDICTORS:\n",
    "        PREDICTORS.append(col)\n",
    "        CATEGORICAL_FEATURES.append(col)\n",
    "    elif col not in PREDICTORS:\n",
    "        NON_CAT_FEATURES.append(col)\n",
    "        PREDICTORS.append(col)\n",
    "        \n",
    "if 'target_attention_features' not in PREDICTORS:\n",
    "    PREDICTORS.append('target_attention_features')\n",
    "    \n",
    "if 'ficc_treasury_spread' not in PREDICTORS:\n",
    "    PREDICTORS.append('ficc_treasury_spread')\n",
    "    NON_CAT_FEATURES.append('ficc_treasury_spread')\n",
    "    \n",
    "for col in ['new_ficc_ycl', 'new_real_time_ficc_ycl']:     \n",
    "    if col not in PREDICTORS:\n",
    "        PREDICTORS.append(col)\n",
    "        NON_CAT_FEATURES.append(col)\n",
    "        \n",
    "encoders = {}\n",
    "fmax = {}\n",
    "for f in CATEGORICAL_FEATURES:\n",
    "    print(f)\n",
    "    if f in ['rating', 'incorporated_state_code', 'trade_type', 'purpose_class']:\n",
    "        fprep = preprocessing.LabelEncoder().fit(categorical_feature_values[f])\n",
    "    else:\n",
    "        fprep = preprocessing.LabelEncoder().fit(processed_data[f].drop_duplicates())\n",
    "    fmax[f] = np.max(fprep.transform(fprep.classes_))\n",
    "    encoders[f] = fprep\n",
    "    \n",
    "with open('encoders.pkl','wb') as file:\n",
    "    pickle.dump(encoders,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b92bf3e9-bd78-4aa1-a031-892a653b3524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 810 ms, sys: 68.3 ms, total: 878 ms\n",
      "Wall time: 877 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "processed_data['trade_history_shortened'] = processed_data['trade_history'].apply(lambda x: x[:TRADE_SEQUENCE_LENGTH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb0d6e5b-5e69-454e-9d53-348fc11ad2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.89 s, sys: 95.4 ms, total: 1.99 s\n",
      "Wall time: 2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "processed_data['trade_history_fixed'] = processed_data['trade_history_shortened'].apply(lambda x: x[::-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "408ba4ae-31be-4b37-be1b-a6c143637c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2023-05-01 00:00:00'), Timestamp('2023-07-11 00:00:00'), 1590785)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.trade_date.min(), processed_data.trade_date.max(), len(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9daf580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data start: 2023-05-01 00:00:00, end: 2023-05-31 00:00:00\n",
      "Test data start: 2023-06-01 00:00:00, end: 2023-06-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "train_filter = (processed_data.trade_date < train_end) & (processed_data.trade_date >= train_start)\n",
    "test_filter = (processed_data.trade_date >= test_start) & (processed_data.trade_date <test_end)\n",
    "                                                            \n",
    "train_dataframe = processed_data[train_filter]\\\n",
    ".sort_values(by='trade_date', ascending=True)\\\n",
    ".reset_index(drop=True)\n",
    "\n",
    "test_dataframe = processed_data[test_filter]\\\n",
    ".sort_values(by='trade_date', ascending=True)\\\n",
    ".reset_index(drop=True)\n",
    "\n",
    "print('Training data start: {}, end: {}'.format(train_dataframe.trade_date.min(),train_dataframe.trade_date.max()))\n",
    "print('Test data start: {}, end: {}'.format(test_dataframe.trade_date.min(),test_dataframe.trade_date.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00361ac5-8d00-45be-a40e-9bd8fdbf09ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-01 00:00:00 2023-05-31 00:00:00 705233\n",
      "2023-06-01 00:00:00 2023-06-30 00:00:00 697202\n"
     ]
    }
   ],
   "source": [
    "print(train_dataframe.trade_date.min(), train_dataframe.trade_date.max(), len(train_dataframe))\n",
    "print(test_dataframe.trade_date.min(), test_dataframe.trade_date.max(), len(test_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "868843a0-6fbf-4b47-9203-68b31a9e1835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed new_ficc_ycl\n",
      "Removed new_real_time_ficc_ycl\n"
     ]
    }
   ],
   "source": [
    "for f in ['new_ficc_ycl', 'new_real_time_ficc_ycl']: \n",
    "    if f in NON_CAT_FEATURES:\n",
    "        NON_CAT_FEATURES.remove(f)\n",
    "    if f in BINARY:\n",
    "        NON_CAT_FEATURES.remove(f)\n",
    "    print(f'Removed {f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f44b3bb4-26e1-43ad-a85b-937a3f912834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_new(df, trade_history_col, yield_history_cols):\n",
    "    global encoders\n",
    "    datalist = []\n",
    "    \n",
    "    for col in yield_history_cols:\n",
    "        datalist.append(np.stack(df[col].to_numpy()))\n",
    "        \n",
    "    datalist.append(np.stack(df[trade_history_col].to_numpy()))\n",
    "    datalist.append(np.stack(df['target_attention_features'].to_numpy()))\n",
    "\n",
    "    noncat_and_binary = []\n",
    "    for f in NON_CAT_FEATURES + BINARY:\n",
    "        noncat_and_binary.append(np.expand_dims(df[f].to_numpy().astype('float32'), axis=1))\n",
    "    datalist.append(np.concatenate(noncat_and_binary, axis=-1))\n",
    "    \n",
    "    for f in CATEGORICAL_FEATURES:\n",
    "        encoded = encoders[f].transform(df[f])\n",
    "        datalist.append(encoded.astype('float32'))\n",
    "    \n",
    "    return datalist\n",
    "\n",
    "def generate_model_new(TRADE_SEQUENCE_LENGTH, yield_history_cols, yield_history_lengths, num_yield_history,\n",
    "                      yield_history_normalizers, trade_history_normalizer, noncat_binary_normalizer):\n",
    "    inputs = []\n",
    "    layer = []\n",
    "\n",
    "    ############## INPUT BLOCK ###################\n",
    "    for i in range(num_yield_history):\n",
    "        inputs.append(layers.Input(name=f\"yield_history_input_{yield_history_cols[i]}\", \n",
    "                                           shape=(yield_history_lengths[i], 1), \n",
    "                                           dtype = tf.float32))\n",
    "    \n",
    "    trade_history_input = layers.Input(name=\"trade_history_input\", \n",
    "                                       shape=(TRADE_SEQUENCE_LENGTH, NUM_FEATURES), \n",
    "                                       dtype = tf.float32) \n",
    "\n",
    "    target_attention_input = layers.Input(name=\"target_attention_input\", \n",
    "                                       shape=(1, 3), \n",
    "                                       dtype = tf.float32) \n",
    "    inputs.append(trade_history_input)\n",
    "    inputs.append(target_attention_input)\n",
    "\n",
    "    inputs.append(layers.Input(\n",
    "        name=\"NON_CAT_AND_BINARY_FEATURES\",\n",
    "        shape=(len(NON_CAT_FEATURES + BINARY),)\n",
    "    ))\n",
    "\n",
    "\n",
    "    layer.append(noncat_binary_normalizer(inputs[num_yield_history+2]))\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "    ############## TRADE HISTORY MODEL #################\n",
    "\n",
    "    lstm_layer = layers.Bidirectional(layers.LSTM(50, \n",
    "                             activation='tanh',\n",
    "                             input_shape=(TRADE_SEQUENCE_LENGTH,NUM_FEATURES),\n",
    "                             return_sequences = True,\n",
    "                             name='LSTM'))\n",
    "\n",
    "    lstm_layer_2 = layers.Bidirectional(layers.LSTM(100, \n",
    "                                                    activation='tanh',\n",
    "                                                    input_shape=(TRADE_SEQUENCE_LENGTH, 50),\n",
    "                                                    return_sequences = True,\n",
    "                                                    name='LSTM_2'))\n",
    "\n",
    "\n",
    "\n",
    "    features = lstm_layer(trade_history_normalizer(inputs[num_yield_history]))\n",
    "    features = lstm_layer_2(features)  \n",
    "    \n",
    "    \n",
    "    attention_sequence = layers.Dense(200, activation='relu', name='attention_dense')(target_attention_input)\n",
    "    attention = layers.Dot(axes=[2, 2])([features, attention_sequence])\n",
    "    attention = layers.Activation('softmax')(attention)\n",
    "\n",
    "    context_vector = layers.Dot(axes=[1, 1])([features, attention])\n",
    "    context_vector = layers.Flatten(name='context_vector_flatten')(context_vector)\n",
    "\n",
    "\n",
    "    trade_history_output = layers.Dense(100, \n",
    "                                        activation='relu')(context_vector)\n",
    "\n",
    "    ####################################################\n",
    "    \n",
    "    ############## YIELD HISTORY MODEL #################\n",
    "    yield_history_outputs = []\n",
    "    for i in range(num_yield_history):\n",
    "        yield_lstm_layer = layers.Bidirectional(layers.LSTM(50, \n",
    "                                 activation='tanh',\n",
    "                                 input_shape=(yield_history_lengths[i], 1),\n",
    "                                 return_sequences = False,\n",
    "                                 name=f'Yield_History_LSTM_{yield_history_cols[i]}'))\n",
    "\n",
    "        yield_features = yield_lstm_layer(yield_history_normalizers[i](inputs[i]))\n",
    "        yield_history_outputs.append(layers.Dense(25, activation='relu')(yield_features))\n",
    " \n",
    "    ####################################################\n",
    "\n",
    "    ############## REFERENCE DATA MODEL ################\n",
    "    global encoders\n",
    "    for f in CATEGORICAL_FEATURES:\n",
    "        fin = layers.Input(shape=(1,), name = f)\n",
    "        inputs.append(fin)\n",
    "        embedded = layers.Flatten(name = f + \"_flat\")( layers.Embedding(input_dim = fmax[f]+1,\n",
    "                                                                        output_dim = max(30,int(np.sqrt(fmax[f]))),\n",
    "                                                                        input_length= 1,\n",
    "                                                                        name = f + \"_embed\")(fin))\n",
    "        layer.append(embedded)\n",
    "\n",
    "\n",
    "    reference_hidden = layers.Dense(400,\n",
    "                                    activation='relu',\n",
    "                                    name='reference_hidden_1')(layers.concatenate(layer, axis=-1))\n",
    "\n",
    "    reference_hidden = layers.BatchNormalization()(reference_hidden)\n",
    "    reference_hidden = layers.Dropout(DROPOUT)(reference_hidden)\n",
    "\n",
    "    reference_hidden2 = layers.Dense(200,activation='relu',name='reference_hidden_2')(reference_hidden)\n",
    "    reference_hidden2 = layers.BatchNormalization()(reference_hidden2)\n",
    "    reference_hidden2 = layers.Dropout(DROPOUT)(reference_hidden2)\n",
    "\n",
    "    reference_output = layers.Dense(100,activation='tanh',name='reference_hidden_3')(reference_hidden2)\n",
    "\n",
    "    ####################################################\n",
    "\n",
    "    feed_forward_input = layers.concatenate(yield_history_outputs+[reference_output, trade_history_output])\n",
    "\n",
    "    hidden = layers.Dense(300,activation='relu')(feed_forward_input)\n",
    "    hidden = layers.BatchNormalization()(hidden)\n",
    "    hidden = layers.Dropout(DROPOUT)(hidden)\n",
    "\n",
    "    hidden2 = layers.Dense(100,activation='relu')(hidden)\n",
    "    hidden2 = layers.BatchNormalization()(hidden2)\n",
    "    hidden2 = layers.Dropout(DROPOUT)(hidden2)\n",
    "    final = layers.Dense(1)(hidden2)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=final)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def create_data_set_and_model(train_dataframe, test_dataframe, trade_history_col, yield_history_cols):\n",
    "    \n",
    "    if not isinstance(trade_history_col, str):\n",
    "        raise ValueError('trade_history_col must be a string')\n",
    "    \n",
    "    if isinstance(yield_history_cols, str):\n",
    "        num_yield_history = 1\n",
    "        yield_history_cols = [yield_history_cols]\n",
    "    else:\n",
    "        num_yield_history = len(yield_history_cols)\n",
    "    \n",
    "    yield_history_lengths = [train_dataframe[x][0].shape[0] for x in yield_history_cols]\n",
    "    \n",
    "    TRADE_SEQUENCE_LENGTH = train_dataframe[trade_history_col][0].shape[0] \n",
    "    \n",
    "    params = {'TRADE_SEQUENCE_LENGTH':TRADE_SEQUENCE_LENGTH, \n",
    "           'yield_history_cols':yield_history_cols, \n",
    "           'yield_history_lengths':yield_history_lengths, \n",
    "           'num_yield_history':num_yield_history }\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    val_idx = np.random.choice(range(len(train_dataframe)), \n",
    "                     size = int(VALIDATION_SPLIT*len(train_dataframe)),\n",
    "                     replace=False)\n",
    "\n",
    "    print(f'TRAINING DATA: N = {len(train_dataframe)-len(val_idx)}, MIN DATE = {train_dataframe.drop(val_idx, axis=0).trade_date.min()}, MAX DATE = {train_dataframe.drop(val_idx, axis=0).trade_date.max()}')\n",
    "    print(f'VALIDATION DATA: N = {len(val_idx)}, MIN DATE = {train_dataframe.iloc[val_idx].trade_date.min()}, MAX DATE = {train_dataframe.iloc[val_idx].trade_date.max()}')\n",
    "    print(f'TEST DATA: N = {len(test_dataframe)}, MIN DATE = {test_dataframe.trade_date.min()}, MAX DATE = {test_dataframe.trade_date.max()}')\n",
    "\n",
    "    x_train = create_input_new(train_dataframe.drop(val_idx, axis=0), trade_history_col, yield_history_cols)\n",
    "    y_train = train_dataframe.drop(val_idx, axis=0)[target_variable]\n",
    "\n",
    "    x_val = create_input_new(train_dataframe.iloc[val_idx], trade_history_col, yield_history_cols)\n",
    "    y_val = train_dataframe.iloc[val_idx][target_variable]\n",
    "\n",
    "    x_test = create_input_new(test_dataframe, trade_history_col, yield_history_cols)\n",
    "    y_test = test_dataframe[target_variable]    \n",
    "    \n",
    "    # Normalization layer for the yield history\n",
    "    yield_history_normalizers = []\n",
    "    for i in range(num_yield_history):\n",
    "        yield_history_normalizers.append(Normalization(name=f'Yield_history_normalizer_{yield_history_cols[i]}'))\n",
    "        yield_history_normalizers[i].adapt(x_train[i],batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Normalization layer for the trade history\n",
    "    trade_history_normalizer = Normalization(name='Trade_history_normalizer')\n",
    "    trade_history_normalizer.adapt(x_train[num_yield_history],batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Normalization layer for the non-categorical and binary features\n",
    "    noncat_binary_normalizer = Normalization(name='Numerical_binary_normalizer')\n",
    "    noncat_binary_normalizer.adapt(x_train[num_yield_history+2], batch_size = BATCH_SIZE)\n",
    "\n",
    "    normalizers = {'yield_history_normalizers': yield_history_normalizers,\n",
    "                  'trade_history_normalizer': trade_history_normalizer,\n",
    "                  'noncat_binary_normalizer': noncat_binary_normalizer}\n",
    "\n",
    "    return  params, normalizers, x_train, y_train, x_val, y_val, x_test, y_test, val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16e33f76-449a-4655-882f-8b3fe873099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_data(x_train, y_train, shuffle=False, shuffle_buffer=1):\n",
    "                     \n",
    "    X=()\n",
    "    for x in x_train:\n",
    "        X += (tf.data.Dataset.from_tensor_slices(x),)\n",
    "        \n",
    "\n",
    "    temp = tf.data.Dataset.zip((X))\n",
    "    del X\n",
    "    dataset = tf.data.Dataset.zip((temp,\n",
    "                        tf.data.Dataset.from_tensor_slices(y_train)))\n",
    "    del temp\n",
    "    if shuffle:\n",
    "        shuffle_buffer = int(len(x_train[0])*shuffle_buffer)\n",
    "        dataset = dataset.shuffle(shuffle_buffer)\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c7eea81-0c51-4ac7-bb58-4aef23343c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_new(params, normalizers, x_train, y_train, x_val, y_val, shuffle, shuffle_buffer=1):\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    TRADE_SEQUENCE_LENGTH = params.get('TRADE_SEQUENCE_LENGTH')\n",
    "    yield_history_cols = params.get('yield_history_cols')\n",
    "    yield_history_lengths = params.get('yield_history_lengths')\n",
    "    num_yield_history = params.get('num_yield_history')\n",
    "      \n",
    "    yield_history_normalizers = normalizers.get('yield_history_normalizers')\n",
    "    trade_history_normalizer = normalizers.get('trade_history_normalizer')\n",
    "    noncat_binary_normalizer = normalizers.get('noncat_binary_normalizer')\n",
    "       \n",
    "    tf.keras.utils.set_random_seed(10)\n",
    "    model = generate_model_new(TRADE_SEQUENCE_LENGTH, yield_history_cols, yield_history_lengths, num_yield_history, \n",
    "                               yield_history_normalizers, trade_history_normalizer, noncat_binary_normalizer)\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H-%M')\n",
    "    \n",
    "    fit_callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=10,\n",
    "        verbose=0,\n",
    "        mode=\"auto\",\n",
    "        restore_best_weights=True),\n",
    "        # time_callback,\n",
    "        CSVLoggerTimeHistory(timestamp+'_training_logs_yield_history.csv', separator=\",\", append=False)]\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        train_ds = create_tf_data(x_train, y_train, shuffle, shuffle_buffer)\n",
    "        train_ds = train_ds.batch(BATCH_SIZE).prefetch(2).cache()\n",
    "        val_ds = create_tf_data(x_val, y_val, shuffle = False)\n",
    "        val_ds = val_ds.batch(BATCH_SIZE).prefetch(2).cache()\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "          loss=keras.losses.MeanAbsoluteError())\n",
    "\n",
    "    history= model.fit(train_ds,\n",
    "                                      validation_data=val_ds,\n",
    "                                        epochs=NUM_EPOCHS,     \n",
    "                                        verbose=1, \n",
    "                                        callbacks=fit_callbacks,\n",
    "                                        use_multiprocessing=True,\n",
    "                                        workers=8)\n",
    "    \n",
    "    return history, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d5a61d0-40f6-42d0-8c22-f448d5e06d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['new_ficc_ycl', 'new_real_time_ficc_ycl']:     \n",
    "    if col in PREDICTORS:\n",
    "        PREDICTORS.remove(col)\n",
    "    if col in NON_CAT_FEATURES:\n",
    "        NON_CAT_FEATURES.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e40d5dc8-31a6-4c39-a36c-8377f2afa2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_history_col = 'trade_history_shortened'\n",
    "yield_history_cols = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e3e4815-ce26-4258-9eee-0f8337010232",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataframe_sub = test_dataframe[test_dataframe.trade_date <= '2023-06-09']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "404fc0cb-bb04-4d79-875b-ba91eb490e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA: N = 634710, MIN DATE = 2023-05-01 00:00:00, MAX DATE = 2023-05-31 00:00:00\n",
      "VALIDATION DATA: N = 70523, MIN DATE = 2023-05-01 00:00:00, MAX DATE = 2023-05-31 00:00:00\n",
      "TEST DATA: N = 235624, MIN DATE = 2023-06-01 00:00:00, MAX DATE = 2023-06-09 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 19:02:23.169227: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-19 19:02:23.173126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-19 19:02:23.175071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-19 19:02:23.176676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-19 19:02:23.876191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-19 19:02:23.877969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-19 19:02:23.879516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-19 19:02:23.881064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12681 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# params, normalizers, x_train, y_train, x_val, y_val, x_test, y_test, val_idx = create_data_set_and_model(train_dataframe, test_dataframe, trade_history_col, yield_history_cols)\n",
    "params, normalizers, x_train, y_train, x_val, y_val, x_test, y_test, val_idx = create_data_set_and_model(train_dataframe, test_dataframe[test_dataframe.trade_date <= '2023-06-09'], trade_history_col, yield_history_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9623aff-b815-4a2b-94f7-6f2e8189dc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "(634710, 2, 6)\n",
      "(634710, 1, 3)\n",
      "(634710, 48)\n",
      "(634710,)\n",
      "(634710,)\n",
      "(634710,)\n",
      "(634710,)\n",
      "(634710,)\n",
      "(634710,)\n",
      "(634710,)\n",
      "(634710,)\n",
      "(634710,)\n",
      "(634710,)\n",
      "(634710,)\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))\n",
    "for i in range(len(x_train)):\n",
    "    print(x_train[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "94ded485-96ec-43a2-a374-6b77bd936e55",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:22:19.572724: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 13007060992 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - ETA: 0s - loss: 43.3011"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:22:44.384006: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 13007060992 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 33s 27ms/step - loss: 43.3011 - val_loss: 39.0171\n",
      "Epoch 2/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 39.3124 - val_loss: 36.5678\n",
      "Epoch 3/75\n",
      "635/635 [==============================] - 23s 36ms/step - loss: 35.7395 - val_loss: 34.0807\n",
      "Epoch 4/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 31.3821 - val_loss: 30.0747\n",
      "Epoch 5/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 26.0626 - val_loss: 25.0375\n",
      "Epoch 6/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 20.3064 - val_loss: 17.4329\n",
      "Epoch 7/75\n",
      "635/635 [==============================] - 23s 36ms/step - loss: 15.3155 - val_loss: 13.0115\n",
      "Epoch 8/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 12.1222 - val_loss: 10.9199\n",
      "Epoch 9/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 10.7983 - val_loss: 10.1960\n",
      "Epoch 10/75\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 10.3896 - val_loss: 10.0005\n",
      "Epoch 11/75\n",
      "635/635 [==============================] - 22s 35ms/step - loss: 10.2143 - val_loss: 9.9242\n",
      "Epoch 12/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 10.1077 - val_loss: 9.8148\n",
      "Epoch 13/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 10.0107 - val_loss: 9.7770\n",
      "Epoch 14/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 9.9210 - val_loss: 9.7425\n",
      "Epoch 15/75\n",
      "635/635 [==============================] - 23s 36ms/step - loss: 9.8427 - val_loss: 9.6704\n",
      "Epoch 16/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 9.7718 - val_loss: 9.6677\n",
      "Epoch 17/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 9.7076 - val_loss: 9.6242\n",
      "Epoch 18/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 9.6500 - val_loss: 9.5949\n",
      "Epoch 19/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 9.5899 - val_loss: 9.5611\n",
      "Epoch 20/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 9.5364 - val_loss: 9.5320\n",
      "Epoch 21/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 9.4794 - val_loss: 9.5064\n",
      "Epoch 22/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 9.4348 - val_loss: 9.4677\n",
      "Epoch 23/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 9.3830 - val_loss: 9.4599\n",
      "Epoch 24/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 9.3404 - val_loss: 9.4327\n",
      "Epoch 25/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 9.2965 - val_loss: 9.4360\n",
      "Epoch 26/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.2539 - val_loss: 9.4160\n",
      "Epoch 27/75\n",
      "635/635 [==============================] - 25s 39ms/step - loss: 9.2192 - val_loss: 9.3840\n",
      "Epoch 28/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 9.1728 - val_loss: 9.3655\n",
      "Epoch 29/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.1343 - val_loss: 9.3491\n",
      "Epoch 30/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 9.0929 - val_loss: 9.3437\n",
      "Epoch 31/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 9.0590 - val_loss: 9.3592\n",
      "Epoch 32/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 9.0187 - val_loss: 9.3225\n",
      "Epoch 33/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 8.9828 - val_loss: 9.3288\n",
      "Epoch 34/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 8.9409 - val_loss: 9.2745\n",
      "Epoch 35/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.9115 - val_loss: 9.2937\n",
      "Epoch 36/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 8.8778 - val_loss: 9.2719\n",
      "Epoch 37/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.8400 - val_loss: 9.2762\n",
      "Epoch 38/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 8.8142 - val_loss: 9.2604\n",
      "Epoch 39/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 8.7845 - val_loss: 9.2665\n",
      "Epoch 40/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 8.7577 - val_loss: 9.2249\n",
      "Epoch 41/75\n",
      "635/635 [==============================] - 23s 36ms/step - loss: 8.7259 - val_loss: 9.2369\n",
      "Epoch 42/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 8.6927 - val_loss: 9.2174\n",
      "Epoch 43/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.6713 - val_loss: 9.1941\n",
      "Epoch 44/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 8.6326 - val_loss: 9.2028\n",
      "Epoch 45/75\n",
      "635/635 [==============================] - 23s 36ms/step - loss: 8.6070 - val_loss: 9.2002\n",
      "Epoch 46/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 8.5831 - val_loss: 9.2280\n",
      "Epoch 47/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 8.5582 - val_loss: 9.2040\n",
      "Epoch 48/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 8.5306 - val_loss: 9.1836\n",
      "Epoch 49/75\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 8.5066 - val_loss: 9.1821\n",
      "Epoch 50/75\n",
      "635/635 [==============================] - 24s 37ms/step - loss: 8.4799 - val_loss: 9.1682\n",
      "Epoch 51/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.4577 - val_loss: 9.1691\n",
      "Epoch 52/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.4342 - val_loss: 9.1589\n",
      "Epoch 53/75\n",
      "635/635 [==============================] - 23s 36ms/step - loss: 8.4053 - val_loss: 9.1916\n",
      "Epoch 54/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 8.3829 - val_loss: 9.1584\n",
      "Epoch 55/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 8.3555 - val_loss: 9.1565\n",
      "Epoch 56/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 8.3323 - val_loss: 9.1465\n",
      "Epoch 57/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.3172 - val_loss: 9.1712\n",
      "Epoch 58/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 8.2959 - val_loss: 9.1205\n",
      "Epoch 59/75\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 8.2743 - val_loss: 9.1543\n",
      "Epoch 60/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.2562 - val_loss: 9.1262\n",
      "Epoch 61/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 8.2307 - val_loss: 9.1523\n",
      "Epoch 62/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.2122 - val_loss: 9.1441\n",
      "Epoch 63/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.1839 - val_loss: 9.1307\n",
      "Epoch 64/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.1691 - val_loss: 9.1488\n",
      "Epoch 65/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 8.1565 - val_loss: 9.1137\n",
      "Epoch 66/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.1324 - val_loss: 9.1304\n",
      "Epoch 67/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.1161 - val_loss: 9.1144\n",
      "Epoch 68/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.1042 - val_loss: 9.0931\n",
      "Epoch 69/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 8.0728 - val_loss: 9.0752\n",
      "Epoch 70/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.0542 - val_loss: 9.0915\n",
      "Epoch 71/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.0418 - val_loss: 9.0671\n",
      "Epoch 72/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 8.0262 - val_loss: 9.0982\n",
      "Epoch 73/75\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 8.0037 - val_loss: 9.0954\n",
      "Epoch 74/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 7.9873 - val_loss: 9.0906\n",
      "Epoch 75/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 7.9738 - val_loss: 9.0490\n",
      "Model training time was 19.53 minutes (1171.81 seconds).\n",
      "Average time for each epoch was 0.20 minutes (11.72 seconds).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:41:56.407761: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 13007060992 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= TRIAL 0, MAE: 9.773586852955418 =========================\n",
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:42:50.700369: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 13007060992 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - ETA: 0s - loss: 43.3010"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:43:25.732405: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 13007060992 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 43s 43ms/step - loss: 43.3010 - val_loss: 39.0538\n",
      "Epoch 2/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 39.3144 - val_loss: 36.6620\n",
      "Epoch 3/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 35.7390 - val_loss: 34.2886\n",
      "Epoch 4/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 31.3824 - val_loss: 30.3149\n",
      "Epoch 5/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 26.0621 - val_loss: 24.7498\n",
      "Epoch 6/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 20.3078 - val_loss: 17.3133\n",
      "Epoch 7/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 15.3147 - val_loss: 12.8398\n",
      "Epoch 8/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 12.1141 - val_loss: 10.8680\n",
      "Epoch 9/75\n",
      "635/635 [==============================] - 22s 35ms/step - loss: 10.7933 - val_loss: 10.1926\n",
      "Epoch 10/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 10.3812 - val_loss: 9.9791\n",
      "Epoch 11/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 10.2056 - val_loss: 9.9088\n",
      "Epoch 12/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 10.0969 - val_loss: 9.8185\n",
      "Epoch 13/75\n",
      "635/635 [==============================] - 23s 36ms/step - loss: 10.0006 - val_loss: 9.7363\n",
      "Epoch 14/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 9.9119 - val_loss: 9.7138\n",
      "Epoch 15/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 9.8388 - val_loss: 9.6850\n",
      "Epoch 16/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 9.7668 - val_loss: 9.6424\n",
      "Epoch 17/75\n",
      "635/635 [==============================] - 23s 36ms/step - loss: 9.6995 - val_loss: 9.6230\n",
      "Epoch 18/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 9.6377 - val_loss: 9.5587\n",
      "Epoch 19/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 9.5793 - val_loss: 9.5394\n",
      "Epoch 20/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 9.5217 - val_loss: 9.5301\n",
      "Epoch 21/75\n",
      "635/635 [==============================] - 23s 36ms/step - loss: 9.4742 - val_loss: 9.4728\n",
      "Epoch 22/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 9.4300 - val_loss: 9.4631\n",
      "Epoch 23/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 9.3677 - val_loss: 9.4449\n",
      "Epoch 24/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 9.3318 - val_loss: 9.4525\n",
      "Epoch 25/75\n",
      "635/635 [==============================] - 17s 26ms/step - loss: 9.2904 - val_loss: 9.4338\n",
      "Epoch 26/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 9.2431 - val_loss: 9.3987\n",
      "Epoch 27/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 9.2072 - val_loss: 9.4102\n",
      "Epoch 28/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 9.1627 - val_loss: 9.3823\n",
      "Epoch 29/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 9.1244 - val_loss: 9.3545\n",
      "Epoch 30/75\n",
      "635/635 [==============================] - 23s 35ms/step - loss: 9.0856 - val_loss: 9.3611\n",
      "Epoch 31/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 9.0458 - val_loss: 9.3563\n",
      "Epoch 32/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 9.0122 - val_loss: 9.3156\n",
      "Epoch 33/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 8.9795 - val_loss: 9.3298\n",
      "Epoch 34/75\n",
      "635/635 [==============================] - 22s 35ms/step - loss: 8.9359 - val_loss: 9.3099\n",
      "Epoch 35/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 8.9075 - val_loss: 9.2911\n",
      "Epoch 36/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 8.8763 - val_loss: 9.2840\n",
      "Epoch 37/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 8.8431 - val_loss: 9.2449\n",
      "Epoch 38/75\n",
      "635/635 [==============================] - 23s 36ms/step - loss: 8.8081 - val_loss: 9.2467\n",
      "Epoch 39/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 8.7824 - val_loss: 9.2566\n",
      "Epoch 40/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 8.7493 - val_loss: 9.2220\n",
      "Epoch 41/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 8.7147 - val_loss: 9.2402\n",
      "Epoch 42/75\n",
      "635/635 [==============================] - 22s 35ms/step - loss: 8.6916 - val_loss: 9.2241\n",
      "Epoch 43/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 8.6661 - val_loss: 9.1932\n",
      "Epoch 44/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 8.6276 - val_loss: 9.2106\n",
      "Epoch 45/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 8.6055 - val_loss: 9.1999\n",
      "Epoch 46/75\n",
      "635/635 [==============================] - 22s 34ms/step - loss: 8.5781 - val_loss: 9.2169\n",
      "Epoch 47/75\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 8.5535 - val_loss: 9.2085\n",
      "Epoch 48/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 8.5258 - val_loss: 9.1877\n",
      "Epoch 49/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 8.5015 - val_loss: 9.1687\n",
      "Epoch 50/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 8.4733 - val_loss: 9.1746\n",
      "Epoch 51/75\n",
      "635/635 [==============================] - 23s 36ms/step - loss: 8.4554 - val_loss: 9.1677\n",
      "Epoch 52/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 8.4272 - val_loss: 9.1499\n",
      "Epoch 53/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 8.3999 - val_loss: 9.1437\n",
      "Epoch 54/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 8.3761 - val_loss: 9.1463\n",
      "Epoch 55/75\n",
      "635/635 [==============================] - 22s 34ms/step - loss: 8.3544 - val_loss: 9.1338\n",
      "Epoch 56/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.3315 - val_loss: 9.1332\n",
      "Epoch 57/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.3113 - val_loss: 9.1384\n",
      "Epoch 58/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 8.2897 - val_loss: 9.1268\n",
      "Epoch 59/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 8.2741 - val_loss: 9.1560\n",
      "Epoch 60/75\n",
      "635/635 [==============================] - 22s 34ms/step - loss: 8.2490 - val_loss: 9.1313\n",
      "Epoch 61/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.2295 - val_loss: 9.1465\n",
      "Epoch 62/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.2098 - val_loss: 9.1139\n",
      "Epoch 63/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.1878 - val_loss: 9.1379\n",
      "Epoch 64/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 8.1752 - val_loss: 9.0973\n",
      "Epoch 65/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 8.1539 - val_loss: 9.1165\n",
      "Epoch 66/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 8.1291 - val_loss: 9.0861\n",
      "Epoch 67/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 8.1151 - val_loss: 9.0974\n",
      "Epoch 68/75\n",
      "635/635 [==============================] - 25s 39ms/step - loss: 8.0940 - val_loss: 9.0686\n",
      "Epoch 69/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 8.0698 - val_loss: 9.0706\n",
      "Epoch 70/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 8.0511 - val_loss: 9.0801\n",
      "Epoch 71/75\n",
      "635/635 [==============================] - 24s 39ms/step - loss: 8.0385 - val_loss: 9.0612\n",
      "Epoch 72/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 8.0141 - val_loss: 9.0873\n",
      "Epoch 73/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 8.0047 - val_loss: 9.0739\n",
      "Epoch 74/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 7.9872 - val_loss: 9.0289\n",
      "Epoch 75/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 7.9716 - val_loss: 9.0409\n",
      "Model training time was 18.31 minutes (1098.68 seconds).\n",
      "Average time for each epoch was 0.18 minutes (10.99 seconds).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 23:01:25.404075: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 13007060992 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= TRIAL 1, MAE: 9.84294248814721 =========================\n",
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 23:02:20.790310: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 13007060992 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - ETA: 0s - loss: 43.3009"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 23:02:47.178703: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 13007060992 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 34s 29ms/step - loss: 43.3009 - val_loss: 39.0029\n",
      "Epoch 2/75\n",
      "635/635 [==============================] - 16s 25ms/step - loss: 39.3134 - val_loss: 36.5829\n",
      "Epoch 3/75\n",
      "635/635 [==============================] - 22s 34ms/step - loss: 35.7392 - val_loss: 34.0274\n",
      "Epoch 4/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 31.3820 - val_loss: 30.0190\n",
      "Epoch 5/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 26.0645 - val_loss: 25.0525\n",
      "Epoch 6/75\n",
      "635/635 [==============================] - 24s 37ms/step - loss: 20.3038 - val_loss: 18.0204\n",
      "Epoch 7/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 15.3098 - val_loss: 12.8501\n",
      "Epoch 8/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 12.1153 - val_loss: 10.8521\n",
      "Epoch 9/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 10.7993 - val_loss: 10.1704\n",
      "Epoch 10/75\n",
      "635/635 [==============================] - 22s 35ms/step - loss: 10.3824 - val_loss: 10.0010\n",
      "Epoch 11/75\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 10.2137 - val_loss: 9.9066\n",
      "Epoch 12/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 10.1024 - val_loss: 9.8218\n",
      "Epoch 13/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 10.0018 - val_loss: 9.7811\n",
      "Epoch 14/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 9.9161 - val_loss: 9.7542\n",
      "Epoch 15/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.8380 - val_loss: 9.6934\n",
      "Epoch 16/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.7648 - val_loss: 9.6902\n",
      "Epoch 17/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.6947 - val_loss: 9.6180\n",
      "Epoch 18/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 9.6396 - val_loss: 9.6276\n",
      "Epoch 19/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.5837 - val_loss: 9.5696\n",
      "Epoch 20/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.5298 - val_loss: 9.5732\n",
      "Epoch 21/75\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 9.4708 - val_loss: 9.5185\n",
      "Epoch 22/75\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 9.4288 - val_loss: 9.4982\n",
      "Epoch 23/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.3712 - val_loss: 9.4642\n",
      "Epoch 24/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 9.3256 - val_loss: 9.4421\n",
      "Epoch 25/75\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 9.2855 - val_loss: 9.4437\n",
      "Epoch 26/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.2450 - val_loss: 9.4320\n",
      "Epoch 27/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.2090 - val_loss: 9.3971\n",
      "Epoch 28/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 9.1642 - val_loss: 9.3632\n",
      "Epoch 29/75\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 9.1231 - val_loss: 9.3591\n",
      "Epoch 30/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.0855 - val_loss: 9.3713\n",
      "Epoch 31/75\n",
      "635/635 [==============================] - 24s 37ms/step - loss: 9.0489 - val_loss: 9.3465\n",
      "Epoch 32/75\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 9.0090 - val_loss: 9.3332\n",
      "Epoch 33/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.9747 - val_loss: 9.3113\n",
      "Epoch 34/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.9339 - val_loss: 9.3156\n",
      "Epoch 35/75\n",
      "635/635 [==============================] - 25s 39ms/step - loss: 8.9087 - val_loss: 9.3184\n",
      "Epoch 36/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.8722 - val_loss: 9.2963\n",
      "Epoch 37/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.8417 - val_loss: 9.2636\n",
      "Epoch 38/75\n",
      "635/635 [==============================] - 12s 20ms/step - loss: 8.8105 - val_loss: 9.2743\n",
      "Epoch 39/75\n",
      "635/635 [==============================] - 22s 34ms/step - loss: 8.7800 - val_loss: 9.2570\n",
      "Epoch 40/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 8.7523 - val_loss: 9.2383\n",
      "Epoch 41/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 8.7137 - val_loss: 9.2279\n",
      "Epoch 42/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 8.6893 - val_loss: 9.2400\n",
      "Epoch 43/75\n",
      "635/635 [==============================] - 22s 35ms/step - loss: 8.6570 - val_loss: 9.2227\n",
      "Epoch 44/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 8.6280 - val_loss: 9.2321\n",
      "Epoch 45/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 8.5978 - val_loss: 9.2370\n",
      "Epoch 46/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 8.5749 - val_loss: 9.2321\n",
      "Epoch 47/75\n",
      "635/635 [==============================] - 22s 35ms/step - loss: 8.5481 - val_loss: 9.1874\n",
      "Epoch 48/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 8.5243 - val_loss: 9.1929\n",
      "Epoch 49/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 8.5049 - val_loss: 9.1864\n",
      "Epoch 50/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 8.4723 - val_loss: 9.1783\n",
      "Epoch 51/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 8.4493 - val_loss: 9.1987\n",
      "Epoch 52/75\n",
      "635/635 [==============================] - 22s 35ms/step - loss: 8.4260 - val_loss: 9.1758\n",
      "Epoch 53/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.3979 - val_loss: 9.1998\n",
      "Epoch 54/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.3721 - val_loss: 9.1913\n",
      "Epoch 55/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.3492 - val_loss: 9.1804\n",
      "Epoch 56/75\n",
      "635/635 [==============================] - 23s 36ms/step - loss: 8.3250 - val_loss: 9.1712\n",
      "Epoch 57/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 8.3080 - val_loss: 9.1769\n",
      "Epoch 58/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 8.2822 - val_loss: 9.1746\n",
      "Epoch 59/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 8.2661 - val_loss: 9.1608\n",
      "Epoch 60/75\n",
      "635/635 [==============================] - 22s 35ms/step - loss: 8.2410 - val_loss: 9.1745\n",
      "Epoch 61/75\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 8.2316 - val_loss: 9.1459\n",
      "Epoch 62/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 8.2018 - val_loss: 9.1391\n",
      "Epoch 63/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 8.1792 - val_loss: 9.1412\n",
      "Epoch 64/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 8.1679 - val_loss: 9.1495\n",
      "Epoch 65/75\n",
      "635/635 [==============================] - 23s 36ms/step - loss: 8.1401 - val_loss: 9.1362\n",
      "Epoch 66/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 8.1227 - val_loss: 9.1746\n",
      "Epoch 67/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 8.1081 - val_loss: 9.1338\n",
      "Epoch 68/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 8.0865 - val_loss: 9.1349\n",
      "Epoch 69/75\n",
      "635/635 [==============================] - 23s 36ms/step - loss: 8.0702 - val_loss: 9.1085\n",
      "Epoch 70/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 8.0448 - val_loss: 9.1284\n",
      "Epoch 71/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 8.0284 - val_loss: 9.1296\n",
      "Epoch 72/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 8.0143 - val_loss: 9.1241\n",
      "Epoch 73/75\n",
      "635/635 [==============================] - 23s 36ms/step - loss: 7.9957 - val_loss: 9.1502\n",
      "Epoch 74/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 7.9727 - val_loss: 9.1138\n",
      "Epoch 75/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 7.9638 - val_loss: 9.0868\n",
      "Model training time was 18.97 minutes (1138.05 seconds).\n",
      "Average time for each epoch was 0.19 minutes (11.38 seconds).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 23:21:43.659682: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 13007060992 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= TRIAL 2, MAE: 9.851067833237337 =========================\n",
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 23:22:36.457283: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 13007060992 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - ETA: 0s - loss: 43.3008"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 23:23:01.401660: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 13007060992 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 36s 33ms/step - loss: 43.3008 - val_loss: 39.0289\n",
      "Epoch 2/75\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 39.3133 - val_loss: 36.4809\n",
      "Epoch 3/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 35.7388 - val_loss: 34.0382\n",
      "Epoch 4/75\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 31.3825 - val_loss: 29.8896\n",
      "Epoch 5/75\n",
      "635/635 [==============================] - 23s 36ms/step - loss: 26.0638 - val_loss: 24.7796\n",
      "Epoch 6/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 20.3095 - val_loss: 18.0250\n",
      "Epoch 7/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 15.3122 - val_loss: 13.1188\n",
      "Epoch 8/75\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 12.1166 - val_loss: 10.9007\n",
      "Epoch 9/75\n",
      "635/635 [==============================] - 23s 36ms/step - loss: 10.7973 - val_loss: 10.1601\n",
      "Epoch 10/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 10.3859 - val_loss: 9.9941\n",
      "Epoch 11/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 10.2148 - val_loss: 9.8837\n",
      "Epoch 12/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 10.1065 - val_loss: 9.8228\n",
      "Epoch 13/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 10.0099 - val_loss: 9.7926\n",
      "Epoch 14/75\n",
      "635/635 [==============================] - 15s 23ms/step - loss: 9.9223 - val_loss: 9.7412\n",
      "Epoch 15/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 9.8465 - val_loss: 9.7229\n",
      "Epoch 16/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 9.7721 - val_loss: 9.6544\n",
      "Epoch 17/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 9.7043 - val_loss: 9.6364\n",
      "Epoch 18/75\n",
      "635/635 [==============================] - 22s 35ms/step - loss: 9.6479 - val_loss: 9.6148\n",
      "Epoch 19/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 9.5910 - val_loss: 9.5753\n",
      "Epoch 20/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 9.5327 - val_loss: 9.5505\n",
      "Epoch 21/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 9.4842 - val_loss: 9.5218\n",
      "Epoch 22/75\n",
      "635/635 [==============================] - 24s 37ms/step - loss: 9.4402 - val_loss: 9.4906\n",
      "Epoch 23/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.3845 - val_loss: 9.4698\n",
      "Epoch 24/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 9.3385 - val_loss: 9.4377\n",
      "Epoch 25/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 9.2944 - val_loss: 9.4494\n",
      "Epoch 26/75\n",
      "635/635 [==============================] - 25s 39ms/step - loss: 9.2563 - val_loss: 9.4452\n",
      "Epoch 27/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 9.2174 - val_loss: 9.4358\n",
      "Epoch 28/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 9.1741 - val_loss: 9.3766\n",
      "Epoch 29/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 9.1319 - val_loss: 9.3520\n",
      "Epoch 30/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 9.0959 - val_loss: 9.3540\n",
      "Epoch 31/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 9.0580 - val_loss: 9.3580\n",
      "Epoch 32/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 9.0247 - val_loss: 9.3325\n",
      "Epoch 33/75\n",
      "635/635 [==============================] - 25s 39ms/step - loss: 8.9891 - val_loss: 9.3270\n",
      "Epoch 34/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 8.9447 - val_loss: 9.3092\n",
      "Epoch 35/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 8.9168 - val_loss: 9.2911\n",
      "Epoch 36/75\n",
      "635/635 [==============================] - 25s 39ms/step - loss: 8.8861 - val_loss: 9.2845\n",
      "Epoch 37/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 8.8461 - val_loss: 9.2493\n",
      "Epoch 38/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 8.8210 - val_loss: 9.2530\n",
      "Epoch 39/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 8.7905 - val_loss: 9.2596\n",
      "Epoch 40/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 8.7611 - val_loss: 9.2339\n",
      "Epoch 41/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 8.7286 - val_loss: 9.2367\n",
      "Epoch 42/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 8.6980 - val_loss: 9.2186\n",
      "Epoch 43/75\n",
      "635/635 [==============================] - 18s 29ms/step - loss: 8.6741 - val_loss: 9.2152\n",
      "Epoch 44/75\n",
      "635/635 [==============================] - 18s 28ms/step - loss: 8.6416 - val_loss: 9.2150\n",
      "Epoch 45/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 8.6138 - val_loss: 9.2336\n",
      "Epoch 46/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 8.5853 - val_loss: 9.2160\n",
      "Epoch 47/75\n",
      "635/635 [==============================] - 23s 36ms/step - loss: 8.5609 - val_loss: 9.2029\n",
      "Epoch 48/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 8.5362 - val_loss: 9.1909\n",
      "Epoch 49/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.5125 - val_loss: 9.1882\n",
      "Epoch 50/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 8.4857 - val_loss: 9.1729\n",
      "Epoch 51/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 8.4597 - val_loss: 9.1903\n",
      "Epoch 52/75\n",
      "635/635 [==============================] - 22s 34ms/step - loss: 8.4335 - val_loss: 9.1524\n",
      "Epoch 53/75\n",
      "635/635 [==============================] - 12s 20ms/step - loss: 8.4058 - val_loss: 9.1565\n",
      "Epoch 54/75\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 8.3830 - val_loss: 9.1547\n",
      "Epoch 55/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.3621 - val_loss: 9.1254\n",
      "Epoch 56/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 8.3422 - val_loss: 9.1526\n",
      "Epoch 57/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.3147 - val_loss: 9.1515\n",
      "Epoch 58/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.2985 - val_loss: 9.1336\n",
      "Epoch 59/75\n",
      "635/635 [==============================] - 26s 42ms/step - loss: 8.2703 - val_loss: 9.1462\n",
      "Epoch 60/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.2533 - val_loss: 9.1331\n",
      "Epoch 61/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.2300 - val_loss: 9.1486\n",
      "Epoch 62/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.2096 - val_loss: 9.1315\n",
      "Epoch 63/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 8.1929 - val_loss: 9.1199\n",
      "Epoch 64/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.1754 - val_loss: 9.1296\n",
      "Epoch 65/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.1555 - val_loss: 9.1260\n",
      "Epoch 66/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 8.1293 - val_loss: 9.0982\n",
      "Epoch 67/75\n",
      "635/635 [==============================] - 17s 27ms/step - loss: 8.1132 - val_loss: 9.1058\n",
      "Epoch 68/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 8.0965 - val_loss: 9.1173\n",
      "Epoch 69/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.0794 - val_loss: 9.0986\n",
      "Epoch 70/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 8.0550 - val_loss: 9.0762\n",
      "Epoch 71/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.0375 - val_loss: 9.0803\n",
      "Epoch 72/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 8.0152 - val_loss: 9.1000\n",
      "Epoch 73/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 7.9972 - val_loss: 9.0657\n",
      "Epoch 74/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 7.9854 - val_loss: 9.0881\n",
      "Epoch 75/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 7.9679 - val_loss: 9.0910\n",
      "Model training time was 19.46 minutes (1167.89 seconds).\n",
      "Average time for each epoch was 0.19 minutes (11.68 seconds).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 23:42:36.262165: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 13007060992 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= TRIAL 3, MAE: 9.765151981736258 =========================\n",
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 23:43:32.090613: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 13007060992 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - ETA: 0s - loss: 43.3011"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 23:43:56.593249: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 13007060992 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 32s 27ms/step - loss: 43.3011 - val_loss: 39.0219\n",
      "Epoch 2/75\n",
      "635/635 [==============================] - 23s 36ms/step - loss: 39.3131 - val_loss: 36.7242\n",
      "Epoch 3/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 35.7388 - val_loss: 34.2763\n",
      "Epoch 4/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 31.3824 - val_loss: 30.0741\n",
      "Epoch 5/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 26.0625 - val_loss: 24.8885\n",
      "Epoch 6/75\n",
      "635/635 [==============================] - 22s 35ms/step - loss: 20.3022 - val_loss: 17.8679\n",
      "Epoch 7/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 15.3116 - val_loss: 13.0940\n",
      "Epoch 8/75\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 12.1142 - val_loss: 10.8310\n",
      "Epoch 9/75\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 10.7986 - val_loss: 10.1744\n",
      "Epoch 10/75\n",
      "635/635 [==============================] - 23s 37ms/step - loss: 10.3895 - val_loss: 9.9771\n",
      "Epoch 11/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 10.2156 - val_loss: 9.9158\n",
      "Epoch 12/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 10.1087 - val_loss: 9.8233\n",
      "Epoch 13/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 10.0093 - val_loss: 9.7848\n",
      "Epoch 14/75\n",
      "635/635 [==============================] - 22s 35ms/step - loss: 9.9214 - val_loss: 9.7374\n",
      "Epoch 15/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 9.8432 - val_loss: 9.6990\n",
      "Epoch 16/75\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 9.7739 - val_loss: 9.6514\n",
      "Epoch 17/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 9.7032 - val_loss: 9.6135\n",
      "Epoch 18/75\n",
      "635/635 [==============================] - 15s 23ms/step - loss: 9.6438 - val_loss: 9.5941\n",
      "Epoch 19/75\n",
      "635/635 [==============================] - 14s 23ms/step - loss: 9.5848 - val_loss: 9.5584\n",
      "Epoch 20/75\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 9.5251 - val_loss: 9.5166\n",
      "Epoch 21/75\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 9.4742 - val_loss: 9.4925\n",
      "Epoch 22/75\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 9.4286 - val_loss: 9.4768\n",
      "Epoch 23/75\n",
      "635/635 [==============================] - 15s 24ms/step - loss: 9.3699 - val_loss: 9.4521\n",
      "Epoch 24/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 9.3301 - val_loss: 9.4362\n",
      "Epoch 25/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 9.2875 - val_loss: 9.4264\n",
      "Epoch 26/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 9.2439 - val_loss: 9.4228\n",
      "Epoch 27/75\n",
      "635/635 [==============================] - 23s 36ms/step - loss: 9.2065 - val_loss: 9.3807\n",
      "Epoch 28/75\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 9.1644 - val_loss: 9.3651\n",
      "Epoch 29/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 9.1191 - val_loss: 9.3389\n",
      "Epoch 30/75\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 9.0815 - val_loss: 9.3430\n",
      "Epoch 31/75\n",
      "635/635 [==============================] - 23s 37ms/step - loss: 9.0473 - val_loss: 9.3294\n",
      "Epoch 32/75\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 9.0090 - val_loss: 9.3263\n",
      "Epoch 33/75\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 8.9752 - val_loss: 9.3160\n",
      "Epoch 34/75\n",
      "635/635 [==============================] - 19s 30ms/step - loss: 8.9339 - val_loss: 9.2797\n",
      "Epoch 35/75\n",
      "635/635 [==============================] - 18s 29ms/step - loss: 8.9068 - val_loss: 9.2911\n",
      "Epoch 36/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 8.8677 - val_loss: 9.2721\n",
      "Epoch 37/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 8.8380 - val_loss: 9.2818\n",
      "Epoch 38/75\n",
      "635/635 [==============================] - 24s 37ms/step - loss: 8.8075 - val_loss: 9.2543\n",
      "Epoch 39/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 8.7801 - val_loss: 9.2536\n",
      "Epoch 40/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 8.7491 - val_loss: 9.2388\n",
      "Epoch 41/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 8.7157 - val_loss: 9.2298\n",
      "Epoch 42/75\n",
      "635/635 [==============================] - 23s 36ms/step - loss: 8.6906 - val_loss: 9.2319\n",
      "Epoch 43/75\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 8.6637 - val_loss: 9.2233\n",
      "Epoch 44/75\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 8.6262 - val_loss: 9.2209\n",
      "Epoch 45/75\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 8.6052 - val_loss: 9.2129\n",
      "Epoch 46/75\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 8.5745 - val_loss: 9.2303\n",
      "Epoch 47/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 8.5480 - val_loss: 9.2153\n",
      "Epoch 48/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.5222 - val_loss: 9.1710\n",
      "Epoch 49/75\n",
      "635/635 [==============================] - 21s 34ms/step - loss: 8.5027 - val_loss: 9.1695\n",
      "Epoch 50/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 8.4756 - val_loss: 9.1824\n",
      "Epoch 51/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 8.4500 - val_loss: 9.1939\n",
      "Epoch 52/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 8.4307 - val_loss: 9.1847\n",
      "Epoch 53/75\n",
      "635/635 [==============================] - 15s 24ms/step - loss: 8.3994 - val_loss: 9.1608\n",
      "Epoch 54/75\n",
      "635/635 [==============================] - 19s 30ms/step - loss: 8.3761 - val_loss: 9.1845\n",
      "Epoch 55/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 8.3528 - val_loss: 9.1612\n",
      "Epoch 56/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 8.3332 - val_loss: 9.1414\n",
      "Epoch 57/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 8.3167 - val_loss: 9.1414\n",
      "Epoch 58/75\n",
      "635/635 [==============================] - 22s 35ms/step - loss: 8.2881 - val_loss: 9.1450\n",
      "Epoch 59/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.2653 - val_loss: 9.1283\n",
      "Epoch 60/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.2550 - val_loss: 9.1305\n",
      "Epoch 61/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.2224 - val_loss: 9.1335\n",
      "Epoch 62/75\n",
      "635/635 [==============================] - 25s 39ms/step - loss: 8.2061 - val_loss: 9.1428\n",
      "Epoch 63/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 8.1852 - val_loss: 9.1111\n",
      "Epoch 64/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 8.1696 - val_loss: 9.1081\n",
      "Epoch 65/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 8.1473 - val_loss: 9.1455\n",
      "Epoch 66/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 8.1276 - val_loss: 9.1461\n",
      "Epoch 67/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 8.1165 - val_loss: 9.1039\n",
      "Epoch 68/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 8.0930 - val_loss: 9.1233\n",
      "Epoch 69/75\n",
      "635/635 [==============================] - 24s 39ms/step - loss: 8.0755 - val_loss: 9.1080\n",
      "Epoch 70/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 8.0636 - val_loss: 9.1000\n",
      "Epoch 71/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 8.0342 - val_loss: 9.1098\n",
      "Epoch 72/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 8.0140 - val_loss: 9.0990\n",
      "Epoch 73/75\n",
      "635/635 [==============================] - 25s 39ms/step - loss: 8.0041 - val_loss: 9.1075\n",
      "Epoch 74/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 7.9898 - val_loss: 9.0925\n",
      "Epoch 75/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 7.9704 - val_loss: 9.0835\n",
      "Model training time was 18.64 minutes (1118.18 seconds).\n",
      "Average time for each epoch was 0.19 minutes (11.18 seconds).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:02:50.695431: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 13007060992 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= TRIAL 4, MAE: 9.8204304028186 =========================\n",
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:03:49.319771: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 13007060992 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - ETA: 0s - loss: 43.3012"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:04:26.713937: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 13007060992 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 45s 46ms/step - loss: 43.3012 - val_loss: 39.0281\n",
      "Epoch 2/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 39.3128 - val_loss: 36.6399\n",
      "Epoch 3/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 35.7382 - val_loss: 34.1940\n",
      "Epoch 4/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 31.3820 - val_loss: 30.1678\n",
      "Epoch 5/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 26.0638 - val_loss: 24.8153\n",
      "Epoch 6/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 20.3029 - val_loss: 17.6100\n",
      "Epoch 7/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 15.3088 - val_loss: 13.2028\n",
      "Epoch 8/75\n",
      "635/635 [==============================] - 22s 35ms/step - loss: 12.1079 - val_loss: 10.9953\n",
      "Epoch 9/75\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 10.7933 - val_loss: 10.1667\n",
      "Epoch 10/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 10.3825 - val_loss: 9.9975\n",
      "Epoch 11/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 10.2104 - val_loss: 9.9532\n",
      "Epoch 12/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 10.0987 - val_loss: 9.8314\n",
      "Epoch 13/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 10.0041 - val_loss: 9.7784\n",
      "Epoch 14/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.9146 - val_loss: 9.7179\n",
      "Epoch 15/75\n",
      "635/635 [==============================] - 15s 24ms/step - loss: 9.8412 - val_loss: 9.7200\n",
      "Epoch 16/75\n",
      "635/635 [==============================] - 17s 27ms/step - loss: 9.7707 - val_loss: 9.6679\n",
      "Epoch 17/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.7036 - val_loss: 9.5952\n",
      "Epoch 18/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 9.6430 - val_loss: 9.5805\n",
      "Epoch 19/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 9.5844 - val_loss: 9.5672\n",
      "Epoch 20/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.5315 - val_loss: 9.5532\n",
      "Epoch 21/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 9.4806 - val_loss: 9.4852\n",
      "Epoch 22/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.4294 - val_loss: 9.4975\n",
      "Epoch 23/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 9.3783 - val_loss: 9.4580\n",
      "Epoch 24/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.3398 - val_loss: 9.4363\n",
      "Epoch 25/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.2932 - val_loss: 9.4258\n",
      "Epoch 26/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 9.2510 - val_loss: 9.3958\n",
      "Epoch 27/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.2167 - val_loss: 9.4224\n",
      "Epoch 28/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.1733 - val_loss: 9.3763\n",
      "Epoch 29/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.1363 - val_loss: 9.3600\n",
      "Epoch 30/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 9.0937 - val_loss: 9.3486\n",
      "Epoch 31/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.0539 - val_loss: 9.3289\n",
      "Epoch 32/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 9.0236 - val_loss: 9.3481\n",
      "Epoch 33/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.9849 - val_loss: 9.3208\n",
      "Epoch 34/75\n",
      "635/635 [==============================] - 24s 37ms/step - loss: 8.9475 - val_loss: 9.3203\n",
      "Epoch 35/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.9179 - val_loss: 9.2990\n",
      "Epoch 36/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.8822 - val_loss: 9.2981\n",
      "Epoch 37/75\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 8.8454 - val_loss: 9.2554\n",
      "Epoch 38/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 8.8176 - val_loss: 9.2852\n",
      "Epoch 39/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 8.7875 - val_loss: 9.2461\n",
      "Epoch 40/75\n",
      "635/635 [==============================] - 22s 35ms/step - loss: 8.7616 - val_loss: 9.2368\n",
      "Epoch 41/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 8.7244 - val_loss: 9.2248\n",
      "Epoch 42/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 8.6981 - val_loss: 9.2231\n",
      "Epoch 43/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 8.6701 - val_loss: 9.2110\n",
      "Epoch 44/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.6306 - val_loss: 9.2181\n",
      "Epoch 45/75\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 8.6150 - val_loss: 9.1960\n",
      "Epoch 46/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.5859 - val_loss: 9.2014\n",
      "Epoch 47/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.5640 - val_loss: 9.1960\n",
      "Epoch 48/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.5357 - val_loss: 9.1638\n",
      "Epoch 49/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.5128 - val_loss: 9.1575\n",
      "Epoch 50/75\n",
      "635/635 [==============================] - 21s 34ms/step - loss: 8.4826 - val_loss: 9.1327\n",
      "Epoch 51/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.4583 - val_loss: 9.1837\n",
      "Epoch 52/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.4342 - val_loss: 9.1541\n",
      "Epoch 53/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.4109 - val_loss: 9.1490\n",
      "Epoch 54/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.3916 - val_loss: 9.1534\n",
      "Epoch 55/75\n",
      "635/635 [==============================] - 20s 32ms/step - loss: 8.3547 - val_loss: 9.1385\n",
      "Epoch 56/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.3423 - val_loss: 9.1311\n",
      "Epoch 57/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.3197 - val_loss: 9.1329\n",
      "Epoch 58/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.2892 - val_loss: 9.1555\n",
      "Epoch 59/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.2743 - val_loss: 9.1338\n",
      "Epoch 60/75\n",
      "635/635 [==============================] - 21s 34ms/step - loss: 8.2513 - val_loss: 9.0993\n",
      "Epoch 61/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.2353 - val_loss: 9.1173\n",
      "Epoch 62/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.2074 - val_loss: 9.1447\n",
      "Epoch 63/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.1918 - val_loss: 9.1161\n",
      "Epoch 64/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.1753 - val_loss: 9.0821\n",
      "Epoch 65/75\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 8.1559 - val_loss: 9.1066\n",
      "Epoch 66/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.1289 - val_loss: 9.1009\n",
      "Epoch 67/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.1145 - val_loss: 9.0920\n",
      "Epoch 68/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.0997 - val_loss: 9.0870\n",
      "Epoch 69/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.0718 - val_loss: 9.0585\n",
      "Epoch 70/75\n",
      "635/635 [==============================] - 20s 31ms/step - loss: 8.0601 - val_loss: 9.0637\n",
      "Epoch 71/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.0364 - val_loss: 9.0983\n",
      "Epoch 72/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.0309 - val_loss: 9.0736\n",
      "Epoch 73/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.0040 - val_loss: 9.0539\n",
      "Epoch 74/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 7.9898 - val_loss: 9.0753\n",
      "Epoch 75/75\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 7.9721 - val_loss: 9.0745\n",
      "Model training time was 18.07 minutes (1084.09 seconds).\n",
      "Average time for each epoch was 0.18 minutes (10.84 seconds).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:22:19.571334: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 13007060992 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= TRIAL 5, MAE: 9.816808938767654 =========================\n",
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:22:56.768272: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 13007060992 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-07-19 00:23:07.469543: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 412803 of 476032\n",
      "2023-07-19 00:23:09.688826: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:405] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - ETA: 0s - loss: 43.3009"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:23:27.895936: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 13007060992 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 38s 30ms/step - loss: 43.3009 - val_loss: 39.0449\n",
      "Epoch 2/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 39.3137 - val_loss: 36.6009\n",
      "Epoch 3/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 35.7388 - val_loss: 34.1695\n",
      "Epoch 4/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 31.3822 - val_loss: 30.1429\n",
      "Epoch 5/75\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 26.0612 - val_loss: 24.6847\n",
      "Epoch 6/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 20.2988 - val_loss: 17.6555\n",
      "Epoch 7/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 15.3096 - val_loss: 13.1471\n",
      "Epoch 8/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 12.1096 - val_loss: 11.0417\n",
      "Epoch 9/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 10.7942 - val_loss: 10.1979\n",
      "Epoch 10/75\n",
      "635/635 [==============================] - 20s 32ms/step - loss: 10.3833 - val_loss: 9.9732\n",
      "Epoch 11/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 10.2122 - val_loss: 9.8948\n",
      "Epoch 12/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 10.1001 - val_loss: 9.7960\n",
      "Epoch 13/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 10.0043 - val_loss: 9.7608\n",
      "Epoch 14/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.9170 - val_loss: 9.7386\n",
      "Epoch 15/75\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 9.8409 - val_loss: 9.6920\n",
      "Epoch 16/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.7688 - val_loss: 9.6701\n",
      "Epoch 17/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.7014 - val_loss: 9.6031\n",
      "Epoch 18/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.6430 - val_loss: 9.5859\n",
      "Epoch 19/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.5868 - val_loss: 9.5387\n",
      "Epoch 20/75\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 9.5265 - val_loss: 9.5223\n",
      "Epoch 21/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.4768 - val_loss: 9.5043\n",
      "Epoch 22/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.4339 - val_loss: 9.5026\n",
      "Epoch 23/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.3712 - val_loss: 9.4710\n",
      "Epoch 24/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.3344 - val_loss: 9.4209\n",
      "Epoch 25/75\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 9.2894 - val_loss: 9.4303\n",
      "Epoch 26/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.2435 - val_loss: 9.3889\n",
      "Epoch 27/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.2083 - val_loss: 9.3936\n",
      "Epoch 28/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.1681 - val_loss: 9.3841\n",
      "Epoch 29/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.1286 - val_loss: 9.3529\n",
      "Epoch 30/75\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 9.0876 - val_loss: 9.3675\n",
      "Epoch 31/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.0566 - val_loss: 9.3340\n",
      "Epoch 32/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.0135 - val_loss: 9.3507\n",
      "Epoch 33/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.9833 - val_loss: 9.3159\n",
      "Epoch 34/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.9417 - val_loss: 9.2863\n",
      "Epoch 35/75\n",
      "635/635 [==============================] - 20s 32ms/step - loss: 8.9110 - val_loss: 9.2709\n",
      "Epoch 36/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.8756 - val_loss: 9.3200\n",
      "Epoch 37/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.8440 - val_loss: 9.2452\n",
      "Epoch 38/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 8.8134 - val_loss: 9.3124\n",
      "Epoch 39/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.7864 - val_loss: 9.2641\n",
      "Epoch 40/75\n",
      "635/635 [==============================] - 22s 34ms/step - loss: 8.7579 - val_loss: 9.2254\n",
      "Epoch 41/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.7213 - val_loss: 9.2834\n",
      "Epoch 42/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.6942 - val_loss: 9.2623\n",
      "Epoch 43/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.6688 - val_loss: 9.2073\n",
      "Epoch 44/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.6370 - val_loss: 9.2211\n",
      "Epoch 45/75\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 8.6058 - val_loss: 9.2078\n",
      "Epoch 46/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.5831 - val_loss: 9.2101\n",
      "Epoch 47/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.5577 - val_loss: 9.2005\n",
      "Epoch 48/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.5311 - val_loss: 9.2012\n",
      "Epoch 49/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.5095 - val_loss: 9.1923\n",
      "Epoch 50/75\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 8.4828 - val_loss: 9.1667\n",
      "Epoch 51/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.4581 - val_loss: 9.1967\n",
      "Epoch 52/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.4300 - val_loss: 9.1876\n",
      "Epoch 53/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.4031 - val_loss: 9.1995\n",
      "Epoch 54/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.3723 - val_loss: 9.1656\n",
      "Epoch 55/75\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 8.3557 - val_loss: 9.1727\n",
      "Epoch 56/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.3287 - val_loss: 9.1646\n",
      "Epoch 57/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.3145 - val_loss: 9.1226\n",
      "Epoch 58/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.2859 - val_loss: 9.1543\n",
      "Epoch 59/75\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 8.2692 - val_loss: 9.1595\n",
      "Epoch 60/75\n",
      "635/635 [==============================] - 19s 29ms/step - loss: 8.2473 - val_loss: 9.1445\n",
      "Epoch 61/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.2215 - val_loss: 9.1329\n",
      "Epoch 62/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.2042 - val_loss: 9.1406\n",
      "Epoch 63/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.1767 - val_loss: 9.1147\n",
      "Epoch 64/75\n",
      "635/635 [==============================] - 22s 35ms/step - loss: 8.1740 - val_loss: 9.0987\n",
      "Epoch 65/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.1429 - val_loss: 9.1384\n",
      "Epoch 66/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.1278 - val_loss: 9.0925\n",
      "Epoch 67/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.1038 - val_loss: 9.1162\n",
      "Epoch 68/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.0833 - val_loss: 9.0833\n",
      "Epoch 69/75\n",
      "635/635 [==============================] - 22s 34ms/step - loss: 8.0677 - val_loss: 9.1044\n",
      "Epoch 70/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.0459 - val_loss: 9.0910\n",
      "Epoch 71/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.0287 - val_loss: 9.0685\n",
      "Epoch 72/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.0130 - val_loss: 9.1012\n",
      "Epoch 73/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 7.9913 - val_loss: 9.1181\n",
      "Epoch 74/75\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 7.9803 - val_loss: 9.0661\n",
      "Epoch 75/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 7.9575 - val_loss: 9.1049\n",
      "Model training time was 15.45 minutes (926.83 seconds).\n",
      "Average time for each epoch was 0.15 minutes (9.27 seconds).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:38:28.211778: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 13007060992 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= TRIAL 6, MAE: 9.85302389281811 =========================\n",
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:39:13.586379: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 13007060992 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-07-19 00:39:24.745646: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 462443 of 476032\n",
      "2023-07-19 00:39:25.039639: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:405] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - ETA: 0s - loss: 43.3010"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:39:39.475487: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 13007060992 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 38s 25ms/step - loss: 43.3010 - val_loss: 39.0396\n",
      "Epoch 2/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 39.3128 - val_loss: 36.5533\n",
      "Epoch 3/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 35.7387 - val_loss: 34.0406\n",
      "Epoch 4/75\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 31.3824 - val_loss: 30.2720\n",
      "Epoch 5/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 26.0625 - val_loss: 24.5545\n",
      "Epoch 6/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 20.3047 - val_loss: 17.7263\n",
      "Epoch 7/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 15.3179 - val_loss: 13.1001\n",
      "Epoch 8/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 12.1198 - val_loss: 10.9011\n",
      "Epoch 9/75\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 10.7975 - val_loss: 10.2138\n",
      "Epoch 10/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 10.3871 - val_loss: 10.0017\n",
      "Epoch 11/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 10.2138 - val_loss: 9.9334\n",
      "Epoch 12/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 10.1026 - val_loss: 9.8382\n",
      "Epoch 13/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 10.0075 - val_loss: 9.7751\n",
      "Epoch 14/75\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 9.9165 - val_loss: 9.7494\n",
      "Epoch 15/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.8390 - val_loss: 9.6830\n",
      "Epoch 16/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.7676 - val_loss: 9.6557\n",
      "Epoch 17/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.7053 - val_loss: 9.6219\n",
      "Epoch 18/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.6443 - val_loss: 9.5858\n",
      "Epoch 19/75\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 9.5881 - val_loss: 9.5545\n",
      "Epoch 20/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 9.5298 - val_loss: 9.5384\n",
      "Epoch 21/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.4771 - val_loss: 9.5028\n",
      "Epoch 22/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.4331 - val_loss: 9.4941\n",
      "Epoch 23/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 9.3781 - val_loss: 9.4770\n",
      "Epoch 24/75\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 9.3372 - val_loss: 9.4561\n",
      "Epoch 25/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 9.2943 - val_loss: 9.4180\n",
      "Epoch 26/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 9.2542 - val_loss: 9.4187\n",
      "Epoch 27/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 9.2121 - val_loss: 9.3938\n",
      "Epoch 28/75\n",
      "635/635 [==============================] - 22s 34ms/step - loss: 9.1751 - val_loss: 9.3908\n",
      "Epoch 29/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.1320 - val_loss: 9.3969\n",
      "Epoch 30/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.0914 - val_loss: 9.3374\n",
      "Epoch 31/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.0554 - val_loss: 9.3589\n",
      "Epoch 32/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 9.0167 - val_loss: 9.3263\n",
      "Epoch 33/75\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 8.9901 - val_loss: 9.3542\n",
      "Epoch 34/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.9471 - val_loss: 9.2955\n",
      "Epoch 35/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.9167 - val_loss: 9.3107\n",
      "Epoch 36/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.8835 - val_loss: 9.2950\n",
      "Epoch 37/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.8487 - val_loss: 9.2742\n",
      "Epoch 38/75\n",
      "635/635 [==============================] - 21s 32ms/step - loss: 8.8208 - val_loss: 9.2794\n",
      "Epoch 39/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.7918 - val_loss: 9.2933\n",
      "Epoch 40/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.7640 - val_loss: 9.2539\n",
      "Epoch 41/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.7309 - val_loss: 9.2497\n",
      "Epoch 42/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.7026 - val_loss: 9.2519\n",
      "Epoch 43/75\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 8.6724 - val_loss: 9.2179\n",
      "Epoch 44/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.6363 - val_loss: 9.2171\n",
      "Epoch 45/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.6181 - val_loss: 9.2170\n",
      "Epoch 46/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.5869 - val_loss: 9.2725\n",
      "Epoch 47/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.5653 - val_loss: 9.2211\n",
      "Epoch 48/75\n",
      "635/635 [==============================] - 21s 34ms/step - loss: 8.5365 - val_loss: 9.1930\n",
      "Epoch 49/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.5097 - val_loss: 9.1644\n",
      "Epoch 50/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.4862 - val_loss: 9.1701\n",
      "Epoch 51/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.4612 - val_loss: 9.1953\n",
      "Epoch 52/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.4348 - val_loss: 9.1701\n",
      "Epoch 53/75\n",
      "635/635 [==============================] - 22s 34ms/step - loss: 8.4114 - val_loss: 9.1623\n",
      "Epoch 54/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.3898 - val_loss: 9.1845\n",
      "Epoch 55/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.3644 - val_loss: 9.1908\n",
      "Epoch 56/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.3428 - val_loss: 9.1864\n",
      "Epoch 57/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.3183 - val_loss: 9.1224\n",
      "Epoch 58/75\n",
      "635/635 [==============================] - 21s 34ms/step - loss: 8.3015 - val_loss: 9.1693\n",
      "Epoch 59/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.2778 - val_loss: 9.1510\n",
      "Epoch 60/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.2560 - val_loss: 9.1364\n",
      "Epoch 61/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.2355 - val_loss: 9.1570\n",
      "Epoch 62/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.2187 - val_loss: 9.1459\n",
      "Epoch 63/75\n",
      "635/635 [==============================] - 21s 34ms/step - loss: 8.1956 - val_loss: 9.1299\n",
      "Epoch 64/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.1808 - val_loss: 9.1130\n",
      "Epoch 65/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.1571 - val_loss: 9.1166\n",
      "Epoch 66/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.1337 - val_loss: 9.1034\n",
      "Epoch 67/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.1228 - val_loss: 9.1201\n",
      "Epoch 68/75\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 8.0986 - val_loss: 9.0955\n",
      "Epoch 69/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.0843 - val_loss: 9.1288\n",
      "Epoch 70/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.0589 - val_loss: 9.1041\n",
      "Epoch 71/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.0453 - val_loss: 9.0834\n",
      "Epoch 72/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.0249 - val_loss: 9.1031\n",
      "Epoch 73/75\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 8.0080 - val_loss: 9.1015\n",
      "Epoch 74/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 7.9932 - val_loss: 9.0779\n",
      "Epoch 75/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 7.9846 - val_loss: 9.0859\n",
      "Model training time was 15.31 minutes (918.88 seconds).\n",
      "Average time for each epoch was 0.15 minutes (9.19 seconds).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:54:41.170997: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 13007060992 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= TRIAL 7, MAE: 9.74230706731968 =========================\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "predictions = []\n",
    "NUM_EPOCHS  = 75\n",
    "\n",
    "for i in range(8):\n",
    "    tf.keras.utils.set_random_seed(10)\n",
    "    history, model = train_model_new(params, normalizers, x_train, y_train, x_val, y_val, True, shuffle_buffer = .75)\n",
    "    pred = model.predict(x_test)\n",
    "    predictions.append(pred)\n",
    "    print('='*25+f' TRIAL {i}, MAE: {mean_absolute_error(pred,y_test)} '+'='*25)\n",
    "    results.append([history, model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a8b3d801-5534-40b8-a26a-d732b36e1951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1: 9.774\n",
      "Trial 2: 9.843\n",
      "Trial 3: 9.851\n",
      "Trial 4: 9.765\n",
      "Trial 5: 9.820\n",
      "Trial 6: 9.817\n",
      "Trial 7: 9.853\n",
      "Trial 8: 9.742\n",
      "Average MAE: 9.808\n"
     ]
    }
   ],
   "source": [
    "#SECOND RUN ON 07/14; 100 EPOCHS\n",
    "t = 0\n",
    "for i, prediction in enumerate(predictions):\n",
    "    print(f'Trial {i+1}: {mean_absolute_error(prediction, y_test):.3f}')\n",
    "    t += mean_absolute_error(prediction, y_test)\n",
    "print(f'Average MAE: {t/len(predictions):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70ac2c25-63e7-43e4-915f-0a72c6fc2df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 18:27:51.158205: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: baseline_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: baseline_model/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7ff79de2b7d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7ff79de426d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7ff79de3c1d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7ff79dc12410> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "model.save('baseline_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a15b1f7-c7af-4101-ac87-cafd0bfc7862",
   "metadata": {},
   "source": [
    "# Pricing Hypothetical Trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b59dba5f-087c-4e6a-a37b-33f91bc0986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_days(row, col):\n",
    "    if pd.isna(row[col]) or pd.isna(row['settlement_date']):\n",
    "        return 0\n",
    "    else: \n",
    "        diff = diff_in_days_two_dates(row[col], row.settlement_date)\n",
    "        if diff <= 0:\n",
    "            return -1\n",
    "        else:\n",
    "            return diff\n",
    "        \n",
    "        \n",
    "def sample_dataframe(df, N):\n",
    "    \n",
    "    group_name = df.name\n",
    "    def index_to_dict(index_row):\n",
    "        is_callable = index_row[0]\n",
    "        interval = str(index_row[1].left)+'-'+str(index_row[1].right)\n",
    "        rating = index_row[2]\n",
    "        return {'is_callable':is_callable, 'interval': interval, 'rating':rating}\n",
    "    \n",
    "    df = df.drop_duplicates(subset='cusip')\n",
    "    \n",
    "    if len(df) < N:\n",
    "        N = len(df)\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    result = df.sample(N, replace=False) \n",
    "    group_id = next(COUNT)\n",
    "    result['group'] = group_id\n",
    "    groupby_id_dict[group_id] = index_to_dict(group_name)\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_synthetic_samples(df, col, vals):\n",
    "    for cusip in df.cusip.unique():\n",
    "        for val in vals:\n",
    "            temp = df[df.cusip == cusip].iloc[0] \n",
    "            if val != temp[col]:\n",
    "                temp[col] = val\n",
    "                df = df.append(temp)\n",
    "    return df\n",
    "\n",
    "def make_summary(col):\n",
    "    # col = 'is_callable'\n",
    "\n",
    "    summary_df = synthetic_sampled_data.groupby(['cusip'])\\\n",
    "    ['predictions']\\\n",
    "    .agg(['std', max_min_f])\\\n",
    "    .rename({'<lambda_0>':'Max-Min'}, axis=1)\\\n",
    "    \n",
    "    summary_df = summary_df.join(synthetic_sampled_data.set_index('cusip')[col])\n",
    "    \n",
    "    summary_df = summary_df.groupby(col).mean()\n",
    "    # summary_df.columns = pd.MultiIndex.from_tuples(summary_df.columns)\n",
    "    \n",
    "    display(summary_df)\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "827b78c1-d2af-4861-9d61-ae2a6d434713",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test_dataframe[test_dataframe.trade_date <= '2023-06-09'].copy()\n",
    "# data['days_to_call'] = data[['settlement_date','next_call_date']].apply(lambda x: get_days(x, 'next_call_date'), axis = 1)/NUM_OF_DAYS_IN_YEAR\n",
    "data['maturity'] = data[['settlement_date','maturity_date']].apply(lambda x: get_days(x, 'maturity_date'), axis = 1)/NUM_OF_DAYS_IN_YEAR\n",
    "data['maturity_bucket'] = pd.cut(data['maturity'], [0, 5, 10, 15, 20, 30, data.maturity.max()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32d3ba48-8c05-4d86-b160-6b63199d228b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 16s, sys: 73.4 ms, total: 1min 16s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "#We just look at 5 rating values for efficiency \n",
    "RATINGS = ['AAA',  'BBB', 'CCC', 'NR']\n",
    "groupby_cols = ['is_callable','maturity_bucket','rating']\n",
    "\n",
    "sampled_data = data[data.rating.isin(RATINGS)].groupby(groupby_cols)\n",
    "COUNT = iter(range(len(sampled_data.groups)))\n",
    "groupby_id_dict = dict()\n",
    "\n",
    "#Sample 20 random cusips from each maturity bucket, rating and callable combination \n",
    "sampled_data = sampled_data.apply(lambda x: sample_dataframe(x, 20)).reset_index(drop=True)\n",
    "\n",
    "%time synthetic_sampled_data = create_synthetic_samples(sampled_data, 'rating', RATINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34fe743f-3ff6-410a-8ffc-bfa3ec00c18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rtrs_control_number</th>\n",
       "      <th>cusip</th>\n",
       "      <th>yield</th>\n",
       "      <th>is_callable</th>\n",
       "      <th>refund_date</th>\n",
       "      <th>accrual_date</th>\n",
       "      <th>dated_date</th>\n",
       "      <th>next_sink_date</th>\n",
       "      <th>coupon</th>\n",
       "      <th>delivery_date</th>\n",
       "      <th>...</th>\n",
       "      <th>abs_last_yield_spread</th>\n",
       "      <th>abs_diff_size</th>\n",
       "      <th>days_duration</th>\n",
       "      <th>trade_history_sum</th>\n",
       "      <th>ted-rate</th>\n",
       "      <th>trade_history_shortened</th>\n",
       "      <th>trade_history_fixed</th>\n",
       "      <th>maturity</th>\n",
       "      <th>maturity_bucket</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023060106012200</td>\n",
       "      <td>13067WRB0</td>\n",
       "      <td>506.300</td>\n",
       "      <td>False</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-08-06</td>\n",
       "      <td>2020-08-06</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.560</td>\n",
       "      <td>2020-08-06</td>\n",
       "      <td>...</td>\n",
       "      <td>196.191</td>\n",
       "      <td>60000.000</td>\n",
       "      <td>549</td>\n",
       "      <td>1339.388</td>\n",
       "      <td>-72.000</td>\n",
       "      <td>[[196.19109531370867, 99.8, 4.602059841156006,...</td>\n",
       "      <td>[[188.88488489691935, 87.3, 5.0, 1.0, 0.0, 5.7...</td>\n",
       "      <td>1.489</td>\n",
       "      <td>(0.0, 5.0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023060208551800</td>\n",
       "      <td>269696KP4</td>\n",
       "      <td>305.000</td>\n",
       "      <td>False</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2018-12-20</td>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2018-12-20</td>\n",
       "      <td>...</td>\n",
       "      <td>1.941</td>\n",
       "      <td>5000.000</td>\n",
       "      <td>1542</td>\n",
       "      <td>-216.378</td>\n",
       "      <td>-81.000</td>\n",
       "      <td>[[-1.9410871431837222, -59.0, 4.17609119415283...</td>\n",
       "      <td>[[-1.9410871431837222, -59.0, 4.17609119415283...</td>\n",
       "      <td>4.192</td>\n",
       "      <td>(0.0, 5.0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023060108789700</td>\n",
       "      <td>161045QG7</td>\n",
       "      <td>289.200</td>\n",
       "      <td>False</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-09-24</td>\n",
       "      <td>2020-09-24</td>\n",
       "      <td>NaT</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2020-09-24</td>\n",
       "      <td>...</td>\n",
       "      <td>16.132</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>1497</td>\n",
       "      <td>-381.400</td>\n",
       "      <td>-72.000</td>\n",
       "      <td>[[-16.132136149069538, -72.6, 4.0, 0.0, 1.0, 5...</td>\n",
       "      <td>[[4.6833993241039025, -64.8, 4.0, 0.0, 0.0, 5....</td>\n",
       "      <td>4.072</td>\n",
       "      <td>(0.0, 5.0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023060103514200</td>\n",
       "      <td>655867TX7</td>\n",
       "      <td>305.600</td>\n",
       "      <td>False</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2016-10-19</td>\n",
       "      <td>2016-10-19</td>\n",
       "      <td>NaT</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2016-10-19</td>\n",
       "      <td>...</td>\n",
       "      <td>13.485</td>\n",
       "      <td>15000.000</td>\n",
       "      <td>1220</td>\n",
       "      <td>-487.650</td>\n",
       "      <td>-72.000</td>\n",
       "      <td>[[13.484656000666575, -84.1, 4.0, 1.0, 0.0, 5....</td>\n",
       "      <td>[[10.384656000666553, -87.2, 4.0, 0.0, 0.0, 5....</td>\n",
       "      <td>3.322</td>\n",
       "      <td>(0.0, 5.0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023060509494600</td>\n",
       "      <td>92818MZH6</td>\n",
       "      <td>284.600</td>\n",
       "      <td>False</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2018-08-06</td>\n",
       "      <td>2018-08-06</td>\n",
       "      <td>NaT</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2018-08-06</td>\n",
       "      <td>...</td>\n",
       "      <td>4.739</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1624</td>\n",
       "      <td>-528.767</td>\n",
       "      <td>-77.000</td>\n",
       "      <td>[[-4.739394888894822, -89.0, 4.397940158843994...</td>\n",
       "      <td>[[-3.163124109371296, -125.1, 4.39794015884399...</td>\n",
       "      <td>4.400</td>\n",
       "      <td>(0.0, 5.0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>2023060105719200</td>\n",
       "      <td>65830RCV9</td>\n",
       "      <td>425.000</td>\n",
       "      <td>True</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-12-17</td>\n",
       "      <td>2019-12-17</td>\n",
       "      <td>2050-01-01</td>\n",
       "      <td>4.000</td>\n",
       "      <td>2019-12-17</td>\n",
       "      <td>...</td>\n",
       "      <td>19.986</td>\n",
       "      <td>5000.000</td>\n",
       "      <td>11536</td>\n",
       "      <td>117.164</td>\n",
       "      <td>-72.000</td>\n",
       "      <td>[[-19.985699461609443, 41.0, 4.301030158996582...</td>\n",
       "      <td>[[-18.485699461609386, 42.5, 4.301030158996582...</td>\n",
       "      <td>31.572</td>\n",
       "      <td>(30.0, 46.136]</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>2023060702745900</td>\n",
       "      <td>888805CJ5</td>\n",
       "      <td>524.900</td>\n",
       "      <td>True</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2021-04-13</td>\n",
       "      <td>2021-04-13</td>\n",
       "      <td>2028-06-01</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2021-04-13</td>\n",
       "      <td>...</td>\n",
       "      <td>62.154</td>\n",
       "      <td>3000000.000</td>\n",
       "      <td>15335</td>\n",
       "      <td>1064.315</td>\n",
       "      <td>-77.000</td>\n",
       "      <td>[[62.15424012169029, 135.8, 6.698969841003418,...</td>\n",
       "      <td>[[64.35424012169022, 138.0, 7.176091194152832,...</td>\n",
       "      <td>41.978</td>\n",
       "      <td>(30.0, 46.136]</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>2023060101614700</td>\n",
       "      <td>13016NFC3</td>\n",
       "      <td>568.000</td>\n",
       "      <td>True</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-06-10</td>\n",
       "      <td>2020-06-10</td>\n",
       "      <td>2027-06-01</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2020-06-10</td>\n",
       "      <td>...</td>\n",
       "      <td>111.783</td>\n",
       "      <td>12055000.000</td>\n",
       "      <td>11778</td>\n",
       "      <td>1866.770</td>\n",
       "      <td>-72.000</td>\n",
       "      <td>[[111.78281923384151, 175.0, 7.431363582611084...</td>\n",
       "      <td>[[109.78281923384151, 173.0, 7.431363582611084...</td>\n",
       "      <td>31.989</td>\n",
       "      <td>(30.0, 46.136]</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>2023060103450000</td>\n",
       "      <td>118217CZ9</td>\n",
       "      <td>546.000</td>\n",
       "      <td>True</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-03-04</td>\n",
       "      <td>2020-03-04</td>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2020-03-04</td>\n",
       "      <td>...</td>\n",
       "      <td>103.148</td>\n",
       "      <td>1500000.000</td>\n",
       "      <td>11684</td>\n",
       "      <td>1296.770</td>\n",
       "      <td>-72.000</td>\n",
       "      <td>[[103.1482038867872, 162.0, 6.397940158843994,...</td>\n",
       "      <td>[[105.1482038867872, 164.0, 6.3324384689331055...</td>\n",
       "      <td>31.989</td>\n",
       "      <td>(30.0, 46.136]</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>2023060101076500</td>\n",
       "      <td>38122NB84</td>\n",
       "      <td>534.000</td>\n",
       "      <td>True</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>2029-06-01</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>...</td>\n",
       "      <td>65.417</td>\n",
       "      <td>2000000.000</td>\n",
       "      <td>15702</td>\n",
       "      <td>1151.008</td>\n",
       "      <td>-72.000</td>\n",
       "      <td>[[65.41674418743281, 150.0, 6.84509801864624, ...</td>\n",
       "      <td>[[65.41674418743281, 150.0, 7.0, 0.0, 1.0, 3.4...</td>\n",
       "      <td>42.989</td>\n",
       "      <td>(30.0, 46.136]</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows  181 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     rtrs_control_number      cusip   yield  is_callable refund_date  \\\n",
       "0       2023060106012200  13067WRB0 506.300        False         NaT   \n",
       "1       2023060208551800  269696KP4 305.000        False         NaT   \n",
       "2       2023060108789700  161045QG7 289.200        False         NaT   \n",
       "3       2023060103514200  655867TX7 305.600        False         NaT   \n",
       "4       2023060509494600  92818MZH6 284.600        False         NaT   \n",
       "..                   ...        ...     ...          ...         ...   \n",
       "495     2023060105719200  65830RCV9 425.000         True         NaT   \n",
       "496     2023060702745900  888805CJ5 524.900         True         NaT   \n",
       "497     2023060101614700  13016NFC3 568.000         True         NaT   \n",
       "498     2023060103450000  118217CZ9 546.000         True         NaT   \n",
       "499     2023060101076500  38122NB84 534.000         True         NaT   \n",
       "\n",
       "    accrual_date dated_date next_sink_date  coupon delivery_date  ...  \\\n",
       "0     2020-08-06 2020-08-06            NaT   0.560    2020-08-06  ...   \n",
       "1     2018-12-20 2018-12-01            NaT   5.000    2018-12-20  ...   \n",
       "2     2020-09-24 2020-09-24            NaT   5.000    2020-09-24  ...   \n",
       "3     2016-10-19 2016-10-19            NaT   5.000    2016-10-19  ...   \n",
       "4     2018-08-06 2018-08-06            NaT   5.000    2018-08-06  ...   \n",
       "..           ...        ...            ...     ...           ...  ...   \n",
       "495   2019-12-17 2019-12-17     2050-01-01   4.000    2019-12-17  ...   \n",
       "496   2021-04-13 2021-04-13     2028-06-01   0.000    2021-04-13  ...   \n",
       "497   2020-06-10 2020-06-10     2027-06-01   0.000    2020-06-10  ...   \n",
       "498   2020-03-04 2020-03-04     2024-06-01   5.000    2020-03-04  ...   \n",
       "499   2021-12-15 2021-12-15     2029-06-01   0.000    2021-12-15  ...   \n",
       "\n",
       "    abs_last_yield_spread abs_diff_size days_duration trade_history_sum  \\\n",
       "0                 196.191     60000.000           549          1339.388   \n",
       "1                   1.941      5000.000          1542          -216.378   \n",
       "2                  16.132     10000.000          1497          -381.400   \n",
       "3                  13.485     15000.000          1220          -487.650   \n",
       "4                   4.739         0.000          1624          -528.767   \n",
       "..                    ...           ...           ...               ...   \n",
       "495                19.986      5000.000         11536           117.164   \n",
       "496                62.154   3000000.000         15335          1064.315   \n",
       "497               111.783  12055000.000         11778          1866.770   \n",
       "498               103.148   1500000.000         11684          1296.770   \n",
       "499                65.417   2000000.000         15702          1151.008   \n",
       "\n",
       "     ted-rate                            trade_history_shortened  \\\n",
       "0     -72.000  [[196.19109531370867, 99.8, 4.602059841156006,...   \n",
       "1     -81.000  [[-1.9410871431837222, -59.0, 4.17609119415283...   \n",
       "2     -72.000  [[-16.132136149069538, -72.6, 4.0, 0.0, 1.0, 5...   \n",
       "3     -72.000  [[13.484656000666575, -84.1, 4.0, 1.0, 0.0, 5....   \n",
       "4     -77.000  [[-4.739394888894822, -89.0, 4.397940158843994...   \n",
       "..        ...                                                ...   \n",
       "495   -72.000  [[-19.985699461609443, 41.0, 4.301030158996582...   \n",
       "496   -77.000  [[62.15424012169029, 135.8, 6.698969841003418,...   \n",
       "497   -72.000  [[111.78281923384151, 175.0, 7.431363582611084...   \n",
       "498   -72.000  [[103.1482038867872, 162.0, 6.397940158843994,...   \n",
       "499   -72.000  [[65.41674418743281, 150.0, 6.84509801864624, ...   \n",
       "\n",
       "                                   trade_history_fixed  maturity  \\\n",
       "0    [[188.88488489691935, 87.3, 5.0, 1.0, 0.0, 5.7...     1.489   \n",
       "1    [[-1.9410871431837222, -59.0, 4.17609119415283...     4.192   \n",
       "2    [[4.6833993241039025, -64.8, 4.0, 0.0, 0.0, 5....     4.072   \n",
       "3    [[10.384656000666553, -87.2, 4.0, 0.0, 0.0, 5....     3.322   \n",
       "4    [[-3.163124109371296, -125.1, 4.39794015884399...     4.400   \n",
       "..                                                 ...       ...   \n",
       "495  [[-18.485699461609386, 42.5, 4.301030158996582...    31.572   \n",
       "496  [[64.35424012169022, 138.0, 7.176091194152832,...    41.978   \n",
       "497  [[109.78281923384151, 173.0, 7.431363582611084...    31.989   \n",
       "498  [[105.1482038867872, 164.0, 6.3324384689331055...    31.989   \n",
       "499  [[65.41674418743281, 150.0, 7.0, 0.0, 1.0, 3.4...    42.989   \n",
       "\n",
       "     maturity_bucket  group  \n",
       "0         (0.0, 5.0]      0  \n",
       "1         (0.0, 5.0]      0  \n",
       "2         (0.0, 5.0]      0  \n",
       "3         (0.0, 5.0]      0  \n",
       "4         (0.0, 5.0]      0  \n",
       "..               ...    ...  \n",
       "495   (30.0, 46.136]     32  \n",
       "496   (30.0, 46.136]     33  \n",
       "497   (30.0, 46.136]     33  \n",
       "498   (30.0, 46.136]     33  \n",
       "499   (30.0, 46.136]     33  \n",
       "\n",
       "[500 rows x 181 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b86e3ce5-b4eb-413b-a864-d10bc8e1c1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rtrs_control_number</th>\n",
       "      <th>cusip</th>\n",
       "      <th>yield</th>\n",
       "      <th>is_callable</th>\n",
       "      <th>refund_date</th>\n",
       "      <th>accrual_date</th>\n",
       "      <th>dated_date</th>\n",
       "      <th>next_sink_date</th>\n",
       "      <th>coupon</th>\n",
       "      <th>delivery_date</th>\n",
       "      <th>...</th>\n",
       "      <th>abs_last_yield_spread</th>\n",
       "      <th>abs_diff_size</th>\n",
       "      <th>days_duration</th>\n",
       "      <th>trade_history_sum</th>\n",
       "      <th>ted-rate</th>\n",
       "      <th>trade_history_shortened</th>\n",
       "      <th>trade_history_fixed</th>\n",
       "      <th>maturity</th>\n",
       "      <th>maturity_bucket</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023060106012200</td>\n",
       "      <td>13067WRB0</td>\n",
       "      <td>506.300</td>\n",
       "      <td>False</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-08-06</td>\n",
       "      <td>2020-08-06</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.560</td>\n",
       "      <td>2020-08-06</td>\n",
       "      <td>...</td>\n",
       "      <td>196.191</td>\n",
       "      <td>60000.000</td>\n",
       "      <td>549</td>\n",
       "      <td>1339.388</td>\n",
       "      <td>-72.000</td>\n",
       "      <td>[[196.19109531370867, 99.8, 4.602059841156006,...</td>\n",
       "      <td>[[188.88488489691935, 87.3, 5.0, 1.0, 0.0, 5.7...</td>\n",
       "      <td>1.489</td>\n",
       "      <td>(0.0, 5.0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023060208551800</td>\n",
       "      <td>269696KP4</td>\n",
       "      <td>305.000</td>\n",
       "      <td>False</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2018-12-20</td>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2018-12-20</td>\n",
       "      <td>...</td>\n",
       "      <td>1.941</td>\n",
       "      <td>5000.000</td>\n",
       "      <td>1542</td>\n",
       "      <td>-216.378</td>\n",
       "      <td>-81.000</td>\n",
       "      <td>[[-1.9410871431837222, -59.0, 4.17609119415283...</td>\n",
       "      <td>[[-1.9410871431837222, -59.0, 4.17609119415283...</td>\n",
       "      <td>4.192</td>\n",
       "      <td>(0.0, 5.0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023060108789700</td>\n",
       "      <td>161045QG7</td>\n",
       "      <td>289.200</td>\n",
       "      <td>False</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-09-24</td>\n",
       "      <td>2020-09-24</td>\n",
       "      <td>NaT</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2020-09-24</td>\n",
       "      <td>...</td>\n",
       "      <td>16.132</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>1497</td>\n",
       "      <td>-381.400</td>\n",
       "      <td>-72.000</td>\n",
       "      <td>[[-16.132136149069538, -72.6, 4.0, 0.0, 1.0, 5...</td>\n",
       "      <td>[[4.6833993241039025, -64.8, 4.0, 0.0, 0.0, 5....</td>\n",
       "      <td>4.072</td>\n",
       "      <td>(0.0, 5.0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023060103514200</td>\n",
       "      <td>655867TX7</td>\n",
       "      <td>305.600</td>\n",
       "      <td>False</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2016-10-19</td>\n",
       "      <td>2016-10-19</td>\n",
       "      <td>NaT</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2016-10-19</td>\n",
       "      <td>...</td>\n",
       "      <td>13.485</td>\n",
       "      <td>15000.000</td>\n",
       "      <td>1220</td>\n",
       "      <td>-487.650</td>\n",
       "      <td>-72.000</td>\n",
       "      <td>[[13.484656000666575, -84.1, 4.0, 1.0, 0.0, 5....</td>\n",
       "      <td>[[10.384656000666553, -87.2, 4.0, 0.0, 0.0, 5....</td>\n",
       "      <td>3.322</td>\n",
       "      <td>(0.0, 5.0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023060509494600</td>\n",
       "      <td>92818MZH6</td>\n",
       "      <td>284.600</td>\n",
       "      <td>False</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2018-08-06</td>\n",
       "      <td>2018-08-06</td>\n",
       "      <td>NaT</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2018-08-06</td>\n",
       "      <td>...</td>\n",
       "      <td>4.739</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1624</td>\n",
       "      <td>-528.767</td>\n",
       "      <td>-77.000</td>\n",
       "      <td>[[-4.739394888894822, -89.0, 4.397940158843994...</td>\n",
       "      <td>[[-3.163124109371296, -125.1, 4.39794015884399...</td>\n",
       "      <td>4.400</td>\n",
       "      <td>(0.0, 5.0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>2023060103450000</td>\n",
       "      <td>118217CZ9</td>\n",
       "      <td>546.000</td>\n",
       "      <td>True</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-03-04</td>\n",
       "      <td>2020-03-04</td>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2020-03-04</td>\n",
       "      <td>...</td>\n",
       "      <td>103.148</td>\n",
       "      <td>1500000.000</td>\n",
       "      <td>11684</td>\n",
       "      <td>1296.770</td>\n",
       "      <td>-72.000</td>\n",
       "      <td>[[103.1482038867872, 162.0, 6.397940158843994,...</td>\n",
       "      <td>[[105.1482038867872, 164.0, 6.3324384689331055...</td>\n",
       "      <td>31.989</td>\n",
       "      <td>(30.0, 46.136]</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>2023060103450000</td>\n",
       "      <td>118217CZ9</td>\n",
       "      <td>546.000</td>\n",
       "      <td>True</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-03-04</td>\n",
       "      <td>2020-03-04</td>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2020-03-04</td>\n",
       "      <td>...</td>\n",
       "      <td>103.148</td>\n",
       "      <td>1500000.000</td>\n",
       "      <td>11684</td>\n",
       "      <td>1296.770</td>\n",
       "      <td>-72.000</td>\n",
       "      <td>[[103.1482038867872, 162.0, 6.397940158843994,...</td>\n",
       "      <td>[[105.1482038867872, 164.0, 6.3324384689331055...</td>\n",
       "      <td>31.989</td>\n",
       "      <td>(30.0, 46.136]</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>2023060101076500</td>\n",
       "      <td>38122NB84</td>\n",
       "      <td>534.000</td>\n",
       "      <td>True</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>2029-06-01</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>...</td>\n",
       "      <td>65.417</td>\n",
       "      <td>2000000.000</td>\n",
       "      <td>15702</td>\n",
       "      <td>1151.008</td>\n",
       "      <td>-72.000</td>\n",
       "      <td>[[65.41674418743281, 150.0, 6.84509801864624, ...</td>\n",
       "      <td>[[65.41674418743281, 150.0, 7.0, 0.0, 1.0, 3.4...</td>\n",
       "      <td>42.989</td>\n",
       "      <td>(30.0, 46.136]</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>2023060101076500</td>\n",
       "      <td>38122NB84</td>\n",
       "      <td>534.000</td>\n",
       "      <td>True</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>2029-06-01</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>...</td>\n",
       "      <td>65.417</td>\n",
       "      <td>2000000.000</td>\n",
       "      <td>15702</td>\n",
       "      <td>1151.008</td>\n",
       "      <td>-72.000</td>\n",
       "      <td>[[65.41674418743281, 150.0, 6.84509801864624, ...</td>\n",
       "      <td>[[65.41674418743281, 150.0, 7.0, 0.0, 1.0, 3.4...</td>\n",
       "      <td>42.989</td>\n",
       "      <td>(30.0, 46.136]</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>2023060101076500</td>\n",
       "      <td>38122NB84</td>\n",
       "      <td>534.000</td>\n",
       "      <td>True</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>2029-06-01</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>...</td>\n",
       "      <td>65.417</td>\n",
       "      <td>2000000.000</td>\n",
       "      <td>15702</td>\n",
       "      <td>1151.008</td>\n",
       "      <td>-72.000</td>\n",
       "      <td>[[65.41674418743281, 150.0, 6.84509801864624, ...</td>\n",
       "      <td>[[65.41674418743281, 150.0, 7.0, 0.0, 1.0, 3.4...</td>\n",
       "      <td>42.989</td>\n",
       "      <td>(30.0, 46.136]</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows  181 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     rtrs_control_number      cusip   yield  is_callable refund_date  \\\n",
       "0       2023060106012200  13067WRB0 506.300        False         NaT   \n",
       "1       2023060208551800  269696KP4 305.000        False         NaT   \n",
       "2       2023060108789700  161045QG7 289.200        False         NaT   \n",
       "3       2023060103514200  655867TX7 305.600        False         NaT   \n",
       "4       2023060509494600  92818MZH6 284.600        False         NaT   \n",
       "..                   ...        ...     ...          ...         ...   \n",
       "498     2023060103450000  118217CZ9 546.000         True         NaT   \n",
       "498     2023060103450000  118217CZ9 546.000         True         NaT   \n",
       "499     2023060101076500  38122NB84 534.000         True         NaT   \n",
       "499     2023060101076500  38122NB84 534.000         True         NaT   \n",
       "499     2023060101076500  38122NB84 534.000         True         NaT   \n",
       "\n",
       "    accrual_date dated_date next_sink_date  coupon delivery_date  ...  \\\n",
       "0     2020-08-06 2020-08-06            NaT   0.560    2020-08-06  ...   \n",
       "1     2018-12-20 2018-12-01            NaT   5.000    2018-12-20  ...   \n",
       "2     2020-09-24 2020-09-24            NaT   5.000    2020-09-24  ...   \n",
       "3     2016-10-19 2016-10-19            NaT   5.000    2016-10-19  ...   \n",
       "4     2018-08-06 2018-08-06            NaT   5.000    2018-08-06  ...   \n",
       "..           ...        ...            ...     ...           ...  ...   \n",
       "498   2020-03-04 2020-03-04     2024-06-01   5.000    2020-03-04  ...   \n",
       "498   2020-03-04 2020-03-04     2024-06-01   5.000    2020-03-04  ...   \n",
       "499   2021-12-15 2021-12-15     2029-06-01   0.000    2021-12-15  ...   \n",
       "499   2021-12-15 2021-12-15     2029-06-01   0.000    2021-12-15  ...   \n",
       "499   2021-12-15 2021-12-15     2029-06-01   0.000    2021-12-15  ...   \n",
       "\n",
       "    abs_last_yield_spread abs_diff_size days_duration trade_history_sum  \\\n",
       "0                 196.191     60000.000           549          1339.388   \n",
       "1                   1.941      5000.000          1542          -216.378   \n",
       "2                  16.132     10000.000          1497          -381.400   \n",
       "3                  13.485     15000.000          1220          -487.650   \n",
       "4                   4.739         0.000          1624          -528.767   \n",
       "..                    ...           ...           ...               ...   \n",
       "498               103.148   1500000.000         11684          1296.770   \n",
       "498               103.148   1500000.000         11684          1296.770   \n",
       "499                65.417   2000000.000         15702          1151.008   \n",
       "499                65.417   2000000.000         15702          1151.008   \n",
       "499                65.417   2000000.000         15702          1151.008   \n",
       "\n",
       "     ted-rate                            trade_history_shortened  \\\n",
       "0     -72.000  [[196.19109531370867, 99.8, 4.602059841156006,...   \n",
       "1     -81.000  [[-1.9410871431837222, -59.0, 4.17609119415283...   \n",
       "2     -72.000  [[-16.132136149069538, -72.6, 4.0, 0.0, 1.0, 5...   \n",
       "3     -72.000  [[13.484656000666575, -84.1, 4.0, 1.0, 0.0, 5....   \n",
       "4     -77.000  [[-4.739394888894822, -89.0, 4.397940158843994...   \n",
       "..        ...                                                ...   \n",
       "498   -72.000  [[103.1482038867872, 162.0, 6.397940158843994,...   \n",
       "498   -72.000  [[103.1482038867872, 162.0, 6.397940158843994,...   \n",
       "499   -72.000  [[65.41674418743281, 150.0, 6.84509801864624, ...   \n",
       "499   -72.000  [[65.41674418743281, 150.0, 6.84509801864624, ...   \n",
       "499   -72.000  [[65.41674418743281, 150.0, 6.84509801864624, ...   \n",
       "\n",
       "                                   trade_history_fixed  maturity  \\\n",
       "0    [[188.88488489691935, 87.3, 5.0, 1.0, 0.0, 5.7...     1.489   \n",
       "1    [[-1.9410871431837222, -59.0, 4.17609119415283...     4.192   \n",
       "2    [[4.6833993241039025, -64.8, 4.0, 0.0, 0.0, 5....     4.072   \n",
       "3    [[10.384656000666553, -87.2, 4.0, 0.0, 0.0, 5....     3.322   \n",
       "4    [[-3.163124109371296, -125.1, 4.39794015884399...     4.400   \n",
       "..                                                 ...       ...   \n",
       "498  [[105.1482038867872, 164.0, 6.3324384689331055...    31.989   \n",
       "498  [[105.1482038867872, 164.0, 6.3324384689331055...    31.989   \n",
       "499  [[65.41674418743281, 150.0, 7.0, 0.0, 1.0, 3.4...    42.989   \n",
       "499  [[65.41674418743281, 150.0, 7.0, 0.0, 1.0, 3.4...    42.989   \n",
       "499  [[65.41674418743281, 150.0, 7.0, 0.0, 1.0, 3.4...    42.989   \n",
       "\n",
       "     maturity_bucket  group  \n",
       "0         (0.0, 5.0]      0  \n",
       "1         (0.0, 5.0]      0  \n",
       "2         (0.0, 5.0]      0  \n",
       "3         (0.0, 5.0]      0  \n",
       "4         (0.0, 5.0]      0  \n",
       "..               ...    ...  \n",
       "498   (30.0, 46.136]     33  \n",
       "498   (30.0, 46.136]     33  \n",
       "499   (30.0, 46.136]     33  \n",
       "499   (30.0, 46.136]     33  \n",
       "499   (30.0, 46.136]     33  \n",
       "\n",
       "[2000 rows x 181 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "998432c9-13c4-4a65-bb06-54ef5b4287f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 37s, sys: 0 ns, total: 1min 37s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%time synthetic_sampled_data = create_synthetic_samples(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4afee367-efc4-4d17-bdd0-537bf29c0b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = create_input_new(sampled_data, trade_history_col, yield_history_cols)\n",
    "X_sample_hypothetical_trades = create_input_new(synthetic_sampled_data, trade_history_col, yield_history_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "11d9895c-6282-4b83-a6d4-545c07c5144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data_predictions  = model.predict(X_sample).flatten()\n",
    "sampled_data_hypothetical_predictions  = model.predict(X_sample_hypothetical_trades).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "890c5d48-1891-423b-a9e5-f8277ebe43a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "sampled_data['predictions'] = sampled_data_predictions\n",
    "synthetic_sampled_data['predictions'] = sampled_data_hypothetical_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a36b9ebd-a640-4ba5-89fa-3d2da373e3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13.70137205875082, 15.0045147946588)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(sampled_data['predictions'], sampled_data['new_ys']), mean_absolute_error(synthetic_sampled_data['predictions'], synthetic_sampled_data['new_ys'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "db6306c0-b6b7-44a2-bdb9-407906c7a4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>std</th>\n",
       "      <th>Max-Min</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_callable</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>5.074</td>\n",
       "      <td>12.128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>4.526</td>\n",
       "      <td>10.882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              std  Max-Min\n",
       "is_callable               \n",
       "False       5.074   12.128\n",
       "True        4.526   10.882"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>std</th>\n",
       "      <th>Max-Min</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_rating</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAA</th>\n",
       "      <td>4.193</td>\n",
       "      <td>9.505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BBB</th>\n",
       "      <td>5.355</td>\n",
       "      <td>12.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCC</th>\n",
       "      <td>9.848</td>\n",
       "      <td>21.623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MR</th>\n",
       "      <td>4.842</td>\n",
       "      <td>12.156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NR</th>\n",
       "      <td>4.841</td>\n",
       "      <td>11.457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  std  Max-Min\n",
       "original_rating               \n",
       "AAA             4.193    9.505\n",
       "BBB             5.355   12.805\n",
       "CCC             9.848   21.623\n",
       "MR              4.842   12.156\n",
       "NR              4.841   11.457"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>std</th>\n",
       "      <th>Max-Min</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maturity_bucket</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(0.0, 5.0]</th>\n",
       "      <td>7.176</td>\n",
       "      <td>17.364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(5.0, 10.0]</th>\n",
       "      <td>5.332</td>\n",
       "      <td>12.661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(10.0, 15.0]</th>\n",
       "      <td>4.369</td>\n",
       "      <td>10.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(15.0, 20.0]</th>\n",
       "      <td>4.153</td>\n",
       "      <td>10.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(20.0, 30.0]</th>\n",
       "      <td>3.586</td>\n",
       "      <td>8.566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(30.0, 46.136]</th>\n",
       "      <td>2.574</td>\n",
       "      <td>6.213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  std  Max-Min\n",
       "maturity_bucket               \n",
       "(0.0, 5.0]      7.176   17.364\n",
       "(5.0, 10.0]     5.332   12.661\n",
       "(10.0, 15.0]    4.369   10.375\n",
       "(15.0, 20.0]    4.153   10.033\n",
       "(20.0, 30.0]    3.586    8.566\n",
       "(30.0, 46.136]  2.574    6.213"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_min_f = lambda x: x.max() - x.min()\n",
    "\n",
    "synthetic_sampled_data['original_rating'] = synthetic_sampled_data['group'].apply(lambda x: groupby_id_dict[x]['rating'])\n",
    "\n",
    "make_summary('is_callable');\n",
    "make_summary('original_rating');\n",
    "make_summary('maturity_bucket');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "af2b1275-b890-4017-aaec-ed1651c69f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stouch .gitignoreynthetic_sampled_data.to_pickle('synthetic_sampled_data_baseline.pkl')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
