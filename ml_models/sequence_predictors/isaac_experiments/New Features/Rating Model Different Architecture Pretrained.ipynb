{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54acdc5b-7dc8-4578-a0bd-02740ba377a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Packages, Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58de4fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 18:30:46.859563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-18 18:30:46.872145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-18 18:30:46.873784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing pandarallel with 16.0 cores\n",
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "TF Version: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import gcsfs\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import seaborn as sns\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle5 as pickle\n",
    "\n",
    "\n",
    "from ficc.utils.nelson_siegel_model import *\n",
    "from ficc.utils.diff_in_days import *\n",
    "from ficc.utils.auxiliary_functions import sqltodf\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "\n",
    "\n",
    "from ficc.data.process_data import process_data\n",
    "from ficc.utils.auxiliary_variables import PREDICTORS, NON_CAT_FEATURES, BINARY, CATEGORICAL_FEATURES, IDENTIFIERS, PURPOSE_CLASS_DICT, NUM_OF_DAYS_IN_YEAR\n",
    "from ficc.utils.gcp_storage_functions import upload_data, download_data\n",
    "from ficc.utils.auxiliary_variables import RELATED_TRADE_BINARY_FEATURES, RELATED_TRADE_NON_CAT_FEATURES, RELATED_TRADE_CATEGORICAL_FEATURES\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from ficc_keras_utils import *\n",
    "import ficc_keras_utils\n",
    "from lgbm_tools import *\n",
    "from ficc_debiasing import *\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "print(f'TF Version: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07c11246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/home/jupyter/ficc/isaac_creds.json\"\n",
    "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "bq_client = bigquery.Client()\n",
    "storage_client = storage.Client()\n",
    "\n",
    "##COMMON VARIABLES\n",
    "#DATA WINDOW\n",
    "train_start = '2023-05-01'\n",
    "train_end = '2023-06-01'\n",
    "test_start = '2023-06-01'\n",
    "test_end = '2023-07-01'\n",
    "#MODEL PARAMETERS \n",
    "VALIDATION_SPLIT = ficc_keras_utils.VALIDATION_SPLIT\n",
    "LEARNING_RATE = ficc_keras_utils.LEARNING_RATE\n",
    "BATCH_SIZE = 1000 #ficc_keras_utils.BATCH_SIZE\n",
    "NUM_EPOCHS = 75 #ficc_keras_utils.NUM_EPOCHS\n",
    "DROPOUT = 0.01 #ficc_keras_utils.DROPOUT\n",
    "\n",
    "##NOTEBOOK SPECIFIC VARIABLES \n",
    "TRADE_SEQUENCE_LENGTH = 2\n",
    "NUM_FEATURES = 6\n",
    "# target_variable = 'new_ys_diff'\n",
    "# target_variable = 'new_ys' \n",
    "target_variable = 'new_ys' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ca18089-58e8-4a2e-9c50-11898bfde236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-01 2023-06-01 2023-06-01 2023-07-01\n"
     ]
    }
   ],
   "source": [
    "print(train_start ,\n",
    "train_end ,\n",
    "test_start ,\n",
    "test_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917f52d4-5834-401a-a2ac-46559494098f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Data and Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "877a208b-c665-48ad-a8da-316b29c256b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File available, loading pickle\n",
      "CPU times: user 7.3 s, sys: 2.71 s, total: 10 s\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "%time processed_data = load_data_from_pickle('processed_file_FULL_2023-07-12-17:22.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e934f6f-e216-4871-aea7-598e8c16140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = processed_data[~(processed_data.trade_history.apply(lambda x: x.shape) == (5,6))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ab6562f-cb41-4dcf-91a5-17d7373d4d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data['trade_history'] = processed_data.trade_history.apply(lambda x: x[:,[0,2,3,4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a792e5f4-76fa-48db-8746-63e50f1603c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data['target_attention_features'] = processed_data['target_attention_features'].apply(lambda x: x[[0],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91f2d6b6-4d22-474c-9482-1c0a333e930a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data['target_attention_features'].iloc[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c184fe-3bff-410b-9bfe-44392274d494",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Checking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e94dfab-86e9-4cc1-91eb-c017cee0d551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2023-05-01 00:00:00'), Timestamp('2023-07-11 00:00:00'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.trade_date.min(), processed_data.trade_date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be6e3073-3be5-4b5c-a7fa-c0f42f623aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yield</th>\n",
       "      <th>new_real_time_ficc_ycl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1746702.000</td>\n",
       "      <td>1746702.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>364.498</td>\n",
       "      <td>323.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>91.917</td>\n",
       "      <td>41.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.100</td>\n",
       "      <td>263.663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>307.000</td>\n",
       "      <td>297.164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>350.300</td>\n",
       "      <td>310.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>411.000</td>\n",
       "      <td>336.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9892.500</td>\n",
       "      <td>541.456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            yield  new_real_time_ficc_ycl\n",
       "count 1746702.000             1746702.000\n",
       "mean      364.498                 323.013\n",
       "std        91.917                  41.183\n",
       "min         0.100                 263.663\n",
       "25%       307.000                 297.164\n",
       "50%       350.300                 310.806\n",
       "75%       411.000                 336.136\n",
       "max      9892.500                 541.456"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data[['yield','new_real_time_ficc_ycl']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2a60e40-35a7-4769-9fd2-399bb2f0cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f5c8ed7-c8f9-433b-92db-2868d3c716f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data['new_ys_realtime'] = processed_data['yield'] - processed_data['new_real_time_ficc_ycl']\n",
    "processed_data['new_ys'] = processed_data['yield'] - processed_data['new_ficc_ycl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a46c03ad-d7d5-4492-bba2-4cf4aef63c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### EXTRA DATA PREPROCESSING #####\n",
    "ttype_dict = { (0,0):'D', (0,1):'S', (1,0):'P' }\n",
    "\n",
    "ys_variants = [\"max_ys\", \"min_ys\", \"max_qty\", \"min_ago\", \"D_min_ago\", \"P_min_ago\", \"S_min_ago\"]\n",
    "ys_feats = [\"_ys\", \"_ttypes\", \"_ago\", \"_qdiff\"]\n",
    "D_prev = dict()\n",
    "P_prev = dict()\n",
    "S_prev = dict()\n",
    "\n",
    "def get_trade_history_columns():\n",
    "    '''\n",
    "    This function is used to create a list of columns\n",
    "    '''\n",
    "    YS_COLS = []\n",
    "    for prefix in ys_variants:\n",
    "        for suffix in ys_feats:\n",
    "            YS_COLS.append(prefix + suffix)\n",
    "    return YS_COLS\n",
    "\n",
    "def extract_feature_from_trade(row, name, trade):\n",
    "    yield_spread = trade[0]\n",
    "    ttypes = ttype_dict[(trade[3],trade[4])] + row.trade_type\n",
    "    seconds_ago = trade[5]\n",
    "    quantity_diff = np.log10(1 + np.abs(10**trade[2] - 10**row.quantity))\n",
    "    return [yield_spread, ttypes,  seconds_ago, quantity_diff]\n",
    "\n",
    "def trade_history_derived_features(row):\n",
    "    trade_history = row.trade_history\n",
    "    trade = trade_history[0]\n",
    "    \n",
    "    D_min_ago_t = D_prev.get(row.cusip,trade)\n",
    "    D_min_ago = 9        \n",
    "\n",
    "    P_min_ago_t = P_prev.get(row.cusip,trade)\n",
    "    P_min_ago = 9\n",
    "    \n",
    "    S_min_ago_t = S_prev.get(row.cusip,trade)\n",
    "    S_min_ago = 9\n",
    "    \n",
    "    max_ys_t = trade; max_ys = trade[0]\n",
    "    min_ys_t = trade; min_ys = trade[0]\n",
    "    max_qty_t = trade; max_qty = trade[2]\n",
    "    min_ago_t = trade; min_ago = trade[5]\n",
    "    \n",
    "    for trade in trade_history[0:]:\n",
    "        #Checking if the first trade in the history is from the same block\n",
    "        if trade[5] == 0: \n",
    "            continue\n",
    " \n",
    "        if trade[0] > max_ys: \n",
    "            max_ys_t = trade\n",
    "            max_ys = trade[0]\n",
    "        elif trade[0] < min_ys: \n",
    "            min_ys_t = trade; \n",
    "            min_ys = trade[0]\n",
    "\n",
    "        if trade[2] > max_qty: \n",
    "            max_qty_t = trade \n",
    "            max_qty = trade[2]\n",
    "        if trade[5] < min_ago: \n",
    "            min_ago_t = trade; \n",
    "            min_ago = trade[5]\n",
    "            \n",
    "        side = ttype_dict[(trade[3],trade[4])]\n",
    "        if side == \"D\":\n",
    "            if trade[5] < D_min_ago: \n",
    "                D_min_ago_t = trade; D_min_ago = trade[5]\n",
    "                D_prev[row.cusip] = trade\n",
    "        elif side == \"P\":\n",
    "            if trade[5] < P_min_ago: \n",
    "                P_min_ago_t = trade; P_min_ago = trade[5]\n",
    "                P_prev[row.cusip] = trade\n",
    "        elif side == \"S\":\n",
    "            if trade[5] < S_min_ago: \n",
    "                S_min_ago_t = trade; S_min_ago = trade[5]\n",
    "                S_prev[row.cusip] = trade\n",
    "        else: \n",
    "            print(\"invalid side\", trade)\n",
    "    \n",
    "    trade_history_dict = {\"max_ys\":max_ys_t,\n",
    "                          \"min_ys\":min_ys_t,\n",
    "                          \"max_qty\":max_qty_t,\n",
    "                          \"min_ago\":min_ago_t,\n",
    "                          \"D_min_ago\":D_min_ago_t,\n",
    "                          \"P_min_ago\":P_min_ago_t,\n",
    "                          \"S_min_ago\":S_min_ago_t}\n",
    "\n",
    "    return_list = []\n",
    "    for variant in ys_variants:\n",
    "        feature_list = extract_feature_from_trade(row,variant,trade_history_dict[variant])\n",
    "        return_list += feature_list\n",
    "    \n",
    "    return return_list\n",
    "\n",
    "def process_data(data): \n",
    "    data['ted-rate'] = (data['t_rate_10'] - data['t_rate_2']) * 100\n",
    "    \n",
    "    data = data[(data.days_to_call == 0) | (data.days_to_call > np.log10(400))]\n",
    "    data = data[(data.days_to_refund == 0) | (data.days_to_refund > np.log10(400))]\n",
    "    data = data[(data.days_to_maturity == 0) | (data.days_to_maturity > np.log10(400))]\n",
    "    data = data[data.days_to_maturity < np.log10(30000)]\n",
    "    data['trade_history_sum'] = data.trade_history.parallel_apply(lambda x: np.sum(x))\n",
    "    data.issue_amount = data.issue_amount.replace([np.inf, -np.inf], np.nan)\n",
    "    data.dropna(inplace=True, subset=PREDICTORS+['trade_history_sum'])\n",
    "    data.purpose_sub_class.fillna(0, inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "964a86ed-8535-4195-8ff8-88f9da5e8999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.2 s, sys: 5.79 s, total: 39.9 s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "temp = processed_data[['cusip','trade_history','quantity','trade_type']].parallel_apply(trade_history_derived_features, axis=1)\n",
    "YS_COLS = get_trade_history_columns()\n",
    "processed_data[YS_COLS] = pd.DataFrame(temp.tolist(), index=processed_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06f9fae8-c191-49ac-af98-2f6b720b663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data['ttypes'] = (processed_data.last_trade_type.astype(str) + processed_data.trade_type.astype(str)).astype('category')\n",
    "processed_data['diff_size'] = (processed_data.par_traded.astype(float) - processed_data.last_size).astype(np.float32)\n",
    "processed_data['abs_last_yield_spread'] = np.abs(processed_data['last_yield_spread'])\n",
    "processed_data['abs_diff_size'] = np.abs(processed_data['diff_size'])\n",
    "processed_data['days_duration'] = (processed_data.last_calc_date - processed_data.last_settlement_date).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddf12468-0634-4ac8-8085-17777a34d796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.3 s, sys: 3.94 s, total: 18.2 s\n",
      "Wall time: 19.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "processed_data['trade_history_sum'] = processed_data.trade_history.parallel_apply(lambda x: np.sum(x))\n",
    "processed_data = processed_data.dropna(subset=['trade_history_sum'])\n",
    "processed_data.purpose_sub_class.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e294b38-2ba3-49ec-930e-8f8d3a5a0187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in NON_CAT_FEATURES:\n",
    "#     print(col + ':' + str(train_dataframe[col].isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26e3d97e-c3b0-4511-b6de-79717352924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['new_ficc_ycl', 'yield_spread']:\n",
    "    if col in NON_CAT_FEATURES:\n",
    "        NON_CAT_FEATURES.remove(col)\n",
    "    if col in PREDICTORS:\n",
    "        PREDICTORS.remove(col)\n",
    "        \n",
    "# for col in ['rating']:\n",
    "#     if col in CATEGORICAL_FEATURES:\n",
    "#         CATEGORICAL_FEATURES.remove(col)\n",
    "#     if col in PREDICTORS:\n",
    "#         PREDICTORS.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "302404ee-098f-4e8f-9bd3-cfc57fc96974",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = process_data(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc47f4e4-9a1a-4ffa-932c-5413827d0731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating\n",
      "incorporated_state_code\n",
      "trade_type\n",
      "purpose_class\n",
      "max_ys_ttypes\n",
      "min_ys_ttypes\n",
      "max_qty_ttypes\n",
      "min_ago_ttypes\n",
      "D_min_ago_ttypes\n",
      "P_min_ago_ttypes\n",
      "S_min_ago_ttypes\n"
     ]
    }
   ],
   "source": [
    "additional_features = ['ttypes', 'diff_size', 'abs_last_yield_spread', 'abs_diff_size', 'days_duration']\n",
    "YS_COLS = ['max_ys_ys', 'max_ys_ttypes', 'max_ys_ago', 'max_ys_qdiff', 'min_ys_ys', 'min_ys_ttypes', 'min_ys_ago', 'min_ys_qdiff', 'max_qty_ys', 'max_qty_ttypes', \\\n",
    "           'max_qty_ago', 'max_qty_qdiff', 'min_ago_ys', 'min_ago_ttypes', 'min_ago_ago', 'min_ago_qdiff', 'D_min_ago_ys', 'D_min_ago_ttypes', 'D_min_ago_ago', 'D_min_ago_qdiff',\\\n",
    "           'P_min_ago_ys', 'P_min_ago_ttypes', 'P_min_ago_ago', 'P_min_ago_qdiff', 'S_min_ago_ys', 'S_min_ago_ttypes', 'S_min_ago_ago', 'S_min_ago_qdiff']\n",
    "    \n",
    "for col in YS_COLS:\n",
    "    if 'ttypes' in col and col not in PREDICTORS:\n",
    "        PREDICTORS.append(col)\n",
    "        CATEGORICAL_FEATURES.append(col)\n",
    "    elif col not in PREDICTORS:\n",
    "        NON_CAT_FEATURES.append(col)\n",
    "        PREDICTORS.append(col)\n",
    "        \n",
    "if 'target_attention_features' not in PREDICTORS:\n",
    "    PREDICTORS.append('target_attention_features')\n",
    "    \n",
    "if 'ficc_treasury_spread' not in PREDICTORS:\n",
    "    PREDICTORS.append('ficc_treasury_spread')\n",
    "    NON_CAT_FEATURES.append('ficc_treasury_spread')\n",
    "    \n",
    "for col in ['new_ficc_ycl', 'new_real_time_ficc_ycl']:     \n",
    "    if col not in PREDICTORS:\n",
    "        PREDICTORS.append(col)\n",
    "        NON_CAT_FEATURES.append(col)\n",
    "        \n",
    "encoders = {}\n",
    "fmax = {}\n",
    "for f in CATEGORICAL_FEATURES:\n",
    "    print(f)\n",
    "    if f in ['rating', 'incorporated_state_code', 'trade_type', 'purpose_class']:\n",
    "        fprep = preprocessing.LabelEncoder().fit(categorical_feature_values[f])\n",
    "    else:\n",
    "        fprep = preprocessing.LabelEncoder().fit(processed_data[f].drop_duplicates())\n",
    "    fmax[f] = np.max(fprep.transform(fprep.classes_))\n",
    "    encoders[f] = fprep\n",
    "    \n",
    "with open('encoders.pkl','wb') as file:\n",
    "    pickle.dump(encoders,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b92bf3e9-bd78-4aa1-a031-892a653b3524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 872 ms, sys: 11.7 ms, total: 884 ms\n",
      "Wall time: 883 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "processed_data['trade_history_shortened'] = processed_data['trade_history'].apply(lambda x: x[:TRADE_SEQUENCE_LENGTH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb0d6e5b-5e69-454e-9d53-348fc11ad2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 931 ms, sys: 34.7 ms, total: 966 ms\n",
      "Wall time: 965 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "processed_data['trade_history_fixed'] = processed_data['trade_history_shortened'].apply(lambda x: x[::-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "408ba4ae-31be-4b37-be1b-a6c143637c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2023-05-01 00:00:00'), Timestamp('2023-07-11 00:00:00'), 1590785)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.trade_date.min(), processed_data.trade_date.max(), len(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91873089-ae3e-4351-ab3e-ded0eafbdbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.to_pickle('processed_data_ratings_comparison.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9daf580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data start: 2023-05-01 00:00:00, end: 2023-05-31 00:00:00\n",
      "Test data start: 2023-06-01 00:00:00, end: 2023-06-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "train_filter = (processed_data.trade_date < train_end) & (processed_data.trade_date >= train_start)\n",
    "test_filter = (processed_data.trade_date >= test_start) & (processed_data.trade_date <test_end)\n",
    "                                                            \n",
    "train_dataframe = processed_data[train_filter]\\\n",
    ".sort_values(by='trade_date', ascending=True)\\\n",
    ".reset_index(drop=True)\n",
    "\n",
    "test_dataframe = processed_data[test_filter]\\\n",
    ".sort_values(by='trade_date', ascending=True)\\\n",
    ".reset_index(drop=True)\n",
    "\n",
    "print('Training data start: {}, end: {}'.format(train_dataframe.trade_date.min(),train_dataframe.trade_date.max()))\n",
    "print('Test data start: {}, end: {}'.format(test_dataframe.trade_date.min(),test_dataframe.trade_date.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00361ac5-8d00-45be-a40e-9bd8fdbf09ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-01 00:00:00 2023-05-31 00:00:00 705233\n",
      "2023-06-01 00:00:00 2023-06-30 00:00:00 697202\n"
     ]
    }
   ],
   "source": [
    "print(train_dataframe.trade_date.min(), train_dataframe.trade_date.max(), len(train_dataframe))\n",
    "print(test_dataframe.trade_date.min(), test_dataframe.trade_date.max(), len(test_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f44b3bb4-26e1-43ad-a85b-937a3f912834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_new(df, trade_history_col):\n",
    "    global encoders\n",
    "    trade_history_input = []\n",
    "    reference_input = []\n",
    "        \n",
    "    trade_history_input.append(np.stack(df[trade_history_col].to_numpy()))\n",
    "    trade_history_input.append(np.stack(df['target_attention_features'].to_numpy()))\n",
    "\n",
    "    noncat_and_binary = []\n",
    "    for f in NON_CAT_FEATURES + BINARY:\n",
    "        noncat_and_binary.append(np.expand_dims(df[f].to_numpy().astype('float32'), axis=1))\n",
    "    reference_input.append(np.concatenate(noncat_and_binary, axis=-1))\n",
    "    \n",
    "    for f in CATEGORICAL_FEATURES:\n",
    "        encoded = encoders[f].transform(df[f])\n",
    "        reference_input.append(encoded.astype('float32'))\n",
    "    \n",
    "    return trade_history_input, reference_input\n",
    "\n",
    "def generate_trade_history_model(TRADE_SEQUENCE_LENGTH, trade_history_normalizer):\n",
    "\n",
    "    inputs = []\n",
    "    \n",
    "    trade_history_input = layers.Input(name=\"trade_history_input\", \n",
    "                                       shape=(TRADE_SEQUENCE_LENGTH, NUM_FEATURES), \n",
    "                                       dtype = tf.float32) \n",
    "\n",
    "    target_attention_input = layers.Input(name=\"target_attention_input\", \n",
    "                                       shape=(1, 3), \n",
    "                                       dtype = tf.float32) \n",
    "    inputs.append(trade_history_input)\n",
    "    inputs.append(target_attention_input)\n",
    "\n",
    "\n",
    "    ############## TRADE HISTORY MODEL #################\n",
    "\n",
    "    lstm_layer = layers.Bidirectional(layers.LSTM(50, \n",
    "                             activation='tanh',\n",
    "                             input_shape=(TRADE_SEQUENCE_LENGTH,NUM_FEATURES),\n",
    "                             return_sequences = True,\n",
    "                             name='LSTM'))\n",
    "\n",
    "    lstm_layer_2 = layers.Bidirectional(layers.LSTM(100, \n",
    "                                                    activation='tanh',\n",
    "                                                    input_shape=(TRADE_SEQUENCE_LENGTH, 50),\n",
    "                                                    return_sequences = True,\n",
    "                                                    name='LSTM_2'))\n",
    "\n",
    "\n",
    "\n",
    "    features = lstm_layer(trade_history_normalizer(inputs[0]))\n",
    "    features = lstm_layer_2(features)  \n",
    "    \n",
    "    \n",
    "    attention_sequence = layers.Dense(200, activation='relu', name='attention_dense')(target_attention_input)\n",
    "    attention = layers.Dot(axes=[2, 2])([features, attention_sequence])\n",
    "    attention = layers.Activation('softmax')(attention)\n",
    "\n",
    "    context_vector = layers.Dot(axes=[1, 1])([features, attention])\n",
    "    context_vector = layers.Flatten(name='context_vector_flatten')(context_vector)\n",
    "\n",
    "\n",
    "    trade_history_output = layers.Dense(100, \n",
    "                                        activation='relu')(context_vector)\n",
    "    \n",
    "    trade_history_output = layers.Dense(1)(trade_history_output)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=trade_history_output)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "\n",
    "def generate_reference_model(noncat_binary_normalizer):\n",
    "    inputs = []\n",
    "    layer = []\n",
    "\n",
    "    inputs.append(layers.Input(\n",
    "        name=\"NON_CAT_AND_BINARY_FEATURES\",\n",
    "        shape=(len(NON_CAT_FEATURES + BINARY),)\n",
    "    ))\n",
    "\n",
    "\n",
    "    layer.append(noncat_binary_normalizer(inputs[0]))\n",
    "    \n",
    "    global encoders\n",
    "    for f in CATEGORICAL_FEATURES:\n",
    "        fin = layers.Input(shape=(1,), name = f)\n",
    "        inputs.append(fin)\n",
    "        embedded = layers.Flatten(name = f + \"_flat\")( layers.Embedding(input_dim = fmax[f]+1,\n",
    "                                                                        output_dim = max(30,int(np.sqrt(fmax[f]))),\n",
    "                                                                        input_length= 1,\n",
    "                                                                        name = f + \"_embed\")(fin))\n",
    "        layer.append(embedded)\n",
    "\n",
    "\n",
    "    reference_hidden = layers.Dense(300,\n",
    "                                    activation='relu',\n",
    "                                    name='reference_hidden_1')(layers.concatenate(layer, axis=-1))\n",
    "    reference_hidden = layers.BatchNormalization()(reference_hidden)\n",
    "    reference_hidden = layers.Dropout(DROPOUT)(reference_hidden)\n",
    "\n",
    "    reference_hidden2 = layers.Dense(200,activation='relu',name='reference_hidden_2')(reference_hidden)\n",
    "    reference_hidden2 = layers.BatchNormalization()(reference_hidden2)\n",
    "    reference_hidden2 = layers.Dropout(DROPOUT)(reference_hidden2)\n",
    "\n",
    "    reference_hidden3 = layers.Dense(100,activation='tanh',name='reference_hidden_3')(reference_hidden2)\n",
    "    reference_hidden3 = layers.BatchNormalization()(reference_hidden3)\n",
    "    reference_hidden3 = layers.Dropout(DROPOUT)(reference_hidden3)\n",
    "    \n",
    "    reference_output = layers.Dense(1, name='reference_hidden_3')(reference_hidden2)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=reference_output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def create_data_set_and_model(train_dataframe, test_dataframe, trade_history_col):\n",
    "    \n",
    "    if not isinstance(trade_history_col, str):\n",
    "        raise ValueError('trade_history_col must be a string')\n",
    "    \n",
    "    TRADE_SEQUENCE_LENGTH = train_dataframe[trade_history_col][0].shape[0] \n",
    "    \n",
    "    params = {'TRADE_SEQUENCE_LENGTH':TRADE_SEQUENCE_LENGTH}\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    val_idx = np.random.choice(range(len(train_dataframe)), \n",
    "                     size = int(VALIDATION_SPLIT*len(train_dataframe)),\n",
    "                     replace=False)\n",
    "\n",
    "    print(f'TRAINING DATA: N = {len(train_dataframe)-len(val_idx)}, MIN DATE = {train_dataframe.drop(val_idx, axis=0).trade_date.min()}, MAX DATE = {train_dataframe.drop(val_idx, axis=0).trade_date.max()}')\n",
    "    print(f'VALIDATION DATA: N = {len(val_idx)}, MIN DATE = {train_dataframe.iloc[val_idx].trade_date.min()}, MAX DATE = {train_dataframe.iloc[val_idx].trade_date.max()}')\n",
    "    print(f'TEST DATA: N = {len(test_dataframe)}, MIN DATE = {test_dataframe.trade_date.min()}, MAX DATE = {test_dataframe.trade_date.max()}')\n",
    "\n",
    "    \n",
    "    trade_history_x_train, reference_x_train = create_input_new(train_dataframe.drop(val_idx, axis=0), trade_history_col)\n",
    "    y_train = train_dataframe.drop(val_idx, axis=0)[target_variable]\n",
    "\n",
    "    trade_history_x_val, reference_x_val = create_input_new(train_dataframe.iloc[val_idx], trade_history_col)\n",
    "    y_val = train_dataframe.iloc[val_idx][target_variable]\n",
    "\n",
    "    trade_history_x_test, reference_x_test = create_input_new(test_dataframe, trade_history_col)\n",
    "    y_test = test_dataframe[target_variable]    \n",
    "    \n",
    "    # Normalization layer for the trade history\n",
    "    trade_history_normalizer = Normalization(name='Trade_history_normalizer')\n",
    "    trade_history_normalizer.adapt(trade_history_x_train[0], batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Normalization layer for the non-categorical and binary features\n",
    "    noncat_binary_normalizer = Normalization(name='Numerical_binary_normalizer')\n",
    "    noncat_binary_normalizer.adapt(reference_x_train[0], batch_size = BATCH_SIZE)\n",
    "\n",
    "    normalizers = {'trade_history_normalizer': trade_history_normalizer,\n",
    "                  'noncat_binary_normalizer': noncat_binary_normalizer}\n",
    "\n",
    "    return  params, normalizers, [trade_history_x_train, reference_x_train], y_train, [trade_history_x_val, reference_x_val], y_val, [trade_history_x_test, reference_x_test], y_test, val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16e33f76-449a-4655-882f-8b3fe873099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_data(x_train, y_train, shuffle=False, shuffle_buffer=1):\n",
    "                     \n",
    "    X=()\n",
    "    for x in x_train:\n",
    "        X += (tf.data.Dataset.from_tensor_slices(x),)\n",
    "        \n",
    "\n",
    "    temp = tf.data.Dataset.zip((X))\n",
    "    del X\n",
    "    dataset = tf.data.Dataset.zip((temp,\n",
    "                        tf.data.Dataset.from_tensor_slices(y_train)))\n",
    "    del temp\n",
    "    if shuffle:\n",
    "        shuffle_buffer = int(len(x_train[0])*shuffle_buffer)\n",
    "        dataset = dataset.shuffle(shuffle_buffer)\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c7eea81-0c51-4ac7-bb58-4aef23343c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_new(params, normalizers, x_train, y_train, x_val, y_val, shuffle, shuffle_buffer=1, \n",
    "                    epochs = {'trade_history_model':NUM_EPOCHS, 'reference_model':NUM_EPOCHS}):\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    TRADE_SEQUENCE_LENGTH = params.get('TRADE_SEQUENCE_LENGTH')\n",
    "      \n",
    "    trade_history_normalizer = normalizers.get('trade_history_normalizer')\n",
    "    noncat_binary_normalizer = normalizers.get('noncat_binary_normalizer')\n",
    "       \n",
    "    tf.keras.utils.set_random_seed(10)\n",
    "    trade_history_model = generate_trade_history_model(TRADE_SEQUENCE_LENGTH, trade_history_normalizer)\n",
    "    reference_model = generate_reference_model(noncat_binary_normalizer)\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H-%M')\n",
    "    \n",
    "    fit_callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=10,\n",
    "        verbose=0,\n",
    "        mode=\"auto\",\n",
    "        restore_best_weights=True),\n",
    "        # time_callback,\n",
    "        # CSVLoggerTimeHistory(timestamp+'_training_logs_yield_history.csv', separator=\",\", append=False)\n",
    "    ]\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        trade_history_train_ds = create_tf_data(x_train[0], y_train, shuffle, shuffle_buffer).batch(BATCH_SIZE).prefetch(2).cache()\n",
    "        trade_history_val_ds = create_tf_data(x_val[0], y_val, shuffle = False).batch(BATCH_SIZE).prefetch(2).cache()\n",
    "        \n",
    "        reference_model_train_ds = create_tf_data(x_train[1], y_train, shuffle, shuffle_buffer).batch(BATCH_SIZE).prefetch(2).cache()\n",
    "        reference_model_val_ds = create_tf_data(x_val[1], y_val, shuffle = False).batch(BATCH_SIZE).prefetch(2).cache()\n",
    "    \n",
    "    trade_history_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "          loss=keras.losses.MeanAbsoluteError())\n",
    "\n",
    "    reference_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "          loss=keras.losses.MeanAbsoluteError())\n",
    "    \n",
    "    trade_history_history = trade_history_model.fit(trade_history_train_ds,\n",
    "                                      validation_data=trade_history_val_ds,\n",
    "                                        epochs=epochs.get('trade_history_model', NUM_EPOCHS),     \n",
    "                                        verbose=1, \n",
    "                                        callbacks=fit_callbacks,\n",
    "                                        use_multiprocessing=True,\n",
    "                                        workers=8)\n",
    "    \n",
    "    reference_history = reference_model.fit(reference_model_train_ds,\n",
    "                                      validation_data=reference_model_val_ds,\n",
    "                                        epochs=epochs.get('reference_model', NUM_EPOCHS),     \n",
    "                                        verbose=1, \n",
    "                                        callbacks=fit_callbacks,\n",
    "                                        use_multiprocessing=True,\n",
    "                                        workers=8)\n",
    "    \n",
    "    return [trade_history_history, reference_history], [trade_history_model, reference_model]\n",
    "\n",
    "\n",
    "def train_model_combined(epochs, trade_history_model, reference_model, x_train, y_train, x_val, y_val, shuffle, shuffle_buffer=1):\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H-%M')\n",
    "    tf.keras.utils.set_random_seed(10)\n",
    "    \n",
    "    fit_callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=10,\n",
    "        verbose=0,\n",
    "        mode=\"auto\",\n",
    "        restore_best_weights=True),\n",
    "        # time_callback,\n",
    "        # CSVLoggerTimeHistory(timestamp+'_training_logs_yield_history.csv', separator=\",\", append=False)\n",
    "    ]\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        trade_history_train_ds = create_tf_data(x_train[0], y_train, shuffle, shuffle_buffer)\n",
    "        trade_history_val_ds = create_tf_data(x_val[0], y_val, shuffle = False)\n",
    "        \n",
    "        reference_model_train_ds = create_tf_data(x_train[1], y_train, shuffle, shuffle_buffer)\n",
    "        reference_model_val_ds = create_tf_data(x_val[1], y_val, shuffle = False)\n",
    "        \n",
    "#         train_ds = tf.data.Dataset.zip((trade_history_train_ds, reference_model_train_ds))\n",
    "#         train_ds = tf.data.Dataset.zip((train_ds, tf.data.Dataset.from_tensor_slices(y_train)))\n",
    "        \n",
    "#         val_ds = tf.data.Dataset.zip((trade_history_val_ds, reference_model_val_ds)).batch(BATCH_SIZE).prefetch(2).cache()\n",
    "#         val_ds = tf.data.Dataset.zip((val_ds, tf.data.Dataset.from_tensor_slices(y_val))).batch(BATCH_SIZE).prefetch(2).cache()\n",
    "\n",
    "    \n",
    "    inputs = [trade_history_model.input, reference_model.input]\n",
    "    output = layers.Dense(1, name='combined_output')(layers.concatenate([trade_history_model.output, reference_model.output], axis=-1, name='hist_ref_concat'))\n",
    "    combined_model = keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    \n",
    "    combined_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "          loss=keras.losses.MeanAbsoluteError())\n",
    "    \n",
    "    combined_history = combined_model.fit([x_train[0], x_train[1]], y_train,\n",
    "                                      validation_data=[[x_val[0], x_val[1]], y_val],\n",
    "                                        epochs=epochs,     \n",
    "                                        verbose=1, \n",
    "                                        callbacks=fit_callbacks,\n",
    "                                        use_multiprocessing=True,\n",
    "                                        workers=8, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    return combined_history, combined_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e40d5dc8-31a6-4c39-a36c-8377f2afa2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_history_col = 'trade_history_shortened'\n",
    "yield_history_cols = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75f184b9-49ed-4632-9f72-d48535e81c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['new_ficc_ycl', 'new_real_time_ficc_ycl']:     \n",
    "    if col in PREDICTORS:\n",
    "        PREDICTORS.remove(col)\n",
    "    if col in NON_CAT_FEATURES:\n",
    "        NON_CAT_FEATURES.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "404fc0cb-bb04-4d79-875b-ba91eb490e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA: N = 634710, MIN DATE = 2023-05-01 00:00:00, MAX DATE = 2023-05-31 00:00:00\n",
      "VALIDATION DATA: N = 70523, MIN DATE = 2023-05-01 00:00:00, MAX DATE = 2023-05-31 00:00:00\n",
      "TEST DATA: N = 235624, MIN DATE = 2023-06-01 00:00:00, MAX DATE = 2023-06-09 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 18:39:06.318509: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-18 18:39:06.349141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-18 18:39:06.351436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-18 18:39:06.353393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-18 18:39:07.490615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-18 18:39:07.492782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-18 18:39:07.494740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-18 18:39:07.496557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10905 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# params, normalizers, x_train, y_train, x_val, y_val, x_test, y_test, val_idx = create_data_set_and_model(train_dataframe, test_dataframe, trade_history_col, yield_history_cols)\n",
    "params, normalizers, x_train, y_train, x_val, y_val, x_test, y_test, val_idx = create_data_set_and_model(train_dataframe, test_dataframe[test_dataframe.trade_date <= '2023-06-09'], trade_history_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9623aff-b815-4a2b-94f7-6f2e8189dc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(634710, 2, 6)\n",
      "(634710, 1, 3)\n",
      "(634710, 48)\n",
      "(634710,)\n",
      "(634710,)\n",
      "(634710,)\n",
      "(634710,)\n",
      "(634710,)\n",
      "(634710,)\n",
      "(634710,)\n",
      "(634710,)\n",
      "(634710,)\n",
      "(634710,)\n",
      "(634710,)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(x_train[0])):\n",
    "    print(x_train[0][i].shape)\n",
    "    \n",
    "for i in range(len(x_train[1])):\n",
    "    print(x_train[1][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a20fa89-0c40-4309-9d92-20284d07f28e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 21:46:15.394097: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630/635 [============================>.] - ETA: 0s - loss: 31.6103"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 21:46:23.839073: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 16s 10ms/step - loss: 31.5019 - val_loss: 17.2299\n",
      "Epoch 2/50\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 15.9286 - val_loss: 15.2582\n",
      "Epoch 3/50\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 15.0458 - val_loss: 14.8120\n",
      "Epoch 4/50\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 14.7097 - val_loss: 14.5384\n",
      "Epoch 5/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 14.4635 - val_loss: 14.2981\n",
      "Epoch 6/50\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 14.2388 - val_loss: 14.1182\n",
      "Epoch 7/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 14.1223 - val_loss: 14.0388\n",
      "Epoch 8/50\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 14.0554 - val_loss: 13.9810\n",
      "Epoch 9/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 14.0041 - val_loss: 13.9339\n",
      "Epoch 10/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.9606 - val_loss: 13.8940\n",
      "Epoch 11/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.9223 - val_loss: 13.8583\n",
      "Epoch 12/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.8868 - val_loss: 13.8263\n",
      "Epoch 13/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.8540 - val_loss: 13.7964\n",
      "Epoch 14/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.8234 - val_loss: 13.7690\n",
      "Epoch 15/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.7953 - val_loss: 13.7430\n",
      "Epoch 16/50\n",
      "635/635 [==============================] - 17s 26ms/step - loss: 13.7692 - val_loss: 13.7195\n",
      "Epoch 17/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.7451 - val_loss: 13.6995\n",
      "Epoch 18/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.7235 - val_loss: 13.6788\n",
      "Epoch 19/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.7038 - val_loss: 13.6613\n",
      "Epoch 20/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.6858 - val_loss: 13.6475\n",
      "Epoch 21/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.6695 - val_loss: 13.6319\n",
      "Epoch 22/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.6542 - val_loss: 13.6183\n",
      "Epoch 23/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.6403 - val_loss: 13.6035\n",
      "Epoch 24/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.6273 - val_loss: 13.5912\n",
      "Epoch 25/50\n",
      "635/635 [==============================] - 17s 27ms/step - loss: 13.6151 - val_loss: 13.5814\n",
      "Epoch 26/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.6037 - val_loss: 13.5711\n",
      "Epoch 27/50\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 13.5926 - val_loss: 13.5620\n",
      "Epoch 28/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5827 - val_loss: 13.5529\n",
      "Epoch 29/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5730 - val_loss: 13.5451\n",
      "Epoch 30/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5639 - val_loss: 13.5363\n",
      "Epoch 31/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5552 - val_loss: 13.5242\n",
      "Epoch 32/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5470 - val_loss: 13.5178\n",
      "Epoch 33/50\n",
      "635/635 [==============================] - 17s 27ms/step - loss: 13.5391 - val_loss: 13.5105\n",
      "Epoch 34/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5316 - val_loss: 13.5022\n",
      "Epoch 35/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5246 - val_loss: 13.4954\n",
      "Epoch 36/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5174 - val_loss: 13.4893\n",
      "Epoch 37/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5109 - val_loss: 13.4839\n",
      "Epoch 38/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5044 - val_loss: 13.4773\n",
      "Epoch 39/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4984 - val_loss: 13.4726\n",
      "Epoch 40/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4924 - val_loss: 13.4678\n",
      "Epoch 41/50\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 13.4867 - val_loss: 13.4640\n",
      "Epoch 42/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4810 - val_loss: 13.4580\n",
      "Epoch 43/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4754 - val_loss: 13.4549\n",
      "Epoch 44/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4700 - val_loss: 13.4493\n",
      "Epoch 45/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4648 - val_loss: 13.4439\n",
      "Epoch 46/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4598 - val_loss: 13.4408\n",
      "Epoch 47/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4549 - val_loss: 13.4355\n",
      "Epoch 48/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4498 - val_loss: 13.4304\n",
      "Epoch 49/50\n",
      "635/635 [==============================] - 17s 27ms/step - loss: 13.4451 - val_loss: 13.4253\n",
      "Epoch 50/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4403 - val_loss: 13.4205\n",
      "Epoch 1/75\n",
      "635/635 [==============================] - 21s 19ms/step - loss: 42.3596 - val_loss: 38.1416\n",
      "Epoch 2/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 37.0046 - val_loss: 33.6969\n",
      "Epoch 3/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 31.1155 - val_loss: 27.7517\n",
      "Epoch 4/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 23.4496 - val_loss: 19.7251\n",
      "Epoch 5/75\n",
      "635/635 [==============================] - 18s 28ms/step - loss: 16.0752 - val_loss: 13.2995\n",
      "Epoch 6/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 12.0638 - val_loss: 10.9651\n",
      "Epoch 7/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 11.0609 - val_loss: 10.5510\n",
      "Epoch 8/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 10.8377 - val_loss: 10.4359\n",
      "Epoch 9/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 10.7175 - val_loss: 10.3246\n",
      "Epoch 10/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 10.6255 - val_loss: 10.2692\n",
      "Epoch 11/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 10.5433 - val_loss: 10.2024\n",
      "Epoch 12/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 10.4727 - val_loss: 10.1479\n",
      "Epoch 13/75\n",
      "635/635 [==============================] - 18s 28ms/step - loss: 10.4102 - val_loss: 10.1088\n",
      "Epoch 14/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 10.3523 - val_loss: 10.0542\n",
      "Epoch 15/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 10.2966 - val_loss: 10.0213\n",
      "Epoch 16/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 10.2503 - val_loss: 9.9845\n",
      "Epoch 17/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 10.2064 - val_loss: 9.9589\n",
      "Epoch 18/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 10.1607 - val_loss: 9.9284\n",
      "Epoch 19/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 10.1225 - val_loss: 9.8889\n",
      "Epoch 20/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 10.0886 - val_loss: 9.8756\n",
      "Epoch 21/75\n",
      "635/635 [==============================] - 17s 27ms/step - loss: 10.0499 - val_loss: 9.8373\n",
      "Epoch 22/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 10.0187 - val_loss: 9.8190\n",
      "Epoch 23/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.9835 - val_loss: 9.8066\n",
      "Epoch 24/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.9513 - val_loss: 9.7773\n",
      "Epoch 25/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.9275 - val_loss: 9.7558\n",
      "Epoch 26/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.8978 - val_loss: 9.7455\n",
      "Epoch 27/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.8734 - val_loss: 9.7281\n",
      "Epoch 28/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.8437 - val_loss: 9.7166\n",
      "Epoch 29/75\n",
      "635/635 [==============================] - 17s 27ms/step - loss: 9.8208 - val_loss: 9.6972\n",
      "Epoch 30/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.7937 - val_loss: 9.6943\n",
      "Epoch 31/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.7733 - val_loss: 9.6692\n",
      "Epoch 32/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.7456 - val_loss: 9.6619\n",
      "Epoch 33/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.7224 - val_loss: 9.6467\n",
      "Epoch 34/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.7048 - val_loss: 9.6357\n",
      "Epoch 35/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.6845 - val_loss: 9.6250\n",
      "Epoch 36/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.6631 - val_loss: 9.6225\n",
      "Epoch 37/75\n",
      "635/635 [==============================] - 17s 27ms/step - loss: 9.6420 - val_loss: 9.6070\n",
      "Epoch 38/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.6245 - val_loss: 9.5917\n",
      "Epoch 39/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.6005 - val_loss: 9.5848\n",
      "Epoch 40/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.5828 - val_loss: 9.5621\n",
      "Epoch 41/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.5671 - val_loss: 9.5665\n",
      "Epoch 42/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.5510 - val_loss: 9.5617\n",
      "Epoch 43/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.5319 - val_loss: 9.5529\n",
      "Epoch 44/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.5176 - val_loss: 9.5364\n",
      "Epoch 45/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 9.4982 - val_loss: 9.5271\n",
      "Epoch 46/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.4785 - val_loss: 9.5181\n",
      "Epoch 47/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.4709 - val_loss: 9.5297\n",
      "Epoch 48/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.4542 - val_loss: 9.4991\n",
      "Epoch 49/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.4405 - val_loss: 9.5186\n",
      "Epoch 50/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.4255 - val_loss: 9.4913\n",
      "Epoch 51/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.4068 - val_loss: 9.4829\n",
      "Epoch 52/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.3961 - val_loss: 9.4961\n",
      "Epoch 53/75\n",
      "635/635 [==============================] - 17s 27ms/step - loss: 9.3835 - val_loss: 9.4732\n",
      "Epoch 54/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.3676 - val_loss: 9.4748\n",
      "Epoch 55/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.3491 - val_loss: 9.4668\n",
      "Epoch 56/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.3358 - val_loss: 9.4591\n",
      "Epoch 57/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.3274 - val_loss: 9.4417\n",
      "Epoch 58/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.3127 - val_loss: 9.4453\n",
      "Epoch 59/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.3043 - val_loss: 9.4476\n",
      "Epoch 60/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.2907 - val_loss: 9.4342\n",
      "Epoch 61/75\n",
      "635/635 [==============================] - 17s 27ms/step - loss: 9.2796 - val_loss: 9.4289\n",
      "Epoch 62/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.2669 - val_loss: 9.4425\n",
      "Epoch 63/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.2558 - val_loss: 9.4383\n",
      "Epoch 64/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.2401 - val_loss: 9.4243\n",
      "Epoch 65/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.2297 - val_loss: 9.4198\n",
      "Epoch 66/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.2217 - val_loss: 9.4176\n",
      "Epoch 67/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.2102 - val_loss: 9.4069\n",
      "Epoch 68/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.1983 - val_loss: 9.3969\n",
      "Epoch 69/75\n",
      "635/635 [==============================] - 17s 27ms/step - loss: 9.1819 - val_loss: 9.3982\n",
      "Epoch 70/75\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 9.1762 - val_loss: 9.4022\n",
      "Epoch 71/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.1662 - val_loss: 9.3928\n",
      "Epoch 72/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.1540 - val_loss: 9.3898\n",
      "Epoch 73/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.1428 - val_loss: 9.3923\n",
      "Epoch 74/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.1325 - val_loss: 9.3796\n",
      "Epoch 75/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.1215 - val_loss: 9.3769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:01:53.667017: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining reference data and trade_history models and training:\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:02:52.597222: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - ETA: 0s - loss: 25.7190"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:03:10.456646: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 29s 36ms/step - loss: 25.7190 - val_loss: 13.2475\n",
      "Epoch 2/25\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 11.7743 - val_loss: 11.1446\n",
      "Epoch 3/25\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 10.7706 - val_loss: 10.5149\n",
      "Epoch 4/25\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 10.3106 - val_loss: 10.1997\n",
      "Epoch 5/25\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 10.0253 - val_loss: 9.9796\n",
      "Epoch 6/25\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 9.8131 - val_loss: 9.8591\n",
      "Epoch 7/25\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 9.6574 - val_loss: 9.7273\n",
      "Epoch 8/25\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 9.5491 - val_loss: 9.6452\n",
      "Epoch 9/25\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 9.4552 - val_loss: 9.5813\n",
      "Epoch 10/25\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 9.3857 - val_loss: 9.5257\n",
      "Epoch 11/25\n",
      "635/635 [==============================] - 19s 30ms/step - loss: 9.3200 - val_loss: 9.5156\n",
      "Epoch 12/25\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.2657 - val_loss: 9.4704\n",
      "Epoch 13/25\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.2127 - val_loss: 9.4395\n",
      "Epoch 14/25\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.1717 - val_loss: 9.4166\n",
      "Epoch 15/25\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.1417 - val_loss: 9.3909\n",
      "Epoch 16/25\n",
      "635/635 [==============================] - 21s 32ms/step - loss: 9.1018 - val_loss: 9.3934\n",
      "Epoch 17/25\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 9.0723 - val_loss: 9.3688\n",
      "Epoch 18/25\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 9.0326 - val_loss: 9.3821\n",
      "Epoch 19/25\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 9.0149 - val_loss: 9.3269\n",
      "Epoch 20/25\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 8.9867 - val_loss: 9.3173\n",
      "Epoch 21/25\n",
      "635/635 [==============================] - 17s 27ms/step - loss: 8.9617 - val_loss: 9.3137\n",
      "Epoch 22/25\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.9402 - val_loss: 9.2772\n",
      "Epoch 23/25\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 8.9186 - val_loss: 9.3206\n",
      "Epoch 24/25\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.8892 - val_loss: 9.2732\n",
      "Epoch 25/25\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.8726 - val_loss: 9.2783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:08:00.318531: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= TRIAL 0, MAE: Trade History: 12.745, Reference: 9.549 =========================\n",
      "========================= TRIAL 0, MAE: Combined Model: 9.520=========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:08:46.933135: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: combined_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: combined_model/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fbe7a790e50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fbd22e536d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fbe75ca80d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fbe7e314110> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:09:16.732989: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - ETA: 0s - loss: 31.5019"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:09:26.047682: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 17s 11ms/step - loss: 31.5019 - val_loss: 17.2299\n",
      "Epoch 2/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 15.9286 - val_loss: 15.2582\n",
      "Epoch 3/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 15.0458 - val_loss: 14.8123\n",
      "Epoch 4/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 14.7097 - val_loss: 14.5387\n",
      "Epoch 5/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 14.4636 - val_loss: 14.2987\n",
      "Epoch 6/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 14.2389 - val_loss: 14.1184\n",
      "Epoch 7/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 14.1222 - val_loss: 14.0387\n",
      "Epoch 8/50\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 14.0553 - val_loss: 13.9812\n",
      "Epoch 9/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 14.0040 - val_loss: 13.9337\n",
      "Epoch 10/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.9606 - val_loss: 13.8935\n",
      "Epoch 11/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.9222 - val_loss: 13.8584\n",
      "Epoch 12/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.8867 - val_loss: 13.8261\n",
      "Epoch 13/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.8541 - val_loss: 13.7964\n",
      "Epoch 14/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.8234 - val_loss: 13.7683\n",
      "Epoch 15/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.7950 - val_loss: 13.7437\n",
      "Epoch 16/50\n",
      "635/635 [==============================] - 17s 26ms/step - loss: 13.7692 - val_loss: 13.7195\n",
      "Epoch 17/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.7451 - val_loss: 13.6983\n",
      "Epoch 18/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.7232 - val_loss: 13.6793\n",
      "Epoch 19/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.7037 - val_loss: 13.6618\n",
      "Epoch 20/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.6856 - val_loss: 13.6484\n",
      "Epoch 21/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.6693 - val_loss: 13.6328\n",
      "Epoch 22/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.6542 - val_loss: 13.6176\n",
      "Epoch 23/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.6401 - val_loss: 13.6029\n",
      "Epoch 24/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.6273 - val_loss: 13.5920\n",
      "Epoch 25/50\n",
      "635/635 [==============================] - 17s 26ms/step - loss: 13.6151 - val_loss: 13.5816\n",
      "Epoch 26/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.6037 - val_loss: 13.5702\n",
      "Epoch 27/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5926 - val_loss: 13.5626\n",
      "Epoch 28/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5824 - val_loss: 13.5528\n",
      "Epoch 29/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5730 - val_loss: 13.5462\n",
      "Epoch 30/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5639 - val_loss: 13.5355\n",
      "Epoch 31/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5552 - val_loss: 13.5248\n",
      "Epoch 32/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5469 - val_loss: 13.5178\n",
      "Epoch 33/50\n",
      "635/635 [==============================] - 17s 26ms/step - loss: 13.5390 - val_loss: 13.5114\n",
      "Epoch 34/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5316 - val_loss: 13.5035\n",
      "Epoch 35/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5244 - val_loss: 13.4950\n",
      "Epoch 36/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5174 - val_loss: 13.4885\n",
      "Epoch 37/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5109 - val_loss: 13.4856\n",
      "Epoch 38/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5046 - val_loss: 13.4775\n",
      "Epoch 39/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4985 - val_loss: 13.4724\n",
      "Epoch 40/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4924 - val_loss: 13.4678\n",
      "Epoch 41/50\n",
      "635/635 [==============================] - 5s 9ms/step - loss: 13.4866 - val_loss: 13.4636\n",
      "Epoch 42/50\n",
      "635/635 [==============================] - 16s 26ms/step - loss: 13.4810 - val_loss: 13.4579\n",
      "Epoch 43/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4755 - val_loss: 13.4554\n",
      "Epoch 44/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4702 - val_loss: 13.4496\n",
      "Epoch 45/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4647 - val_loss: 13.4450\n",
      "Epoch 46/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4597 - val_loss: 13.4408\n",
      "Epoch 47/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4547 - val_loss: 13.4358\n",
      "Epoch 48/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4498 - val_loss: 13.4296\n",
      "Epoch 49/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4449 - val_loss: 13.4255\n",
      "Epoch 50/50\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 13.4403 - val_loss: 13.4207\n",
      "Epoch 1/75\n",
      "635/635 [==============================] - 23s 19ms/step - loss: 42.3596 - val_loss: 38.1489\n",
      "Epoch 2/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 37.0049 - val_loss: 33.7066\n",
      "Epoch 3/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 31.1159 - val_loss: 27.7829\n",
      "Epoch 4/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 23.4505 - val_loss: 19.6224\n",
      "Epoch 5/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 16.0781 - val_loss: 13.2740\n",
      "Epoch 6/75\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 12.0661 - val_loss: 10.9741\n",
      "Epoch 7/75\n",
      "635/635 [==============================] - 16s 25ms/step - loss: 11.0613 - val_loss: 10.5483\n",
      "Epoch 8/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 10.8380 - val_loss: 10.4373\n",
      "Epoch 9/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 10.7177 - val_loss: 10.3325\n",
      "Epoch 10/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 10.6246 - val_loss: 10.2706\n",
      "Epoch 11/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 10.5425 - val_loss: 10.1975\n",
      "Epoch 12/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 10.4713 - val_loss: 10.1585\n",
      "Epoch 13/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 10.4080 - val_loss: 10.1096\n",
      "Epoch 14/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 10.3497 - val_loss: 10.0623\n",
      "Epoch 15/75\n",
      "635/635 [==============================] - 17s 26ms/step - loss: 10.2935 - val_loss: 10.0174\n",
      "Epoch 16/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 10.2490 - val_loss: 9.9850\n",
      "Epoch 17/75\n",
      "635/635 [==============================] - 5s 9ms/step - loss: 10.2035 - val_loss: 9.9428\n",
      "Epoch 18/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 10.1582 - val_loss: 9.9157\n",
      "Epoch 19/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 10.1200 - val_loss: 9.8936\n",
      "Epoch 20/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 10.0856 - val_loss: 9.8694\n",
      "Epoch 21/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 10.0449 - val_loss: 9.8380\n",
      "Epoch 22/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 10.0167 - val_loss: 9.8260\n",
      "Epoch 23/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.9799 - val_loss: 9.8133\n",
      "Epoch 24/75\n",
      "635/635 [==============================] - 17s 27ms/step - loss: 9.9480 - val_loss: 9.7772\n",
      "Epoch 25/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.9246 - val_loss: 9.7611\n",
      "Epoch 26/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.8933 - val_loss: 9.7662\n",
      "Epoch 27/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.8673 - val_loss: 9.7335\n",
      "Epoch 28/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.8388 - val_loss: 9.7185\n",
      "Epoch 29/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.8156 - val_loss: 9.7053\n",
      "Epoch 30/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.7909 - val_loss: 9.6898\n",
      "Epoch 31/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.7693 - val_loss: 9.6845\n",
      "Epoch 32/75\n",
      "635/635 [==============================] - 17s 27ms/step - loss: 9.7445 - val_loss: 9.6649\n",
      "Epoch 33/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.7208 - val_loss: 9.6551\n",
      "Epoch 34/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.6994 - val_loss: 9.6437\n",
      "Epoch 35/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.6810 - val_loss: 9.6301\n",
      "Epoch 36/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.6601 - val_loss: 9.6206\n",
      "Epoch 37/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.6368 - val_loss: 9.6009\n",
      "Epoch 38/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.6204 - val_loss: 9.6008\n",
      "Epoch 39/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.5954 - val_loss: 9.5952\n",
      "Epoch 40/75\n",
      "635/635 [==============================] - 18s 28ms/step - loss: 9.5779 - val_loss: 9.5731\n",
      "Epoch 41/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.5635 - val_loss: 9.5735\n",
      "Epoch 42/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.5487 - val_loss: 9.5606\n",
      "Epoch 43/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.5257 - val_loss: 9.5516\n",
      "Epoch 44/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.5109 - val_loss: 9.5434\n",
      "Epoch 45/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.4905 - val_loss: 9.5376\n",
      "Epoch 46/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.4758 - val_loss: 9.5317\n",
      "Epoch 47/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.4626 - val_loss: 9.5391\n",
      "Epoch 48/75\n",
      "635/635 [==============================] - 18s 28ms/step - loss: 9.4482 - val_loss: 9.5105\n",
      "Epoch 49/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.4312 - val_loss: 9.5309\n",
      "Epoch 50/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.4166 - val_loss: 9.5040\n",
      "Epoch 51/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.3987 - val_loss: 9.4841\n",
      "Epoch 52/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.3872 - val_loss: 9.4886\n",
      "Epoch 53/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.3744 - val_loss: 9.4751\n",
      "Epoch 54/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.3536 - val_loss: 9.4743\n",
      "Epoch 55/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.3396 - val_loss: 9.4702\n",
      "Epoch 56/75\n",
      "635/635 [==============================] - 16s 25ms/step - loss: 9.3256 - val_loss: 9.4580\n",
      "Epoch 57/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.3173 - val_loss: 9.4477\n",
      "Epoch 58/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.3017 - val_loss: 9.4486\n",
      "Epoch 59/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.2929 - val_loss: 9.4394\n",
      "Epoch 60/75\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 9.2798 - val_loss: 9.4322\n",
      "Epoch 61/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.2674 - val_loss: 9.4452\n",
      "Epoch 62/75\n",
      "635/635 [==============================] - 20s 31ms/step - loss: 9.2556 - val_loss: 9.4226\n",
      "Epoch 63/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.2433 - val_loss: 9.4272\n",
      "Epoch 64/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.2324 - val_loss: 9.4201\n",
      "Epoch 65/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.2221 - val_loss: 9.4208\n",
      "Epoch 66/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.2111 - val_loss: 9.4158\n",
      "Epoch 67/75\n",
      "635/635 [==============================] - 18s 29ms/step - loss: 9.2012 - val_loss: 9.4124\n",
      "Epoch 68/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 9.1913 - val_loss: 9.4060\n",
      "Epoch 69/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.1706 - val_loss: 9.3930\n",
      "Epoch 70/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.1680 - val_loss: 9.3968\n",
      "Epoch 71/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.1566 - val_loss: 9.3974\n",
      "Epoch 72/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.1436 - val_loss: 9.4043\n",
      "Epoch 73/75\n",
      "635/635 [==============================] - 19s 31ms/step - loss: 9.1333 - val_loss: 9.3935\n",
      "Epoch 74/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.1242 - val_loss: 9.3875\n",
      "Epoch 75/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.1110 - val_loss: 9.3787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:25:39.587980: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining reference data and trade_history models and training:\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:26:55.038220: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - ETA: 0s - loss: 25.7268"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:27:21.419193: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 33s 41ms/step - loss: 25.7268 - val_loss: 13.2116\n",
      "Epoch 2/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 11.7794 - val_loss: 11.1645\n",
      "Epoch 3/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 10.7702 - val_loss: 10.5209\n",
      "Epoch 4/25\n",
      "635/635 [==============================] - 14s 23ms/step - loss: 10.3098 - val_loss: 10.1821\n",
      "Epoch 5/25\n",
      "635/635 [==============================] - 22s 35ms/step - loss: 10.0219 - val_loss: 9.9850\n",
      "Epoch 6/25\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 9.8085 - val_loss: 9.8525\n",
      "Epoch 7/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.6539 - val_loss: 9.7200\n",
      "Epoch 8/25\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 9.5434 - val_loss: 9.6316\n",
      "Epoch 9/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.4482 - val_loss: 9.5747\n",
      "Epoch 10/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.3820 - val_loss: 9.5244\n",
      "Epoch 11/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.3179 - val_loss: 9.4971\n",
      "Epoch 12/25\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 9.2632 - val_loss: 9.4462\n",
      "Epoch 13/25\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 9.2094 - val_loss: 9.4191\n",
      "Epoch 14/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.1680 - val_loss: 9.4003\n",
      "Epoch 15/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.1378 - val_loss: 9.3897\n",
      "Epoch 16/25\n",
      "635/635 [==============================] - 24s 37ms/step - loss: 9.0983 - val_loss: 9.3717\n",
      "Epoch 17/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.0690 - val_loss: 9.3288\n",
      "Epoch 18/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.0274 - val_loss: 9.3400\n",
      "Epoch 19/25\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 9.0092 - val_loss: 9.3162\n",
      "Epoch 20/25\n",
      "635/635 [==============================] - 16s 25ms/step - loss: 8.9777 - val_loss: 9.2912\n",
      "Epoch 21/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.9501 - val_loss: 9.2990\n",
      "Epoch 22/25\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 8.9278 - val_loss: 9.2719\n",
      "Epoch 23/25\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 8.9071 - val_loss: 9.2860\n",
      "Epoch 24/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.8801 - val_loss: 9.2434\n",
      "Epoch 25/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.8638 - val_loss: 9.2446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:33:44.831526: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= TRIAL 1, MAE: Trade History: 12.745, Reference: 9.548 =========================\n",
      "========================= TRIAL 1, MAE: Combined Model: 9.479=========================\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:34:43.494958: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632/635 [============================>.] - ETA: 0s - loss: 31.5644"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:34:55.204201: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 17s 15ms/step - loss: 31.5019 - val_loss: 17.2299\n",
      "Epoch 2/50\n",
      "635/635 [==============================] - 18s 28ms/step - loss: 15.9286 - val_loss: 15.2582\n",
      "Epoch 3/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 15.0458 - val_loss: 14.8114\n",
      "Epoch 4/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 14.7095 - val_loss: 14.5386\n",
      "Epoch 5/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 14.4634 - val_loss: 14.2978\n",
      "Epoch 6/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 14.2388 - val_loss: 14.1180\n",
      "Epoch 7/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 14.1223 - val_loss: 14.0391\n",
      "Epoch 8/50\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 14.0554 - val_loss: 13.9810\n",
      "Epoch 9/50\n",
      "635/635 [==============================] - 15s 23ms/step - loss: 14.0042 - val_loss: 13.9335\n",
      "Epoch 10/50\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 13.9607 - val_loss: 13.8939\n",
      "Epoch 11/50\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 13.9223 - val_loss: 13.8590\n",
      "Epoch 12/50\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 13.8868 - val_loss: 13.8264\n",
      "Epoch 13/50\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 13.8542 - val_loss: 13.7964\n",
      "Epoch 14/50\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 13.8235 - val_loss: 13.7693\n",
      "Epoch 15/50\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 13.7952 - val_loss: 13.7439\n",
      "Epoch 16/50\n",
      "635/635 [==============================] - 18s 28ms/step - loss: 13.7693 - val_loss: 13.7202\n",
      "Epoch 17/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.7453 - val_loss: 13.6986\n",
      "Epoch 18/50\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 13.7234 - val_loss: 13.6789\n",
      "Epoch 19/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.7041 - val_loss: 13.6628\n",
      "Epoch 20/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.6858 - val_loss: 13.6480\n",
      "Epoch 21/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.6694 - val_loss: 13.6338\n",
      "Epoch 22/50\n",
      "635/635 [==============================] - 17s 26ms/step - loss: 13.6544 - val_loss: 13.6177\n",
      "Epoch 23/50\n",
      "635/635 [==============================] - 9s 13ms/step - loss: 13.6403 - val_loss: 13.6029\n",
      "Epoch 24/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.6273 - val_loss: 13.5929\n",
      "Epoch 25/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.6152 - val_loss: 13.5821\n",
      "Epoch 26/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.6037 - val_loss: 13.5706\n",
      "Epoch 27/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.5927 - val_loss: 13.5627\n",
      "Epoch 28/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.5826 - val_loss: 13.5523\n",
      "Epoch 29/50\n",
      "635/635 [==============================] - 18s 29ms/step - loss: 13.5731 - val_loss: 13.5451\n",
      "Epoch 30/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.5639 - val_loss: 13.5355\n",
      "Epoch 31/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.5553 - val_loss: 13.5244\n",
      "Epoch 32/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.5469 - val_loss: 13.5178\n",
      "Epoch 33/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.5393 - val_loss: 13.5105\n",
      "Epoch 34/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.5317 - val_loss: 13.5028\n",
      "Epoch 35/50\n",
      "635/635 [==============================] - 18s 29ms/step - loss: 13.5245 - val_loss: 13.4957\n",
      "Epoch 36/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.5175 - val_loss: 13.4890\n",
      "Epoch 37/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.5110 - val_loss: 13.4833\n",
      "Epoch 38/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.5045 - val_loss: 13.4779\n",
      "Epoch 39/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.4984 - val_loss: 13.4724\n",
      "Epoch 40/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.4924 - val_loss: 13.4680\n",
      "Epoch 41/50\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 13.4867 - val_loss: 13.4645\n",
      "Epoch 42/50\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 13.4811 - val_loss: 13.4592\n",
      "Epoch 43/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.4755 - val_loss: 13.4571\n",
      "Epoch 44/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.4700 - val_loss: 13.4504\n",
      "Epoch 45/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.4647 - val_loss: 13.4455\n",
      "Epoch 46/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.4597 - val_loss: 13.4402\n",
      "Epoch 47/50\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 13.4551 - val_loss: 13.4370\n",
      "Epoch 48/50\n",
      "635/635 [==============================] - 17s 28ms/step - loss: 13.4501 - val_loss: 13.4292\n",
      "Epoch 49/50\n",
      "635/635 [==============================] - 7s 10ms/step - loss: 13.4449 - val_loss: 13.4254\n",
      "Epoch 50/50\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 13.4402 - val_loss: 13.4212\n",
      "Epoch 1/75\n",
      "635/635 [==============================] - 21s 19ms/step - loss: 42.3596 - val_loss: 38.1436\n",
      "Epoch 2/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 37.0047 - val_loss: 33.7051\n",
      "Epoch 3/75\n",
      "635/635 [==============================] - 18s 28ms/step - loss: 31.1158 - val_loss: 27.8472\n",
      "Epoch 4/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 23.4507 - val_loss: 19.5857\n",
      "Epoch 5/75\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 16.0765 - val_loss: 13.3465\n",
      "Epoch 6/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 12.0651 - val_loss: 10.9620\n",
      "Epoch 7/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 11.0610 - val_loss: 10.5540\n",
      "Epoch 8/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 10.8383 - val_loss: 10.4235\n",
      "Epoch 9/75\n",
      "635/635 [==============================] - 20s 31ms/step - loss: 10.7183 - val_loss: 10.3287\n",
      "Epoch 10/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 10.6273 - val_loss: 10.2662\n",
      "Epoch 11/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 10.5473 - val_loss: 10.2001\n",
      "Epoch 12/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 10.4761 - val_loss: 10.1571\n",
      "Epoch 13/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 10.4142 - val_loss: 10.1221\n",
      "Epoch 14/75\n",
      "635/635 [==============================] - 19s 30ms/step - loss: 10.3558 - val_loss: 10.0720\n",
      "Epoch 15/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 10.2997 - val_loss: 10.0250\n",
      "Epoch 16/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 10.2539 - val_loss: 9.9883\n",
      "Epoch 17/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 10.2107 - val_loss: 9.9545\n",
      "Epoch 18/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 10.1634 - val_loss: 9.9265\n",
      "Epoch 19/75\n",
      "635/635 [==============================] - 20s 31ms/step - loss: 10.1242 - val_loss: 9.8927\n",
      "Epoch 20/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 10.0895 - val_loss: 9.8741\n",
      "Epoch 21/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 10.0512 - val_loss: 9.8476\n",
      "Epoch 22/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 10.0182 - val_loss: 9.8197\n",
      "Epoch 23/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.9834 - val_loss: 9.8063\n",
      "Epoch 24/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.9511 - val_loss: 9.7842\n",
      "Epoch 25/75\n",
      "635/635 [==============================] - 20s 31ms/step - loss: 9.9272 - val_loss: 9.7612\n",
      "Epoch 26/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.8954 - val_loss: 9.7561\n",
      "Epoch 27/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.8692 - val_loss: 9.7297\n",
      "Epoch 28/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.8398 - val_loss: 9.7234\n",
      "Epoch 29/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.8183 - val_loss: 9.6959\n",
      "Epoch 30/75\n",
      "635/635 [==============================] - 20s 31ms/step - loss: 9.7900 - val_loss: 9.6858\n",
      "Epoch 31/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.7711 - val_loss: 9.6786\n",
      "Epoch 32/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.7435 - val_loss: 9.6752\n",
      "Epoch 33/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.7215 - val_loss: 9.6454\n",
      "Epoch 34/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.6982 - val_loss: 9.6414\n",
      "Epoch 35/75\n",
      "635/635 [==============================] - 20s 31ms/step - loss: 9.6807 - val_loss: 9.6331\n",
      "Epoch 36/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.6615 - val_loss: 9.6271\n",
      "Epoch 37/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.6375 - val_loss: 9.6055\n",
      "Epoch 38/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.6210 - val_loss: 9.5927\n",
      "Epoch 39/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.5979 - val_loss: 9.5993\n",
      "Epoch 40/75\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 9.5827 - val_loss: 9.5674\n",
      "Epoch 41/75\n",
      "635/635 [==============================] - 17s 27ms/step - loss: 9.5637 - val_loss: 9.5655\n",
      "Epoch 42/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.5470 - val_loss: 9.5700\n",
      "Epoch 43/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.5273 - val_loss: 9.5677\n",
      "Epoch 44/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.5127 - val_loss: 9.5573\n",
      "Epoch 45/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.4943 - val_loss: 9.5451\n",
      "Epoch 46/75\n",
      "635/635 [==============================] - 19s 31ms/step - loss: 9.4770 - val_loss: 9.5237\n",
      "Epoch 47/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.4660 - val_loss: 9.5237\n",
      "Epoch 48/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.4503 - val_loss: 9.5118\n",
      "Epoch 49/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.4354 - val_loss: 9.5196\n",
      "Epoch 50/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.4210 - val_loss: 9.4996\n",
      "Epoch 51/75\n",
      "635/635 [==============================] - 20s 31ms/step - loss: 9.4023 - val_loss: 9.4966\n",
      "Epoch 52/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.3933 - val_loss: 9.4960\n",
      "Epoch 53/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.3813 - val_loss: 9.4768\n",
      "Epoch 54/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.3606 - val_loss: 9.4884\n",
      "Epoch 55/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.3445 - val_loss: 9.4709\n",
      "Epoch 56/75\n",
      "635/635 [==============================] - 19s 31ms/step - loss: 9.3314 - val_loss: 9.4707\n",
      "Epoch 57/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.3250 - val_loss: 9.4621\n",
      "Epoch 58/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.3093 - val_loss: 9.4667\n",
      "Epoch 59/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.2997 - val_loss: 9.4515\n",
      "Epoch 60/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.2852 - val_loss: 9.4550\n",
      "Epoch 61/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.2740 - val_loss: 9.4711\n",
      "Epoch 62/75\n",
      "635/635 [==============================] - 19s 29ms/step - loss: 9.2614 - val_loss: 9.4413\n",
      "Epoch 63/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.2497 - val_loss: 9.4443\n",
      "Epoch 64/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.2395 - val_loss: 9.4435\n",
      "Epoch 65/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.2255 - val_loss: 9.4337\n",
      "Epoch 66/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.2155 - val_loss: 9.4146\n",
      "Epoch 67/75\n",
      "635/635 [==============================] - 19s 30ms/step - loss: 9.2050 - val_loss: 9.4268\n",
      "Epoch 68/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.1945 - val_loss: 9.4081\n",
      "Epoch 69/75\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 9.1766 - val_loss: 9.3927\n",
      "Epoch 70/75\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 9.1713 - val_loss: 9.4036\n",
      "Epoch 71/75\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 9.1618 - val_loss: 9.3993\n",
      "Epoch 72/75\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 9.1487 - val_loss: 9.3949\n",
      "Epoch 73/75\n",
      "635/635 [==============================] - 18s 29ms/step - loss: 9.1371 - val_loss: 9.3912\n",
      "Epoch 74/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.1278 - val_loss: 9.3718\n",
      "Epoch 75/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.1168 - val_loss: 9.3762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:56:31.736895: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining reference data and trade_history models and training:\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:57:43.070433: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634/635 [============================>.] - ETA: 0s - loss: 25.7451"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:57:58.025358: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 22s 23ms/step - loss: 25.7309 - val_loss: 13.2360\n",
      "Epoch 2/25\n",
      "635/635 [==============================] - 24s 37ms/step - loss: 11.7819 - val_loss: 11.1635\n",
      "Epoch 3/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 10.7816 - val_loss: 10.5317\n",
      "Epoch 4/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 10.3231 - val_loss: 10.2034\n",
      "Epoch 5/25\n",
      "635/635 [==============================] - 15s 23ms/step - loss: 10.0318 - val_loss: 9.9964\n",
      "Epoch 6/25\n",
      "635/635 [==============================] - 22s 35ms/step - loss: 9.8133 - val_loss: 9.8534\n",
      "Epoch 7/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.6543 - val_loss: 9.7379\n",
      "Epoch 8/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.5411 - val_loss: 9.6360\n",
      "Epoch 9/25\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 9.4437 - val_loss: 9.5664\n",
      "Epoch 10/25\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 9.3732 - val_loss: 9.5181\n",
      "Epoch 11/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.3072 - val_loss: 9.5065\n",
      "Epoch 12/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.2545 - val_loss: 9.4692\n",
      "Epoch 13/25\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 9.1987 - val_loss: 9.4178\n",
      "Epoch 14/25\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 9.1567 - val_loss: 9.3905\n",
      "Epoch 15/25\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 9.1289 - val_loss: 9.4086\n",
      "Epoch 16/25\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 9.0895 - val_loss: 9.3621\n",
      "Epoch 17/25\n",
      "635/635 [==============================] - 22s 35ms/step - loss: 9.0601 - val_loss: 9.3365\n",
      "Epoch 18/25\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.0158 - val_loss: 9.3324\n",
      "Epoch 19/25\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 8.9977 - val_loss: 9.3080\n",
      "Epoch 20/25\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 8.9727 - val_loss: 9.2960\n",
      "Epoch 21/25\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 8.9449 - val_loss: 9.2776\n",
      "Epoch 22/25\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 8.9226 - val_loss: 9.2619\n",
      "Epoch 23/25\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 8.9001 - val_loss: 9.3088\n",
      "Epoch 24/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.8724 - val_loss: 9.2505\n",
      "Epoch 25/25\n",
      "635/635 [==============================] - 24s 37ms/step - loss: 8.8585 - val_loss: 9.2373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 23:04:15.973577: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= TRIAL 2, MAE: Trade History: 12.745, Reference: 9.531 =========================\n",
      "========================= TRIAL 2, MAE: Combined Model: 9.496=========================\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 23:05:06.253307: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "633/635 [============================>.] - ETA: 0s - loss: 31.5417"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 23:05:22.006619: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 23s 17ms/step - loss: 31.5019 - val_loss: 17.2299\n",
      "Epoch 2/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 15.9286 - val_loss: 15.2582\n",
      "Epoch 3/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 15.0458 - val_loss: 14.8114\n",
      "Epoch 4/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 14.7096 - val_loss: 14.5388\n",
      "Epoch 5/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 14.4634 - val_loss: 14.2984\n",
      "Epoch 6/50\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 14.2388 - val_loss: 14.1181\n",
      "Epoch 7/50\n",
      "635/635 [==============================] - 16s 26ms/step - loss: 14.1222 - val_loss: 14.0389\n",
      "Epoch 8/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 14.0554 - val_loss: 13.9807\n",
      "Epoch 9/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 14.0042 - val_loss: 13.9341\n",
      "Epoch 10/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.9608 - val_loss: 13.8941\n",
      "Epoch 11/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.9223 - val_loss: 13.8586\n",
      "Epoch 12/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.8869 - val_loss: 13.8257\n",
      "Epoch 13/50\n",
      "635/635 [==============================] - 18s 29ms/step - loss: 13.8542 - val_loss: 13.7968\n",
      "Epoch 14/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.8236 - val_loss: 13.7693\n",
      "Epoch 15/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.7953 - val_loss: 13.7435\n",
      "Epoch 16/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.7694 - val_loss: 13.7194\n",
      "Epoch 17/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.7452 - val_loss: 13.6994\n",
      "Epoch 18/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.7235 - val_loss: 13.6792\n",
      "Epoch 19/50\n",
      "635/635 [==============================] - 18s 28ms/step - loss: 13.7040 - val_loss: 13.6623\n",
      "Epoch 20/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.6858 - val_loss: 13.6474\n",
      "Epoch 21/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.6693 - val_loss: 13.6335\n",
      "Epoch 22/50\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 13.6543 - val_loss: 13.6167\n",
      "Epoch 23/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.6404 - val_loss: 13.6027\n",
      "Epoch 24/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.6273 - val_loss: 13.5935\n",
      "Epoch 25/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.6152 - val_loss: 13.5821\n",
      "Epoch 26/50\n",
      "635/635 [==============================] - 18s 29ms/step - loss: 13.6036 - val_loss: 13.5709\n",
      "Epoch 27/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.5927 - val_loss: 13.5620\n",
      "Epoch 28/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.5827 - val_loss: 13.5535\n",
      "Epoch 29/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.5728 - val_loss: 13.5443\n",
      "Epoch 30/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.5638 - val_loss: 13.5350\n",
      "Epoch 31/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.5553 - val_loss: 13.5253\n",
      "Epoch 32/50\n",
      "635/635 [==============================] - 12s 20ms/step - loss: 13.5470 - val_loss: 13.5178\n",
      "Epoch 33/50\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 13.5392 - val_loss: 13.5108\n",
      "Epoch 34/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.5316 - val_loss: 13.5022\n",
      "Epoch 35/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.5245 - val_loss: 13.4952\n",
      "Epoch 36/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.5174 - val_loss: 13.4890\n",
      "Epoch 37/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.5110 - val_loss: 13.4833\n",
      "Epoch 38/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.5047 - val_loss: 13.4796\n",
      "Epoch 39/50\n",
      "635/635 [==============================] - 18s 29ms/step - loss: 13.4983 - val_loss: 13.4727\n",
      "Epoch 40/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.4925 - val_loss: 13.4678\n",
      "Epoch 41/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.4868 - val_loss: 13.4645\n",
      "Epoch 42/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.4810 - val_loss: 13.4581\n",
      "Epoch 43/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.4756 - val_loss: 13.4536\n",
      "Epoch 44/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.4701 - val_loss: 13.4515\n",
      "Epoch 45/50\n",
      "635/635 [==============================] - 18s 29ms/step - loss: 13.4649 - val_loss: 13.4453\n",
      "Epoch 46/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.4597 - val_loss: 13.4404\n",
      "Epoch 47/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.4550 - val_loss: 13.4344\n",
      "Epoch 48/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.4500 - val_loss: 13.4302\n",
      "Epoch 49/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.4449 - val_loss: 13.4263\n",
      "Epoch 50/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.4403 - val_loss: 13.4208\n",
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 23:13:08.724542: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 438801 of 476032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/635 [..............................] - ETA: 1:00 - loss: 52.3196   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 23:13:09.810549: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:405] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 30s 26ms/step - loss: 42.3595 - val_loss: 38.1496\n",
      "Epoch 2/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 37.0049 - val_loss: 33.7028\n",
      "Epoch 3/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 31.1157 - val_loss: 27.7506\n",
      "Epoch 4/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 23.4509 - val_loss: 19.6364\n",
      "Epoch 5/75\n",
      "635/635 [==============================] - 19s 30ms/step - loss: 16.0791 - val_loss: 13.2656\n",
      "Epoch 6/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 12.0661 - val_loss: 10.9762\n",
      "Epoch 7/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 11.0619 - val_loss: 10.5452\n",
      "Epoch 8/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 10.8387 - val_loss: 10.4359\n",
      "Epoch 9/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 10.7174 - val_loss: 10.3267\n",
      "Epoch 10/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 10.6249 - val_loss: 10.2696\n",
      "Epoch 11/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 10.5452 - val_loss: 10.2023\n",
      "Epoch 12/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 10.4743 - val_loss: 10.1575\n",
      "Epoch 13/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 10.4116 - val_loss: 10.1104\n",
      "Epoch 14/75\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 10.3551 - val_loss: 10.0620\n",
      "Epoch 15/75\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 10.2982 - val_loss: 10.0187\n",
      "Epoch 16/75\n",
      "635/635 [==============================] - 18s 29ms/step - loss: 10.2505 - val_loss: 9.9861\n",
      "Epoch 17/75\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 10.2076 - val_loss: 9.9518\n",
      "Epoch 18/75\n",
      "635/635 [==============================] - 7s 12ms/step - loss: 10.1610 - val_loss: 9.9187\n",
      "Epoch 19/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 10.1216 - val_loss: 9.8892\n",
      "Epoch 20/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 10.0869 - val_loss: 9.8729\n",
      "Epoch 21/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 10.0499 - val_loss: 9.8387\n",
      "Epoch 22/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 10.0174 - val_loss: 9.8246\n",
      "Epoch 23/75\n",
      "635/635 [==============================] - 20s 32ms/step - loss: 9.9830 - val_loss: 9.8097\n",
      "Epoch 24/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.9507 - val_loss: 9.7774\n",
      "Epoch 25/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.9274 - val_loss: 9.7600\n",
      "Epoch 26/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.8966 - val_loss: 9.7575\n",
      "Epoch 27/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.8701 - val_loss: 9.7327\n",
      "Epoch 28/75\n",
      "635/635 [==============================] - 20s 31ms/step - loss: 9.8419 - val_loss: 9.7205\n",
      "Epoch 29/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.8192 - val_loss: 9.7038\n",
      "Epoch 30/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.7925 - val_loss: 9.6913\n",
      "Epoch 31/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.7734 - val_loss: 9.6766\n",
      "Epoch 32/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.7472 - val_loss: 9.6719\n",
      "Epoch 33/75\n",
      "635/635 [==============================] - 19s 31ms/step - loss: 9.7230 - val_loss: 9.6472\n",
      "Epoch 34/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.7025 - val_loss: 9.6472\n",
      "Epoch 35/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.6842 - val_loss: 9.6343\n",
      "Epoch 36/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 9.6653 - val_loss: 9.6372\n",
      "Epoch 37/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.6424 - val_loss: 9.6056\n",
      "Epoch 38/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 9.6247 - val_loss: 9.5962\n",
      "Epoch 39/75\n",
      "635/635 [==============================] - 19s 29ms/step - loss: 9.6005 - val_loss: 9.5867\n",
      "Epoch 40/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.5867 - val_loss: 9.5667\n",
      "Epoch 41/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.5678 - val_loss: 9.5680\n",
      "Epoch 42/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.5512 - val_loss: 9.5563\n",
      "Epoch 43/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.5339 - val_loss: 9.5670\n",
      "Epoch 44/75\n",
      "635/635 [==============================] - 19s 30ms/step - loss: 9.5174 - val_loss: 9.5465\n",
      "Epoch 45/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.4981 - val_loss: 9.5253\n",
      "Epoch 46/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.4805 - val_loss: 9.5225\n",
      "Epoch 47/75\n",
      "635/635 [==============================] - 7s 12ms/step - loss: 9.4694 - val_loss: 9.5387\n",
      "Epoch 48/75\n",
      "635/635 [==============================] - 7s 12ms/step - loss: 9.4557 - val_loss: 9.5050\n",
      "Epoch 49/75\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 9.4397 - val_loss: 9.5204\n",
      "Epoch 50/75\n",
      "635/635 [==============================] - 16s 24ms/step - loss: 9.4258 - val_loss: 9.5015\n",
      "Epoch 51/75\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 9.4066 - val_loss: 9.4815\n",
      "Epoch 52/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.3945 - val_loss: 9.4807\n",
      "Epoch 53/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.3844 - val_loss: 9.4709\n",
      "Epoch 54/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.3657 - val_loss: 9.4693\n",
      "Epoch 55/75\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 9.3489 - val_loss: 9.4573\n",
      "Epoch 56/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.3365 - val_loss: 9.4754\n",
      "Epoch 57/75\n",
      "635/635 [==============================] - 16s 25ms/step - loss: 9.3272 - val_loss: 9.4489\n",
      "Epoch 58/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.3131 - val_loss: 9.4462\n",
      "Epoch 59/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.3029 - val_loss: 9.4400\n",
      "Epoch 60/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.2904 - val_loss: 9.4566\n",
      "Epoch 61/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.2783 - val_loss: 9.4567\n",
      "Epoch 62/75\n",
      "635/635 [==============================] - 20s 32ms/step - loss: 9.2677 - val_loss: 9.4374\n",
      "Epoch 63/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.2535 - val_loss: 9.4405\n",
      "Epoch 64/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.2397 - val_loss: 9.4287\n",
      "Epoch 65/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.2322 - val_loss: 9.4231\n",
      "Epoch 66/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.2192 - val_loss: 9.4267\n",
      "Epoch 67/75\n",
      "635/635 [==============================] - 15s 23ms/step - loss: 9.2088 - val_loss: 9.4245\n",
      "Epoch 68/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.1996 - val_loss: 9.4124\n",
      "Epoch 69/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.1803 - val_loss: 9.4078\n",
      "Epoch 70/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.1783 - val_loss: 9.4150\n",
      "Epoch 71/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.1669 - val_loss: 9.4026\n",
      "Epoch 72/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.1544 - val_loss: 9.4024\n",
      "Epoch 73/75\n",
      "635/635 [==============================] - 19s 30ms/step - loss: 9.1411 - val_loss: 9.3925\n",
      "Epoch 74/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.1325 - val_loss: 9.3957\n",
      "Epoch 75/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.1206 - val_loss: 9.3891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 23:26:39.395981: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining reference data and trade_history models and training:\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 23:27:55.745070: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "633/635 [============================>.] - ETA: 0s - loss: 25.7657"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 23:28:21.745916: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 33s 41ms/step - loss: 25.7292 - val_loss: 13.2465\n",
      "Epoch 2/25\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 11.7813 - val_loss: 11.1645\n",
      "Epoch 3/25\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 10.7853 - val_loss: 10.5293\n",
      "Epoch 4/25\n",
      "635/635 [==============================] - 14s 21ms/step - loss: 10.3255 - val_loss: 10.2027\n",
      "Epoch 5/25\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 10.0364 - val_loss: 9.9888\n",
      "Epoch 6/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.8196 - val_loss: 9.8587\n",
      "Epoch 7/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.6629 - val_loss: 9.7407\n",
      "Epoch 8/25\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 9.5531 - val_loss: 9.6520\n",
      "Epoch 9/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.4603 - val_loss: 9.5784\n",
      "Epoch 10/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.3895 - val_loss: 9.5352\n",
      "Epoch 11/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.3271 - val_loss: 9.5066\n",
      "Epoch 12/25\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 9.2726 - val_loss: 9.4824\n",
      "Epoch 13/25\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 9.2170 - val_loss: 9.4301\n",
      "Epoch 14/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.1774 - val_loss: 9.4187\n",
      "Epoch 15/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.1450 - val_loss: 9.4037\n",
      "Epoch 16/25\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 9.1055 - val_loss: 9.3896\n",
      "Epoch 17/25\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 9.0764 - val_loss: 9.3734\n",
      "Epoch 18/25\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 9.0335 - val_loss: 9.3723\n",
      "Epoch 19/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.0180 - val_loss: 9.3459\n",
      "Epoch 20/25\n",
      "635/635 [==============================] - 23s 37ms/step - loss: 8.9862 - val_loss: 9.3157\n",
      "Epoch 21/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.9616 - val_loss: 9.3108\n",
      "Epoch 22/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.9382 - val_loss: 9.2840\n",
      "Epoch 23/25\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 8.9210 - val_loss: 9.3424\n",
      "Epoch 24/25\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 8.8906 - val_loss: 9.2672\n",
      "Epoch 25/25\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 8.8750 - val_loss: 9.2865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 23:34:42.880429: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= TRIAL 3, MAE: Trade History: 12.746, Reference: 9.544 =========================\n",
      "========================= TRIAL 3, MAE: Combined Model: 9.501=========================\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 23:35:39.216683: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634/635 [============================>.] - ETA: 0s - loss: 31.5184"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 23:35:49.694469: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 16s 13ms/step - loss: 31.5019 - val_loss: 17.2299\n",
      "Epoch 2/50\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 15.9286 - val_loss: 15.2582\n",
      "Epoch 3/50\n",
      "635/635 [==============================] - 15s 23ms/step - loss: 15.0458 - val_loss: 14.8120\n",
      "Epoch 4/50\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 14.7097 - val_loss: 14.5388\n",
      "Epoch 5/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 14.4635 - val_loss: 14.2981\n",
      "Epoch 6/50\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 14.2389 - val_loss: 14.1179\n",
      "Epoch 7/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 14.1221 - val_loss: 14.0387\n",
      "Epoch 8/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 14.0553 - val_loss: 13.9807\n",
      "Epoch 9/50\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 14.0039 - val_loss: 13.9344\n",
      "Epoch 10/50\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 13.9606 - val_loss: 13.8941\n",
      "Epoch 11/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.9222 - val_loss: 13.8580\n",
      "Epoch 12/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.8867 - val_loss: 13.8263\n",
      "Epoch 13/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.8540 - val_loss: 13.7969\n",
      "Epoch 14/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.8234 - val_loss: 13.7687\n",
      "Epoch 15/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.7949 - val_loss: 13.7428\n",
      "Epoch 16/50\n",
      "635/635 [==============================] - 17s 27ms/step - loss: 13.7692 - val_loss: 13.7205\n",
      "Epoch 17/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.7452 - val_loss: 13.7000\n",
      "Epoch 18/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.7233 - val_loss: 13.6784\n",
      "Epoch 19/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.7036 - val_loss: 13.6618\n",
      "Epoch 20/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.6858 - val_loss: 13.6476\n",
      "Epoch 21/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.6692 - val_loss: 13.6321\n",
      "Epoch 22/50\n",
      "635/635 [==============================] - 18s 29ms/step - loss: 13.6541 - val_loss: 13.6173\n",
      "Epoch 23/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.6400 - val_loss: 13.6032\n",
      "Epoch 24/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.6272 - val_loss: 13.5916\n",
      "Epoch 25/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.6151 - val_loss: 13.5811\n",
      "Epoch 26/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.6036 - val_loss: 13.5703\n",
      "Epoch 27/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.5927 - val_loss: 13.5618\n",
      "Epoch 28/50\n",
      "635/635 [==============================] - 14s 23ms/step - loss: 13.5823 - val_loss: 13.5545\n",
      "Epoch 29/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.5730 - val_loss: 13.5442\n",
      "Epoch 30/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.5639 - val_loss: 13.5349\n",
      "Epoch 31/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.5554 - val_loss: 13.5258\n",
      "Epoch 32/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.5468 - val_loss: 13.5176\n",
      "Epoch 33/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.5392 - val_loss: 13.5113\n",
      "Epoch 34/50\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 13.5316 - val_loss: 13.5025\n",
      "Epoch 35/50\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 13.5245 - val_loss: 13.4959\n",
      "Epoch 36/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.5175 - val_loss: 13.4886\n",
      "Epoch 37/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.5109 - val_loss: 13.4846\n",
      "Epoch 38/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.5045 - val_loss: 13.4779\n",
      "Epoch 39/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.4984 - val_loss: 13.4739\n",
      "Epoch 40/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.4925 - val_loss: 13.4681\n",
      "Epoch 41/50\n",
      "635/635 [==============================] - 19s 30ms/step - loss: 13.4866 - val_loss: 13.4649\n",
      "Epoch 42/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.4811 - val_loss: 13.4590\n",
      "Epoch 43/50\n",
      "635/635 [==============================] - 7s 12ms/step - loss: 13.4755 - val_loss: 13.4553\n",
      "Epoch 44/50\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 13.4701 - val_loss: 13.4496\n",
      "Epoch 45/50\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 13.4648 - val_loss: 13.4434\n",
      "Epoch 46/50\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 13.4598 - val_loss: 13.4403\n",
      "Epoch 47/50\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 13.4547 - val_loss: 13.4356\n",
      "Epoch 48/50\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 13.4500 - val_loss: 13.4296\n",
      "Epoch 49/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4449 - val_loss: 13.4253\n",
      "Epoch 50/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4404 - val_loss: 13.4211\n",
      "Epoch 1/75\n",
      "635/635 [==============================] - 21s 20ms/step - loss: 42.3596 - val_loss: 38.1435\n",
      "Epoch 2/75\n",
      "635/635 [==============================] - 7s 12ms/step - loss: 37.0050 - val_loss: 33.7049\n",
      "Epoch 3/75\n",
      "635/635 [==============================] - 19s 30ms/step - loss: 31.1155 - val_loss: 27.8044\n",
      "Epoch 4/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 23.4507 - val_loss: 19.6538\n",
      "Epoch 5/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 16.0786 - val_loss: 13.2063\n",
      "Epoch 6/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 12.0667 - val_loss: 10.9789\n",
      "Epoch 7/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 11.0607 - val_loss: 10.5430\n",
      "Epoch 8/75\n",
      "635/635 [==============================] - 19s 30ms/step - loss: 10.8377 - val_loss: 10.4294\n",
      "Epoch 9/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 10.7168 - val_loss: 10.3269\n",
      "Epoch 10/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 10.6260 - val_loss: 10.2728\n",
      "Epoch 11/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 10.5457 - val_loss: 10.2001\n",
      "Epoch 12/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 10.4746 - val_loss: 10.1600\n",
      "Epoch 13/75\n",
      "635/635 [==============================] - 20s 31ms/step - loss: 10.4129 - val_loss: 10.1179\n",
      "Epoch 14/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 10.3538 - val_loss: 10.0687\n",
      "Epoch 15/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 10.2981 - val_loss: 10.0150\n",
      "Epoch 16/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 10.2509 - val_loss: 9.9887\n",
      "Epoch 17/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 10.2057 - val_loss: 9.9466\n",
      "Epoch 18/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 10.1602 - val_loss: 9.9279\n",
      "Epoch 19/75\n",
      "635/635 [==============================] - 19s 30ms/step - loss: 10.1213 - val_loss: 9.8927\n",
      "Epoch 20/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 10.0863 - val_loss: 9.8768\n",
      "Epoch 21/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 10.0496 - val_loss: 9.8552\n",
      "Epoch 22/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 10.0164 - val_loss: 9.8226\n",
      "Epoch 23/75\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.9818 - val_loss: 9.8078\n",
      "Epoch 24/75\n",
      "635/635 [==============================] - 20s 31ms/step - loss: 9.9502 - val_loss: 9.7852\n",
      "Epoch 25/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.9278 - val_loss: 9.7608\n",
      "Epoch 26/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.8950 - val_loss: 9.7588\n",
      "Epoch 27/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.8715 - val_loss: 9.7501\n",
      "Epoch 28/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.8422 - val_loss: 9.7355\n",
      "Epoch 29/75\n",
      "635/635 [==============================] - 20s 32ms/step - loss: 9.8182 - val_loss: 9.7075\n",
      "Epoch 30/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.7930 - val_loss: 9.6953\n",
      "Epoch 31/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.7732 - val_loss: 9.6977\n",
      "Epoch 32/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.7464 - val_loss: 9.6769\n",
      "Epoch 33/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.7235 - val_loss: 9.6567\n",
      "Epoch 34/75\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 9.7029 - val_loss: 9.6469\n",
      "Epoch 35/75\n",
      "635/635 [==============================] - 17s 26ms/step - loss: 9.6843 - val_loss: 9.6376\n",
      "Epoch 36/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.6650 - val_loss: 9.6368\n",
      "Epoch 37/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.6439 - val_loss: 9.6188\n",
      "Epoch 38/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.6251 - val_loss: 9.6047\n",
      "Epoch 39/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.6025 - val_loss: 9.6038\n",
      "Epoch 40/75\n",
      "635/635 [==============================] - 20s 31ms/step - loss: 9.5860 - val_loss: 9.5791\n",
      "Epoch 41/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.5681 - val_loss: 9.5758\n",
      "Epoch 42/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.5544 - val_loss: 9.5729\n",
      "Epoch 43/75\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 9.5351 - val_loss: 9.5685\n",
      "Epoch 44/75\n",
      "635/635 [==============================] - 12s 18ms/step - loss: 9.5182 - val_loss: 9.5624\n",
      "Epoch 45/75\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 9.4980 - val_loss: 9.5565\n",
      "Epoch 46/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 9.4839 - val_loss: 9.5262\n",
      "Epoch 47/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 9.4713 - val_loss: 9.5367\n",
      "Epoch 48/75\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 9.4539 - val_loss: 9.5199\n",
      "Epoch 49/75\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 9.4390 - val_loss: 9.5291\n",
      "Epoch 50/75\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 9.4257 - val_loss: 9.5084\n",
      "Epoch 51/75\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 9.4075 - val_loss: 9.4993\n",
      "Epoch 52/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.3963 - val_loss: 9.5031\n",
      "Epoch 53/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.3824 - val_loss: 9.4913\n",
      "Epoch 54/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.3650 - val_loss: 9.4956\n",
      "Epoch 55/75\n",
      "635/635 [==============================] - 20s 31ms/step - loss: 9.3495 - val_loss: 9.4939\n",
      "Epoch 56/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.3364 - val_loss: 9.4806\n",
      "Epoch 57/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.3288 - val_loss: 9.4708\n",
      "Epoch 58/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.3117 - val_loss: 9.4736\n",
      "Epoch 59/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.3015 - val_loss: 9.4676\n",
      "Epoch 60/75\n",
      "635/635 [==============================] - 19s 29ms/step - loss: 9.2918 - val_loss: 9.4656\n",
      "Epoch 61/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.2804 - val_loss: 9.4688\n",
      "Epoch 62/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.2679 - val_loss: 9.4819\n",
      "Epoch 63/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.2539 - val_loss: 9.4612\n",
      "Epoch 64/75\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 9.2409 - val_loss: 9.4510\n",
      "Epoch 65/75\n",
      "635/635 [==============================] - 7s 12ms/step - loss: 9.2309 - val_loss: 9.4586\n",
      "Epoch 66/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.2223 - val_loss: 9.4496\n",
      "Epoch 67/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.2098 - val_loss: 9.4439\n",
      "Epoch 68/75\n",
      "635/635 [==============================] - 18s 29ms/step - loss: 9.2001 - val_loss: 9.4276\n",
      "Epoch 69/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.1811 - val_loss: 9.4233\n",
      "Epoch 70/75\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 9.1764 - val_loss: 9.4262\n",
      "Epoch 71/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.1677 - val_loss: 9.4266\n",
      "Epoch 72/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.1501 - val_loss: 9.4342\n",
      "Epoch 73/75\n",
      "635/635 [==============================] - 20s 31ms/step - loss: 9.1412 - val_loss: 9.4228\n",
      "Epoch 74/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.1314 - val_loss: 9.4074\n",
      "Epoch 75/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.1229 - val_loss: 9.4189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 23:57:34.470580: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining reference data and trade_history models and training:\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 23:58:50.070418: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - ETA: 0s - loss: 25.7296"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 23:59:12.895108: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 30s 38ms/step - loss: 25.7296 - val_loss: 13.2227\n",
      "Epoch 2/25\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 11.7865 - val_loss: 11.1840\n",
      "Epoch 3/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 10.7962 - val_loss: 10.5534\n",
      "Epoch 4/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 10.3307 - val_loss: 10.2220\n",
      "Epoch 5/25\n",
      "635/635 [==============================] - 24s 37ms/step - loss: 10.0354 - val_loss: 9.9895\n",
      "Epoch 6/25\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 9.8195 - val_loss: 9.8556\n",
      "Epoch 7/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.6596 - val_loss: 9.7508\n",
      "Epoch 8/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.5474 - val_loss: 9.6398\n",
      "Epoch 9/25\n",
      "635/635 [==============================] - 24s 37ms/step - loss: 9.4536 - val_loss: 9.5870\n",
      "Epoch 10/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.3842 - val_loss: 9.5264\n",
      "Epoch 11/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.3178 - val_loss: 9.5187\n",
      "Epoch 12/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.2667 - val_loss: 9.4741\n",
      "Epoch 13/25\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 9.2121 - val_loss: 9.4403\n",
      "Epoch 14/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.1719 - val_loss: 9.4193\n",
      "Epoch 15/25\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 9.1396 - val_loss: 9.3933\n",
      "Epoch 16/25\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 9.1011 - val_loss: 9.4107\n",
      "Epoch 17/25\n",
      "635/635 [==============================] - 22s 35ms/step - loss: 9.0721 - val_loss: 9.3524\n",
      "Epoch 18/25\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 9.0292 - val_loss: 9.3806\n",
      "Epoch 19/25\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 9.0120 - val_loss: 9.3464\n",
      "Epoch 20/25\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 8.9832 - val_loss: 9.3072\n",
      "Epoch 21/25\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 8.9562 - val_loss: 9.3056\n",
      "Epoch 22/25\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 8.9335 - val_loss: 9.2889\n",
      "Epoch 23/25\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 8.9139 - val_loss: 9.3060\n",
      "Epoch 24/25\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 8.8862 - val_loss: 9.2667\n",
      "Epoch 25/25\n",
      "635/635 [==============================] - 24s 38ms/step - loss: 8.8721 - val_loss: 9.2721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:05:18.151455: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= TRIAL 4, MAE: Trade History: 12.746, Reference: 9.519 =========================\n",
      "========================= TRIAL 4, MAE: Combined Model: 9.516=========================\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:06:10.155430: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - ETA: 0s - loss: 31.5019"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:06:23.678218: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 23s 14ms/step - loss: 31.5019 - val_loss: 17.2299\n",
      "Epoch 2/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 15.9286 - val_loss: 15.2582\n",
      "Epoch 3/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 15.0456 - val_loss: 14.8115\n",
      "Epoch 4/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 14.7095 - val_loss: 14.5383\n",
      "Epoch 5/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 14.4632 - val_loss: 14.2973\n",
      "Epoch 6/50\n",
      "635/635 [==============================] - 18s 29ms/step - loss: 14.2385 - val_loss: 14.1177\n",
      "Epoch 7/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 14.1223 - val_loss: 14.0388\n",
      "Epoch 8/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 14.0553 - val_loss: 13.9805\n",
      "Epoch 9/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 14.0041 - val_loss: 13.9339\n",
      "Epoch 10/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.9607 - val_loss: 13.8939\n",
      "Epoch 11/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.9223 - val_loss: 13.8581\n",
      "Epoch 12/50\n",
      "635/635 [==============================] - 15s 24ms/step - loss: 13.8866 - val_loss: 13.8261\n",
      "Epoch 13/50\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 13.8540 - val_loss: 13.7968\n",
      "Epoch 14/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.8233 - val_loss: 13.7686\n",
      "Epoch 15/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.7950 - val_loss: 13.7442\n",
      "Epoch 16/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.7692 - val_loss: 13.7204\n",
      "Epoch 17/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.7451 - val_loss: 13.6996\n",
      "Epoch 18/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.7232 - val_loss: 13.6782\n",
      "Epoch 19/50\n",
      "635/635 [==============================] - 18s 28ms/step - loss: 13.7036 - val_loss: 13.6623\n",
      "Epoch 20/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.6857 - val_loss: 13.6478\n",
      "Epoch 21/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.6694 - val_loss: 13.6331\n",
      "Epoch 22/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.6543 - val_loss: 13.6185\n",
      "Epoch 23/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.6401 - val_loss: 13.6032\n",
      "Epoch 24/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.6273 - val_loss: 13.5913\n",
      "Epoch 25/50\n",
      "635/635 [==============================] - 18s 29ms/step - loss: 13.6150 - val_loss: 13.5816\n",
      "Epoch 26/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.6035 - val_loss: 13.5712\n",
      "Epoch 27/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.5925 - val_loss: 13.5625\n",
      "Epoch 28/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.5824 - val_loss: 13.5528\n",
      "Epoch 29/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.5729 - val_loss: 13.5443\n",
      "Epoch 30/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.5639 - val_loss: 13.5346\n",
      "Epoch 31/50\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 13.5553 - val_loss: 13.5243\n",
      "Epoch 32/50\n",
      "635/635 [==============================] - 15s 23ms/step - loss: 13.5468 - val_loss: 13.5183\n",
      "Epoch 33/50\n",
      "635/635 [==============================] - 7s 12ms/step - loss: 13.5391 - val_loss: 13.5110\n",
      "Epoch 34/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.5317 - val_loss: 13.5028\n",
      "Epoch 35/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.5245 - val_loss: 13.4961\n",
      "Epoch 36/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.5172 - val_loss: 13.4895\n",
      "Epoch 37/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.5108 - val_loss: 13.4838\n",
      "Epoch 38/50\n",
      "635/635 [==============================] - 18s 29ms/step - loss: 13.5043 - val_loss: 13.4788\n",
      "Epoch 39/50\n",
      "635/635 [==============================] - 7s 12ms/step - loss: 13.4982 - val_loss: 13.4738\n",
      "Epoch 40/50\n",
      "635/635 [==============================] - 7s 12ms/step - loss: 13.4923 - val_loss: 13.4690\n",
      "Epoch 41/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.4868 - val_loss: 13.4637\n",
      "Epoch 42/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.4808 - val_loss: 13.4584\n",
      "Epoch 43/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.4754 - val_loss: 13.4542\n",
      "Epoch 44/50\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 13.4699 - val_loss: 13.4508\n",
      "Epoch 45/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.4646 - val_loss: 13.4450\n",
      "Epoch 46/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.4597 - val_loss: 13.4411\n",
      "Epoch 47/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.4549 - val_loss: 13.4372\n",
      "Epoch 48/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.4497 - val_loss: 13.4306\n",
      "Epoch 49/50\n",
      "635/635 [==============================] - 7s 12ms/step - loss: 13.4447 - val_loss: 13.4244\n",
      "Epoch 50/50\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 13.4403 - val_loss: 13.4210\n",
      "Epoch 1/75\n",
      "635/635 [==============================] - 22s 20ms/step - loss: 42.3595 - val_loss: 38.1540\n",
      "Epoch 2/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 37.0047 - val_loss: 33.7312\n",
      "Epoch 3/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 31.1152 - val_loss: 27.8302\n",
      "Epoch 4/75\n",
      "635/635 [==============================] - 20s 31ms/step - loss: 23.4496 - val_loss: 19.7804\n",
      "Epoch 5/75\n",
      "635/635 [==============================] - 7s 12ms/step - loss: 16.0768 - val_loss: 13.2648\n",
      "Epoch 6/75\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 12.0656 - val_loss: 10.9949\n",
      "Epoch 7/75\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 11.0607 - val_loss: 10.5512\n",
      "Epoch 8/75\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 10.8369 - val_loss: 10.4295\n",
      "Epoch 9/75\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 10.7158 - val_loss: 10.3261\n",
      "Epoch 10/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 10.6230 - val_loss: 10.2599\n",
      "Epoch 11/75\n",
      "635/635 [==============================] - 17s 27ms/step - loss: 10.5424 - val_loss: 10.2009\n",
      "Epoch 12/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 10.4717 - val_loss: 10.1544\n",
      "Epoch 13/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 10.4090 - val_loss: 10.1022\n",
      "Epoch 14/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 10.3514 - val_loss: 10.0583\n",
      "Epoch 15/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 10.2955 - val_loss: 10.0232\n",
      "Epoch 16/75\n",
      "635/635 [==============================] - 7s 10ms/step - loss: 10.2483 - val_loss: 9.9788\n",
      "Epoch 17/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 10.2054 - val_loss: 9.9381\n",
      "Epoch 18/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 10.1598 - val_loss: 9.9182\n",
      "Epoch 19/75\n",
      "635/635 [==============================] - 17s 27ms/step - loss: 10.1214 - val_loss: 9.8874\n",
      "Epoch 20/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 10.0873 - val_loss: 9.8617\n",
      "Epoch 21/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 10.0481 - val_loss: 9.8397\n",
      "Epoch 22/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 10.0179 - val_loss: 9.8124\n",
      "Epoch 23/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.9801 - val_loss: 9.7991\n",
      "Epoch 24/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.9524 - val_loss: 9.7730\n",
      "Epoch 25/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.9284 - val_loss: 9.7551\n",
      "Epoch 26/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.8960 - val_loss: 9.7487\n",
      "Epoch 27/75\n",
      "635/635 [==============================] - 17s 27ms/step - loss: 9.8698 - val_loss: 9.7301\n",
      "Epoch 28/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.8430 - val_loss: 9.7172\n",
      "Epoch 29/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.8189 - val_loss: 9.7000\n",
      "Epoch 30/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.7936 - val_loss: 9.6820\n",
      "Epoch 31/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.7737 - val_loss: 9.6644\n",
      "Epoch 32/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.7448 - val_loss: 9.6651\n",
      "Epoch 33/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.7245 - val_loss: 9.6487\n",
      "Epoch 34/75\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 9.7041 - val_loss: 9.6519\n",
      "Epoch 35/75\n",
      "635/635 [==============================] - 16s 26ms/step - loss: 9.6871 - val_loss: 9.6214\n",
      "Epoch 36/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.6662 - val_loss: 9.6149\n",
      "Epoch 37/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.6441 - val_loss: 9.6010\n",
      "Epoch 38/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.6277 - val_loss: 9.5926\n",
      "Epoch 39/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.6014 - val_loss: 9.5976\n",
      "Epoch 40/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.5865 - val_loss: 9.5687\n",
      "Epoch 41/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.5709 - val_loss: 9.5688\n",
      "Epoch 42/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.5549 - val_loss: 9.5506\n",
      "Epoch 43/75\n",
      "635/635 [==============================] - 13s 21ms/step - loss: 9.5351 - val_loss: 9.5495\n",
      "Epoch 44/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.5199 - val_loss: 9.5528\n",
      "Epoch 45/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.5027 - val_loss: 9.5344\n",
      "Epoch 46/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.4839 - val_loss: 9.5204\n",
      "Epoch 47/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.4728 - val_loss: 9.5375\n",
      "Epoch 48/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.4556 - val_loss: 9.5113\n",
      "Epoch 49/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.4407 - val_loss: 9.5171\n",
      "Epoch 50/75\n",
      "635/635 [==============================] - 15s 23ms/step - loss: 9.4265 - val_loss: 9.4919\n",
      "Epoch 51/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.4059 - val_loss: 9.4935\n",
      "Epoch 52/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.3960 - val_loss: 9.4861\n",
      "Epoch 53/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.3828 - val_loss: 9.4784\n",
      "Epoch 54/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.3654 - val_loss: 9.4758\n",
      "Epoch 55/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.3486 - val_loss: 9.4731\n",
      "Epoch 56/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.3372 - val_loss: 9.4741\n",
      "Epoch 57/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.3263 - val_loss: 9.4625\n",
      "Epoch 58/75\n",
      "635/635 [==============================] - 17s 27ms/step - loss: 9.3130 - val_loss: 9.4635\n",
      "Epoch 59/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.3043 - val_loss: 9.4548\n",
      "Epoch 60/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.2905 - val_loss: 9.4542\n",
      "Epoch 61/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.2767 - val_loss: 9.4449\n",
      "Epoch 62/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.2686 - val_loss: 9.4360\n",
      "Epoch 63/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.2551 - val_loss: 9.4390\n",
      "Epoch 64/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.2392 - val_loss: 9.4348\n",
      "Epoch 65/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 9.2303 - val_loss: 9.4301\n",
      "Epoch 66/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 9.2213 - val_loss: 9.4297\n",
      "Epoch 67/75\n",
      "635/635 [==============================] - 16s 26ms/step - loss: 9.2088 - val_loss: 9.4098\n",
      "Epoch 68/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.2006 - val_loss: 9.4143\n",
      "Epoch 69/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.1798 - val_loss: 9.4081\n",
      "Epoch 70/75\n",
      "635/635 [==============================] - 5s 9ms/step - loss: 9.1763 - val_loss: 9.4030\n",
      "Epoch 71/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.1692 - val_loss: 9.4006\n",
      "Epoch 72/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.1511 - val_loss: 9.4023\n",
      "Epoch 73/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.1424 - val_loss: 9.3935\n",
      "Epoch 74/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.1341 - val_loss: 9.3896\n",
      "Epoch 75/75\n",
      "635/635 [==============================] - 17s 27ms/step - loss: 9.1219 - val_loss: 9.3906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:24:14.934179: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining reference data and trade_history models and training:\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:25:00.327835: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "633/635 [============================>.] - ETA: 0s - loss: 25.7531"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:25:22.876667: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 28s 35ms/step - loss: 25.7166 - val_loss: 13.2261\n",
      "Epoch 2/25\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 11.7702 - val_loss: 11.1514\n",
      "Epoch 3/25\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 10.7686 - val_loss: 10.5135\n",
      "Epoch 4/25\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 10.3160 - val_loss: 10.1947\n",
      "Epoch 5/25\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 10.0295 - val_loss: 9.9838\n",
      "Epoch 6/25\n",
      "635/635 [==============================] - 20s 31ms/step - loss: 9.8157 - val_loss: 9.8493\n",
      "Epoch 7/25\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.6580 - val_loss: 9.7526\n",
      "Epoch 8/25\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.5480 - val_loss: 9.6330\n",
      "Epoch 9/25\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.4542 - val_loss: 9.5636\n",
      "Epoch 10/25\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.3829 - val_loss: 9.5124\n",
      "Epoch 11/25\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 9.3194 - val_loss: 9.5088\n",
      "Epoch 12/25\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.2649 - val_loss: 9.4636\n",
      "Epoch 13/25\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.2088 - val_loss: 9.4344\n",
      "Epoch 14/25\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.1670 - val_loss: 9.4006\n",
      "Epoch 15/25\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.1410 - val_loss: 9.3993\n",
      "Epoch 16/25\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 9.0995 - val_loss: 9.3985\n",
      "Epoch 17/25\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.0693 - val_loss: 9.3504\n",
      "Epoch 18/25\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.0300 - val_loss: 9.3472\n",
      "Epoch 19/25\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.0133 - val_loss: 9.3244\n",
      "Epoch 20/25\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.9818 - val_loss: 9.3020\n",
      "Epoch 21/25\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 8.9606 - val_loss: 9.3020\n",
      "Epoch 22/25\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.9340 - val_loss: 9.2717\n",
      "Epoch 23/25\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.9133 - val_loss: 9.2934\n",
      "Epoch 24/25\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.8873 - val_loss: 9.2651\n",
      "Epoch 25/25\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.8717 - val_loss: 9.2534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:29:59.001950: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= TRIAL 5, MAE: Trade History: 12.746, Reference: 9.549 =========================\n",
      "========================= TRIAL 5, MAE: Combined Model: 9.483=========================\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:30:45.940303: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632/635 [============================>.] - ETA: 0s - loss: 31.5644"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:30:55.179429: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 14s 11ms/step - loss: 31.5019 - val_loss: 17.2299\n",
      "Epoch 2/50\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 15.9286 - val_loss: 15.2582\n",
      "Epoch 3/50\n",
      "635/635 [==============================] - 15s 24ms/step - loss: 15.0456 - val_loss: 14.8115\n",
      "Epoch 4/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 14.7095 - val_loss: 14.5385\n",
      "Epoch 5/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 14.4632 - val_loss: 14.2980\n",
      "Epoch 6/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 14.2385 - val_loss: 14.1180\n",
      "Epoch 7/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 14.1219 - val_loss: 14.0385\n",
      "Epoch 8/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 14.0551 - val_loss: 13.9802\n",
      "Epoch 9/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 14.0038 - val_loss: 13.9339\n",
      "Epoch 10/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.9605 - val_loss: 13.8934\n",
      "Epoch 11/50\n",
      "635/635 [==============================] - 17s 27ms/step - loss: 13.9221 - val_loss: 13.8584\n",
      "Epoch 12/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.8865 - val_loss: 13.8256\n",
      "Epoch 13/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.8539 - val_loss: 13.7964\n",
      "Epoch 14/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.8231 - val_loss: 13.7684\n",
      "Epoch 15/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.7949 - val_loss: 13.7432\n",
      "Epoch 16/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.7690 - val_loss: 13.7198\n",
      "Epoch 17/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.7450 - val_loss: 13.6995\n",
      "Epoch 18/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.7234 - val_loss: 13.6792\n",
      "Epoch 19/50\n",
      "635/635 [==============================] - 15s 23ms/step - loss: 13.7037 - val_loss: 13.6608\n",
      "Epoch 20/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 13.6859 - val_loss: 13.6469\n",
      "Epoch 21/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.6693 - val_loss: 13.6330\n",
      "Epoch 22/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.6543 - val_loss: 13.6179\n",
      "Epoch 23/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.6402 - val_loss: 13.6034\n",
      "Epoch 24/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.6273 - val_loss: 13.5925\n",
      "Epoch 25/50\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 13.6150 - val_loss: 13.5812\n",
      "Epoch 26/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.6037 - val_loss: 13.5703\n",
      "Epoch 27/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5926 - val_loss: 13.5622\n",
      "Epoch 28/50\n",
      "635/635 [==============================] - 17s 26ms/step - loss: 13.5824 - val_loss: 13.5527\n",
      "Epoch 29/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5729 - val_loss: 13.5443\n",
      "Epoch 30/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5637 - val_loss: 13.5344\n",
      "Epoch 31/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5552 - val_loss: 13.5254\n",
      "Epoch 32/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5469 - val_loss: 13.5174\n",
      "Epoch 33/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5391 - val_loss: 13.5101\n",
      "Epoch 34/50\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 13.5315 - val_loss: 13.5031\n",
      "Epoch 35/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5245 - val_loss: 13.4962\n",
      "Epoch 36/50\n",
      "635/635 [==============================] - 16s 26ms/step - loss: 13.5174 - val_loss: 13.4897\n",
      "Epoch 37/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5109 - val_loss: 13.4836\n",
      "Epoch 38/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5044 - val_loss: 13.4770\n",
      "Epoch 39/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4983 - val_loss: 13.4739\n",
      "Epoch 40/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4924 - val_loss: 13.4682\n",
      "Epoch 41/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4867 - val_loss: 13.4640\n",
      "Epoch 42/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4810 - val_loss: 13.4600\n",
      "Epoch 43/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4755 - val_loss: 13.4563\n",
      "Epoch 44/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4701 - val_loss: 13.4494\n",
      "Epoch 45/50\n",
      "635/635 [==============================] - 17s 27ms/step - loss: 13.4646 - val_loss: 13.4449\n",
      "Epoch 46/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4596 - val_loss: 13.4421\n",
      "Epoch 47/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4549 - val_loss: 13.4372\n",
      "Epoch 48/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4498 - val_loss: 13.4312\n",
      "Epoch 49/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.4452 - val_loss: 13.4265\n",
      "Epoch 50/50\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 13.4404 - val_loss: 13.4217\n",
      "Epoch 1/75\n",
      "635/635 [==============================] - 32s 36ms/step - loss: 42.3596 - val_loss: 38.1528\n",
      "Epoch 2/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 37.0049 - val_loss: 33.6933\n",
      "Epoch 3/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 31.1154 - val_loss: 27.7945\n",
      "Epoch 4/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 23.4494 - val_loss: 19.6231\n",
      "Epoch 5/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 16.0783 - val_loss: 13.2346\n",
      "Epoch 6/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 12.0682 - val_loss: 10.9842\n",
      "Epoch 7/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 11.0617 - val_loss: 10.5503\n",
      "Epoch 8/75\n",
      "635/635 [==============================] - 17s 27ms/step - loss: 10.8389 - val_loss: 10.4400\n",
      "Epoch 9/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 10.7203 - val_loss: 10.3359\n",
      "Epoch 10/75\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 10.6271 - val_loss: 10.2685\n",
      "Epoch 11/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 10.5459 - val_loss: 10.2125\n",
      "Epoch 12/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 10.4734 - val_loss: 10.1521\n",
      "Epoch 13/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 10.4117 - val_loss: 10.1035\n",
      "Epoch 14/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 10.3528 - val_loss: 10.0570\n",
      "Epoch 15/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 10.2964 - val_loss: 10.0153\n",
      "Epoch 16/75\n",
      "635/635 [==============================] - 17s 26ms/step - loss: 10.2481 - val_loss: 9.9820\n",
      "Epoch 17/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 10.2045 - val_loss: 9.9506\n",
      "Epoch 18/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 10.1599 - val_loss: 9.9227\n",
      "Epoch 19/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 10.1206 - val_loss: 9.8973\n",
      "Epoch 20/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 10.0848 - val_loss: 9.8642\n",
      "Epoch 21/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 10.0453 - val_loss: 9.8391\n",
      "Epoch 22/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 10.0154 - val_loss: 9.8160\n",
      "Epoch 23/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.9802 - val_loss: 9.8106\n",
      "Epoch 24/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.9480 - val_loss: 9.7793\n",
      "Epoch 25/75\n",
      "635/635 [==============================] - 17s 27ms/step - loss: 9.9259 - val_loss: 9.7605\n",
      "Epoch 26/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.8954 - val_loss: 9.7578\n",
      "Epoch 27/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.8666 - val_loss: 9.7297\n",
      "Epoch 28/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.8403 - val_loss: 9.7175\n",
      "Epoch 29/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.8175 - val_loss: 9.7023\n",
      "Epoch 30/75\n",
      "635/635 [==============================] - 7s 10ms/step - loss: 9.7910 - val_loss: 9.6929\n",
      "Epoch 31/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.7704 - val_loss: 9.6775\n",
      "Epoch 32/75\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 9.7457 - val_loss: 9.6724\n",
      "Epoch 33/75\n",
      "635/635 [==============================] - 16s 25ms/step - loss: 9.7203 - val_loss: 9.6521\n",
      "Epoch 34/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.7011 - val_loss: 9.6472\n",
      "Epoch 35/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.6834 - val_loss: 9.6316\n",
      "Epoch 36/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.6618 - val_loss: 9.6209\n",
      "Epoch 37/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.6402 - val_loss: 9.6100\n",
      "Epoch 38/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.6234 - val_loss: 9.6081\n",
      "Epoch 39/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.5972 - val_loss: 9.5992\n",
      "Epoch 40/75\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 9.5837 - val_loss: 9.5765\n",
      "Epoch 41/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.5672 - val_loss: 9.5828\n",
      "Epoch 42/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.5491 - val_loss: 9.5727\n",
      "Epoch 43/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.5299 - val_loss: 9.5611\n",
      "Epoch 44/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.5153 - val_loss: 9.5527\n",
      "Epoch 45/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.4957 - val_loss: 9.5395\n",
      "Epoch 46/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.4785 - val_loss: 9.5346\n",
      "Epoch 47/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.4675 - val_loss: 9.5444\n",
      "Epoch 48/75\n",
      "635/635 [==============================] - 17s 27ms/step - loss: 9.4517 - val_loss: 9.5261\n",
      "Epoch 49/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.4368 - val_loss: 9.5286\n",
      "Epoch 50/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.4217 - val_loss: 9.5156\n",
      "Epoch 51/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.4003 - val_loss: 9.5102\n",
      "Epoch 52/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.3942 - val_loss: 9.5059\n",
      "Epoch 53/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.3800 - val_loss: 9.4919\n",
      "Epoch 54/75\n",
      "635/635 [==============================] - 7s 10ms/step - loss: 9.3622 - val_loss: 9.5047\n",
      "Epoch 55/75\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 9.3453 - val_loss: 9.4815\n",
      "Epoch 56/75\n",
      "635/635 [==============================] - 18s 28ms/step - loss: 9.3316 - val_loss: 9.4781\n",
      "Epoch 57/75\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 9.3229 - val_loss: 9.4623\n",
      "Epoch 58/75\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 9.3074 - val_loss: 9.4754\n",
      "Epoch 59/75\n",
      "635/635 [==============================] - 7s 12ms/step - loss: 9.2983 - val_loss: 9.4605\n",
      "Epoch 60/75\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 9.2874 - val_loss: 9.4687\n",
      "Epoch 61/75\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 9.2723 - val_loss: 9.4615\n",
      "Epoch 62/75\n",
      "635/635 [==============================] - 18s 29ms/step - loss: 9.2623 - val_loss: 9.4646\n",
      "Epoch 63/75\n",
      "635/635 [==============================] - 7s 10ms/step - loss: 9.2494 - val_loss: 9.4565\n",
      "Epoch 64/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.2389 - val_loss: 9.4426\n",
      "Epoch 65/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.2272 - val_loss: 9.4340\n",
      "Epoch 66/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.2169 - val_loss: 9.4462\n",
      "Epoch 67/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.2054 - val_loss: 9.4533\n",
      "Epoch 68/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.1958 - val_loss: 9.4387\n",
      "Epoch 69/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.1754 - val_loss: 9.4152\n",
      "Epoch 70/75\n",
      "635/635 [==============================] - 17s 26ms/step - loss: 9.1712 - val_loss: 9.4276\n",
      "Epoch 71/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.1603 - val_loss: 9.4292\n",
      "Epoch 72/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.1488 - val_loss: 9.4078\n",
      "Epoch 73/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.1379 - val_loss: 9.4131\n",
      "Epoch 74/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 9.1287 - val_loss: 9.4241\n",
      "Epoch 75/75\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9.1158 - val_loss: 9.4079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:46:46.848564: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining reference data and trade_history models and training:\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:47:43.333165: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - ETA: 0s - loss: 25.7412"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:47:54.777040: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 18s 18ms/step - loss: 25.7412 - val_loss: 13.2354\n",
      "Epoch 2/25\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 11.7851 - val_loss: 11.1550\n",
      "Epoch 3/25\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 10.7741 - val_loss: 10.5192\n",
      "Epoch 4/25\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 10.3149 - val_loss: 10.1984\n",
      "Epoch 5/25\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 10.0291 - val_loss: 9.9992\n",
      "Epoch 6/25\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.8155 - val_loss: 9.8562\n",
      "Epoch 7/25\n",
      "635/635 [==============================] - 21s 33ms/step - loss: 9.6571 - val_loss: 9.7416\n",
      "Epoch 8/25\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.5475 - val_loss: 9.6460\n",
      "Epoch 9/25\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.4545 - val_loss: 9.5736\n",
      "Epoch 10/25\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.3828 - val_loss: 9.5249\n",
      "Epoch 11/25\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.3168 - val_loss: 9.5276\n",
      "Epoch 12/25\n",
      "635/635 [==============================] - 21s 34ms/step - loss: 9.2627 - val_loss: 9.4797\n",
      "Epoch 13/25\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 9.2122 - val_loss: 9.4310\n",
      "Epoch 14/25\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.1710 - val_loss: 9.4043\n",
      "Epoch 15/25\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.1393 - val_loss: 9.3918\n",
      "Epoch 16/25\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.1016 - val_loss: 9.3712\n",
      "Epoch 17/25\n",
      "635/635 [==============================] - 15s 24ms/step - loss: 9.0704 - val_loss: 9.3453\n",
      "Epoch 18/25\n",
      "635/635 [==============================] - 15s 23ms/step - loss: 9.0299 - val_loss: 9.3584\n",
      "Epoch 19/25\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.0132 - val_loss: 9.3177\n",
      "Epoch 20/25\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.9823 - val_loss: 9.2997\n",
      "Epoch 21/25\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 8.9554 - val_loss: 9.2970\n",
      "Epoch 22/25\n",
      "635/635 [==============================] - 11s 18ms/step - loss: 8.9333 - val_loss: 9.2852\n",
      "Epoch 23/25\n",
      "635/635 [==============================] - 19s 30ms/step - loss: 8.9115 - val_loss: 9.3002\n",
      "Epoch 24/25\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.8847 - val_loss: 9.2601\n",
      "Epoch 25/25\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 8.8684 - val_loss: 9.2536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:52:42.714253: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= TRIAL 6, MAE: Trade History: 12.747, Reference: 9.533 =========================\n",
      "========================= TRIAL 6, MAE: Combined Model: 9.513=========================\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:53:30.397837: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - ETA: 0s - loss: 31.5019"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:53:39.519367: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 14s 11ms/step - loss: 31.5019 - val_loss: 17.2299\n",
      "Epoch 2/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 15.9286 - val_loss: 15.2582\n",
      "Epoch 3/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 15.0456 - val_loss: 14.8119\n",
      "Epoch 4/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 14.7094 - val_loss: 14.5383\n",
      "Epoch 5/50\n",
      "635/635 [==============================] - 14s 22ms/step - loss: 14.4633 - val_loss: 14.2976\n",
      "Epoch 6/50\n",
      "635/635 [==============================] - 8s 12ms/step - loss: 14.2387 - val_loss: 14.1181\n",
      "Epoch 7/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 14.1221 - val_loss: 14.0384\n",
      "Epoch 8/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 14.0551 - val_loss: 13.9799\n",
      "Epoch 9/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 14.0039 - val_loss: 13.9340\n",
      "Epoch 10/50\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 13.9606 - val_loss: 13.8937\n",
      "Epoch 11/50\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 13.9222 - val_loss: 13.8582\n",
      "Epoch 12/50\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 13.8866 - val_loss: 13.8259\n",
      "Epoch 13/50\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 13.8541 - val_loss: 13.7968\n",
      "Epoch 14/50\n",
      "635/635 [==============================] - 16s 24ms/step - loss: 13.8233 - val_loss: 13.7694\n",
      "Epoch 15/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.7950 - val_loss: 13.7432\n",
      "Epoch 16/50\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 13.7692 - val_loss: 13.7201\n",
      "Epoch 17/50\n",
      "635/635 [==============================] - 4s 7ms/step - loss: 13.7450 - val_loss: 13.6993\n",
      "Epoch 18/50\n",
      "635/635 [==============================] - 4s 7ms/step - loss: 13.7234 - val_loss: 13.6795\n",
      "Epoch 19/50\n",
      "635/635 [==============================] - 4s 7ms/step - loss: 13.7038 - val_loss: 13.6619\n",
      "Epoch 20/50\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 13.6857 - val_loss: 13.6485\n",
      "Epoch 21/50\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 13.6694 - val_loss: 13.6319\n",
      "Epoch 22/50\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 13.6541 - val_loss: 13.6169\n",
      "Epoch 23/50\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 13.6402 - val_loss: 13.6025\n",
      "Epoch 24/50\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 13.6273 - val_loss: 13.5932\n",
      "Epoch 25/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.6151 - val_loss: 13.5814\n",
      "Epoch 26/50\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 13.6036 - val_loss: 13.5698\n",
      "Epoch 27/50\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 13.5926 - val_loss: 13.5631\n",
      "Epoch 28/50\n",
      "635/635 [==============================] - 4s 7ms/step - loss: 13.5825 - val_loss: 13.5528\n",
      "Epoch 29/50\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 13.5729 - val_loss: 13.5449\n",
      "Epoch 30/50\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 13.5639 - val_loss: 13.5354\n",
      "Epoch 31/50\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 13.5553 - val_loss: 13.5251\n",
      "Epoch 32/50\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 13.5469 - val_loss: 13.5186\n",
      "Epoch 33/50\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 13.5392 - val_loss: 13.5117\n",
      "Epoch 34/50\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 13.5317 - val_loss: 13.5029\n",
      "Epoch 35/50\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 13.5246 - val_loss: 13.4957\n",
      "Epoch 36/50\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 13.5174 - val_loss: 13.4902\n",
      "Epoch 37/50\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 13.5110 - val_loss: 13.4834\n",
      "Epoch 38/50\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 13.5045 - val_loss: 13.4782\n",
      "Epoch 39/50\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 13.4982 - val_loss: 13.4726\n",
      "Epoch 40/50\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 13.4925 - val_loss: 13.4684\n",
      "Epoch 41/50\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 13.4867 - val_loss: 13.4637\n",
      "Epoch 42/50\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 13.4810 - val_loss: 13.4598\n",
      "Epoch 43/50\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 13.4752 - val_loss: 13.4548\n",
      "Epoch 44/50\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 13.4700 - val_loss: 13.4514\n",
      "Epoch 45/50\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 13.4648 - val_loss: 13.4451\n",
      "Epoch 46/50\n",
      "635/635 [==============================] - 10s 15ms/step - loss: 13.4596 - val_loss: 13.4393\n",
      "Epoch 47/50\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 13.4548 - val_loss: 13.4369\n",
      "Epoch 48/50\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 13.4496 - val_loss: 13.4313\n",
      "Epoch 49/50\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 13.4450 - val_loss: 13.4244\n",
      "Epoch 50/50\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 13.4403 - val_loss: 13.4199\n",
      "Epoch 1/75\n",
      "635/635 [==============================] - 20s 19ms/step - loss: 42.3595 - val_loss: 38.1542\n",
      "Epoch 2/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 37.0046 - val_loss: 33.7017\n",
      "Epoch 3/75\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 31.1155 - val_loss: 27.8778\n",
      "Epoch 4/75\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 23.4514 - val_loss: 19.7591\n",
      "Epoch 5/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 16.0768 - val_loss: 13.3245\n",
      "Epoch 6/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 12.0638 - val_loss: 10.9835\n",
      "Epoch 7/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 11.0582 - val_loss: 10.5431\n",
      "Epoch 8/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 10.8342 - val_loss: 10.4235\n",
      "Epoch 9/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 10.7150 - val_loss: 10.3262\n",
      "Epoch 10/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 10.6233 - val_loss: 10.2615\n",
      "Epoch 11/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 10.5428 - val_loss: 10.2038\n",
      "Epoch 12/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 10.4729 - val_loss: 10.1682\n",
      "Epoch 13/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 10.4113 - val_loss: 10.1132\n",
      "Epoch 14/75\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 10.3539 - val_loss: 10.0673\n",
      "Epoch 15/75\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 10.2974 - val_loss: 10.0244\n",
      "Epoch 16/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 10.2504 - val_loss: 9.9884\n",
      "Epoch 17/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 10.2064 - val_loss: 9.9495\n",
      "Epoch 18/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 10.1608 - val_loss: 9.9237\n",
      "Epoch 19/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 10.1215 - val_loss: 9.8879\n",
      "Epoch 20/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 10.0864 - val_loss: 9.8826\n",
      "Epoch 21/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 10.0489 - val_loss: 9.8486\n",
      "Epoch 22/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 10.0186 - val_loss: 9.8178\n",
      "Epoch 23/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 9.9819 - val_loss: 9.8137\n",
      "Epoch 24/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 9.9522 - val_loss: 9.7817\n",
      "Epoch 25/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 9.9270 - val_loss: 9.7640\n",
      "Epoch 26/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.8977 - val_loss: 9.7538\n",
      "Epoch 27/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.8703 - val_loss: 9.7280\n",
      "Epoch 28/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.8419 - val_loss: 9.7154\n",
      "Epoch 29/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 9.8187 - val_loss: 9.7001\n",
      "Epoch 30/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.7928 - val_loss: 9.6791\n",
      "Epoch 31/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.7729 - val_loss: 9.6727\n",
      "Epoch 32/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.7454 - val_loss: 9.6531\n",
      "Epoch 33/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.7220 - val_loss: 9.6493\n",
      "Epoch 34/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 9.7016 - val_loss: 9.6308\n",
      "Epoch 35/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.6828 - val_loss: 9.6145\n",
      "Epoch 36/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.6618 - val_loss: 9.6210\n",
      "Epoch 37/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 9.6395 - val_loss: 9.6015\n",
      "Epoch 38/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 9.6220 - val_loss: 9.5852\n",
      "Epoch 39/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 9.5995 - val_loss: 9.5800\n",
      "Epoch 40/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.5813 - val_loss: 9.5644\n",
      "Epoch 41/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 9.5646 - val_loss: 9.5604\n",
      "Epoch 42/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 9.5500 - val_loss: 9.5493\n",
      "Epoch 43/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 9.5304 - val_loss: 9.5478\n",
      "Epoch 44/75\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 9.5169 - val_loss: 9.5419\n",
      "Epoch 45/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.4957 - val_loss: 9.5385\n",
      "Epoch 46/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.4807 - val_loss: 9.5278\n",
      "Epoch 47/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 9.4682 - val_loss: 9.5177\n",
      "Epoch 48/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 9.4531 - val_loss: 9.5072\n",
      "Epoch 49/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 9.4358 - val_loss: 9.5181\n",
      "Epoch 50/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 9.4236 - val_loss: 9.5021\n",
      "Epoch 51/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 9.4027 - val_loss: 9.5042\n",
      "Epoch 52/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 9.3933 - val_loss: 9.4951\n",
      "Epoch 53/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 9.3793 - val_loss: 9.4776\n",
      "Epoch 54/75\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 9.3634 - val_loss: 9.4807\n",
      "Epoch 55/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.3456 - val_loss: 9.4668\n",
      "Epoch 56/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 9.3338 - val_loss: 9.4611\n",
      "Epoch 57/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.3268 - val_loss: 9.4549\n",
      "Epoch 58/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 9.3125 - val_loss: 9.4556\n",
      "Epoch 59/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 9.3022 - val_loss: 9.4601\n",
      "Epoch 60/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 9.2895 - val_loss: 9.4456\n",
      "Epoch 61/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.2755 - val_loss: 9.4430\n",
      "Epoch 62/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 9.2661 - val_loss: 9.4387\n",
      "Epoch 63/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 9.2532 - val_loss: 9.4400\n",
      "Epoch 64/75\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 9.2376 - val_loss: 9.4298\n",
      "Epoch 65/75\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 9.2275 - val_loss: 9.4291\n",
      "Epoch 66/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.2170 - val_loss: 9.4309\n",
      "Epoch 67/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.2060 - val_loss: 9.4205\n",
      "Epoch 68/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.1956 - val_loss: 9.4117\n",
      "Epoch 69/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.1790 - val_loss: 9.3966\n",
      "Epoch 70/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.1730 - val_loss: 9.4069\n",
      "Epoch 71/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.1649 - val_loss: 9.4049\n",
      "Epoch 72/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.1506 - val_loss: 9.3875\n",
      "Epoch 73/75\n",
      "635/635 [==============================] - 5s 7ms/step - loss: 9.1368 - val_loss: 9.3998\n",
      "Epoch 74/75\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 9.1313 - val_loss: 9.3913\n",
      "Epoch 75/75\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 9.1162 - val_loss: 9.3840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 01:06:11.826746: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining reference data and trade_history models and training:\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 01:06:55.120277: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634/635 [============================>.] - ETA: 0s - loss: 25.7267"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 01:07:17.763687: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 29s 35ms/step - loss: 25.7125 - val_loss: 13.2159\n",
      "Epoch 2/25\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 11.7876 - val_loss: 11.1801\n",
      "Epoch 3/25\n",
      "635/635 [==============================] - 9s 15ms/step - loss: 10.7914 - val_loss: 10.5324\n",
      "Epoch 4/25\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 10.3307 - val_loss: 10.2058\n",
      "Epoch 5/25\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 10.0375 - val_loss: 9.9849\n",
      "Epoch 6/25\n",
      "635/635 [==============================] - 15s 24ms/step - loss: 9.8200 - val_loss: 9.8670\n",
      "Epoch 7/25\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 9.6602 - val_loss: 9.7312\n",
      "Epoch 8/25\n",
      "635/635 [==============================] - 9s 13ms/step - loss: 9.5508 - val_loss: 9.6440\n",
      "Epoch 9/25\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.4558 - val_loss: 9.5819\n",
      "Epoch 10/25\n",
      "635/635 [==============================] - 9s 13ms/step - loss: 9.3861 - val_loss: 9.5249\n",
      "Epoch 11/25\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.3207 - val_loss: 9.5114\n",
      "Epoch 12/25\n",
      "635/635 [==============================] - 18s 29ms/step - loss: 9.2677 - val_loss: 9.4833\n",
      "Epoch 13/25\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.2135 - val_loss: 9.4474\n",
      "Epoch 14/25\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.1708 - val_loss: 9.4126\n",
      "Epoch 15/25\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 9.1388 - val_loss: 9.3886\n",
      "Epoch 16/25\n",
      "635/635 [==============================] - 9s 13ms/step - loss: 9.0994 - val_loss: 9.3826\n",
      "Epoch 17/25\n",
      "635/635 [==============================] - 9s 13ms/step - loss: 9.0661 - val_loss: 9.3794\n",
      "Epoch 18/25\n",
      "635/635 [==============================] - 18s 28ms/step - loss: 9.0268 - val_loss: 9.3614\n",
      "Epoch 19/25\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 9.0105 - val_loss: 9.3158\n",
      "Epoch 20/25\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 8.9764 - val_loss: 9.2982\n",
      "Epoch 21/25\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 8.9563 - val_loss: 9.3003\n",
      "Epoch 22/25\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 8.9305 - val_loss: 9.2726\n",
      "Epoch 23/25\n",
      "635/635 [==============================] - 9s 14ms/step - loss: 8.9116 - val_loss: 9.3136\n",
      "Epoch 24/25\n",
      "635/635 [==============================] - 18s 28ms/step - loss: 8.8798 - val_loss: 9.2718\n",
      "Epoch 25/25\n",
      "635/635 [==============================] - 8s 13ms/step - loss: 8.8644 - val_loss: 9.2769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 01:11:27.869450: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 11435245568 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= TRIAL 7, MAE: Trade History: 12.744, Reference: 9.523 =========================\n",
      "========================= TRIAL 7, MAE: Combined Model: 9.529=========================\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "predictions = []\n",
    "MAE = []\n",
    "NUM_EPOCHS = 75\n",
    "\n",
    "for i in range(8):\n",
    "    tf.keras.utils.set_random_seed(10)\n",
    "    histories, models = train_model_new(params, normalizers, x_train, y_train, x_val, y_val, True, shuffle_buffer = .75, epochs = {'trade_history_model':50, 'reference_model':NUM_EPOCHS})\n",
    "    pred_trade_history = models[0].predict(x_test[0])\n",
    "    pred_reference = models[1].predict(x_test[1])\n",
    "    print('Combining reference data and trade_history models and training:')\n",
    "    combined_history, combined_model = train_model_combined(25, models[0], models[1], x_train, y_train, x_val, y_val, True, shuffle_buffer=.75)\n",
    "    pred_combined = combined_model.predict([x_test[0],x_test[1]])\n",
    "    \n",
    "    predictions.append([pred_trade_history,pred_reference, pred_combined])\n",
    "    trade_history_MAE = mean_absolute_error(pred_trade_history,y_test)\n",
    "    reference_MAE = mean_absolute_error(pred_reference,y_test)\n",
    "    combined_MAE = mean_absolute_error(pred_combined,y_test)\n",
    "    MAE.append([trade_history_MAE,reference_MAE, combined_MAE])\n",
    "    \n",
    "    print('='*25+f' TRIAL {i}, MAE: Trade History: {trade_history_MAE:.3f}, Reference: {reference_MAE:.3f} '+'='*25)\n",
    "    print('='*25+f' TRIAL {i}, MAE: Combined Model: {combined_MAE:.3f}'+'='*25)\n",
    "    \n",
    "    if i == 0:\n",
    "        combined_model.save('combined_model')\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f8e5a26-31f6-4d46-b1e2-d9104d84ae67",
   "metadata": {},
   "source": [
    "combined_model.save('combined_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65a130aa-2578-495c-86ac-6fda0214865b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12.884853862620792, 9.664670530642082, 9.734403541827938],\n",
       " [12.885950745722045, 9.602860682752429, 9.67904592390172],\n",
       " [12.885547284006874, 9.636652157375993, 9.685912449378543],\n",
       " [12.884900076428922, 9.646545978977242, 9.694077417575542],\n",
       " [12.88636594648258, 9.6124096343337, 9.693760341072883],\n",
       " [12.885736518089105, 9.654095307492312, 9.634134517008983]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "59fdf35a-cc71-4a1a-9452-1811c7b5f061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.544788262214652\n",
      "9.624181506945037\n",
      "9.603551627217575\n",
      "9.630965299839303\n",
      "9.660185577435245\n",
      "9.600846136543083\n",
      "Average: 9.610753068365815\n"
     ]
    }
   ],
   "source": [
    "mean = []\n",
    "for mae in MAE:\n",
    "    mean.append(mae[-1])\n",
    "    print(mae[-1])\n",
    "print(f'Average: {sum(mean)/len(mean)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47c5c2e-f5bc-4caa-abc1-a387c18f1973",
   "metadata": {},
   "source": [
    "THIRD RUN 15/07; 75 EPOCHS 25 EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6ed03ab1-00db-41e1-9b06-306bf9e9190d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12.686785229529745, 9.755793584794, 9.626803578941479],\n",
       " [12.68543648848834, 9.704650349382945, 9.596396903641113],\n",
       " [12.687292684071222, 9.751268367268784, 9.714188953038796],\n",
       " [12.684486975474103, 9.764472778810486, 9.694741502677074],\n",
       " [12.686453401009002, 9.75069207037754, 9.67305636732385],\n",
       " [12.683873468805832, 9.750673455224073, 9.691850878420926],\n",
       " [12.687405916841312, 9.712301077367504, 9.70132130349329],\n",
       " [12.688395820408788, 9.798180082294317, 9.673725604833994]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "78c2de28-dc3a-4248-87cd-5791d622924d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.626803578941479\n",
      "9.596396903641113\n",
      "9.714188953038796\n",
      "9.694741502677074\n",
      "9.67305636732385\n",
      "9.691850878420926\n",
      "9.70132130349329\n",
      "9.673725604833994\n",
      "Average: 9.671510636546314\n"
     ]
    }
   ],
   "source": [
    "mean = []\n",
    "for mae in MAE:\n",
    "    mean.append(mae[-1])\n",
    "    print(mae[-1])\n",
    "print(f'Average: {sum(mean)/len(mean)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d3898ede-e5c1-491a-89d3-9ffc753f1767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('trained_model.pkl', 'wb') as f: \n",
    "    pickle.dump(MAE, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfbed07-db99-4fe1-a136-69f90867d380",
   "metadata": {},
   "source": [
    "SECOND RUN 14/07; 75 EPOCHS 25 EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dae1e335-67fc-47bc-aafe-162db3c736b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12.687332703726891, 9.75820506016119, 9.64473226532462],\n",
       " [12.687519494699352, 9.709219321891661, 9.571949381539332],\n",
       " [12.686222233921466, 9.773561948090235, 9.651582925384842],\n",
       " [12.685440483532743, 9.770017258663625, 9.616265581668335],\n",
       " [12.685580418973062, 9.743915899426293, 9.610144937238896],\n",
       " [12.68732822199087, 9.809103491614499, 9.664799207049594],\n",
       " [12.686245808210336, 9.73155712080094, 9.634556508598592],\n",
       " [12.6854449017254, 9.72472503936161, 9.640937044012379]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a02a07d-f05b-446f-a622-563ec0717993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.64473226532462\n",
      "9.571949381539332\n",
      "9.651582925384842\n",
      "9.616265581668335\n",
      "9.610144937238896\n",
      "9.664799207049594\n",
      "9.634556508598592\n",
      "9.640937044012379\n",
      "Average: 9.629370981352075\n"
     ]
    }
   ],
   "source": [
    "mean = []\n",
    "for mae in MAE:\n",
    "    mean.append(mae[-1])\n",
    "    print(mae[-1])\n",
    "print(f'Average: {sum(mean)/len(mean)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f1d487-3407-4827-be13-5b4e7d5cc4ba",
   "metadata": {},
   "source": [
    "FIRST RUN 14/07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c224d28f-d5c1-4246-ad9a-d7c3501adbe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12.750836081212, 9.709164749250036, 9.833257690311669],\n",
       " [12.750539814460222, 9.694631344753219, 9.7018498771275],\n",
       " [12.751355369463464, 9.702590116089727, 9.721349447420366],\n",
       " [12.751615484333085, 9.72402659841592, 9.710762415822632],\n",
       " [12.75142301945446, 9.67591248355224, 9.719721634541589],\n",
       " [12.752904275163205, 9.670816813629783, 9.752365562575061],\n",
       " [12.749830045356374, 9.738991989111, 9.669802102792266],\n",
       " [12.749911332368585, 9.70245904310648, 9.708890139092066]]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "97089793-e123-454c-a83a-96ee5a3ee7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.833257690311669\n",
      "9.7018498771275\n",
      "9.721349447420366\n",
      "9.710762415822632\n",
      "9.719721634541589\n",
      "9.752365562575061\n",
      "9.669802102792266\n",
      "9.708890139092066\n",
      "Average: 9.727249858710392\n"
     ]
    }
   ],
   "source": [
    "mean = []\n",
    "for mae in MAE:\n",
    "    mean.append(mae[-1])\n",
    "    print(mae[-1])\n",
    "print(f'Average: {sum(mean)/len(mean)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c4738d03-0f69-44cc-b5bc-962722a2b6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1: 9.723\n",
      "Trial 2: 9.791\n",
      "Trial 3: 9.754\n",
      "Trial 4: 9.785\n",
      "Trial 5: 9.785\n",
      "Trial 6: 9.765\n",
      "Average MAE: 9.767\n"
     ]
    }
   ],
   "source": [
    "t = 0\n",
    "for i, prediction in enumerate(predictions):\n",
    "    print(f'Trial {i+1}: {mean_absolute_error(prediction, y_test):.3f}')\n",
    "    t += mean_absolute_error(prediction, y_test)\n",
    "print(f'Average MAE: {t/len(predictions):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "140e57b7-2709-4bc9-8925-05f86535edb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 00:49:25.325596: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 9036103680 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.218766129071318\n"
     ]
    }
   ],
   "source": [
    "print(f'{mean_absolute_error(model.predict(x_test), y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bf97edcf-0c1f-451f-a04a-8f6d9c8526a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe_rmval = train_dataframe.drop(val_idx)\n",
    "val_dataframe = train_dataframe.loc[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8585d38e-ec11-48c2-9ea1-297a2c0f7c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_pred = model.predict(x_train)\n",
    "train_dataframe_rmval['prediction'] = tr_pred\n",
    "val_pred = model.predict(x_val)\n",
    "val_dataframe['prediction'] = val_pred\n",
    "ts_pred = model.predict(x_test)\n",
    "test_dataframe['prediction'] = ts_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fdd4f4ab-0406-41ac-8024-d1e5351f6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe_rmval['error'] = tr_pred.flatten() - y_train\n",
    "val_dataframe['error'] = val_pred.flatten() - y_val\n",
    "test_dataframe['error'] = ts_pred.flatten() - y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f19deaeb-dcf3-4a5c-874f-6c077af2d57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN MAE: 7.926396271765109\n",
      "VAL MAE: 8.547524310685908\n",
      "TEST MAE: 9.890944111123545\n"
     ]
    }
   ],
   "source": [
    "print(f'TRAIN MAE: {mean_absolute_error(tr_pred,y_train)}')\n",
    "print(f'VAL MAE: {mean_absolute_error(val_pred, y_val)}')\n",
    "print(f'TEST MAE: {mean_absolute_error(ts_pred,y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d6484acd-fcad-4da8-800d-b3e81df820fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SET BIAS      : -0.36, SD: 13.68, Max: 429.44, Min: -2833.66, 75%: 4.55, 25%: -5.40\n",
      "VALIDATION SET BIAS : -0.39, SD: 15.35, Max: 316.50, Min: -2832.28, 75%: 4.95, 25%: -5.78\n",
      "TEST SET BIAS       : 0.49, SD: 16.44, Max: 349.55, Min: -2087.98, 75%: 6.78, 25%: -5.88\n"
     ]
    }
   ],
   "source": [
    "summarize_col(train_dataframe_rmval.error, 'TRAIN SET BIAS')\n",
    "summarize_col(val_dataframe.error, 'VALIDATION SET BIAS')\n",
    "summarize_col(test_dataframe.error, 'TEST SET BIAS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "57af236b-fade-4803-9463-0240c7889a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(30,5))\n",
    "for i, axes in enumerate(ax):\n",
    "    if i == 0: \n",
    "        filter_column_outliers(train_dataframe_rmval, 'error', .99, .01).error.hist(ax=axes, bins = 100)\n",
    "        axes.set_title('Train Errors')\n",
    "    if i == 1: \n",
    "        filter_column_outliers(val_dataframe, 'error', .99, .01).error.hist(ax=axes, bins = 100)\n",
    "        axes.set_title('Val Errors')\n",
    "    if i == 2: \n",
    "        filter_column_outliers(test_dataframe, 'error', .99, .01).error.hist(ax=axes, bins = 100)\n",
    "        axes.set_title('Test Errors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "adc08de0-a3cd-4032-972a-6f5f5d4ce6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original bias: -0.36, Original MAE: 7.93, Corrected bias: -0.00, Corrected MAE: 7.91\n",
      "Original bias: -0.39, Original MAE: 8.55, Corrected bias: -0.00, Corrected MAE: 8.53\n",
      "Original bias: 0.49, Original MAE: 9.89, Corrected bias: -0.00, Corrected MAE: 9.90\n",
      "Original bias: 0.49, Original MAE: 9.89, Corrected bias: 0.85, Corrected MAE: 9.90\n"
     ]
    }
   ],
   "source": [
    "debias_constant(train_dataframe_rmval.prediction, train_dataframe_rmval[target_variable], train_dataframe_rmval.error.mean())\n",
    "debias_constant(val_dataframe.prediction, val_dataframe[target_variable], val_dataframe.error.mean())\n",
    "debias_constant(test_dataframe.prediction, test_dataframe[target_variable], test_dataframe.error.mean())\n",
    "debias_constant(test_dataframe.prediction, test_dataframe[target_variable], train_dataframe_rmval.error.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b46cb7-0726-41dc-aa07-fd5bd529de34",
   "metadata": {},
   "source": [
    "# Pricing Hypothetical Trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c2e72775-97c1-4783-a03b-c3024a2df763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_days(row, col):\n",
    "    if pd.isna(row[col]) or pd.isna(row['settlement_date']):\n",
    "        return 0\n",
    "    else: \n",
    "        diff = diff_in_days_two_dates(row[col], row.settlement_date)\n",
    "        if diff <= 0:\n",
    "            return -1\n",
    "        else:\n",
    "            return diff\n",
    "        \n",
    "        \n",
    "def sample_dataframe(df, N):\n",
    "    \n",
    "    group_name = df.name\n",
    "    def index_to_dict(index_row):\n",
    "        is_callable = index_row[0]\n",
    "        interval = str(index_row[1].left)+'-'+str(index_row[1].right)\n",
    "        rating = index_row[2]\n",
    "        return {'is_callable':is_callable, 'interval': interval, 'rating':rating}\n",
    "    \n",
    "    df = df.drop_duplicates(subset='cusip')\n",
    "    \n",
    "    if len(df) < N:\n",
    "        N = len(df)\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    result = df.sample(N, replace=False) \n",
    "    group_id = next(COUNT)\n",
    "    result['group'] = group_id\n",
    "    groupby_id_dict[group_id] = index_to_dict(group_name)\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_synthetic_samples(df):\n",
    "    RATINGS = ['AAA',  'BBB', 'CCC', 'NR']\n",
    "    for cusip in df.cusip.unique():\n",
    "        temp = df[df.cusip == cusip].iloc[0]\n",
    "        for rating in RATINGS:\n",
    "            if rating != temp.rating:\n",
    "                temp.rating = rating\n",
    "                df = df.append(temp)\n",
    "    return df\n",
    "\n",
    "def make_summary(col):\n",
    "    # col = 'is_callable'\n",
    "\n",
    "    summary_df = synthetic_sampled_data.groupby(['cusip'])\\\n",
    "    ['predictions']\\\n",
    "    .agg(['std', max_min_f])\\\n",
    "    .rename({'<lambda_0>':'Max-Min'}, axis=1)\\\n",
    "    \n",
    "    summary_df = summary_df.join(synthetic_sampled_data.set_index('cusip')[col])\n",
    "    \n",
    "    summary_df = summary_df.groupby(col).mean()\n",
    "    # summary_df.columns = pd.MultiIndex.from_tuples(summary_df.columns)\n",
    "    \n",
    "    display(summary_df)\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "216d09d0-8af5-4505-b85b-ecf18a12a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test_dataframe[test_dataframe.trade_date <= '2023-06-09'].copy()\n",
    "# data['days_to_call'] = data[['settlement_date','next_call_date']].apply(lambda x: get_days(x, 'next_call_date'), axis = 1)/NUM_OF_DAYS_IN_YEAR\n",
    "data['maturity'] = data[['settlement_date','maturity_date']].apply(lambda x: get_days(x, 'maturity_date'), axis = 1)/NUM_OF_DAYS_IN_YEAR\n",
    "data['maturity_bucket'] = pd.cut(data['maturity'], [0, 5, 10, 15, 20, 30, data.maturity.max()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "336b06a7-fc88-4785-81b7-c2f566e84b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "RATINGS = ['AAA',  'BBB', 'CCC', 'NR', 'MR']\n",
    "groupby_cols = ['is_callable','maturity_bucket','rating']\n",
    "\n",
    "sampled_data = data[data.rating.isin(RATINGS)].groupby(groupby_cols)\n",
    "COUNT = iter(range(len(sampled_data.groups)))\n",
    "groupby_id_dict = dict()\n",
    "sampled_data = sampled_data.apply(lambda x: sample_dataframe(x, 20)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cfe18d91-b534-4cc4-bfe0-2f0f53ba1a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 44s, sys: 0 ns, total: 1min 44s\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%time synthetic_sampled_data = create_synthetic_samples(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "784a817e-46aa-4d21-b5bf-57663b6bd2f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_input_new() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24480/2325527416.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_input_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrade_history_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myield_history_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_sample_hypothetical_trades\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_input_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynthetic_sampled_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrade_history_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myield_history_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: create_input_new() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "X_sample = create_input_new(sampled_data, trade_history_col, yield_history_cols)\n",
    "X_sample_hypothetical_trades = create_input_new(synthetic_sampled_data, trade_history_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdab58c6-5d90-4ac4-93d2-3021c286d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data_predictions  = model.predict(X_sample).flatten()\n",
    "sampled_data_hypothetical_predictions  = model.predict(X_sample_hypothetical_trades).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32982cbe-67d3-4181-b18b-23df1e71e4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data['predictions'] = sampled_data_predictions\n",
    "synthetic_sampled_data['predictions'] = sampled_data_hypothetical_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dc7d66-a2d4-4438-92c6-e18265601998",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(sampled_data['predictions'], sampled_data['new_ys']), mean_absolute_error(synthetic_sampled_data['predictions'], synthetic_sampled_data['new_ys'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47e2e27-b4c9-48ea-aa40-054ab79277b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_min_f = lambda x: x.max() - x.min()\n",
    "\n",
    "synthetic_sampled_data['original_rating'] = synthetic_sampled_data['group'].apply(lambda x: groupby_id_dict[x]['rating'])\n",
    "\n",
    "make_summary('is_callable');\n",
    "make_summary('original_rating');\n",
    "make_summary('maturity_bucket');"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
