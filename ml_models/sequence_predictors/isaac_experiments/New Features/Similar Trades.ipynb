{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "789ed489-072b-4496-b59d-761d3a701edb",
   "metadata": {},
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd41ed2-4206-424f-8218-a5d4cd189eb4",
   "metadata": {},
   "source": [
    "This notebook looks at yields from cusips in the same series as a proof-of-concept for future feature engineering. The logic is simple: cusips in the same series are informative about each other, and providing features that capture that should allow for better accuracy. This should be particularly useful where there is no trade history in the same cusip. \n",
    "\n",
    "This notebook does the following, using data from 2023-01:\n",
    "1. Check the median new_ys, which is the naive estimate for the conditional median, as a baseline MAE benchmark \n",
    "2. Calculate the maximum potential accuracy gains from using average new_ys of length X for cusips in the same series without accounting for leakage (trades must be Y seconds ago to be seen in production) \n",
    "3. Calculate the realistic accuracy gains from using average new_ys of length X for cusips in the same series with accounting for leakage by masking recent trades \n",
    "4. Repeat (3) but excluding trades from the same cusip, since those features are redundant as they are already modelled by the trade history model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dfde9f-2daa-4f9f-b633-2238856dc47f",
   "metadata": {},
   "source": [
    "Notes on a team member's adjacent work on similar trades:\n",
    "- similar trades based on bq queries \n",
    "- similar trade history fed through separate LSTM, with/without attention, best without attention .2bps \n",
    "- last similar trade features (similar to trade hist derived features) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54acdc5b-7dc8-4578-a0bd-02740ba377a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Packages, Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58de4fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import gcsfs\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle5 as pickle\n",
    "import os\n",
    "\n",
    "pd.set_option('max_colwidth', 1000)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07c11246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/home/jupyter/ficc/isaac_creds.json\"\n",
    "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "bq_client = bigquery.Client()\n",
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917f52d4-5834-401a-a2ac-46559494098f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load data and basic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29fc421e-1f46-4eac-ae37-d944b2071c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_pickle(path, bucket = 'isaac_data'):\n",
    "    if os.path.isfile(path):\n",
    "        print('File available, loading pickle')\n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "    else:\n",
    "        print(f'File not available, downloading from cloud storage and saving to {path}')\n",
    "        fs = gcsfs.GCSFileSystem(project='eng-reactor-287421')\n",
    "        gc_path = os.path.join(bucket, path)\n",
    "        print(gc_path)\n",
    "        with fs.open(gc_path) as gf:\n",
    "            data = pd.read_pickle(gf)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45c8ad64",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = pd.read_pickle('working_dataset.pkl')\n",
    "processed_data = processed_data.sort_values(by='trade_datetime', ascending=True)\n",
    "processed_data['cusip_series'] = processed_data['cusip'].str[:6]\n",
    "# df = processed_data[processed_data.trade_date <= '2023-01-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3d906ad-4104-4aec-b1f0-ab9a07db3c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2023-01-03 00:00:00'), Timestamp('2023-09-29 00:00:00'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.trade_date.min(), processed_data.trade_date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9899e1f-682b-4522-b620-09b675f6b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = processed_data[processed_data.trade_date >= '2023-07-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93c6e7b8-a385-41b4-90c4-cac89d59d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle('similar_trades.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daf764d8-84d0-41b5-a53e-8d58b99f3155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data):\n",
    "    print(f'Split the data into train and test sets at a {TRAIN_TEST_SPLIT} train/test ratio')\n",
    "    train_size = int(len(data) * TRAIN_TEST_SPLIT)\n",
    "    test_size = len(data) - train_size\n",
    "    print(f'Train set size: {train_size}\\t\\tTest set size: {test_size}')\n",
    "    train_data = data.head(train_size)\n",
    "    test_data = data.tail(test_size)\n",
    "    assert train_data.iloc[-1].trade_datetime <= test_data.iloc[0].trade_datetime, 'Most recent trade from the train data should be before the oldest trade from the test data'\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f1e0ebc6-6e15-4a8a-82fa-ee61ee62781f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the data into train and test sets at a 0.85 train/test ratio\n",
      "Train set size: 703810\t\tTest set size: 124202\n"
     ]
    }
   ],
   "source": [
    "TRAIN_TEST_SPLIT = 0.85\n",
    "train_data, test_data = train_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b962dc05-54b0-42a2-806f-94f877aa50ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cusip</th>\n",
       "      <th>yield</th>\n",
       "      <th>trade_date</th>\n",
       "      <th>coupon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>122547LE0</td>\n",
       "      <td>322.200</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>3.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>122547LE0</td>\n",
       "      <td>322.200</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>3.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>54466HJL1</td>\n",
       "      <td>259.000</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>54466HJL1</td>\n",
       "      <td>259.000</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>54466HJJ6</td>\n",
       "      <td>255.000</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703785</th>\n",
       "      <td>64972C7J6</td>\n",
       "      <td>412.200</td>\n",
       "      <td>2023-01-26</td>\n",
       "      <td>4.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703792</th>\n",
       "      <td>4424354C4</td>\n",
       "      <td>260.000</td>\n",
       "      <td>2023-01-26</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703811</th>\n",
       "      <td>7962532J0</td>\n",
       "      <td>239.600</td>\n",
       "      <td>2023-01-26</td>\n",
       "      <td>5.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703810</th>\n",
       "      <td>7962532J0</td>\n",
       "      <td>260.000</td>\n",
       "      <td>2023-01-26</td>\n",
       "      <td>5.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703809</th>\n",
       "      <td>7962532J0</td>\n",
       "      <td>260.000</td>\n",
       "      <td>2023-01-26</td>\n",
       "      <td>5.250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>275509 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            cusip   yield trade_date  coupon\n",
       "11      122547LE0 322.200 2023-01-03   3.250\n",
       "12      122547LE0 322.200 2023-01-03   3.250\n",
       "14      54466HJL1 259.000 2023-01-03   5.000\n",
       "13      54466HJL1 259.000 2023-01-03   5.000\n",
       "17      54466HJJ6 255.000 2023-01-03   5.000\n",
       "...           ...     ...        ...     ...\n",
       "703785  64972C7J6 412.200 2023-01-26   4.000\n",
       "703792  4424354C4 260.000 2023-01-26   5.000\n",
       "703811  7962532J0 239.600 2023-01-26   5.250\n",
       "703810  7962532J0 260.000 2023-01-26   5.250\n",
       "703809  7962532J0 260.000 2023-01-26   5.250\n",
       "\n",
       "[275509 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = train_data[['cusip','yield','trade_date','coupon']]\n",
    "grouped = fd.groupby('cusip')\n",
    "filtered_groups = grouped.filter(lambda group: group['cusip'].size > 10)\n",
    "filtered_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bab4c5-0787-481f-8cc9-93fed5426fbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Simple Rolling Mean (no masking):\n",
    "\n",
    "this is just a proof of concept, with known leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "671ce050-e2d2-4806-9818-7fa177e201d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 µs, sys: 0 ns, total: 12 µs\n",
      "Wall time: 15.7 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "series_cols = []\n",
    "cusip_cols = []\n",
    "for i in [1,2,5,10,15,20, 30, 50]:\n",
    "    series_cols.append(f'series_mean_last_{i}')\n",
    "    cusip_cols.append(f'cusip_mean_last_{i}')\n",
    "    #closed='left' masks the current trade, so no leakage on that front, only leakage from recent trades \n",
    "    df[series_cols[-1]] = df.groupby('cusip_series')['new_ys'].rolling(i, min_periods=1, closed= \"left\").mean().droplevel(0) #using all cusips in series\n",
    "    df[cusip_cols[-1]] = df.groupby('cusip')['new_ys'].rolling(i, min_periods=1, closed= \"left\").mean().droplevel(0) #only using same cusip in series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "cb632916-9051-4512-b704-f702c1759346",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[series_cols] = df[series_cols].fillna(0)\n",
    "df[cusip_cols] = df[cusip_cols].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7a5df74a-8635-4ed3-8452-1d01930e1aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MAE using Median: 54.326\n",
      "Baseline MAE using Mean: 54.500\n",
      "MAE using series_mean_last_1: 32.639\n",
      "MAE using cusip_mean_last_1: 21.579\n",
      "MAE using series_mean_last_2: 37.626\n",
      "MAE using cusip_mean_last_2: 22.773\n",
      "MAE using series_mean_last_5: 41.259\n",
      "MAE using cusip_mean_last_5: 23.714\n",
      "MAE using series_mean_last_10: 42.225\n",
      "MAE using cusip_mean_last_10: 24.123\n",
      "MAE using series_mean_last_15: 42.462\n",
      "MAE using cusip_mean_last_15: 24.327\n",
      "MAE using series_mean_last_20: 42.618\n",
      "MAE using cusip_mean_last_20: 24.459\n",
      "MAE using series_mean_last_30: 42.815\n",
      "MAE using cusip_mean_last_30: 24.632\n",
      "MAE using series_mean_last_50: 42.961\n",
      "MAE using cusip_mean_last_50: 24.812\n"
     ]
    }
   ],
   "source": [
    "median_baseline = mean_absolute_error(test_data[\"new_ys\"], np.repeat(train_data[\"new_ys\"].median(), len(test_data)))\n",
    "print(f'Baseline MAE using Median: {median_baseline:.3f}')\n",
    "mean_baseline = mean_absolute_error(test_data[\"new_ys\"], np.repeat(train_data[\"new_ys\"].mean(), len(test_data)))\n",
    "print(f'Baseline MAE using Mean: {mean_baseline:.3f}')\n",
    "for col1, col2 in zip(series_cols, cusip_cols):\n",
    "    print(f'MAE using {col1}: {mean_absolute_error(test_data[\"new_ys\"], test_data[col1]):.3f}')\n",
    "    print(f'MAE using {col2}: {mean_absolute_error(test_data[\"new_ys\"], test_data[col2]):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "03fbb49d-4c5a-407d-862a-a812c2949da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# median_baseline = mean_absolute_error(test_data[\"new_ys\"], np.repeat(train_data[\"new_ys\"].median(), len(df)))\n",
    "# print(f'Baseline MAE using Median: {median_baseline:.3f}')\n",
    "# mean_baseline = mean_absolute_error(test_data[\"new_ys\"], np.repeat(train_data[\"new_ys\"].mean(), len(df)))\n",
    "# print(f'Baseline MAE using Mean: {mean_baseline:.3f}')\n",
    "# for col1, col2 in zip(series_cols, cusip_cols):\n",
    "#     print(f'MAE using {col1}: {mean_absolute_error(test_data[\"new_ys\"], test_data[col1]):.3f}')\n",
    "#     print(f'MAE using {col2}: {mean_absolute_error(test_data[\"new_ys\"], test_data[col2]):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "03368300-903d-4fa8-b1c4-a67193aee649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cusip</th>\n",
       "      <th>trade_datetime</th>\n",
       "      <th>new_ys</th>\n",
       "      <th>ex_cusip_masked_series_average_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115085AY8</td>\n",
       "      <td>2023-01-03 06:00:02</td>\n",
       "      <td>-15.442</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39765</th>\n",
       "      <td>115085AR3</td>\n",
       "      <td>2023-01-03 16:12:30</td>\n",
       "      <td>41.711</td>\n",
       "      <td>-15.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39766</th>\n",
       "      <td>115085AR3</td>\n",
       "      <td>2023-01-03 16:12:30</td>\n",
       "      <td>42.211</td>\n",
       "      <td>-15.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39767</th>\n",
       "      <td>115085AR3</td>\n",
       "      <td>2023-01-03 16:12:30</td>\n",
       "      <td>27.511</td>\n",
       "      <td>-15.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39769</th>\n",
       "      <td>115085AR3</td>\n",
       "      <td>2023-01-03 16:12:30</td>\n",
       "      <td>38.411</td>\n",
       "      <td>-15.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49908</th>\n",
       "      <td>115085AB8</td>\n",
       "      <td>2023-01-04 10:16:46</td>\n",
       "      <td>20.184</td>\n",
       "      <td>38.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49953</th>\n",
       "      <td>115085AB8</td>\n",
       "      <td>2023-01-04 10:16:52</td>\n",
       "      <td>25.384</td>\n",
       "      <td>38.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49958</th>\n",
       "      <td>115085AB8</td>\n",
       "      <td>2023-01-04 10:16:52</td>\n",
       "      <td>20.184</td>\n",
       "      <td>38.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49959</th>\n",
       "      <td>115085AB8</td>\n",
       "      <td>2023-01-04 10:16:52</td>\n",
       "      <td>14.084</td>\n",
       "      <td>38.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87339</th>\n",
       "      <td>115085AM4</td>\n",
       "      <td>2023-01-04 16:06:33</td>\n",
       "      <td>51.781</td>\n",
       "      <td>14.084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94387</th>\n",
       "      <td>115085AB8</td>\n",
       "      <td>2023-01-05 10:00:30</td>\n",
       "      <td>21.900</td>\n",
       "      <td>51.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94389</th>\n",
       "      <td>115085AB8</td>\n",
       "      <td>2023-01-05 10:00:30</td>\n",
       "      <td>21.900</td>\n",
       "      <td>51.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94392</th>\n",
       "      <td>115085AB8</td>\n",
       "      <td>2023-01-05 10:00:36</td>\n",
       "      <td>26.200</td>\n",
       "      <td>51.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101667</th>\n",
       "      <td>115085AR3</td>\n",
       "      <td>2023-01-05 11:25:00</td>\n",
       "      <td>48.639</td>\n",
       "      <td>26.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101680</th>\n",
       "      <td>115085AR3</td>\n",
       "      <td>2023-01-05 11:25:13</td>\n",
       "      <td>48.653</td>\n",
       "      <td>26.200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            cusip      trade_datetime  new_ys  \\\n",
       "0       115085AY8 2023-01-03 06:00:02 -15.442   \n",
       "39765   115085AR3 2023-01-03 16:12:30  41.711   \n",
       "39766   115085AR3 2023-01-03 16:12:30  42.211   \n",
       "39767   115085AR3 2023-01-03 16:12:30  27.511   \n",
       "39769   115085AR3 2023-01-03 16:12:30  38.411   \n",
       "49908   115085AB8 2023-01-04 10:16:46  20.184   \n",
       "49953   115085AB8 2023-01-04 10:16:52  25.384   \n",
       "49958   115085AB8 2023-01-04 10:16:52  20.184   \n",
       "49959   115085AB8 2023-01-04 10:16:52  14.084   \n",
       "87339   115085AM4 2023-01-04 16:06:33  51.781   \n",
       "94387   115085AB8 2023-01-05 10:00:30  21.900   \n",
       "94389   115085AB8 2023-01-05 10:00:30  21.900   \n",
       "94392   115085AB8 2023-01-05 10:00:36  26.200   \n",
       "101667  115085AR3 2023-01-05 11:25:00  48.639   \n",
       "101680  115085AR3 2023-01-05 11:25:13  48.653   \n",
       "\n",
       "        ex_cusip_masked_series_average_1  \n",
       "0                                  0.000  \n",
       "39765                            -15.442  \n",
       "39766                            -15.442  \n",
       "39767                            -15.442  \n",
       "39769                            -15.442  \n",
       "49908                             38.411  \n",
       "49953                             38.411  \n",
       "49958                             38.411  \n",
       "49959                             38.411  \n",
       "87339                             14.084  \n",
       "94387                             51.781  \n",
       "94389                             51.781  \n",
       "94392                             51.781  \n",
       "101667                            26.200  \n",
       "101680                            26.200  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.cusip_series=='115085'][['cusip', 'trade_datetime', 'new_ys','ex_cusip_masked_series_average_1']].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1092b5-9d3c-414e-be96-cff9c9e2a146",
   "metadata": {},
   "source": [
    "## Simple rolling mean without leakage:\n",
    "masking <= 60seconds trades in the same series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1d1565-2e24-4ec7-9d2e-b51f61ac6733",
   "metadata": {},
   "source": [
    "### Rolling function definitions and time trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "501248c0-fa6f-4852-aea4-ffa75c0d6512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "wrapper = None\n",
    "#using a wrapper function for mp.pool is convenient, but it must be instantiated before pool otherwise mp.pool.map will throw an \"AttributeError: Can't pickle local object\"\n",
    "#a quick and easy fix is just to define wrapper first then reassign it to the wrapper function later on \n",
    "\n",
    "def _masked_fuction(data, agg_func, target_col = 'new_ys', max_sequence_length = 5, seconds_ago_mask = 60):\n",
    "    '''Applies a function to target_col while masking trades less than X seconds ago.'''\n",
    "    \n",
    "    row_id = data.iloc[-1]['id']\n",
    "    target_datetime = data.iloc[-1]['trade_datetime']\n",
    "    data = data.iloc[:-1]  \n",
    "    data = data[(target_datetime - data['trade_datetime']).dt.total_seconds() >= seconds_ago_mask]\n",
    "    if len(data) == 0: \n",
    "        return row_id, 0\n",
    "    return row_id, agg_func(data.iloc[-max_sequence_length:][target_col])\n",
    "\n",
    "def _masked_fuction_ex_cusip(data, agg_func, target_col = 'new_ys', max_sequence_length = 5, seconds_ago_mask = 60):\n",
    "    '''Applies a function to target_col while masking trades less than X seconds ago.'''\n",
    "    \n",
    "    row_id = data.iloc[-1]['id']\n",
    "    target_datetime, target_cusip = data.iloc[-1][['trade_datetime', 'cusip']].values\n",
    "    data = data.iloc[:-1]  \n",
    "    data = data[(target_datetime - data['trade_datetime']).dt.total_seconds() >= seconds_ago_mask]\n",
    "    data = data[data.cusip!=target_cusip]\n",
    "    if len(data) == 0: \n",
    "        return row_id, 0\n",
    "    return row_id, agg_func(data.tail(max_sequence_length)[target_col])\n",
    "\n",
    "def calculate_masked_func(data, agg_func, ex_cusip = False, groupby_col = 'cusip_series', target_col = 'new_ys', max_sequence_length = 5, seconds_ago_mask = 60, mp = None, max_window = 100):\n",
    "    '''\n",
    "    Calculates average of target_col while masking trades less than X seconds ago with option for multiprocessing.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    if mp is not None and not isinstance(mp, int):\n",
    "        raise TypeError(f'mp must be an integer for number of jobs for mp.pool, instead got mp = {mp} of type {type(mp)}')\n",
    "        \n",
    "    data['id'] = range(len(data))\n",
    "    groupby = data.groupby(groupby_col).rolling(max_window, min_periods=0)\n",
    "    resample = list(groupby) \n",
    "    \n",
    "    if ex_cusip:\n",
    "        func = _masked_fuction_ex_cusip\n",
    "    else: \n",
    "        func = _masked_fuction\n",
    "        \n",
    "    if not mp:\n",
    "        result = [func(x, \n",
    "                      agg_func,\n",
    "                      target_col = target_col,\n",
    "                      max_sequence_length = max_sequence_length,\n",
    "                      seconds_ago_mask = seconds_ago_mask)\n",
    "              for x in resample]\n",
    "    else:\n",
    "        global wrapper\n",
    "        def wrapper(x):\n",
    "            '''Wrapper function for func that takes only one argument foreasier compatibility with mp.pool'''\n",
    "            return func(x, \n",
    "                      agg_func,\n",
    "                      target_col = target_col,\n",
    "                      max_sequence_length = max_sequence_length,\n",
    "                      seconds_ago_mask = seconds_ago_mask)\n",
    "            \n",
    "        with multiprocessing.Pool(processes=mp) as pool:  \n",
    "            result = pool.map(wrapper, resample)\n",
    "\n",
    "    data.drop('id', axis = 1, inplace=True)    \n",
    "    return pd.Series({key: value for key, value in result})\n",
    "\n",
    "def calculate_masked_average(data, ex_cusip = False, groupby_col = 'cusip_series', target_col = 'new_ys', max_sequence_length = 5, seconds_ago_mask = 60, mp = None, max_window = 100):\n",
    "    return calculate_masked_func(data, \n",
    "                                 lambda y: y.mean(), \n",
    "                                 ex_cusip = ex_cusip, \n",
    "                                 groupby_col = groupby_col, \n",
    "                                 target_col = target_col, \n",
    "                                 max_sequence_length = max_sequence_length, \n",
    "                                 seconds_ago_mask = seconds_ago_mask, \n",
    "                                 mp = mp, \n",
    "                                 max_window = max_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b384eaf-4a81-4a46-983b-f4ef0693c051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(N, do_all = True):\n",
    "    toy = df.iloc[:N].copy()\n",
    "    toy.reset_index(drop=True, inplace=True)\n",
    "    if do_all:\n",
    "        %time temp1 = calculate_masked_average(toy[['trade_datetime','cusip_series','new_ys']])\n",
    "        %time temp2 = calculate_masked_average(toy[['trade_datetime','cusip_series','new_ys']], mp = 4)\n",
    "    %time temp3 = calculate_masked_average(toy[['trade_datetime','cusip_series','new_ys']], mp = 16)\n",
    "    %time temp4 = calculate_masked_average(toy[['trade_datetime','cusip_series','new_ys']], mp = multiprocessing.cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "2e70f7e4-db2d-4b07-8e2b-8367f55a307c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.37 s, sys: 0 ns, total: 2.37 s\n",
      "Wall time: 2.38 s\n",
      "CPU times: user 296 ms, sys: 604 ms, total: 899 ms\n",
      "Wall time: 1.8 s\n",
      "CPU times: user 159 ms, sys: 2.87 s, total: 3.03 s\n",
      "Wall time: 3.87 s\n",
      "CPU times: user 115 ms, sys: 5.31 s, total: 5.42 s\n",
      "Wall time: 5.84 s\n"
     ]
    }
   ],
   "source": [
    "run_trial(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "bdd30eaf-411c-462b-aff3-8c96a79c849b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.9 s, sys: 0 ns, total: 12.9 s\n",
      "Wall time: 12.8 s\n",
      "CPU times: user 1.22 s, sys: 109 ms, total: 1.33 s\n",
      "Wall time: 4.62 s\n",
      "CPU times: user 884 ms, sys: 1.86 s, total: 2.75 s\n",
      "Wall time: 3.62 s\n",
      "CPU times: user 1.87 s, sys: 3.62 s, total: 5.49 s\n",
      "Wall time: 5.95 s\n"
     ]
    }
   ],
   "source": [
    "run_trial(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "9b0de488-c286-4327-948c-11858326ead1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 21s, sys: 0 ns, total: 1min 21s\n",
      "Wall time: 1min 21s\n",
      "CPU times: user 11.6 s, sys: 0 ns, total: 11.6 s\n",
      "Wall time: 28.5 s\n",
      "CPU times: user 8.35 s, sys: 0 ns, total: 8.35 s\n",
      "Wall time: 11.5 s\n",
      "CPU times: user 7.75 s, sys: 3.8 s, total: 11.5 s\n",
      "Wall time: 14.3 s\n"
     ]
    }
   ],
   "source": [
    "run_trial(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "1e30877d-18c9-4c4c-a062-f447f761e1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 41s, sys: 0 ns, total: 2min 41s\n",
      "Wall time: 2min 41s\n",
      "CPU times: user 15.3 s, sys: 0 ns, total: 15.3 s\n",
      "Wall time: 58.2 s\n",
      "CPU times: user 19.4 s, sys: 0 ns, total: 19.4 s\n",
      "Wall time: 30.8 s\n",
      "CPU times: user 19.8 s, sys: 2.23 s, total: 22.1 s\n",
      "Wall time: 24 s\n"
     ]
    }
   ],
   "source": [
    "run_trial(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "646be500-ab92-4395-8fb5-7b210d3d79be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 2.91 s, total: 1min 4s\n",
      "Wall time: 1min 36s\n",
      "CPU times: user 1min 10s, sys: 3.83 s, total: 1min 14s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "run_trial(400000, do_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "77e3949d-cc39-4cb2-96f9-837b9dd5bb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 13s, sys: 5.17 s, total: 2min 18s\n",
      "Wall time: 2min 49s\n"
     ]
    }
   ],
   "source": [
    "%time temp = calculate_masked_average(df[['trade_datetime','cusip_series','new_ys']], mp = multiprocessing.cpu_count() - 1, max_sequence_length = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "ef789545-3939-49fd-8ddc-ee5fa78820be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 53s, sys: 7.21 s, total: 3min\n",
      "Wall time: 4min 4s\n"
     ]
    }
   ],
   "source": [
    "%time temp = calculate_masked_average(df[['trade_datetime', 'cusip', 'cusip_series','new_ys']], ex_cusip=True, mp = multiprocessing.cpu_count() - 1, max_sequence_length = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519174eb-5fc3-48b4-b509-3134e2aa3d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time temp = calculate_masked_average(df[['trade_datetime', 'cusip', 'cusip_series','new_ys']], ex_cusip=True, mp = multiprocessing.cpu_count() - 1, max_sequence_length = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c221337-deea-49c9-a4d1-0f2a9a0e3d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "def profile_decorator(func):\n",
    "    def inner_wrapper(*args, **kwargs):\n",
    "        profile = cProfile.Profile()\n",
    "        profile.enable()\n",
    "\n",
    "        result = func(*args, **kwargs)\n",
    "\n",
    "        profile.disable()\n",
    "        profile.print_stats(sort='cumulative')\n",
    "\n",
    "        return result\n",
    "\n",
    "    return inner_wrapper\n",
    "\n",
    "@profile_decorator\n",
    "def timed_trial(N):\n",
    "    toy = df.iloc[:N].copy()\n",
    "    toy.reset_index(drop=True, inplace=True)\n",
    "    temp1 = calculate_masked_average(toy[['trade_datetime','cusip_series','new_ys']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6355602-e01a-40e0-848c-eefa08928779",
   "metadata": {},
   "source": [
    "### Further Optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b17b63a-dd96-469b-ad83-5760388ad3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "wrapper = None\n",
    "#using a wrapper function for mp.pool is convenient, but it must be instantiated before pool otherwise mp.pool.map will throw an \"AttributeError: Can't pickle local object\"\n",
    "#a quick and easy fix is just to define wrapper first then reassign it to the wrapper function later on \n",
    "\n",
    "def _masked_fuction_2(data, agg_func, windows, mask_cusip = False, target_col = 'new_ys', seconds_ago_mask = 60):\n",
    "    '''Applies a function to target_col while masking trades less than X seconds ago.'''\n",
    "    \n",
    "    row_id = data.iloc[-1]['id']\n",
    "    target_datetime = data.iloc[-1]['trade_datetime']\n",
    "    target_cusip = data.iloc[-1]['cusip']\n",
    "    data = data.iloc[:-1]  \n",
    "    data = data[(target_datetime - data['trade_datetime']).dt.total_seconds() >= seconds_ago_mask]\n",
    "    if mask_cusip:\n",
    "        data = data[data.cusip!=target_cusip]\n",
    "    if len(data) == 0: \n",
    "        return row_id, None\n",
    "    f = lambda size: agg_func(data.iloc[-size:][target_col])\n",
    "    res = list(map(f, windows))\n",
    "    return row_id, res\n",
    "\n",
    "def calculate_masked_func_2(data, agg_func, windows, mask_cusip = False, groupby_col = 'cusip_series', target_col = 'new_ys', seconds_ago_mask = 60, mp = None, max_window = 100):\n",
    "    '''\n",
    "    Calculates average of target_col while masking trades less than X seconds ago with option for multiprocessing.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    if mp is not None and not isinstance(mp, int):\n",
    "        raise TypeError(f'mp must be an integer for number of jobs for mp.pool, instead got mp = {mp} of type {type(mp)}')\n",
    "        \n",
    "    data['id'] = range(len(data))\n",
    "    groupby = data.groupby(groupby_col).rolling(max_window, min_periods=0)\n",
    "    resample = list(groupby) \n",
    "\n",
    "        \n",
    "    if not mp:\n",
    "        result = [_masked_fuction_2(data=x, \n",
    "                                    agg_func = agg_func,\n",
    "                                    windows = windows,\n",
    "                                    mask_cusip = mask_cusip,\n",
    "                                    target_col = target_col,\n",
    "                                    seconds_ago_mask = seconds_ago_mask)\n",
    "              for x in resample]\n",
    "    else:\n",
    "        global wrapper\n",
    "        def wrapper(x):\n",
    "            '''Wrapper function for func that takes only one argument foreasier compatibility with mp.pool'''\n",
    "            return _masked_fuction_2(data=x, \n",
    "                                        agg_func = agg_func,\n",
    "                                        windows = windows,\n",
    "                                        mask_cusip = mask_cusip,\n",
    "                                        target_col = target_col,\n",
    "                                        seconds_ago_mask = seconds_ago_mask)\n",
    "            \n",
    "        with multiprocessing.Pool(processes=mp) as pool:  \n",
    "            result = pool.map(wrapper, resample)\n",
    "\n",
    "    data.drop('id', axis = 1, inplace=True)    \n",
    "    return pd.Series({key: value for key, value in result})\n",
    "\n",
    "def calculate_masked_average_2(data, windows, mask_cusip = False, groupby_col = 'cusip_series', target_col = 'new_ys', seconds_ago_mask = 60, mp = None, max_window = 100):\n",
    "    return calculate_masked_func_2(data = data, \n",
    "                                 agg_func = lambda y: y.mean(), \n",
    "                                 windows = windows,\n",
    "                                 mask_cusip = mask_cusip, \n",
    "                                 groupby_col = groupby_col, \n",
    "                                 target_col = target_col, \n",
    "                                 seconds_ago_mask = seconds_ago_mask, \n",
    "                                 mp = mp, \n",
    "                                 max_window = max_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0f4adb-caf9-4fb5-8111-36f8cd0b21db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "res = []\n",
    "masked_cols = [f'ex_cusip_masked_series_average_{i}' for i in [1,2,5,10,15,20,30,50]]\n",
    "for i in [1,2, 5, 10, 15, 20, 30, 50]:\n",
    "    %time temp = calculate_masked_average(df[['trade_datetime', 'cusip', 'cusip_series','new_ys']], ex_cusip=True, mp = multiprocessing.cpu_count() - 1,max_sequence_length = i)\n",
    "    res.append(temp)\n",
    "df[masked_cols] = pd.concat(res, axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "51da4b92-7153-44f3-9a0e-d8c86ff560b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 52s, sys: 998 ms, total: 5min 53s\n",
      "Wall time: 5min 53s\n",
      "CPU times: user 6min 31s, sys: 286 ms, total: 6min 32s\n",
      "Wall time: 6min 32s\n",
      "CPU times: user 55.6 s, sys: 13.9 s, total: 1min 9s\n",
      "Wall time: 2min 47s\n",
      "CPU times: user 28.6 s, sys: 4.93 s, total: 33.5 s\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "N = 100000\n",
    "toy = df.iloc[:N]\n",
    "%time temp1 = calculate_masked_average_2(toy, [1], mask_cusip=True)\n",
    "%time temp2 = calculate_masked_average_2(toy, [1,2,5], mask_cusip=True)\n",
    "%time temp3 = calculate_masked_average_2(toy, [1], mask_cusip=True, mp = 4)\n",
    "%time temp4 = calculate_masked_average_2(toy, [1,2,5], mask_cusip=True, mp = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "11d0da09-2760-4bbf-9732-00828f613d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.6 s, sys: 16.5 s, total: 47.1 s\n",
      "Wall time: 52.6 s\n"
     ]
    }
   ],
   "source": [
    "%time temp4 = calculate_masked_average_2(toy, [1,2,5], mask_cusip=True, mp = multiprocessing.cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f1833e95-7978-480c-aff8-33ae1287c9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 48s, sys: 17.5 s, total: 2min 5s\n",
      "Wall time: 3min 34s\n"
     ]
    }
   ],
   "source": [
    "%time temp5 = calculate_masked_average_2(df.iloc[:500000][['trade_datetime','cusip','cusip_series','new_ys']], [1, 2, 10,15, 20, 30, 50], mask_cusip=True, mp = multiprocessing.cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1b1036b-d1dc-4bae-9339-925d31eabaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_df(series):\n",
    "    temp = series.copy()\n",
    "    fill_size = len(temp.dropna().iloc[0])\n",
    "    fill_list = [0 for _ in range(fill_size)]\n",
    "    temp.loc[temp.isna()] = temp.loc[temp.isna()].apply(lambda x: fill_list)\n",
    "    return pd.DataFrame(temp.to_list(), index=temp.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad3106-7541-4d2c-9ac9-5e8ddec75c30",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Application and Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "4920e29c-952c-4be0-b361-0b8a3fb2ff3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22min 12s, sys: 1.2 s, total: 22min 13s\n",
      "Wall time: 22min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "temp = calculate_masked_average(df[['trade_datetime','cusip_series','new_ys']])\n",
    "df['masked_series_average_5'] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "cacbf0e9-0197-4efc-979e-6e467f2ed0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 28s, sys: 1.12 s, total: 21min 29s\n",
      "Wall time: 21min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "temp = calculate_masked_average(df[['trade_datetime','cusip','new_ys']], groupby_col='cusip')\n",
    "df['masked_cusip_average_5'] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "e93f0102-f725-41d8-810a-3740d32d6ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 35min 51s, sys: 8.46 s, total: 2h 36min\n",
      "Wall time: 2h 36min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in [1,2, 10,15,20, 30, 50]:\n",
    "    temp = calculate_masked_average(df[['trade_datetime','cusip_series','new_ys']], max_sequence_length = i)\n",
    "    df[f'masked_series_average_{i}'] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "d319b1c4-26de-4c20-8232-705e26bbb4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked_series_average_1: 49.789678\n",
      "masked_series_average_2: 48.607362\n",
      "masked_series_average_10: 44.738403\n",
      "masked_series_average_15: 44.297552\n",
      "masked_series_average_20: 44.058594\n",
      "masked_series_average_30: 43.856427\n",
      "masked_series_average_50: 43.769231\n"
     ]
    }
   ],
   "source": [
    "for i in [1,2, 10,15,20, 30, 50]:\n",
    "    print(f'masked_series_average_{i}: {mean_absolute_error(df[\"new_ys\"], df[f\"masked_series_average_{i}\"]):3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fcff9e3-460f-41c6-a235-306717536dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 46s, sys: 8.72 s, total: 2min 55s\n",
      "Wall time: 3min 54s\n",
      "CPU times: user 2min 45s, sys: 8.48 s, total: 2min 53s\n",
      "Wall time: 4min 7s\n",
      "CPU times: user 2min 48s, sys: 8.42 s, total: 2min 57s\n",
      "Wall time: 4min 9s\n",
      "CPU times: user 2min 54s, sys: 8.66 s, total: 3min 3s\n",
      "Wall time: 4min 16s\n",
      "CPU times: user 2min 48s, sys: 9.32 s, total: 2min 57s\n",
      "Wall time: 4min 5s\n",
      "CPU times: user 2min 46s, sys: 12.5 s, total: 2min 58s\n",
      "Wall time: 3min 54s\n",
      "CPU times: user 2min 49s, sys: 10.3 s, total: 2min 59s\n",
      "Wall time: 3min 59s\n",
      "CPU times: user 2min 45s, sys: 10.3 s, total: 2min 55s\n",
      "Wall time: 3min 52s\n",
      "CPU times: user 22min 25s, sys: 1min 16s, total: 23min 41s\n",
      "Wall time: 32min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = []\n",
    "masked_cols = [f'ex_cusip_masked_series_average_{i}' for i in [1,2,5,10,15,20,30,50]]\n",
    "for i in [1,2, 5, 10, 15, 20, 30, 50]:\n",
    "    %time temp = calculate_masked_average(df[['trade_datetime', 'cusip', 'cusip_series','new_ys']], ex_cusip=True, mp = multiprocessing.cpu_count() - 1,max_sequence_length = i)\n",
    "    res.append(temp)\n",
    "df[masked_cols] = pd.concat(res, axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0eba7b44-3550-4cb5-af2b-0efd0474e3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ex_cusip_masked_series_average_1: 59.117016\n",
      "ex_cusip_masked_series_average_2: 57.200173\n",
      "ex_cusip_masked_series_average_10: 51.364681\n",
      "ex_cusip_masked_series_average_15: 50.571629\n",
      "ex_cusip_masked_series_average_20: 50.134837\n",
      "ex_cusip_masked_series_average_30: 49.708728\n",
      "ex_cusip_masked_series_average_50: 49.360656\n"
     ]
    }
   ],
   "source": [
    "for i in [1,2, 10,15,20, 30, 50]:\n",
    "    print(f'ex_cusip_masked_series_average_{i}: {mean_absolute_error(df[\"new_ys\"], df[f\"ex_cusip_masked_series_average_{i}\"]):3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61af64e8-0a38-4670-aec5-f453475288d8",
   "metadata": {},
   "source": [
    "Simple autoregression with ols and quantile rgression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6b83dc4-b8ff-45cd-bed2-a5636c39ea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.quantile_regression import QuantReg\n",
    "\n",
    "def perform_ols(train_data, test_data, predictors, target='new_ys', print_result = True, return_vals = False):\n",
    "    '''Estimates OLS model using statsmodels api with predefined set of predictors'''\n",
    "    x = sm.add_constant(train_data[predictors])\n",
    "    y = train_data[target]\n",
    "    lm = sm.OLS(y, x).fit()\n",
    "    preds = lm.predict(sm.add_constant(test_data[predictors]))\n",
    "    \n",
    "    \n",
    "    if print_result: \n",
    "        print(F'OLS MAE using features {predictors}: \\n{mean_absolute_error(test_data[target], preds):3f}')\n",
    "        display(lm.summary())\n",
    "    if return_vals: \n",
    "        return lm, mean_absolute_error(test_data[target], preds)\n",
    "    \n",
    "def perform_qreg(train_data, test_data, predictors, target='new_ys', print_result = True, return_vals = False):\n",
    "    '''Estimates 50th quantile model using statsmodels smf api with predefined set of predictors. Equivalent to minimizing MAE'''\n",
    "    x = sm.add_constant(train_data[predictors])\n",
    "    y = train_data[target]\n",
    "    lm = QuantReg(y, x).fit()\n",
    "    preds = lm.predict(sm.add_constant(test_data[predictors]))\n",
    "    \n",
    "    if print_result: \n",
    "        print(F'OLS MAE using features {predictors}: \\n{mean_absolute_error(test_data[target], preds):3f}')\n",
    "        display(lm.summary())\n",
    "    if return_vals: \n",
    "        return lm, mean_absolute_error(test_data[target], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd2cb1b9-aa9d-4e32-8c1e-ecb562da3965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n"
     ]
    }
   ],
   "source": [
    "masked_cols = [f'ex_cusip_masked_series_average_{i}' for i in [1,2,5,10,15,20,30,50]]\n",
    "permutations = []\n",
    "def permute(path, masked_cols):\n",
    "    global permutations\n",
    "    for i, col in enumerate(masked_cols):\n",
    "        new_path = path + [col]\n",
    "        permutations.append(new_path)\n",
    "        permute(new_path, masked_cols[i+1:])\n",
    "permute([], masked_cols)\n",
    "print(len(permutations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "acbba62a-40fa-41be-82c7-2ecf29805f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 26s, sys: 14min 11s, total: 25min 38s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "summary_df = pd.DataFrame()\n",
    "for cols in permutations:\n",
    "    lm, mae = perform_ols(train_data,\n",
    "                          test_data,\n",
    "                          cols,\n",
    "                         print_result=False,\n",
    "                         return_vals=True)\n",
    "    summary_df = summary_df.append({'ncols': len(cols), 'cols': cols, 'MAE':mae, 'model':lm}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "5d7743b2-2348-4ae2-a5f8-c554efa99725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time \n",
    "# def _qreg_helper(col):\n",
    "#     return perform_qreg(train_data, test_data, col, print_result = False, return_vals = True)\n",
    "\n",
    "# summary_df_qreg = pd.DataFrame()\n",
    "# with multiprocessing.Pool(processes=4) as pool:  \n",
    "#     result = pool.map(_qreg_helper, permutations)\n",
    "    \n",
    "#     # summary_df = summary_df.append({'ncols': len(cols), 'cols': cols, 'MAE':mae, 'model':lm}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "e1051fe9-4ee0-44fa-9bd6-4336b17cf1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10h 41min 34s, sys: 13h 10min 55s, total: 23h 52min 29s\n",
      "Wall time: 1h 11min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "summary_df2 = pd.DataFrame()\n",
    "for cols in permutations:\n",
    "    lm, mae = perform_qreg(train_data,\n",
    "                          test_data,\n",
    "                          cols,\n",
    "                         print_result=False,\n",
    "                         return_vals=True)\n",
    "    summary_df2 = summary_df2.append({'ncols': len(cols), 'cols': cols, 'MAE':mae, 'model':lm}, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "d513ef79-a325-434d-aa75-fda9acaa4e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ncols</th>\n",
       "      <th>cols</th>\n",
       "      <th>MAE</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>5.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_2, ex_cusip_masked_series_average_15, ex_cusip_masked_series_average_20, ex_cusip_masked_series_average_50]</td>\n",
       "      <td>50.092</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04dffd10&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_2, ex_cusip_masked_series_average_5, ex_cusip_masked_series_average_15, ex_cusip_masked_series_average_20, ex_cusip_masked_series_average_50]</td>\n",
       "      <td>50.092</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d050810d0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>4.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_15, ex_cusip_masked_series_average_20, ex_cusip_masked_series_average_50]</td>\n",
       "      <td>50.092</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04871890&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>4.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_2, ex_cusip_masked_series_average_20, ex_cusip_masked_series_average_50]</td>\n",
       "      <td>50.092</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04e6a610&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_2, ex_cusip_masked_series_average_5, ex_cusip_masked_series_average_20, ex_cusip_masked_series_average_50]</td>\n",
       "      <td>50.092</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04d40ed0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_2, ex_cusip_masked_series_average_5]</td>\n",
       "      <td>51.748</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04859e10&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>1.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_5]</td>\n",
       "      <td>51.750</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04928110&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_2]</td>\n",
       "      <td>52.415</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04a5dfd0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>1.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_2]</td>\n",
       "      <td>52.418</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d048590d0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_1]</td>\n",
       "      <td>52.646</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04a5dcd0&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>255 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ncols                                                                                                                                                                                                             cols    MAE                                                                                    model\n",
       "54   5.000                                    [ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_2, ex_cusip_masked_series_average_15, ex_cusip_masked_series_average_20, ex_cusip_masked_series_average_50] 50.092  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04dffd10>\n",
       "23   6.000  [ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_2, ex_cusip_masked_series_average_5, ex_cusip_masked_series_average_15, ex_cusip_masked_series_average_20, ex_cusip_masked_series_average_50] 50.092  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d050810d0>\n",
       "117  4.000                                                                      [ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_15, ex_cusip_masked_series_average_20, ex_cusip_masked_series_average_50] 50.092  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04871890>\n",
       "61   4.000                                                                       [ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_2, ex_cusip_masked_series_average_20, ex_cusip_masked_series_average_50] 50.092  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04e6a610>\n",
       "30   5.000                                     [ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_2, ex_cusip_masked_series_average_5, ex_cusip_masked_series_average_20, ex_cusip_masked_series_average_50] 50.092  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04d40ed0>\n",
       "..     ...                                                                                                                                                                                                              ...    ...                                                                                      ...\n",
       "129  2.000                                                                                                                                             [ex_cusip_masked_series_average_2, ex_cusip_masked_series_average_5] 51.748  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04859e10>\n",
       "192  1.000                                                                                                                                                                               [ex_cusip_masked_series_average_5] 51.750  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04928110>\n",
       "1    2.000                                                                                                                                             [ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_2] 52.415  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04a5dfd0>\n",
       "128  1.000                                                                                                                                                                               [ex_cusip_masked_series_average_2] 52.418  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d048590d0>\n",
       "0    1.000                                                                                                                                                                               [ex_cusip_masked_series_average_1] 52.646  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04a5dcd0>\n",
       "\n",
       "[255 rows x 4 columns]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df.sort_values('MAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "0a23c2f1-8b66-4195-b63c-550412646208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ncols</th>\n",
       "      <th>cols</th>\n",
       "      <th>MAE</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>4.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_10, ex_cusip_masked_series_average_15, ex_cusip_masked_series_average_50]</td>\n",
       "      <td>48.916</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04d14950&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>3.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_10, ex_cusip_masked_series_average_50]</td>\n",
       "      <td>48.916</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d057dc050&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>5.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_10, ex_cusip_masked_series_average_15, ex_cusip_masked_series_average_30, ex_cusip_masked_series_average_50]</td>\n",
       "      <td>48.917</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04d14490&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>5.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_2, ex_cusip_masked_series_average_10, ex_cusip_masked_series_average_15, ex_cusip_masked_series_average_50]</td>\n",
       "      <td>48.917</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7fa04c816dd0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>6.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_10, ex_cusip_masked_series_average_15, ex_cusip_masked_series_average_20, ex_cusip_masked_series_average_30, ex_cusip_masked_series_average_50]</td>\n",
       "      <td>48.917</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04d14c50&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_2, ex_cusip_masked_series_average_5]</td>\n",
       "      <td>50.649</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04d2a490&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>1.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_5]</td>\n",
       "      <td>50.651</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04798b50&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>1.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_2]</td>\n",
       "      <td>51.606</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04d22750&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_2]</td>\n",
       "      <td>51.609</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9fffb31610&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_1]</td>\n",
       "      <td>51.977</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d05f00610&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>255 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ncols                                                                                                                                                                                                               cols    MAE                                                                                    model\n",
       "105  4.000                                                                        [ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_10, ex_cusip_masked_series_average_15, ex_cusip_masked_series_average_50] 48.916  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04d14950>\n",
       "112  3.000                                                                                                           [ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_10, ex_cusip_masked_series_average_50] 48.916  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d057dc050>\n",
       "104  5.000                                     [ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_10, ex_cusip_masked_series_average_15, ex_cusip_masked_series_average_30, ex_cusip_masked_series_average_50] 48.917  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04d14490>\n",
       "42   5.000                                      [ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_2, ex_cusip_masked_series_average_10, ex_cusip_masked_series_average_15, ex_cusip_masked_series_average_50] 48.917  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7fa04c816dd0>\n",
       "101  6.000  [ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_10, ex_cusip_masked_series_average_15, ex_cusip_masked_series_average_20, ex_cusip_masked_series_average_30, ex_cusip_masked_series_average_50] 48.917  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04d14c50>\n",
       "..     ...                                                                                                                                                                                                                ...    ...                                                                                      ...\n",
       "129  2.000                                                                                                                                               [ex_cusip_masked_series_average_2, ex_cusip_masked_series_average_5] 50.649  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04d2a490>\n",
       "192  1.000                                                                                                                                                                                 [ex_cusip_masked_series_average_5] 50.651  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04798b50>\n",
       "128  1.000                                                                                                                                                                                 [ex_cusip_masked_series_average_2] 51.606  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04d22750>\n",
       "1    2.000                                                                                                                                               [ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_2] 51.609  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9fffb31610>\n",
       "0    1.000                                                                                                                                                                                 [ex_cusip_masked_series_average_1] 51.977  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d05f00610>\n",
       "\n",
       "[255 rows x 4 columns]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df2.sort_values('MAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "57753440-e2b1-46df-8e53-4e70d4bd58ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ncols</th>\n",
       "      <th>cols</th>\n",
       "      <th>MAE</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>1.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_50]</td>\n",
       "      <td>50.113</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0466e890&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>1.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_30]</td>\n",
       "      <td>50.325</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0493ae90&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>1.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_20]</td>\n",
       "      <td>50.549</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0493a110&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>1.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_15]</td>\n",
       "      <td>50.742</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0494a050&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>1.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_10]</td>\n",
       "      <td>51.135</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0496e590&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>1.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_5]</td>\n",
       "      <td>51.750</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04928110&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>1.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_2]</td>\n",
       "      <td>52.418</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d048590d0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_1]</td>\n",
       "      <td>52.646</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04a5dcd0&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ncols                                 cols    MAE                                                                                    model\n",
       "254  1.000  [ex_cusip_masked_series_average_50] 50.113  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0466e890>\n",
       "252  1.000  [ex_cusip_masked_series_average_30] 50.325  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0493ae90>\n",
       "248  1.000  [ex_cusip_masked_series_average_20] 50.549  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0493a110>\n",
       "240  1.000  [ex_cusip_masked_series_average_15] 50.742  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0494a050>\n",
       "224  1.000  [ex_cusip_masked_series_average_10] 51.135  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0496e590>\n",
       "192  1.000   [ex_cusip_masked_series_average_5] 51.750  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04928110>\n",
       "128  1.000   [ex_cusip_masked_series_average_2] 52.418  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d048590d0>\n",
       "0    1.000   [ex_cusip_masked_series_average_1] 52.646  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04a5dcd0>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df[summary_df.cols.apply(len) == 1].sort_values('MAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "5d682c74-7383-4713-95dc-100c65d4cdf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ncols</th>\n",
       "      <th>cols</th>\n",
       "      <th>MAE</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>1.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_50]</td>\n",
       "      <td>48.958</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d050b25d0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>1.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_30]</td>\n",
       "      <td>49.167</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d050b2f50&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>1.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_20]</td>\n",
       "      <td>49.357</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04709290&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>1.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_15]</td>\n",
       "      <td>49.541</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04e31e90&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>1.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_10]</td>\n",
       "      <td>49.909</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d093e4a10&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>1.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_5]</td>\n",
       "      <td>50.651</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04798b50&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>1.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_2]</td>\n",
       "      <td>51.606</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04d22750&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_1]</td>\n",
       "      <td>51.977</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d05f00610&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ncols                                 cols    MAE                                                                                    model\n",
       "254  1.000  [ex_cusip_masked_series_average_50] 48.958  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d050b25d0>\n",
       "252  1.000  [ex_cusip_masked_series_average_30] 49.167  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d050b2f50>\n",
       "248  1.000  [ex_cusip_masked_series_average_20] 49.357  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04709290>\n",
       "240  1.000  [ex_cusip_masked_series_average_15] 49.541  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04e31e90>\n",
       "224  1.000  [ex_cusip_masked_series_average_10] 49.909  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d093e4a10>\n",
       "192  1.000   [ex_cusip_masked_series_average_5] 50.651  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04798b50>\n",
       "128  1.000   [ex_cusip_masked_series_average_2] 51.606  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04d22750>\n",
       "0    1.000   [ex_cusip_masked_series_average_1] 51.977  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d05f00610>"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df2[summary_df2.cols.apply(len) == 1].sort_values('MAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ce7dc949-0834-43a9-9b44-7d331b61f0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ncols</th>\n",
       "      <th>cols</th>\n",
       "      <th>MAE</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_50]</td>\n",
       "      <td>50.093</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04869350&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_2, ex_cusip_masked_series_average_50]</td>\n",
       "      <td>50.096</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d049288d0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_5, ex_cusip_masked_series_average_50]</td>\n",
       "      <td>50.110</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0496e050&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_10, ex_cusip_masked_series_average_50]</td>\n",
       "      <td>50.111</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04957350&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_15, ex_cusip_masked_series_average_50]</td>\n",
       "      <td>50.112</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0493a8d0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_20, ex_cusip_masked_series_average_50]</td>\n",
       "      <td>50.113</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0493acd0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_30, ex_cusip_masked_series_average_50]</td>\n",
       "      <td>50.121</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0493add0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_30]</td>\n",
       "      <td>50.298</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d048698d0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_2, ex_cusip_masked_series_average_30]</td>\n",
       "      <td>50.304</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d049345d0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_5, ex_cusip_masked_series_average_30]</td>\n",
       "      <td>50.319</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0496e7d0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_10, ex_cusip_masked_series_average_30]</td>\n",
       "      <td>50.321</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d049578d0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_15, ex_cusip_masked_series_average_30]</td>\n",
       "      <td>50.322</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0494a890&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_20, ex_cusip_masked_series_average_30]</td>\n",
       "      <td>50.326</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0493aed0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_20]</td>\n",
       "      <td>50.518</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d048694d0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_2, ex_cusip_masked_series_average_20]</td>\n",
       "      <td>50.526</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04934b10&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_5, ex_cusip_masked_series_average_20]</td>\n",
       "      <td>50.547</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04900990&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_10, ex_cusip_masked_series_average_20]</td>\n",
       "      <td>50.549</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d049574d0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_15, ex_cusip_masked_series_average_20]</td>\n",
       "      <td>50.550</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0494ae10&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_15]</td>\n",
       "      <td>50.709</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04802e90&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_2, ex_cusip_masked_series_average_15]</td>\n",
       "      <td>50.720</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d048c3390&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_10, ex_cusip_masked_series_average_15]</td>\n",
       "      <td>50.740</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0496ee10&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_5, ex_cusip_masked_series_average_15]</td>\n",
       "      <td>50.744</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04911750&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_10]</td>\n",
       "      <td>51.102</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0481d750&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_2, ex_cusip_masked_series_average_10]</td>\n",
       "      <td>51.119</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d048dc950&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_5, ex_cusip_masked_series_average_10]</td>\n",
       "      <td>51.143</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04928ed0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_5]</td>\n",
       "      <td>51.729</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04e791d0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_2, ex_cusip_masked_series_average_5]</td>\n",
       "      <td>51.748</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04859e10&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.000</td>\n",
       "      <td>[ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_2]</td>\n",
       "      <td>52.415</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04a5dfd0&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ncols                                                                    cols    MAE                                                                                    model\n",
       "127  2.000   [ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_50] 50.093  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04869350>\n",
       "191  2.000   [ex_cusip_masked_series_average_2, ex_cusip_masked_series_average_50] 50.096  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d049288d0>\n",
       "223  2.000   [ex_cusip_masked_series_average_5, ex_cusip_masked_series_average_50] 50.110  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0496e050>\n",
       "239  2.000  [ex_cusip_masked_series_average_10, ex_cusip_masked_series_average_50] 50.111  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04957350>\n",
       "247  2.000  [ex_cusip_masked_series_average_15, ex_cusip_masked_series_average_50] 50.112  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0493a8d0>\n",
       "251  2.000  [ex_cusip_masked_series_average_20, ex_cusip_masked_series_average_50] 50.113  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0493acd0>\n",
       "253  2.000  [ex_cusip_masked_series_average_30, ex_cusip_masked_series_average_50] 50.121  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0493add0>\n",
       "125  2.000   [ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_30] 50.298  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d048698d0>\n",
       "189  2.000   [ex_cusip_masked_series_average_2, ex_cusip_masked_series_average_30] 50.304  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d049345d0>\n",
       "221  2.000   [ex_cusip_masked_series_average_5, ex_cusip_masked_series_average_30] 50.319  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0496e7d0>\n",
       "237  2.000  [ex_cusip_masked_series_average_10, ex_cusip_masked_series_average_30] 50.321  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d049578d0>\n",
       "245  2.000  [ex_cusip_masked_series_average_15, ex_cusip_masked_series_average_30] 50.322  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0494a890>\n",
       "249  2.000  [ex_cusip_masked_series_average_20, ex_cusip_masked_series_average_30] 50.326  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0493aed0>\n",
       "121  2.000   [ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_20] 50.518  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d048694d0>\n",
       "185  2.000   [ex_cusip_masked_series_average_2, ex_cusip_masked_series_average_20] 50.526  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04934b10>\n",
       "217  2.000   [ex_cusip_masked_series_average_5, ex_cusip_masked_series_average_20] 50.547  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04900990>\n",
       "233  2.000  [ex_cusip_masked_series_average_10, ex_cusip_masked_series_average_20] 50.549  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d049574d0>\n",
       "241  2.000  [ex_cusip_masked_series_average_15, ex_cusip_masked_series_average_20] 50.550  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0494ae10>\n",
       "113  2.000   [ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_15] 50.709  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04802e90>\n",
       "177  2.000   [ex_cusip_masked_series_average_2, ex_cusip_masked_series_average_15] 50.720  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d048c3390>\n",
       "225  2.000  [ex_cusip_masked_series_average_10, ex_cusip_masked_series_average_15] 50.740  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0496ee10>\n",
       "209  2.000   [ex_cusip_masked_series_average_5, ex_cusip_masked_series_average_15] 50.744  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04911750>\n",
       "97   2.000   [ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_10] 51.102  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d0481d750>\n",
       "161  2.000   [ex_cusip_masked_series_average_2, ex_cusip_masked_series_average_10] 51.119  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d048dc950>\n",
       "193  2.000   [ex_cusip_masked_series_average_5, ex_cusip_masked_series_average_10] 51.143  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04928ed0>\n",
       "65   2.000    [ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_5] 51.729  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04e791d0>\n",
       "129  2.000    [ex_cusip_masked_series_average_2, ex_cusip_masked_series_average_5] 51.748  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04859e10>\n",
       "1    2.000    [ex_cusip_masked_series_average_1, ex_cusip_masked_series_average_2] 52.415  <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f9d04a5dfd0>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df[summary_df.cols.apply(len) == 2].sort_values('MAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "44004d4b-c04b-419f-acb5-7035d3c2c25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "summary_df.groupby('ncols')['MAE'].mean().plot(ax=ax, label='ols');\n",
    "summary_df2.groupby('ncols')['MAE'].mean().plot(ax=ax, label='quantile');\n",
    "ax.set_ylabel('Average MAE');\n",
    "ax.set_xlabel('Number of Columns');\n",
    "ax.set_title('Average MAE using ex_cusip_series_average against ncols');\n",
    "ax.axhline(mean_absolute_error(test_data[\"new_ys\"], np.repeat(train_data[\"new_ys\"].median(), len(test_data))));\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f3b20269-bfa2-4c76-b2f5-751319cbbc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS MAE using features ['ex_cusip_masked_series_average_1', 'ex_cusip_masked_series_average_2', 'ex_cusip_masked_series_average_5', 'ex_cusip_masked_series_average_10', 'ex_cusip_masked_series_average_15', 'ex_cusip_masked_series_average_20', 'ex_cusip_masked_series_average_30', 'ex_cusip_masked_series_average_50']: \n",
      "50.103047\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>new_ys</td>      <th>  R-squared:         </th>  <td>   0.078</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.077</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   7391.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 09 Nov 2023</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:30:33</td>     <th>  Log-Likelihood:    </th> <td>-4.1594e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>703810</td>      <th>  AIC:               </th>  <td>8.319e+06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>703801</td>      <th>  BIC:               </th>  <td>8.319e+06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     8</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                  <td></td>                     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                             <td>   32.2443</td> <td>    0.145</td> <td>  223.015</td> <td> 0.000</td> <td>   31.961</td> <td>   32.528</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_1</th>  <td>    0.0388</td> <td>    0.004</td> <td>   10.536</td> <td> 0.000</td> <td>    0.032</td> <td>    0.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_2</th>  <td>    0.0178</td> <td>    0.005</td> <td>    3.740</td> <td> 0.000</td> <td>    0.008</td> <td>    0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_5</th>  <td>    0.0263</td> <td>    0.005</td> <td>    5.641</td> <td> 0.000</td> <td>    0.017</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_10</th> <td>   -0.0782</td> <td>    0.008</td> <td>  -10.418</td> <td> 0.000</td> <td>   -0.093</td> <td>   -0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_15</th> <td>    0.1017</td> <td>    0.012</td> <td>    8.504</td> <td> 0.000</td> <td>    0.078</td> <td>    0.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_20</th> <td>   -0.1035</td> <td>    0.013</td> <td>   -8.238</td> <td> 0.000</td> <td>   -0.128</td> <td>   -0.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_30</th> <td>    0.0711</td> <td>    0.013</td> <td>    5.643</td> <td> 0.000</td> <td>    0.046</td> <td>    0.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_50</th> <td>    0.4159</td> <td>    0.010</td> <td>   41.187</td> <td> 0.000</td> <td>    0.396</td> <td>    0.436</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>   <td> 0.000</td>    <th>  Jarque-Bera (JB):  </th> <td>145679439806.883</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>            <td>29.757</td>    <th>  Prob(JB):          </th>     <td>    0.00</td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>2231.036</td>   <th>  Cond. No.          </th>     <td>    289.</td>    \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 new_ys   R-squared:                       0.078\n",
       "Model:                            OLS   Adj. R-squared:                  0.077\n",
       "Method:                 Least Squares   F-statistic:                     7391.\n",
       "Date:                Thu, 09 Nov 2023   Prob (F-statistic):               0.00\n",
       "Time:                        20:30:33   Log-Likelihood:            -4.1594e+06\n",
       "No. Observations:              703810   AIC:                         8.319e+06\n",
       "Df Residuals:                  703801   BIC:                         8.319e+06\n",
       "Df Model:                           8                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=====================================================================================================\n",
       "                                        coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------------------\n",
       "const                                32.2443      0.145    223.015      0.000      31.961      32.528\n",
       "ex_cusip_masked_series_average_1      0.0388      0.004     10.536      0.000       0.032       0.046\n",
       "ex_cusip_masked_series_average_2      0.0178      0.005      3.740      0.000       0.008       0.027\n",
       "ex_cusip_masked_series_average_5      0.0263      0.005      5.641      0.000       0.017       0.035\n",
       "ex_cusip_masked_series_average_10    -0.0782      0.008    -10.418      0.000      -0.093      -0.063\n",
       "ex_cusip_masked_series_average_15     0.1017      0.012      8.504      0.000       0.078       0.125\n",
       "ex_cusip_masked_series_average_20    -0.1035      0.013     -8.238      0.000      -0.128      -0.079\n",
       "ex_cusip_masked_series_average_30     0.0711      0.013      5.643      0.000       0.046       0.096\n",
       "ex_cusip_masked_series_average_50     0.4159      0.010     41.187      0.000       0.396       0.436\n",
       "==============================================================================\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):     145679439806.883\n",
       "Skew:                          29.757   Prob(JB):                         0.00\n",
       "Kurtosis:                    2231.036   Cond. No.                         289.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS MAE using features ['ex_cusip_masked_series_average_2', 'ex_cusip_masked_series_average_5', 'ex_cusip_masked_series_average_10', 'ex_cusip_masked_series_average_15', 'ex_cusip_masked_series_average_20', 'ex_cusip_masked_series_average_30', 'ex_cusip_masked_series_average_50']: \n",
      "50.105466\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>new_ys</td>      <th>  R-squared:         </th>  <td>   0.077</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.077</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   8430.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 09 Nov 2023</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:30:33</td>     <th>  Log-Likelihood:    </th> <td>-4.1594e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>703810</td>      <th>  AIC:               </th>  <td>8.319e+06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>703802</td>      <th>  BIC:               </th>  <td>8.319e+06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                  <td></td>                     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                             <td>   32.2468</td> <td>    0.145</td> <td>  223.015</td> <td> 0.000</td> <td>   31.963</td> <td>   32.530</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_2</th>  <td>    0.0599</td> <td>    0.003</td> <td>   23.249</td> <td> 0.000</td> <td>    0.055</td> <td>    0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_5</th>  <td>    0.0166</td> <td>    0.005</td> <td>    3.642</td> <td> 0.000</td> <td>    0.008</td> <td>    0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_10</th> <td>   -0.0753</td> <td>    0.008</td> <td>  -10.034</td> <td> 0.000</td> <td>   -0.090</td> <td>   -0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_15</th> <td>    0.1001</td> <td>    0.012</td> <td>    8.365</td> <td> 0.000</td> <td>    0.077</td> <td>    0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_20</th> <td>   -0.1040</td> <td>    0.013</td> <td>   -8.271</td> <td> 0.000</td> <td>   -0.129</td> <td>   -0.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_30</th> <td>    0.0736</td> <td>    0.013</td> <td>    5.841</td> <td> 0.000</td> <td>    0.049</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_50</th> <td>    0.4178</td> <td>    0.010</td> <td>   41.377</td> <td> 0.000</td> <td>    0.398</td> <td>    0.438</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>   <td> 0.000</td>    <th>  Jarque-Bera (JB):  </th> <td>145489709677.470</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>            <td>29.744</td>    <th>  Prob(JB):          </th>     <td>    0.00</td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>2229.584</td>   <th>  Cond. No.          </th>     <td>    267.</td>    \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 new_ys   R-squared:                       0.077\n",
       "Model:                            OLS   Adj. R-squared:                  0.077\n",
       "Method:                 Least Squares   F-statistic:                     8430.\n",
       "Date:                Thu, 09 Nov 2023   Prob (F-statistic):               0.00\n",
       "Time:                        20:30:33   Log-Likelihood:            -4.1594e+06\n",
       "No. Observations:              703810   AIC:                         8.319e+06\n",
       "Df Residuals:                  703802   BIC:                         8.319e+06\n",
       "Df Model:                           7                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=====================================================================================================\n",
       "                                        coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------------------\n",
       "const                                32.2468      0.145    223.015      0.000      31.963      32.530\n",
       "ex_cusip_masked_series_average_2      0.0599      0.003     23.249      0.000       0.055       0.065\n",
       "ex_cusip_masked_series_average_5      0.0166      0.005      3.642      0.000       0.008       0.026\n",
       "ex_cusip_masked_series_average_10    -0.0753      0.008    -10.034      0.000      -0.090      -0.061\n",
       "ex_cusip_masked_series_average_15     0.1001      0.012      8.365      0.000       0.077       0.124\n",
       "ex_cusip_masked_series_average_20    -0.1040      0.013     -8.271      0.000      -0.129      -0.079\n",
       "ex_cusip_masked_series_average_30     0.0736      0.013      5.841      0.000       0.049       0.098\n",
       "ex_cusip_masked_series_average_50     0.4178      0.010     41.377      0.000       0.398       0.438\n",
       "==============================================================================\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):     145489709677.470\n",
       "Skew:                          29.744   Prob(JB):                         0.00\n",
       "Kurtosis:                    2229.584   Cond. No.                         267.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perform_ols(train_data, \n",
    "            test_data, \n",
    "            train_data.filter(regex='ex_cusip_masked_series_average').columns.tolist()\n",
    "           )\n",
    "\n",
    "perform_ols(train_data, \n",
    "            test_data, \n",
    "            train_data.filter(regex='ex_cusip_masked_series_average').columns.tolist()[1:]\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbfc197-d253-4727-8022-102a3d3b3f95",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Work in progress ideas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753ab4b1-9ab9-496b-a251-30ec50c976c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Simple autoregression with added features:\n",
    "- max & min\n",
    "- within-cusip normalized volatility using cusip-level ARCH?\n",
    "- lifetime volatility (?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "96aa6a60-6a51-49ba-b3e7-738f77cfa2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = multiprocessing.cpu_count()/2\n",
    "# mp  = 4 \n",
    "series_features = {'series_max': lambda data: calculate_masked_func(data, lambda y: y.max(), ex_cusip = True, max_sequence_length = 10, mp = mp),\n",
    "                    'series_min': lambda data: calculate_masked_func(data, lambda y: y.min(), ex_cusip = True, max_sequence_length = 10, mp = mp),\n",
    "                    'series_std': lambda data: calculate_masked_func(data, lambda y: y.median(), ex_cusip = True, max_sequence_length = 10, mp = mp),\n",
    "                    'series_vol': lambda data: calculate_masked_func(data, lambda y: y.std(), ex_cusip = True, max_sequence_length = 10, mp = mp)}\n",
    "\n",
    "combined_features = lambda data: calculate_masked_func(data, lambda y: (y.max(), y.min(), y.median(), y.std()), ex_cusip = True, max_sequence_length = 10, mp = mp)\n",
    "new_cols = list(series_features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2a05956c-6fc0-4a05-8ce5-6dcaf81b2885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 39s, sys: 50.3 s, total: 7min 29s\n",
      "Wall time: 8min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "res = combined_features(df)\n",
    "res.loc[res==0] = [(0,0,0,0) for _ in range(sum(res==0))]\n",
    "df[new_cols] = pd.DataFrame(res.to_list(), index=res.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "cf0d94a7-bc52-4210-9bb0-9b74458edfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in new_cols:\n",
    "    df[col] = df[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5ffd380e-9719-4b16-902c-73cfe8eab12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_cusip_cols = ['ex_cusip_masked_series_average_1',\n",
    "                 'ex_cusip_masked_series_average_2',\n",
    "                 'ex_cusip_masked_series_average_5',\n",
    "                 'ex_cusip_masked_series_average_10',\n",
    "                 'ex_cusip_masked_series_average_15',\n",
    "                 'ex_cusip_masked_series_average_20',\n",
    "                 'ex_cusip_masked_series_average_30',\n",
    "                 'ex_cusip_masked_series_average_50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a4a72a82-d145-443a-9827-68a61cf319d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS MAE using features ['ex_cusip_masked_series_average_1', 'ex_cusip_masked_series_average_2', 'ex_cusip_masked_series_average_5', 'ex_cusip_masked_series_average_10', 'ex_cusip_masked_series_average_15', 'ex_cusip_masked_series_average_20', 'ex_cusip_masked_series_average_30', 'ex_cusip_masked_series_average_50']: \n",
      "50.020145\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>new_ys</td>      <th>  R-squared:         </th>  <td>   0.079</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.079</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   7498.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 13 Nov 2023</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:58:53</td>     <th>  Log-Likelihood:    </th> <td>-4.1590e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>703810</td>      <th>  AIC:               </th>  <td>8.318e+06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>703801</td>      <th>  BIC:               </th>  <td>8.318e+06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     8</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                  <td></td>                     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                             <td>   31.8315</td> <td>    0.145</td> <td>  219.163</td> <td> 0.000</td> <td>   31.547</td> <td>   32.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_1</th>  <td>    0.0399</td> <td>    0.004</td> <td>   10.888</td> <td> 0.000</td> <td>    0.033</td> <td>    0.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_2</th>  <td>    0.0166</td> <td>    0.005</td> <td>    3.492</td> <td> 0.000</td> <td>    0.007</td> <td>    0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_5</th>  <td>    0.0224</td> <td>    0.005</td> <td>    4.838</td> <td> 0.000</td> <td>    0.013</td> <td>    0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_10</th> <td>   -0.0648</td> <td>    0.007</td> <td>   -8.657</td> <td> 0.000</td> <td>   -0.079</td> <td>   -0.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_15</th> <td>    0.0960</td> <td>    0.012</td> <td>    8.070</td> <td> 0.000</td> <td>    0.073</td> <td>    0.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_20</th> <td>   -0.1037</td> <td>    0.013</td> <td>   -8.292</td> <td> 0.000</td> <td>   -0.128</td> <td>   -0.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_30</th> <td>    0.0768</td> <td>    0.012</td> <td>    6.418</td> <td> 0.000</td> <td>    0.053</td> <td>    0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_50</th> <td>    0.4132</td> <td>    0.009</td> <td>   44.830</td> <td> 0.000</td> <td>    0.395</td> <td>    0.431</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>   <td> 0.000</td>    <th>  Jarque-Bera (JB):  </th> <td>146395270104.647</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>            <td>29.805</td>    <th>  Prob(JB):          </th>     <td>    0.00</td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>2236.504</td>   <th>  Cond. No.          </th>     <td>    291.</td>    \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 new_ys   R-squared:                       0.079\n",
       "Model:                            OLS   Adj. R-squared:                  0.079\n",
       "Method:                 Least Squares   F-statistic:                     7498.\n",
       "Date:                Mon, 13 Nov 2023   Prob (F-statistic):               0.00\n",
       "Time:                        23:58:53   Log-Likelihood:            -4.1590e+06\n",
       "No. Observations:              703810   AIC:                         8.318e+06\n",
       "Df Residuals:                  703801   BIC:                         8.318e+06\n",
       "Df Model:                           8                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=====================================================================================================\n",
       "                                        coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------------------\n",
       "const                                31.8315      0.145    219.163      0.000      31.547      32.116\n",
       "ex_cusip_masked_series_average_1      0.0399      0.004     10.888      0.000       0.033       0.047\n",
       "ex_cusip_masked_series_average_2      0.0166      0.005      3.492      0.000       0.007       0.026\n",
       "ex_cusip_masked_series_average_5      0.0224      0.005      4.838      0.000       0.013       0.031\n",
       "ex_cusip_masked_series_average_10    -0.0648      0.007     -8.657      0.000      -0.079      -0.050\n",
       "ex_cusip_masked_series_average_15     0.0960      0.012      8.070      0.000       0.073       0.119\n",
       "ex_cusip_masked_series_average_20    -0.1037      0.013     -8.292      0.000      -0.128      -0.079\n",
       "ex_cusip_masked_series_average_30     0.0768      0.012      6.418      0.000       0.053       0.100\n",
       "ex_cusip_masked_series_average_50     0.4132      0.009     44.830      0.000       0.395       0.431\n",
       "==============================================================================\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):     146395270104.647\n",
       "Skew:                          29.805   Prob(JB):                         0.00\n",
       "Kurtosis:                    2236.504   Cond. No.                         291.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perform_ols(train_data, \n",
    "            test_data, \n",
    "            ex_cusip_cols\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8feac801-2774-4f84-9806-b2562cad2d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS MAE using features ['ex_cusip_masked_series_average_1', 'ex_cusip_masked_series_average_2', 'ex_cusip_masked_series_average_5', 'ex_cusip_masked_series_average_10', 'ex_cusip_masked_series_average_15', 'ex_cusip_masked_series_average_20', 'ex_cusip_masked_series_average_30', 'ex_cusip_masked_series_average_50', 'series_max', 'series_min', 'series_std', 'series_vol']: \n",
      "49.378319\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>new_ys</td>      <th>  R-squared:         </th>  <td>   0.088</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.088</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   5654.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 13 Nov 2023</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:58:46</td>     <th>  Log-Likelihood:    </th> <td>-4.1554e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>703810</td>      <th>  AIC:               </th>  <td>8.311e+06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>703797</td>      <th>  BIC:               </th>  <td>8.311e+06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    12</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                  <td></td>                     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                             <td>   36.0851</td> <td>    0.163</td> <td>  221.891</td> <td> 0.000</td> <td>   35.766</td> <td>   36.404</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_1</th>  <td>    0.0155</td> <td>    0.004</td> <td>    4.214</td> <td> 0.000</td> <td>    0.008</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_2</th>  <td>    0.0380</td> <td>    0.005</td> <td>    8.015</td> <td> 0.000</td> <td>    0.029</td> <td>    0.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_5</th>  <td>    0.0103</td> <td>    0.005</td> <td>    2.218</td> <td> 0.027</td> <td>    0.001</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_10</th> <td>    0.1070</td> <td>    0.013</td> <td>    8.069</td> <td> 0.000</td> <td>    0.081</td> <td>    0.133</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_15</th> <td>    0.0770</td> <td>    0.012</td> <td>    6.466</td> <td> 0.000</td> <td>    0.054</td> <td>    0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_20</th> <td>   -0.1231</td> <td>    0.012</td> <td>   -9.876</td> <td> 0.000</td> <td>   -0.147</td> <td>   -0.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_30</th> <td>    0.0671</td> <td>    0.012</td> <td>    5.634</td> <td> 0.000</td> <td>    0.044</td> <td>    0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_50</th> <td>    0.3251</td> <td>    0.009</td> <td>   35.125</td> <td> 0.000</td> <td>    0.307</td> <td>    0.343</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>series_max</th>                        <td>    0.1101</td> <td>    0.004</td> <td>   26.622</td> <td> 0.000</td> <td>    0.102</td> <td>    0.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>series_min</th>                        <td>    0.0259</td> <td>    0.007</td> <td>    3.885</td> <td> 0.000</td> <td>    0.013</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>series_std</th>                        <td>   -0.0424</td> <td>    0.007</td> <td>   -6.495</td> <td> 0.000</td> <td>   -0.055</td> <td>   -0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>series_vol</th>                        <td>   -0.4072</td> <td>    0.013</td> <td>  -32.488</td> <td> 0.000</td> <td>   -0.432</td> <td>   -0.383</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>   <td> 0.000</td>    <th>  Jarque-Bera (JB):  </th> <td>153555648395.230</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>            <td>30.489</td>    <th>  Prob(JB):          </th>     <td>    0.00</td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>2290.476</td>   <th>  Cond. No.          </th>     <td>    428.</td>    \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 new_ys   R-squared:                       0.088\n",
       "Model:                            OLS   Adj. R-squared:                  0.088\n",
       "Method:                 Least Squares   F-statistic:                     5654.\n",
       "Date:                Mon, 13 Nov 2023   Prob (F-statistic):               0.00\n",
       "Time:                        23:58:46   Log-Likelihood:            -4.1554e+06\n",
       "No. Observations:              703810   AIC:                         8.311e+06\n",
       "Df Residuals:                  703797   BIC:                         8.311e+06\n",
       "Df Model:                          12                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=====================================================================================================\n",
       "                                        coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------------------\n",
       "const                                36.0851      0.163    221.891      0.000      35.766      36.404\n",
       "ex_cusip_masked_series_average_1      0.0155      0.004      4.214      0.000       0.008       0.023\n",
       "ex_cusip_masked_series_average_2      0.0380      0.005      8.015      0.000       0.029       0.047\n",
       "ex_cusip_masked_series_average_5      0.0103      0.005      2.218      0.027       0.001       0.019\n",
       "ex_cusip_masked_series_average_10     0.1070      0.013      8.069      0.000       0.081       0.133\n",
       "ex_cusip_masked_series_average_15     0.0770      0.012      6.466      0.000       0.054       0.100\n",
       "ex_cusip_masked_series_average_20    -0.1231      0.012     -9.876      0.000      -0.147      -0.099\n",
       "ex_cusip_masked_series_average_30     0.0671      0.012      5.634      0.000       0.044       0.090\n",
       "ex_cusip_masked_series_average_50     0.3251      0.009     35.125      0.000       0.307       0.343\n",
       "series_max                            0.1101      0.004     26.622      0.000       0.102       0.118\n",
       "series_min                            0.0259      0.007      3.885      0.000       0.013       0.039\n",
       "series_std                           -0.0424      0.007     -6.495      0.000      -0.055      -0.030\n",
       "series_vol                           -0.4072      0.013    -32.488      0.000      -0.432      -0.383\n",
       "==============================================================================\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):     153555648395.230\n",
       "Skew:                          30.489   Prob(JB):                         0.00\n",
       "Kurtosis:                    2290.476   Cond. No.                         428.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perform_ols(train_data, \n",
    "            test_data, \n",
    "            ex_cusip_cols + new_cols\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cf8b7610-5c2a-4cf0-a4cc-21b977e09696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS MAE using features ['ex_cusip_masked_series_average_1', 'ex_cusip_masked_series_average_10', 'ex_cusip_masked_series_average_50', 'series_max', 'series_min', 'series_std', 'series_vol']: \n",
      "48.116883\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>QuantReg Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>new_ys</td>      <th>  Pseudo R-squared:  </th> <td> 0.08822</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>             <td>QuantReg</td>     <th>  Bandwidth:         </th> <td>   2.798</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>          <td>Least Squares</td>  <th>  Sparsity:          </th> <td>   122.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Tue, 14 Nov 2023</td> <th>  No. Observations:  </th>  <td>703810</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>00:09:37</td>     <th>  Df Residuals:      </th>  <td>703802</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  Df Model:          </th>  <td>     7</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                  <td></td>                     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                             <td>   19.0410</td> <td>    0.112</td> <td>  169.482</td> <td> 0.000</td> <td>   18.821</td> <td>   19.261</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_1</th>  <td>    0.0272</td> <td>    0.001</td> <td>   23.179</td> <td> 0.000</td> <td>    0.025</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_10</th> <td>    0.3108</td> <td>    0.008</td> <td>   39.320</td> <td> 0.000</td> <td>    0.295</td> <td>    0.326</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_cusip_masked_series_average_50</th> <td>    0.3799</td> <td>    0.003</td> <td>  131.636</td> <td> 0.000</td> <td>    0.374</td> <td>    0.386</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>series_max</th>                        <td>    0.0757</td> <td>    0.003</td> <td>   26.659</td> <td> 0.000</td> <td>    0.070</td> <td>    0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>series_min</th>                        <td>   -0.0202</td> <td>    0.005</td> <td>   -4.401</td> <td> 0.000</td> <td>   -0.029</td> <td>   -0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>series_std</th>                        <td>   -0.0425</td> <td>    0.004</td> <td>   -9.443</td> <td> 0.000</td> <td>   -0.051</td> <td>   -0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>series_vol</th>                        <td>   -0.5070</td> <td>    0.009</td> <td>  -58.625</td> <td> 0.000</td> <td>   -0.524</td> <td>   -0.490</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                         QuantReg Regression Results                          \n",
       "==============================================================================\n",
       "Dep. Variable:                 new_ys   Pseudo R-squared:              0.08822\n",
       "Model:                       QuantReg   Bandwidth:                       2.798\n",
       "Method:                 Least Squares   Sparsity:                        122.6\n",
       "Date:                Tue, 14 Nov 2023   No. Observations:               703810\n",
       "Time:                        00:09:37   Df Residuals:                   703802\n",
       "                                        Df Model:                            7\n",
       "=====================================================================================================\n",
       "                                        coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------------------\n",
       "const                                19.0410      0.112    169.482      0.000      18.821      19.261\n",
       "ex_cusip_masked_series_average_1      0.0272      0.001     23.179      0.000       0.025       0.030\n",
       "ex_cusip_masked_series_average_10     0.3108      0.008     39.320      0.000       0.295       0.326\n",
       "ex_cusip_masked_series_average_50     0.3799      0.003    131.636      0.000       0.374       0.386\n",
       "series_max                            0.0757      0.003     26.659      0.000       0.070       0.081\n",
       "series_min                           -0.0202      0.005     -4.401      0.000      -0.029      -0.011\n",
       "series_std                           -0.0425      0.004     -9.443      0.000      -0.051      -0.034\n",
       "series_vol                           -0.5070      0.009    -58.625      0.000      -0.524      -0.490\n",
       "=====================================================================================================\n",
       "\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perform_qreg(train_data, \n",
    "            test_data, \n",
    "            ['ex_cusip_masked_series_average_1', 'ex_cusip_masked_series_average_10', 'ex_cusip_masked_series_average_50'] + new_cols\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103e1714-9699-47b1-8b27-57674351a5d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9748df81-e57c-4521-a049-59835e0f0dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pickle5 as pickle\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2cd6a80-5a00-4ecb-b7c8-f6114cb7a6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "BINARY, CATEGORICAL_FEATURES, NON_CAT_FEATURES = None, None, None\n",
    "\n",
    "def reset_model_features():\n",
    "    '''Function resets the model features, which are global variables, to their original state for convenience when running new architectures'''\n",
    "    global NON_CAT_FEATURES, BINARY, CATEGORICAL_FEATURES\n",
    "    \n",
    "    BINARY = ['callable',\n",
    "          'sinking',\n",
    "          'zerocoupon',\n",
    "          'is_non_transaction_based_compensation',\n",
    "          'is_general_obligation',\n",
    "          'callable_at_cav',\n",
    "          'extraordinary_make_whole_call',\n",
    "          'make_whole_call',\n",
    "          'has_unexpired_lines_of_credit',\n",
    "          'escrow_exists']\n",
    "\n",
    "    CATEGORICAL_FEATURES = ['rating',\n",
    "                            'incorporated_state_code',\n",
    "                            'trade_type',\n",
    "                            'purpose_class',\n",
    "                            'max_ys_ttypes',\n",
    "                            'min_ys_ttypes',\n",
    "                            'max_qty_ttypes',\n",
    "                            'min_ago_ttypes',\n",
    "                            'D_min_ago_ttypes',\n",
    "                            'P_min_ago_ttypes',\n",
    "                            'S_min_ago_ttypes']\n",
    "\n",
    "    NON_CAT_FEATURES = ['quantity',\n",
    "                        'days_to_maturity',\n",
    "                         'days_to_call',\n",
    "                         'coupon',\n",
    "                         'issue_amount',\n",
    "                         'last_seconds_ago',\n",
    "                         'last_yield_spread',\n",
    "                         'days_to_settle',\n",
    "                         'days_to_par',\n",
    "                         'maturity_amount',\n",
    "                         'issue_price',\n",
    "                         'orig_principal_amount',\n",
    "                         'max_amount_outstanding',\n",
    "                         'accrued_days',\n",
    "                         'days_in_interest_payment',\n",
    "                         'A/E',\n",
    "                         'ficc_treasury_spread',\n",
    "                         'max_ys_ys',\n",
    "                         'max_ys_ago',\n",
    "                         'max_ys_qdiff',\n",
    "                         'min_ys_ys',\n",
    "                         'min_ys_ago',\n",
    "                         'min_ys_qdiff',\n",
    "                         'max_qty_ys',\n",
    "                         'max_qty_ago',\n",
    "                         'max_qty_qdiff',\n",
    "                         'min_ago_ys',\n",
    "                         'min_ago_ago',\n",
    "                         'min_ago_qdiff',\n",
    "                         'D_min_ago_ys',\n",
    "                         'D_min_ago_ago',\n",
    "                         'D_min_ago_qdiff',\n",
    "                         'P_min_ago_ys',\n",
    "                         'P_min_ago_ago',\n",
    "                         'P_min_ago_qdiff',\n",
    "                         'S_min_ago_ys',\n",
    "                         'S_min_ago_ago',\n",
    "                         'S_min_ago_qdiff']\n",
    "\n",
    "def modify_features(cols, how, where=None):\n",
    "    global CATEGORICAL_FEATURES, PREDICTORS, NON_CAT_FEATURES, BINARY\n",
    "    \n",
    "    if how not in ['add','remove']:\n",
    "        raise ValueError(\"'how' argument must be one off the following: ['add','remove']\")\n",
    "        \n",
    "    if where not in ['categorical','binary', 'numeric']:\n",
    "        raise ValueError(\"'where' argument must be one off the following: ['categorical','binary', 'numeric']\")\n",
    "    \n",
    "    if not isinstance(cols, list):\n",
    "        raise TypeError(f\"'cols' argument must be a list, received {type(cols)}\")\n",
    "    \n",
    "        \n",
    "    if how == 'remove':\n",
    "        for col in cols: \n",
    "            if col in CATEGORICAL_FEATURES:\n",
    "                CATEGORICAL_FEATURES.remove(col)\n",
    "            if col in BINARY:\n",
    "                BINARY.remove(col)\n",
    "            if col in NON_CAT_FEATURES:\n",
    "                NON_CAT_FEATURES.remove(col)\n",
    "            # if col in PREDICTORS:\n",
    "            #     PREDICTORS.remove(col)\n",
    "            \n",
    "    if how == 'add':\n",
    "        for col in cols: \n",
    "            if col not in CATEGORICAL_FEATURES and where=='categorical':\n",
    "                CATEGORICAL_FEATURES.append(col)\n",
    "            if col not in BINARY and where=='binary':\n",
    "                BINARY.append(col)\n",
    "            if col not in NON_CAT_FEATURES and where=='numeric':\n",
    "                NON_CAT_FEATURES.append(col)\n",
    "            # if col not in PREDICTORS:\n",
    "            #     PREDICTORS.append(col)\n",
    "                \n",
    "reset_model_features()\n",
    "with open('encoders.pkl', 'rb') as f: \n",
    "    encoders = pickle.load(f)\n",
    "fmax = {key: len(value.classes_) for key, value in encoders.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f44b3bb4-26e1-43ad-a85b-937a3f912834",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL AND DATA FUNCTIONS\n",
    "def create_input(df, trade_history_col):\n",
    "    global encoders\n",
    "    datalist = []\n",
    "        \n",
    "    datalist.append(np.stack(df[trade_history_col].to_numpy()))\n",
    "    datalist.append(np.stack(df['target_attention_features'].to_numpy()))\n",
    "\n",
    "    noncat_and_binary = []\n",
    "    for f in NON_CAT_FEATURES + BINARY:\n",
    "        noncat_and_binary.append(np.expand_dims(df[f].to_numpy().astype('float64'), axis=1))\n",
    "    datalist.append(np.concatenate(noncat_and_binary, axis=-1))\n",
    "    \n",
    "    for f in CATEGORICAL_FEATURES:\n",
    "        encoded = encoders[f].transform(df[f])\n",
    "        datalist.append(encoded.astype('float32'))\n",
    "    \n",
    "    return datalist\n",
    "\n",
    "def generate_model_default(TRADE_SEQUENCE_LENGTH, trade_history_normalizer, noncat_binary_normalizer):\n",
    "    inputs = []\n",
    "    layer = []\n",
    "    \n",
    "    trade_history_input = layers.Input(name=\"trade_history_input\", \n",
    "                                       shape=(TRADE_SEQUENCE_LENGTH, NUM_FEATURES), \n",
    "                                       dtype = tf.float32) \n",
    "\n",
    "    target_attention_input = layers.Input(name=\"target_attention_input\", \n",
    "                                       shape=(1, 3), \n",
    "                                       dtype = tf.float32) \n",
    "    inputs.append(trade_history_input)\n",
    "    inputs.append(target_attention_input)\n",
    "\n",
    "    inputs.append(layers.Input(\n",
    "        name=\"NON_CAT_AND_BINARY_FEATURES\",\n",
    "        shape=(len(NON_CAT_FEATURES + BINARY),)\n",
    "    ))\n",
    "\n",
    "\n",
    "    layer.append(noncat_binary_normalizer(inputs[2]))\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "    ############## TRADE HISTORY MODEL #################\n",
    "\n",
    "    lstm_layer = layers.Bidirectional(layers.LSTM(50, \n",
    "                             activation='tanh',\n",
    "                             input_shape=(TRADE_SEQUENCE_LENGTH,NUM_FEATURES),\n",
    "                             return_sequences = True,\n",
    "                             name='LSTM'))\n",
    "\n",
    "    lstm_layer_2 = layers.Bidirectional(layers.LSTM(100, \n",
    "                                                    activation='tanh',\n",
    "                                                    input_shape=(TRADE_SEQUENCE_LENGTH, 50),\n",
    "                                                    return_sequences = True,\n",
    "                                                    name='LSTM_2'))\n",
    "\n",
    "\n",
    "\n",
    "    features = lstm_layer(trade_history_normalizer(inputs[0]))\n",
    "    features = lstm_layer_2(features)  \n",
    "    \n",
    "    \n",
    "    attention_sequence = layers.Dense(200, activation='relu', name='attention_dense')(target_attention_input)\n",
    "    attention = layers.Dot(axes=[2, 2])([features, attention_sequence])\n",
    "    attention = layers.Activation('softmax')(attention)\n",
    "\n",
    "    context_vector = layers.Dot(axes=[1, 1])([features, attention])\n",
    "    context_vector = layers.Flatten(name='context_vector_flatten')(context_vector)\n",
    "\n",
    "\n",
    "    trade_history_output = layers.Dense(100, \n",
    "                                        activation='relu')(context_vector)\n",
    "    \n",
    " \n",
    "    ####################################################\n",
    "\n",
    "    ############## REFERENCE DATA MODEL ################\n",
    "    global encoders\n",
    "    for f in CATEGORICAL_FEATURES:\n",
    "        \n",
    "        fin = layers.Input(shape=(1,), name = f)\n",
    "        inputs.append(fin)\n",
    "        embedded = layers.Flatten(name = f + \"_flat\")( layers.Embedding(input_dim = fmax[f],\n",
    "                                                                        output_dim = max(30,int(np.sqrt(fmax[f]))),\n",
    "                                                                        input_length= 1,\n",
    "                                                                        name = f + \"_embed\")(fin))\n",
    "        layer.append(embedded)\n",
    "\n",
    "\n",
    "    reference_hidden = layers.Dense(400,\n",
    "                                    activation='relu',\n",
    "                                    name='reference_hidden_1')(layers.concatenate(layer, axis=-1))\n",
    "    reference_hidden = layers.BatchNormalization()(reference_hidden)\n",
    "    reference_hidden = layers.Dropout(DROPOUT)(reference_hidden)\n",
    "\n",
    "    reference_hidden2 = layers.Dense(200,activation='relu',name='reference_hidden_2')(reference_hidden)\n",
    "    reference_hidden2 = layers.BatchNormalization()(reference_hidden2)\n",
    "    reference_hidden2 = layers.Dropout(DROPOUT)(reference_hidden2)\n",
    "\n",
    "    reference_output = layers.Dense(100,activation='relu',name='reference_hidden_3')(reference_hidden2)\n",
    "    ####################################################\n",
    "\n",
    "    feed_forward_input = layers.concatenate([reference_output, trade_history_output])\n",
    "    \n",
    "    hidden = layers.Dense(300,activation='relu')(feed_forward_input)\n",
    "    hidden = layers.BatchNormalization()(hidden)\n",
    "    hidden = layers.Dropout(DROPOUT)(hidden)\n",
    "\n",
    "    hidden2 = layers.Dense(100,activation='relu')(hidden)\n",
    "    hidden2 = layers.BatchNormalization()(hidden2)\n",
    "    hidden2 = layers.Dropout(DROPOUT)(hidden2)\n",
    "\n",
    "    final = layers.Dense(1)(hidden2)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=final)\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "          loss=keras.losses.MeanAbsoluteError())\n",
    "    \n",
    "    return model\n",
    "    \n",
    "\n",
    "def generate_model_bottleneck(TRADE_SEQUENCE_LENGTH, trade_history_normalizer, noncat_binary_normalizer):\n",
    "    inputs = []\n",
    "    layer = []\n",
    "    \n",
    "    trade_history_input = layers.Input(name=\"trade_history_input\", \n",
    "                                       shape=(TRADE_SEQUENCE_LENGTH, NUM_FEATURES), \n",
    "                                       dtype = tf.float32) \n",
    "\n",
    "    target_attention_input = layers.Input(name=\"target_attention_input\", \n",
    "                                       shape=(1, 3), \n",
    "                                       dtype = tf.float32) \n",
    "    inputs.append(trade_history_input)\n",
    "    inputs.append(target_attention_input)\n",
    "\n",
    "    inputs.append(layers.Input(\n",
    "        name=\"NON_CAT_AND_BINARY_FEATURES\",\n",
    "        shape=(len(NON_CAT_FEATURES + BINARY),)\n",
    "    ))\n",
    "\n",
    "\n",
    "    layer.append(noncat_binary_normalizer(inputs[2]))\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "    ############## TRADE HISTORY MODEL #################\n",
    "\n",
    "    lstm_layer = layers.Bidirectional(layers.LSTM(50, \n",
    "                             activation='tanh',\n",
    "                             input_shape=(TRADE_SEQUENCE_LENGTH,NUM_FEATURES),\n",
    "                             return_sequences = True,\n",
    "                             name='LSTM'))\n",
    "\n",
    "    lstm_layer_2 = layers.Bidirectional(layers.LSTM(100, \n",
    "                                                    activation='tanh',\n",
    "                                                    input_shape=(TRADE_SEQUENCE_LENGTH, 50),\n",
    "                                                    return_sequences = True,\n",
    "                                                    name='LSTM_2'))\n",
    "\n",
    "\n",
    "\n",
    "    features = lstm_layer(trade_history_normalizer(inputs[0]))\n",
    "    features = lstm_layer_2(features)  \n",
    "    \n",
    "    \n",
    "    attention_sequence = layers.Dense(200, activation='relu', name='attention_dense')(target_attention_input)\n",
    "    attention = layers.Dot(axes=[2, 2])([features, attention_sequence])\n",
    "    attention = layers.Activation('softmax')(attention)\n",
    "\n",
    "    context_vector = layers.Dot(axes=[1, 1])([features, attention])\n",
    "    context_vector = layers.Flatten(name='context_vector_flatten')(context_vector)\n",
    "\n",
    "\n",
    "    trade_history_output = layers.Dense(100, \n",
    "                                        activation='relu')(context_vector)\n",
    "    \n",
    "    trade_history_output = layers.Dense(1)(trade_history_output)\n",
    " \n",
    "    ####################################################\n",
    "\n",
    "    ############## REFERENCE DATA MODEL ################\n",
    "    global encoders\n",
    "    for f in CATEGORICAL_FEATURES:\n",
    "        \n",
    "        fin = layers.Input(shape=(1,), name = f)\n",
    "        inputs.append(fin)\n",
    "        embedded = layers.Flatten(name = f + \"_flat\")( layers.Embedding(input_dim = fmax[f],\n",
    "                                                                        output_dim = max(30,int(np.sqrt(fmax[f]))),\n",
    "                                                                        input_length= 1,\n",
    "                                                                        name = f + \"_embed\")(fin))\n",
    "        layer.append(embedded)\n",
    "\n",
    "\n",
    "    reference_hidden = layers.Dense(300,\n",
    "                                    activation='relu',\n",
    "                                    name='reference_hidden_1')(layers.concatenate(layer, axis=-1))\n",
    "    reference_hidden = layers.BatchNormalization()(reference_hidden)\n",
    "    reference_hidden = layers.Dropout(DROPOUT)(reference_hidden)\n",
    "\n",
    "    reference_hidden2 = layers.Dense(200,activation='relu',name='reference_hidden_2')(reference_hidden)\n",
    "    reference_hidden2 = layers.BatchNormalization()(reference_hidden2)\n",
    "    reference_hidden2 = layers.Dropout(DROPOUT)(reference_hidden2)\n",
    "\n",
    "    reference_hidden3 = layers.Dense(100,activation='relu',name='reference_hidden_3')(reference_hidden2)\n",
    "    reference_hidden3 = layers.BatchNormalization()(reference_hidden3)\n",
    "    reference_hidden3 = layers.Dropout(DROPOUT)(reference_hidden3)\n",
    "    \n",
    "    reference_output = layers.Dense(1, name='reference_output')(reference_hidden3)\n",
    "    ####################################################\n",
    "\n",
    "    feed_forward_input = layers.concatenate([reference_output, trade_history_output])\n",
    "    \n",
    "    final = layers.Dense(1)(feed_forward_input)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=final)\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "          loss=keras.losses.MeanAbsoluteError())\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generate_model_ensemble(TRADE_SEQUENCE_LENGTH, trade_history_normalizer, noncat_binary_normalizer, ensemble_size):\n",
    "    models = []\n",
    "    for i in range(ensemble_size):\n",
    "        if i < ensemble_size/2:\n",
    "            models.append(generate_model_default(TRADE_SEQUENCE_LENGTH, trade_history_normalizer, noncat_binary_normalizer))\n",
    "        else:\n",
    "            models.append(generate_model_bottleneck(TRADE_SEQUENCE_LENGTH, trade_history_normalizer, noncat_binary_normalizer))\n",
    "            \n",
    "    input_layer = models[0].input\n",
    "    output_list = [model(input_layer) for model in models]\n",
    "    ensemble_model = keras.Model(inputs = input_layer, outputs = output_list)\n",
    "    ensemble_model.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE), \n",
    "                 loss=[keras.losses.MeanAbsoluteError() for i in range(ensemble_size)])\n",
    "\n",
    "    return ensemble_model\n",
    "\n",
    "def create_data_set_and_model(train_dataframe, test_dataframe, trade_history_col):\n",
    "    \n",
    "    if not isinstance(trade_history_col, str):\n",
    "        raise ValueError('trade_history_col must be a string')\n",
    "    \n",
    "    TRADE_SEQUENCE_LENGTH = train_dataframe[trade_history_col][0].shape[0] \n",
    "    \n",
    "    params = {'TRADE_SEQUENCE_LENGTH':TRADE_SEQUENCE_LENGTH}\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    val_idx = np.random.choice(range(len(train_dataframe)), \n",
    "                     size = int(VALIDATION_SPLIT*len(train_dataframe)),\n",
    "                     replace=False)\n",
    "\n",
    "    print(f'TRAINING DATA: N = {len(train_dataframe)-len(val_idx)}, MIN DATE = {train_dataframe.drop(val_idx, axis=0).trade_date.min()}, MAX DATE = {train_dataframe.drop(val_idx, axis=0).trade_date.max()}')\n",
    "    print(f'VALIDATION DATA: N = {len(val_idx)}, MIN DATE = {train_dataframe.iloc[val_idx].trade_date.min()}, MAX DATE = {train_dataframe.iloc[val_idx].trade_date.max()}')\n",
    "    print(f'TEST DATA: N = {len(test_dataframe)}, MIN DATE = {test_dataframe.trade_date.min()}, MAX DATE = {test_dataframe.trade_date.max()}')\n",
    "\n",
    "    x_train = create_input(train_dataframe.drop(val_idx, axis=0), trade_history_col)\n",
    "    y_train = train_dataframe.drop(val_idx, axis=0)[target_variable]\n",
    "\n",
    "    x_val = create_input(train_dataframe.iloc[val_idx], trade_history_col)\n",
    "    y_val = train_dataframe.iloc[val_idx][target_variable]\n",
    "\n",
    "    x_test = create_input(test_dataframe, trade_history_col)\n",
    "    y_test = test_dataframe[target_variable]    \n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        # Normalization layer for the trade history\n",
    "        trade_history_normalizer = Normalization(name='Trade_history_normalizer')\n",
    "        trade_history_normalizer.adapt(x_train[0],batch_size=BATCH_SIZE)\n",
    "\n",
    "        # Normalization layer for the non-categorical and binary features\n",
    "        noncat_binary_normalizer = Normalization(name='Numerical_binary_normalizer')\n",
    "        noncat_binary_normalizer.adapt(x_train[2], batch_size = BATCH_SIZE)\n",
    "\n",
    "    normalizers = {'trade_history_normalizer': trade_history_normalizer,\n",
    "                  'noncat_binary_normalizer': noncat_binary_normalizer}\n",
    "\n",
    "    return  params, normalizers, x_train, y_train, x_val, y_val, x_test, y_test, val_idx\n",
    "\n",
    "\n",
    "def create_tf_data(x_train, y_train, shuffle=False, shuffle_buffer=1):\n",
    "                     \n",
    "    X=()\n",
    "    for x in x_train:\n",
    "        X += (tf.data.Dataset.from_tensor_slices(x),)\n",
    "        \n",
    "\n",
    "    temp = tf.data.Dataset.zip((X))\n",
    "    del X\n",
    "    dataset = tf.data.Dataset.zip((temp,\n",
    "                        tf.data.Dataset.from_tensor_slices(y_train)))\n",
    "    del temp\n",
    "    if shuffle:\n",
    "        shuffle_buffer = int(len(x_train[0])*shuffle_buffer)\n",
    "        dataset = dataset.shuffle(shuffle_buffer)\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "def train_model(params, normalizers, x_train, y_train, x_val, y_val, shuffle, shuffle_buffer=1):\n",
    "\n",
    "    TRADE_SEQUENCE_LENGTH = params.get('TRADE_SEQUENCE_LENGTH')\n",
    "    trade_history_normalizer = normalizers.get('trade_history_normalizer')\n",
    "    noncat_binary_normalizer = normalizers.get('noncat_binary_normalizer')\n",
    "       \n",
    "    tf.keras.utils.set_random_seed(10)\n",
    "    \n",
    "    if model_to_use == 'default':\n",
    "        model = generate_model_default(TRADE_SEQUENCE_LENGTH,trade_history_normalizer, noncat_binary_normalizer)\n",
    "        \n",
    "    elif model_to_use=='bottleneck': \n",
    "        model = generate_model_bottleneck(TRADE_SEQUENCE_LENGTH, trade_history_normalizer, noncat_binary_normalizer)\n",
    "        \n",
    "    elif model_to_use=='ensemble': \n",
    "        model = generate_model_ensemble(TRADE_SEQUENCE_LENGTH, trade_history_normalizer, noncat_binary_normalizer, ensemble_size)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f'Invalid model specified, {model_to_use}')\n",
    "        \n",
    "        \n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H-%M')\n",
    "    \n",
    "    fit_callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=10,\n",
    "        verbose=0,\n",
    "        mode=\"auto\",\n",
    "        restore_best_weights=True),\n",
    "        ]\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        train_ds = create_tf_data(x_train, y_train, shuffle, shuffle_buffer)\n",
    "        train_ds = train_ds.batch(BATCH_SIZE).prefetch(2).cache()\n",
    "        val_ds = create_tf_data(x_val, y_val, shuffle = False)\n",
    "        val_ds = val_ds.batch(BATCH_SIZE).prefetch(2).cache()\n",
    "\n",
    "    history= model.fit(train_ds,\n",
    "                                      validation_data=val_ds,\n",
    "                                        epochs=NUM_EPOCHS,     \n",
    "                                        verbose=1, \n",
    "                                        callbacks=fit_callbacks,\n",
    "                                        use_multiprocessing=True,\n",
    "                                        workers=8)\n",
    "    \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "997c074b-e2ea-47d4-ac6e-463b86dec3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_experiment(experiment_dir, parameters, model):\n",
    "    if not os.path.isdir(experiment_dir):\n",
    "        print(f'{experiment_dir} does not exist. Creating now')\n",
    "        os.makedirs(experiment_dir)\n",
    "    \n",
    "    if not os.path.isdir(os.path.join(experiment_dir, 'model')):\n",
    "        os.makedirs(os.path.join(experiment_dir, 'model'))\n",
    "    \n",
    "    parameters_path = os.path.join(experiment_dir, 'parameters.json')\n",
    "    \n",
    "    with open(parameters_path, 'w') as metadata_file:\n",
    "        json.dump(parameters, metadata_file)\n",
    "\n",
    "    i = 0\n",
    "    model_path = f'model/model_{i}'\n",
    "    while os.path.isdir(os.path.join(experiment_dir, model_path)):\n",
    "        i+=1\n",
    "        model_path = f'model/model_{i}'\n",
    "    model.save(os.path.join(experiment_dir, model_path))\n",
    "    print(f'Model saved to {os.path.join(experiment_dir, model_path)}')\n",
    "    \n",
    "def run_experiment(n_runs, experiment_prefix = None):\n",
    "    tz=pytz.timezone('US/Pacific')\n",
    "    timestamp = datetime.now(tz).strftime(\"%Y-%m-%d_%H:%M\")\n",
    "    results = []\n",
    "    predictions = []\n",
    "    \n",
    "    if experiment_prefix:\n",
    "        experiment_dir = f'experiments/{experiment_prefix}/{target_variable}_experiment_{timestamp}'\n",
    "    else:\n",
    "        experiment_dir = f'experiments/{target_variable}_experiment_{timestamp}'\n",
    "\n",
    "    parameters = {'train_start':train_start,\n",
    "                    'train_end':train_end,\n",
    "                    'test_start':test_start,\n",
    "                    'test_end':test_end,\n",
    "                    'train_size':len(train_dataframe),\n",
    "                    'test_size':len(test_dataframe),\n",
    "                    'VALIDATION_SPLIT':VALIDATION_SPLIT,\n",
    "                    'LEARNING_RATE':LEARNING_RATE,\n",
    "                    'BATCH_SIZE':BATCH_SIZE,\n",
    "                    'NUM_EPOCHS':NUM_EPOCHS,\n",
    "                    'DROPOUT':DROPOUT,\n",
    "                    'TRADE_SEQUENCE_LENGTH':TRADE_SEQUENCE_LENGTH,\n",
    "                    'NUM_FEATURES':NUM_FEATURES,\n",
    "                    'target_variable':target_variable,\n",
    "                    'features':BINARY+NON_CAT_FEATURES+CATEGORICAL_FEATURES,\n",
    "                    'shuffle_buffer':shuffle_buffer,\n",
    "                    'use_bottleneck_model':use_bottleneck_model}\n",
    " \n",
    "    print(f'Experiment results will be saved to {experiment_dir}\\n')\n",
    "    print(f'Performing {n_runs} runs with parameters: {parameters}')\n",
    "    for i in range(n_runs):\n",
    "        tf.keras.utils.set_random_seed(10)\n",
    "        history, model = train_model(params, normalizers, x_train, y_train, x_val, y_val, True, shuffle_buffer = shuffle_buffer)\n",
    "        pred = model.predict(x_test, batch_size=10000)\n",
    "        predictions.append(pred)\n",
    "        print('='*25+f' TRIAL {i}, MAE: {mean_absolute_error(pred,y_test)} '+'='*25)\n",
    "        results.append([history, model])\n",
    "        log_experiment(experiment_dir, parameters, model)\n",
    "    \n",
    "    with open(os.path.join(experiment_dir, 'predictions.pkl'), 'wb') as f: \n",
    "        pickle.dump(predictions, f)\n",
    "        \n",
    "    with open(os.path.join(experiment_dir, 'results.pkl'), 'wb') as f: \n",
    "        pickle.dump([x[0] for x in results], f)\n",
    "        \n",
    "    return results\n",
    "\n",
    "\n",
    "def addflag(flag, condition, name):\n",
    "    empty = flag == \"none\"\n",
    "    flag[condition & empty] = name\n",
    "    flag[condition & ~empty] = flag[condition & ~empty] + \" & \" + name\n",
    "    \n",
    "def addcol(data, newname, newvals, warn=False):\n",
    "    if newname in data.columns:\n",
    "        if warn: print( f\"Warning: replacing duplicate column {newname}\" )\n",
    "        data[newname] = newvals\n",
    "    else:\n",
    "        newcol = pd.Series(newvals, index = data.index, name=newname)\n",
    "        data = pd.concat([data,newcol],axis=1)\n",
    "    return data\n",
    "\n",
    "def mkcases(df):\n",
    "    flag = pd.Series(\"none\", index=df.index)\n",
    "\n",
    "    addflag(flag, df.last_yield.isna(), \"no last yld\")\n",
    "    addflag(flag, df.last_yield < 150, \"last yld < 1.5%\")\n",
    "    addflag(flag, df.last_yield.between(150,700), \"1.5% <= last yld <= 7%\")\n",
    "    addflag(flag, df.last_yield > 700, \"last yld > 7%\")\n",
    "    addflag(flag, df.when_issued, \"when issued\")\n",
    "    \n",
    "    print( flag.value_counts(dropna=False) )\n",
    "    return flag.astype('category')\n",
    "\n",
    "def mean_absolute_deviation(pred, truth):\n",
    "    pred, truth = np.array(pred).reshape(-1,1), np.array(truth).reshape(-1,1)\n",
    "    err = abs(pred - truth)\n",
    "    return np.median(err)\n",
    "\n",
    "def compare_mae(df, prediction_cols, groupby_cols, target_variable):\n",
    "    \n",
    "    if not isinstance(prediction_cols, list):\n",
    "        raise TypeError(f'prediction_cols must be a list, got {type(prediction_cols)}, {type(groupby_cols)} instead')\n",
    "    \n",
    "    if groupby_cols and not isinstance(groupby_cols, list):\n",
    "        raise TypeError(f'groupby_cols must be a list or None, got {type(groupby_cols)} instead')\n",
    "    \n",
    "    print(f'{f\" Analysis for target: {target_variable} \":=^75}')\n",
    "    \n",
    "    nan_counts = df[prediction_cols].isna().sum() \n",
    "    \n",
    "    for x,y  in df[prediction_cols].isna().sum().iteritems():\n",
    "        print(f'Prediction col {x} has {y} nan values')\n",
    "    \n",
    "    df = df.dropna(subset=prediction_cols)\n",
    "\n",
    "    if groupby_cols:\n",
    "        temp = df[[target_variable, 'cases'] + prediction_cols + groupby_cols]\\\n",
    "                .groupby(groupby_cols, observed=True)\\\n",
    "                .apply(lambda x: [mean_absolute_error(x[target_variable], x[col]) for col in prediction_cols] + \\\n",
    "                        [mean_absolute_deviation(x[target_variable], x[col]) for col in prediction_cols] + [len(x)])   \n",
    "        temp = pd.DataFrame(temp.to_list(), index = zip(['Overall']*len(temp),temp.index))\n",
    "\n",
    "        temp2 = df[[target_variable, 'cases'] + prediction_cols + groupby_cols]\\\n",
    "                .groupby(['cases']+ groupby_cols, observed=True)\\\n",
    "                .apply(lambda x: [mean_absolute_error(x[target_variable], x[col]) for col in prediction_cols] + \\\n",
    "                        [mean_absolute_deviation(x[target_variable], x[col]) for col in prediction_cols] + [len(x)])   \n",
    "        temp2 = pd.DataFrame(temp2.to_list(), index = temp2.index)\n",
    "        summary = pd.concat([temp, temp2], axis=0)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        temp2 = df[[target_variable, 'cases'] + prediction_cols]\\\n",
    "                .groupby('cases', observed=True)\\\n",
    "                .apply(lambda x: [mean_absolute_error(x[target_variable], x[col]) for col in prediction_cols] + \\\n",
    "                        [mean_absolute_deviation(x[target_variable], x[col]) for col in prediction_cols] + [len(x)])   \n",
    "        temp2 = pd.DataFrame(temp2.to_list(), index = temp2.index)\n",
    "        \n",
    "        temp = pd.DataFrame([mean_absolute_error(df[target_variable], df[col]) for col in prediction_cols] + \\\n",
    "                        [mean_absolute_deviation(df[target_variable], df[col]) for col in prediction_cols] + [len(df)], columns=['Overall']).T\n",
    "    \n",
    "    summary = pd.concat([temp, temp2], axis=0)  \n",
    "    mae_col = ['MAE']*len(prediction_cols)\n",
    "    mad_col = ['MAD']*len(prediction_cols)\n",
    "    columns= list(zip(mae_col, prediction_cols)) + list(zip(mad_col, prediction_cols)) + [('', 'N')]\n",
    "    summary.columns=pd.MultiIndex.from_tuples(columns)\n",
    "    \n",
    "    if groupby_cols:\n",
    "        summary.index=pd.MultiIndex.from_tuples(summary.index, names = ['cases']+groupby_cols)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    summary[('', 'N')] = summary[('', 'N')].astype(int)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "957c4110-3a86-440d-8044-5ccda2cb2b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = '2023-01-01'\n",
    "train_end = '2023-01-29'\n",
    "test_start = '2023-01-29'\n",
    "test_end = '2023-01-31'\n",
    "\n",
    "VALIDATION_SPLIT = 0.1\n",
    "LEARNING_RATE = 0.0007\n",
    "BATCH_SIZE = 10000\n",
    "NUM_EPOCHS = 75 \n",
    "DROPOUT = 0.1 \n",
    "TRADE_SEQUENCE_LENGTH = 5\n",
    "NUM_FEATURES = 6\n",
    "target_variable = 'new_ys' \n",
    "trade_history_col = 'trade_history'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a6ce7742",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filter = (df.trade_date < train_end) & (df.trade_date >= train_start)\n",
    "test_filter = (df.trade_date >= test_start) & (df.trade_date <test_end)\n",
    "\n",
    "train_dataframe = df[train_filter]\\\n",
    ".sort_values(by='trade_date', ascending=True)\\\n",
    ".reset_index(drop=True).copy()\n",
    "\n",
    "test_dataframe = df[test_filter]\\\n",
    ".sort_values(by='trade_date', ascending=True)\\\n",
    ".reset_index(drop=True).copy()\n",
    "\n",
    "train_dataframe['last_seconds_ago'] = train_dataframe['last_seconds_ago'].fillna(0)\n",
    "train_dataframe['last_yield_spread'] = train_dataframe['last_yield_spread'].fillna(0)\n",
    "\n",
    "test_dataframe['last_seconds_ago'] = test_dataframe['last_seconds_ago'].fillna(0)\n",
    "test_dataframe['last_yield_spread'] = test_dataframe['last_yield_spread'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9986d0ed-2ba3-4c0e-a05d-324abb691eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA: N = 673339, MIN DATE = 2023-01-03 00:00:00, MAX DATE = 2023-01-27 00:00:00\n",
      "VALIDATION DATA: N = 74815, MIN DATE = 2023-01-03 00:00:00, MAX DATE = 2023-01-27 00:00:00\n",
      "TEST DATA: N = 38521, MIN DATE = 2023-01-30 00:00:00, MAX DATE = 2023-01-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "reset_model_features()\n",
    "modify_features(ex_cusip_cols + new_cols, 'add', 'numeric')\n",
    "params, normalizers, x_train, y_train, x_val, y_val, x_test, y_test, val_idx = create_data_set_and_model(train_dataframe, \n",
    "                                                                                                         test_dataframe, \n",
    "                                                                                                         trade_history_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5d7932bd-f2a0-4853-9e81-2c8ec13eb237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "(673339, 5, 6)\n",
      "(673339, 1, 3)\n",
      "(673339, 60)\n",
      "(673339,)\n",
      "(673339,)\n",
      "(673339,)\n",
      "(673339,)\n",
      "(673339,)\n",
      "(673339,)\n",
      "(673339,)\n",
      "(673339,)\n",
      "(673339,)\n",
      "(673339,)\n",
      "(673339,)\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))\n",
    "for i in range(len(x_train)):\n",
    "    print(x_train[i].shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "00cd4c7e-6d52-40bd-99b8-5fc2d0b90559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median baseline MAE: 51.868274\n",
      "qreg baseline MAE: 47.098567\n",
      "qreg baseline + ['series_max', 'series_min', 'series_std', 'series_vol'] MAE: 46.492946\n"
     ]
    }
   ],
   "source": [
    "median_baseline = mean_absolute_error(test_dataframe[\"new_ys\"], np.repeat(train_dataframe[\"new_ys\"].median(), len(test_dataframe)))\n",
    "_, mae1 = perform_qreg(train_dataframe, \n",
    "            test_dataframe, \n",
    "            ex_cusip_cols,\n",
    "             print_result=False,\n",
    "             return_vals = True\n",
    "           )\n",
    "\n",
    "_, mae2 = perform_qreg(train_dataframe, \n",
    "            test_dataframe, \n",
    "            ex_cusip_cols + new_cols,\n",
    "             print_result=False,\n",
    "             return_vals = True\n",
    "           )\n",
    "\n",
    "print(f'median baseline MAE: {median_baseline:2f}')\n",
    "print(f'qreg baseline MAE: {mae1:2f}')\n",
    "print(f'qreg baseline + {new_cols} MAE: {mae2:2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f58ca39-75c8-4f68-94dd-a30ce2d110bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment results will be saved to experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-13_16:47\n",
      "\n",
      "Performing 1 runs with parameters: {'train_start': '2023-01-01', 'train_end': '2023-01-29', 'test_start': '2023-01-29', 'test_end': '2023-01-31', 'train_size': 748154, 'test_size': 38521, 'VALIDATION_SPLIT': 0.1, 'LEARNING_RATE': 0.0007, 'BATCH_SIZE': 10000, 'NUM_EPOCHS': 150, 'DROPOUT': 0.1, 'TRADE_SEQUENCE_LENGTH': 5, 'NUM_FEATURES': 6, 'target_variable': 'new_ys', 'features': ['callable', 'sinking', 'zerocoupon', 'is_non_transaction_based_compensation', 'is_general_obligation', 'callable_at_cav', 'extraordinary_make_whole_call', 'make_whole_call', 'has_unexpired_lines_of_credit', 'escrow_exists', 'quantity', 'days_to_maturity', 'days_to_call', 'coupon', 'issue_amount', 'last_seconds_ago', 'last_yield_spread', 'days_to_settle', 'days_to_par', 'maturity_amount', 'issue_price', 'orig_principal_amount', 'max_amount_outstanding', 'accrued_days', 'days_in_interest_payment', 'A/E', 'ficc_treasury_spread', 'max_ys_ys', 'max_ys_ago', 'max_ys_qdiff', 'min_ys_ys', 'min_ys_ago', 'min_ys_qdiff', 'max_qty_ys', 'max_qty_ago', 'max_qty_qdiff', 'min_ago_ys', 'min_ago_ago', 'min_ago_qdiff', 'D_min_ago_ys', 'D_min_ago_ago', 'D_min_ago_qdiff', 'P_min_ago_ys', 'P_min_ago_ago', 'P_min_ago_qdiff', 'S_min_ago_ys', 'S_min_ago_ago', 'S_min_ago_qdiff', 'ex_cusip_masked_series_average_1', 'ex_cusip_masked_series_average_2', 'ex_cusip_masked_series_average_5', 'ex_cusip_masked_series_average_10', 'ex_cusip_masked_series_average_15', 'ex_cusip_masked_series_average_20', 'ex_cusip_masked_series_average_30', 'ex_cusip_masked_series_average_50', 'series_max', 'series_min', 'series_std', 'series_vol', 'rating', 'incorporated_state_code', 'trade_type', 'purpose_class', 'max_ys_ttypes', 'min_ys_ttypes', 'max_qty_ttypes', 'min_ago_ttypes', 'D_min_ago_ttypes', 'P_min_ago_ttypes', 'S_min_ago_ttypes'], 'shuffle_buffer': 0.75, 'use_bottleneck_model': False}\n",
      "Epoch 1/150\n",
      "68/68 [==============================] - 37s 276ms/step - loss: 58.5963 - val_loss: 55.4102\n",
      "Epoch 2/150\n",
      "68/68 [==============================] - 6s 81ms/step - loss: 55.9216 - val_loss: 48.4232\n",
      "Epoch 3/150\n",
      "68/68 [==============================] - 6s 83ms/step - loss: 53.5658 - val_loss: 44.5612\n",
      "Epoch 4/150\n",
      "68/68 [==============================] - 6s 83ms/step - loss: 50.5188 - val_loss: 41.1603\n",
      "Epoch 5/150\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 46.5542 - val_loss: 40.5329\n",
      "Epoch 6/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 41.5714 - val_loss: 39.6599\n",
      "Epoch 7/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 35.5977 - val_loss: 39.2299\n",
      "Epoch 8/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 29.0415 - val_loss: 28.4084\n",
      "Epoch 9/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 22.7425 - val_loss: 20.1346\n",
      "Epoch 10/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 17.7539 - val_loss: 15.7446\n",
      "Epoch 11/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 14.9308 - val_loss: 14.1203\n",
      "Epoch 12/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 13.7764 - val_loss: 13.4845\n",
      "Epoch 13/150\n",
      "68/68 [==============================] - 5s 77ms/step - loss: 13.2977 - val_loss: 12.8153\n",
      "Epoch 14/150\n",
      "68/68 [==============================] - 6s 83ms/step - loss: 13.0826 - val_loss: 12.5289\n",
      "Epoch 15/150\n",
      "68/68 [==============================] - 7s 98ms/step - loss: 12.9305 - val_loss: 12.5867\n",
      "Epoch 16/150\n",
      "68/68 [==============================] - 5s 78ms/step - loss: 12.7725 - val_loss: 12.3888\n",
      "Epoch 17/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 12.6581 - val_loss: 12.5795\n",
      "Epoch 18/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 12.5800 - val_loss: 12.2348\n",
      "Epoch 19/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 12.4864 - val_loss: 12.1248\n",
      "Epoch 20/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 12.3895 - val_loss: 12.1546\n",
      "Epoch 21/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 12.3404 - val_loss: 12.0579\n",
      "Epoch 22/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 12.2573 - val_loss: 12.1263\n",
      "Epoch 23/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 12.1966 - val_loss: 12.0805\n",
      "Epoch 24/150\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 12.1287 - val_loss: 11.9111\n",
      "Epoch 25/150\n",
      "68/68 [==============================] - 6s 82ms/step - loss: 12.0711 - val_loss: 12.0111\n",
      "Epoch 26/150\n",
      "68/68 [==============================] - 6s 83ms/step - loss: 12.0157 - val_loss: 11.9620\n",
      "Epoch 27/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 11.9734 - val_loss: 11.9824\n",
      "Epoch 28/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.9323 - val_loss: 11.8449\n",
      "Epoch 29/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 11.8863 - val_loss: 11.8040\n",
      "Epoch 30/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 11.8385 - val_loss: 11.8281\n",
      "Epoch 31/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.8132 - val_loss: 11.8517\n",
      "Epoch 32/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.7675 - val_loss: 11.8240\n",
      "Epoch 33/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.7094 - val_loss: 11.8130\n",
      "Epoch 34/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.6649 - val_loss: 11.8653\n",
      "Epoch 35/150\n",
      "68/68 [==============================] - 5s 81ms/step - loss: 11.6378 - val_loss: 11.6910\n",
      "Epoch 36/150\n",
      "68/68 [==============================] - 6s 81ms/step - loss: 11.6083 - val_loss: 11.7753\n",
      "Epoch 37/150\n",
      "68/68 [==============================] - 5s 77ms/step - loss: 11.5725 - val_loss: 11.7178\n",
      "Epoch 38/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.5373 - val_loss: 11.9600\n",
      "Epoch 39/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.5020 - val_loss: 11.6656\n",
      "Epoch 40/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.4566 - val_loss: 11.6737\n",
      "Epoch 41/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 11.4406 - val_loss: 11.5968\n",
      "Epoch 42/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 11.4337 - val_loss: 11.6459\n",
      "Epoch 43/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 11.3941 - val_loss: 11.5355\n",
      "Epoch 44/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 11.3375 - val_loss: 11.5707\n",
      "Epoch 45/150\n",
      "68/68 [==============================] - 5s 77ms/step - loss: 11.3293 - val_loss: 11.5391\n",
      "Epoch 46/150\n",
      "68/68 [==============================] - 6s 83ms/step - loss: 11.3155 - val_loss: 11.5955\n",
      "Epoch 47/150\n",
      "68/68 [==============================] - 6s 83ms/step - loss: 11.2665 - val_loss: 11.5337\n",
      "Epoch 48/150\n",
      "68/68 [==============================] - 6s 83ms/step - loss: 11.2394 - val_loss: 11.5557\n",
      "Epoch 49/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 11.2418 - val_loss: 11.5796\n",
      "Epoch 50/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.2216 - val_loss: 11.8263\n",
      "Epoch 51/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.1939 - val_loss: 11.5928\n",
      "Epoch 52/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 11.1571 - val_loss: 11.5922\n",
      "Epoch 53/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.1316 - val_loss: 11.5324\n",
      "Epoch 54/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.1071 - val_loss: 11.5242\n",
      "Epoch 55/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.1097 - val_loss: 11.4755\n",
      "Epoch 56/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.0596 - val_loss: 11.6461\n",
      "Epoch 57/150\n",
      "68/68 [==============================] - 6s 82ms/step - loss: 11.0753 - val_loss: 11.4929\n",
      "Epoch 58/150\n",
      "68/68 [==============================] - 6s 82ms/step - loss: 11.0173 - val_loss: 11.4768\n",
      "Epoch 59/150\n",
      "68/68 [==============================] - 5s 78ms/step - loss: 11.0087 - val_loss: 11.6068\n",
      "Epoch 60/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 10.9914 - val_loss: 11.5574\n",
      "Epoch 61/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 10.9813 - val_loss: 11.8391\n",
      "Epoch 62/150\n",
      "68/68 [==============================] - 5s 74ms/step - loss: 10.9994 - val_loss: 11.4728\n",
      "Epoch 63/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 10.9302 - val_loss: 11.4169\n",
      "Epoch 64/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 10.9282 - val_loss: 11.4287\n",
      "Epoch 65/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.9022 - val_loss: 11.4307\n",
      "Epoch 66/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 10.8920 - val_loss: 11.3786\n",
      "Epoch 67/150\n",
      "68/68 [==============================] - 5s 78ms/step - loss: 10.8822 - val_loss: 11.5648\n",
      "Epoch 68/150\n",
      "68/68 [==============================] - 6s 81ms/step - loss: 10.8358 - val_loss: 11.4293\n",
      "Epoch 69/150\n",
      "68/68 [==============================] - 6s 81ms/step - loss: 10.8200 - val_loss: 11.5864\n",
      "Epoch 70/150\n",
      "68/68 [==============================] - 6s 82ms/step - loss: 10.8110 - val_loss: 11.3823\n",
      "Epoch 71/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.7911 - val_loss: 11.5516\n",
      "Epoch 72/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.7777 - val_loss: 11.5269\n",
      "Epoch 73/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 10.7616 - val_loss: 11.3405\n",
      "Epoch 74/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.7469 - val_loss: 11.3816\n",
      "Epoch 75/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.7427 - val_loss: 11.8135\n",
      "Epoch 76/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.7095 - val_loss: 11.6148\n",
      "Epoch 77/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.6925 - val_loss: 11.6580\n",
      "Epoch 78/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.7141 - val_loss: 11.7189\n",
      "Epoch 79/150\n",
      "68/68 [==============================] - 6s 82ms/step - loss: 10.7140 - val_loss: 11.5483\n",
      "Epoch 80/150\n",
      "68/68 [==============================] - 6s 83ms/step - loss: 10.6954 - val_loss: 11.5567\n",
      "Epoch 81/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 10.6558 - val_loss: 11.5312\n",
      "Epoch 82/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.6558 - val_loss: 11.4001\n",
      "Epoch 83/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 10.6394 - val_loss: 11.3806\n",
      "========================= TRIAL 0, MAE: 42.21009446540194 =========================\n",
      "experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-13_16:47 does not exist. Creating now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_19_layer_call_fn, lstm_cell_19_layer_call_and_return_conditional_losses, lstm_cell_20_layer_call_fn, lstm_cell_20_layer_call_and_return_conditional_losses, lstm_cell_22_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f722cc1c450> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f7227781dd0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f721f343ed0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f721f33f050> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-13_16:47/model/model_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_19_layer_call_fn, lstm_cell_19_layer_call_and_return_conditional_losses, lstm_cell_20_layer_call_fn, lstm_cell_20_layer_call_and_return_conditional_losses, lstm_cell_22_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f722cc1c450> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f7227781dd0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f721f343ed0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f721f33f050> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "shuffle_buffer = .75\n",
    "use_bottleneck_model = False\n",
    "NUM_EPOCHS = 150\n",
    "verbose = 1\n",
    "model_to_use='default'\n",
    "ytw_result = run_experiment(1, experiment_prefix='similar_trades_small_experiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0c300d18-33fc-4085-b2e4-60bdf67e35b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_model = keras.models.load_model('experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-13_16:47/model/model_0')\n",
    "# test_dataframe['prediction_1'] = default_model.predict(x_test, batch_size=10000)\n",
    "# mean_absolute_error(test_dataframe['new_ys'] , test_dataframe['prediction_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f3e7933f-a5f6-4ef2-9922-e8bd60ff1e01",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA: N = 673339, MIN DATE = 2023-01-03 00:00:00, MAX DATE = 2023-01-27 00:00:00\n",
      "VALIDATION DATA: N = 74815, MIN DATE = 2023-01-03 00:00:00, MAX DATE = 2023-01-27 00:00:00\n",
      "TEST DATA: N = 38521, MIN DATE = 2023-01-30 00:00:00, MAX DATE = 2023-01-30 00:00:00\n",
      "Experiment results will be saved to experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-13_17:16\n",
      "\n",
      "Performing 1 runs with parameters: {'train_start': '2023-01-01', 'train_end': '2023-01-29', 'test_start': '2023-01-29', 'test_end': '2023-01-31', 'train_size': 748154, 'test_size': 38521, 'VALIDATION_SPLIT': 0.1, 'LEARNING_RATE': 0.0007, 'BATCH_SIZE': 10000, 'NUM_EPOCHS': 150, 'DROPOUT': 0.1, 'TRADE_SEQUENCE_LENGTH': 5, 'NUM_FEATURES': 6, 'target_variable': 'new_ys', 'features': ['callable', 'sinking', 'zerocoupon', 'is_non_transaction_based_compensation', 'is_general_obligation', 'callable_at_cav', 'extraordinary_make_whole_call', 'make_whole_call', 'has_unexpired_lines_of_credit', 'escrow_exists', 'quantity', 'days_to_maturity', 'days_to_call', 'coupon', 'issue_amount', 'last_seconds_ago', 'last_yield_spread', 'days_to_settle', 'days_to_par', 'maturity_amount', 'issue_price', 'orig_principal_amount', 'max_amount_outstanding', 'accrued_days', 'days_in_interest_payment', 'A/E', 'ficc_treasury_spread', 'max_ys_ys', 'max_ys_ago', 'max_ys_qdiff', 'min_ys_ys', 'min_ys_ago', 'min_ys_qdiff', 'max_qty_ys', 'max_qty_ago', 'max_qty_qdiff', 'min_ago_ys', 'min_ago_ago', 'min_ago_qdiff', 'D_min_ago_ys', 'D_min_ago_ago', 'D_min_ago_qdiff', 'P_min_ago_ys', 'P_min_ago_ago', 'P_min_ago_qdiff', 'S_min_ago_ys', 'S_min_ago_ago', 'S_min_ago_qdiff', 'rating', 'incorporated_state_code', 'trade_type', 'purpose_class', 'max_ys_ttypes', 'min_ys_ttypes', 'max_qty_ttypes', 'min_ago_ttypes', 'D_min_ago_ttypes', 'P_min_ago_ttypes', 'S_min_ago_ttypes'], 'shuffle_buffer': 0.75, 'use_bottleneck_model': False}\n",
      "Epoch 1/150\n",
      "68/68 [==============================] - 38s 268ms/step - loss: 58.5611 - val_loss: 55.2719\n",
      "Epoch 2/150\n",
      "68/68 [==============================] - 5s 74ms/step - loss: 55.9156 - val_loss: 48.5985\n",
      "Epoch 3/150\n",
      "68/68 [==============================] - 5s 74ms/step - loss: 53.5684 - val_loss: 44.8489\n",
      "Epoch 4/150\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 50.5227 - val_loss: 41.3670\n",
      "Epoch 5/150\n",
      "68/68 [==============================] - 6s 82ms/step - loss: 46.5624 - val_loss: 40.9757\n",
      "Epoch 6/150\n",
      "68/68 [==============================] - 6s 82ms/step - loss: 41.5864 - val_loss: 40.5659\n",
      "Epoch 7/150\n",
      "68/68 [==============================] - 5s 78ms/step - loss: 35.6338 - val_loss: 37.9203\n",
      "Epoch 8/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 29.0827 - val_loss: 28.0319\n",
      "Epoch 9/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 22.7456 - val_loss: 20.0887\n",
      "Epoch 10/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 17.7123 - val_loss: 16.2404\n",
      "Epoch 11/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 14.9238 - val_loss: 14.2379\n",
      "Epoch 12/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 13.7781 - val_loss: 13.4042\n",
      "Epoch 13/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 13.3432 - val_loss: 12.8841\n",
      "Epoch 14/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 13.1034 - val_loss: 12.6909\n",
      "Epoch 15/150\n",
      "68/68 [==============================] - 5s 76ms/step - loss: 12.9494 - val_loss: 12.6413\n",
      "Epoch 16/150\n",
      "68/68 [==============================] - 6s 82ms/step - loss: 12.8024 - val_loss: 12.3499\n",
      "Epoch 17/150\n",
      "68/68 [==============================] - 6s 82ms/step - loss: 12.6852 - val_loss: 12.2905\n",
      "Epoch 18/150\n",
      "68/68 [==============================] - 5s 75ms/step - loss: 12.5995 - val_loss: 12.2795\n",
      "Epoch 19/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 12.5009 - val_loss: 12.1998\n",
      "Epoch 20/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 12.4048 - val_loss: 12.1123\n",
      "Epoch 21/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 12.3255 - val_loss: 12.1217\n",
      "Epoch 22/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 12.2700 - val_loss: 12.0651\n",
      "Epoch 23/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 12.1991 - val_loss: 12.0686\n",
      "Epoch 24/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 12.1435 - val_loss: 11.9742\n",
      "Epoch 25/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 12.0934 - val_loss: 12.2279\n",
      "Epoch 26/150\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 12.0456 - val_loss: 12.0355\n",
      "Epoch 27/150\n",
      "68/68 [==============================] - 5s 80ms/step - loss: 11.9757 - val_loss: 11.9755\n",
      "Epoch 28/150\n",
      "68/68 [==============================] - 5s 80ms/step - loss: 11.9546 - val_loss: 12.1885\n",
      "Epoch 29/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.8979 - val_loss: 11.9256\n",
      "Epoch 30/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.8659 - val_loss: 11.9609\n",
      "Epoch 31/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 11.8051 - val_loss: 11.8232\n",
      "Epoch 32/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.7515 - val_loss: 11.6948\n",
      "Epoch 33/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.7337 - val_loss: 12.0301\n",
      "Epoch 34/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.6892 - val_loss: 11.7485\n",
      "Epoch 35/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.6325 - val_loss: 11.7402\n",
      "Epoch 36/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.6383 - val_loss: 11.8353\n",
      "Epoch 37/150\n",
      "68/68 [==============================] - 5s 81ms/step - loss: 11.5811 - val_loss: 11.7863\n",
      "Epoch 38/150\n",
      "68/68 [==============================] - 6s 83ms/step - loss: 11.5416 - val_loss: 11.8591\n",
      "Epoch 39/150\n",
      "68/68 [==============================] - 5s 75ms/step - loss: 11.5041 - val_loss: 11.7264\n",
      "Epoch 40/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.4815 - val_loss: 11.6728\n",
      "Epoch 41/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.4531 - val_loss: 11.7123\n",
      "Epoch 42/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.4507 - val_loss: 11.6975\n",
      "Epoch 43/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 11.4144 - val_loss: 11.5950\n",
      "Epoch 44/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.3686 - val_loss: 11.6984\n",
      "Epoch 45/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.3391 - val_loss: 11.6115\n",
      "Epoch 46/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.3095 - val_loss: 11.5732\n",
      "Epoch 47/150\n",
      "68/68 [==============================] - 5s 76ms/step - loss: 11.2819 - val_loss: 11.6917\n",
      "Epoch 48/150\n",
      "68/68 [==============================] - 6s 81ms/step - loss: 11.2902 - val_loss: 11.6915\n",
      "Epoch 49/150\n",
      "68/68 [==============================] - 6s 82ms/step - loss: 11.2638 - val_loss: 11.7355\n",
      "Epoch 50/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 11.2165 - val_loss: 11.5922\n",
      "Epoch 51/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.1790 - val_loss: 11.5630\n",
      "Epoch 52/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.1552 - val_loss: 11.6246\n",
      "Epoch 53/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.1257 - val_loss: 11.5950\n",
      "Epoch 54/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 11.1378 - val_loss: 11.5033\n",
      "Epoch 55/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 11.1110 - val_loss: 11.7274\n",
      "Epoch 56/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.0931 - val_loss: 11.6164\n",
      "Epoch 57/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 11.0804 - val_loss: 11.5172\n",
      "Epoch 58/150\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 11.0453 - val_loss: 11.4619\n",
      "Epoch 59/150\n",
      "68/68 [==============================] - 5s 80ms/step - loss: 11.0318 - val_loss: 11.4316\n",
      "Epoch 60/150\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 11.0008 - val_loss: 11.4162\n",
      "Epoch 61/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.9878 - val_loss: 11.5011\n",
      "Epoch 62/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.9583 - val_loss: 11.4868\n",
      "Epoch 63/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.9353 - val_loss: 11.3490\n",
      "Epoch 64/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.9227 - val_loss: 11.4340\n",
      "Epoch 65/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.9115 - val_loss: 11.4105\n",
      "Epoch 66/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.9024 - val_loss: 11.4135\n",
      "Epoch 67/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.9034 - val_loss: 11.4523\n",
      "Epoch 68/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.8474 - val_loss: 11.3803\n",
      "Epoch 69/150\n",
      "68/68 [==============================] - 5s 81ms/step - loss: 10.8384 - val_loss: 11.4177\n",
      "Epoch 70/150\n",
      "68/68 [==============================] - 6s 81ms/step - loss: 10.8297 - val_loss: 11.4000\n",
      "Epoch 71/150\n",
      "68/68 [==============================] - 5s 76ms/step - loss: 10.7856 - val_loss: 11.3382\n",
      "Epoch 72/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.7949 - val_loss: 11.4692\n",
      "Epoch 73/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.7854 - val_loss: 11.3793\n",
      "Epoch 74/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 10.7662 - val_loss: 11.3570\n",
      "Epoch 75/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 10.7406 - val_loss: 11.3291\n",
      "Epoch 76/150\n",
      "68/68 [==============================] - 5s 73ms/step - loss: 10.7407 - val_loss: 11.2811\n",
      "Epoch 77/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.7257 - val_loss: 11.3778\n",
      "Epoch 78/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.7400 - val_loss: 11.3191\n",
      "Epoch 79/150\n",
      "68/68 [==============================] - 5s 77ms/step - loss: 10.6790 - val_loss: 11.4610\n",
      "Epoch 80/150\n",
      "68/68 [==============================] - 6s 83ms/step - loss: 10.6768 - val_loss: 11.2257\n",
      "Epoch 81/150\n",
      "68/68 [==============================] - 6s 80ms/step - loss: 10.6560 - val_loss: 11.3364\n",
      "Epoch 82/150\n",
      "68/68 [==============================] - 5s 74ms/step - loss: 10.6360 - val_loss: 11.2569\n",
      "Epoch 83/150\n",
      "68/68 [==============================] - 5s 71ms/step - loss: 10.6698 - val_loss: 11.2558\n",
      "Epoch 84/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.6234 - val_loss: 11.2149\n",
      "Epoch 85/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.6064 - val_loss: 11.2636\n",
      "Epoch 86/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.6200 - val_loss: 11.3663\n",
      "Epoch 87/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.6161 - val_loss: 11.2084\n",
      "Epoch 88/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.5821 - val_loss: 11.4158\n",
      "Epoch 89/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.5704 - val_loss: 11.2023\n",
      "Epoch 90/150\n",
      "68/68 [==============================] - 5s 78ms/step - loss: 10.5652 - val_loss: 11.3804\n",
      "Epoch 91/150\n",
      "68/68 [==============================] - 6s 83ms/step - loss: 10.5674 - val_loss: 11.2497\n",
      "Epoch 92/150\n",
      "68/68 [==============================] - 6s 82ms/step - loss: 10.5325 - val_loss: 11.4262\n",
      "Epoch 93/150\n",
      "68/68 [==============================] - 5s 76ms/step - loss: 10.5261 - val_loss: 11.5407\n",
      "Epoch 94/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.5226 - val_loss: 11.2282\n",
      "Epoch 95/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.4901 - val_loss: 11.2866\n",
      "Epoch 96/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.5316 - val_loss: 11.5475\n",
      "Epoch 97/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.4915 - val_loss: 11.3481\n",
      "Epoch 98/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.4880 - val_loss: 11.3289\n",
      "Epoch 99/150\n",
      "68/68 [==============================] - 5s 72ms/step - loss: 10.4783 - val_loss: 11.6708\n",
      "========================= TRIAL 0, MAE: 200.7450210227265 =========================\n",
      "experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-13_17:16 does not exist. Creating now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_31_layer_call_fn, lstm_cell_31_layer_call_and_return_conditional_losses, lstm_cell_32_layer_call_fn, lstm_cell_32_layer_call_and_return_conditional_losses, lstm_cell_34_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f7222552950> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f7222539250> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f722d152090> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f722d152b90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-13_17:16/model/model_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_31_layer_call_fn, lstm_cell_31_layer_call_and_return_conditional_losses, lstm_cell_32_layer_call_fn, lstm_cell_32_layer_call_and_return_conditional_losses, lstm_cell_34_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f7222552950> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f7222539250> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f722d152090> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f722d152b90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "reset_model_features()\n",
    "params, normalizers, x_train, y_train, x_val, y_val, x_test, y_test, val_idx = create_data_set_and_model(train_dataframe, \n",
    "                                                                                                         test_dataframe, \n",
    "                                                                                                         trade_history_col)\n",
    "ytw_result = run_experiment(1, experiment_prefix='similar_trades_small_experiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fa7b90-8dbc-4fd0-b4c2-8218335a8ce7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Bigger Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3826d85b-cdf7-481d-a91e-f272bbd5e04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2023-07-03 00:00:00'), Timestamp('2023-09-29 00:00:00'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.trade_date.min(), df.trade_date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2169452d-2402-4943-97de-3e4a3bfbf4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1726c57b-70da-434e-805b-28b8ecd07261",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 11s, sys: 26.6 s, total: 9min 38s\n",
      "Wall time: 13min 1s\n",
      "CPU times: user 9min 9s, sys: 28.2 s, total: 9min 37s\n",
      "Wall time: 12min 45s\n",
      "CPU times: user 9min 22s, sys: 26.9 s, total: 9min 48s\n",
      "Wall time: 12min 52s\n",
      "CPU times: user 9min 33s, sys: 28.7 s, total: 10min 2s\n",
      "Wall time: 13min 6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-17:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 412, in _handle_workers\n",
      "    pool._maintain_pool()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 248, in _maintain_pool\n",
      "    self._repopulate_pool()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 241, in _repopulate_pool\n",
      "    w.start()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 112, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/context.py\", line 277, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/popen_fork.py\", line 20, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/popen_fork.py\", line 70, in _launch\n",
      "    self.pid = os.fork()\n",
      "OSError: [Errno 12] Cannot allocate memory\n",
      "\n",
      "Process ForkPoolWorker-153:\n",
      "Process ForkPoolWorker-152:\n",
      "Process ForkPoolWorker-155:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 411, in _recv_bytes\n",
      "    return self._recv(size)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 411, in _recv_bytes\n",
      "    return self._recv(size)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 411, in _recv_bytes\n",
      "    return self._recv(size)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "MemoryError\n",
      "MemoryError\n",
      "MemoryError\n",
      "Process ForkPoolWorker-127:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 354, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "ModuleNotFoundError: No module named 'essing'\n",
      "Process ForkPoolWorker-130:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 354, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "_pickle.UnpicklingError: invalid load key, '\\x00'.\n",
      "Process ForkPoolWorker-132:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 354, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "_pickle.UnpicklingError: invalid load key, '\\x5c'.\n",
      "Process ForkPoolWorker-135:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 354, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "ValueError: unsupported pickle protocol: 161\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'terminate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24285/3166572668.py\u001b[0m in \u001b[0;36mcalculate_masked_func\u001b[0;34m(data, agg_func, ex_cusip, groupby_col, target_col, max_sequence_length, seconds_ago_mask, mp, max_window)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    267\u001b[0m         '''\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24285/3166572668.py\u001b[0m in \u001b[0;36mcalculate_masked_average\u001b[0;34m(data, ex_cusip, groupby_col, target_col, max_sequence_length, seconds_ago_mask, mp, max_window)\u001b[0m\n\u001b[1;32m     52\u001b[0m                                  \u001b[0mseconds_ago_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseconds_ago_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                                  \u001b[0mmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                                  max_window = max_window)\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_24285/3166572668.py\u001b[0m in \u001b[0;36mcalculate_masked_func\u001b[0;34m(data, agg_func, ex_cusip, groupby_col, target_col, max_sequence_length, seconds_ago_mask, mp, max_window)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_terminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/util.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wr, _finalizer_registry, sub_debug, getpid)\u001b[0m\n\u001b[1;32m    222\u001b[0m                 sub_debug('finalizer calling %s with args %s and kwargs %s',\n\u001b[1;32m    223\u001b[0m                           self._callback, self._args, self._kwargs)\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weakref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_terminate_pool\u001b[0;34m(cls, taskqueue, inqueue, outqueue, pool, worker_handler, task_handler, result_handler, cache)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'joining task handler'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/process.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m         '''\n\u001b[1;32m    123\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'terminate'"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24285/3166572668.py\u001b[0m in \u001b[0;36mcalculate_masked_average\u001b[0;34m(data, ex_cusip, groupby_col, target_col, max_sequence_length, seconds_ago_mask, mp, max_window)\u001b[0m\n\u001b[1;32m     52\u001b[0m                                  \u001b[0mseconds_ago_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseconds_ago_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                                  \u001b[0mmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                                  max_window = max_window)\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_24285/3166572668.py\u001b[0m in \u001b[0;36mcalculate_masked_func\u001b[0;34m(data, agg_func, ex_cusip, groupby_col, target_col, max_sequence_length, seconds_ago_mask, mp, max_window)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mgroupby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroupby_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrolling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_periods\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mresample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mex_cusip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/window/rolling.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_slice_axis\u001b[0;34m(self, slice_obj, axis)\u001b[0m\n\u001b[1;32m   1577\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_positional_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1579\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_slice\u001b[0;34m(self, slobj, axis)\u001b[0m\n\u001b[1;32m   3823\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3824\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3825\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3826\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/internals.pyx\u001b[0m in \u001b[0;36mpandas._libs.internals.BlockManager.get_slice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/internals.pyx\u001b[0m in \u001b[0;36mpandas._libs.internals.BlockManager._get_index_slice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, blocks, axes, verify_integrity)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;31m# Constructors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m     def __init__(\n\u001b[0m\u001b[1;32m    878\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBlock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1334\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m                 \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m                 \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24285/3166572668.py\u001b[0m in \u001b[0;36mcalculate_masked_average\u001b[0;34m(data, ex_cusip, groupby_col, target_col, max_sequence_length, seconds_ago_mask, mp, max_window)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                  \u001b[0mmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                                  max_window = max_window)\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_24285/3166572668.py\u001b[0m in \u001b[0;36mcalculate_masked_func\u001b[0;34m(data, agg_func, ex_cusip, groupby_col, target_col, max_sequence_length, seconds_ago_mask, mp, max_window)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mgroupby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroupby_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrolling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_periods\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mresample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/window/rolling.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1542\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_slice_axis\u001b[0;34m(self, slice_obj, axis)\u001b[0m\n\u001b[1;32m   1578\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_positional_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1579\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_slice\u001b[0;34m(self, slobj, axis)\u001b[0m\n\u001b[1;32m   3824\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3825\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3826\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/internals.pyx\u001b[0m in \u001b[0;36mpandas._libs.internals.BlockManager.get_slice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/internals.pyx\u001b[0m in \u001b[0;36mpandas._libs.internals.BlockManager._get_index_slice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/internals.pyx\u001b[0m in \u001b[0;36mpandas._libs.internals.NDArrayBackedBlock.getitem_block_index\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/internals.pyx\u001b[0m in \u001b[0;36mpandas._libs.internals.NDArrayBackedBlock.getitem_block_index\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/arrays/datetimelike.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_getitem_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1333\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m                 \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m                 \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m                 \u001b[0;31m# multi-line %%time case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = []\n",
    "masked_cols = [f'ex_cusip_masked_series_average_{i}' for i in [1,2,5,10,15,20,30,50]]\n",
    "for i in [1,2, 5, 10, 15, 20, 30, 50]:\n",
    "    %time temp = calculate_masked_average(df[['trade_datetime', 'cusip', 'cusip_series','new_ys']], ex_cusip=True, mp = multiprocessing.cpu_count() - 1,max_sequence_length = i)\n",
    "    res.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "401221ea-eb88-4fb0-9f6e-1020bd0df5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 39s, sys: 52.7 s, total: 11min 31s\n",
      "Wall time: 18min 7s\n"
     ]
    }
   ],
   "source": [
    "%time temp2 = calculate_masked_average_2(df[['trade_datetime','cusip','cusip_series','new_ys']], windows = [1, 2, 10, 15, 20, 30, 50], mask_cusip=True, mp = multiprocessing.cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "a59a29dd-acd6-43d2-92e1-1df6c849339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_cols = [f'ex_cusip_masked_series_average_{i}' for i in [1, 2, 10, 15, 20, 30, 50]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "0da634d7-3f39-4805-b7e9-dee01649c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[masked_cols] = to_df(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "42c5fc72-d64b-4428-a8f6-0ebf3ef82a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = multiprocessing.cpu_count()-1\n",
    "series_features = {'series_max': lambda data: calculate_masked_func(data, lambda y: y.max(), ex_cusip = True, max_sequence_length = 10, mp = mp),\n",
    "                    'series_min': lambda data: calculate_masked_func(data, lambda y: y.min(), ex_cusip = True, max_sequence_length = 10, mp = mp),\n",
    "                    'series_std': lambda data: calculate_masked_func(data, lambda y: y.median(), ex_cusip = True, max_sequence_length = 10, mp = mp),\n",
    "                    'series_vol': lambda data: calculate_masked_func(data, lambda y: y.std(), ex_cusip = True, max_sequence_length = 10, mp = mp)}\n",
    "\n",
    "combined_features = lambda data: calculate_masked_func_2(data, lambda y: (y.max(), y.min(), y.median(), y.std()), windows = [0], mask_cusip = True, mp = mp)\n",
    "new_cols = list(series_features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "8469c26d-8488-4437-b61f-640eeaed628d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 46s, sys: 52.9 s, total: 10min 39s\n",
      "Wall time: 14min 30s\n"
     ]
    }
   ],
   "source": [
    "%time ttemp = combined_features(df[['trade_datetime', 'cusip', 'cusip_series','new_ys']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "6eb4d0eb-a2a3-4fd3-8968-5c1688a23e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[new_cols] = to_df(ttemp.apply(lambda x: x[0] if x else None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2947db91-ca77-4983-b954-8f7bc190ce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_pickle('similar_trades_3month.pkl')\n",
    "df = pd.read_pickle('similar_trades_3month.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "806642e4-a03b-4ba7-8992-5ba908cb8b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2023-07-03 00:00:00'), Timestamp('2023-09-29 00:00:00'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.trade_date.min(), df.trade_date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcb9736c-3656-46f0-b94c-8d65b49a0eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = '2023-07-01'\n",
    "train_end = '2023-09-25'\n",
    "test_start = '2023-09-25'\n",
    "test_end = '2023-09-29'\n",
    "\n",
    "VALIDATION_SPLIT = 0.1\n",
    "LEARNING_RATE = 0.0007\n",
    "BATCH_SIZE = 10000\n",
    "NUM_EPOCHS = 75 \n",
    "DROPOUT = 0.1 \n",
    "TRADE_SEQUENCE_LENGTH = 5\n",
    "NUM_FEATURES = 6\n",
    "target_variable = 'new_ys' \n",
    "trade_history_col = 'trade_history'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "775c47c3-bcb4-4407-8631-c722938c273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['series_vol'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7854e1d-72e8-47c2-bfd5-51a0040ec333",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filter = (df.trade_date < train_end) & (df.trade_date >= train_start)\n",
    "test_filter = (df.trade_date >= test_start) & (df.trade_date <test_end)\n",
    "\n",
    "train_dataframe = df[train_filter]\\\n",
    ".sort_values(by='trade_date', ascending=True)\\\n",
    ".reset_index(drop=True).copy()\n",
    "\n",
    "test_dataframe = df[test_filter]\\\n",
    ".sort_values(by='trade_date', ascending=True)\\\n",
    ".reset_index(drop=True).copy()\n",
    "\n",
    "train_dataframe['last_seconds_ago'] = train_dataframe['last_seconds_ago'].fillna(0)\n",
    "train_dataframe['last_yield_spread'] = train_dataframe['last_yield_spread'].fillna(0)\n",
    "\n",
    "test_dataframe['last_seconds_ago'] = test_dataframe['last_seconds_ago'].fillna(0)\n",
    "test_dataframe['last_yield_spread'] = test_dataframe['last_yield_spread'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "d4818670-7115-40d3-b697-65ef740fef26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA: N = 2191544, MIN DATE = 2023-07-03 00:00:00, MAX DATE = 2023-09-22 00:00:00\n",
      "VALIDATION DATA: N = 243504, MIN DATE = 2023-07-03 00:00:00, MAX DATE = 2023-09-22 00:00:00\n",
      "TEST DATA: N = 216931, MIN DATE = 2023-09-25 00:00:00, MAX DATE = 2023-09-28 00:00:00\n"
     ]
    }
   ],
   "source": [
    "reset_model_features()\n",
    "modify_features(masked_cols + new_cols, 'add', 'numeric')\n",
    "params, normalizers, x_train, y_train, x_val, y_val, x_test, y_test, val_idx = create_data_set_and_model(train_dataframe, \n",
    "                                                                                                         test_dataframe, \n",
    "                                                                                                         trade_history_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "6a059307-61b8-4b9f-8655-3281bbac1d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median baseline MAE: 45.694182\n",
      "qreg baseline MAE: 40.851194\n",
      "qreg baseline + ['series_max', 'series_min', 'series_std', 'series_vol'] MAE: 39.623604\n"
     ]
    }
   ],
   "source": [
    "median_baseline = mean_absolute_error(test_dataframe[\"new_ys\"], np.repeat(train_dataframe[\"new_ys\"].median(), len(test_dataframe)))\n",
    "_, mae1 = perform_qreg(train_dataframe, \n",
    "            test_dataframe, \n",
    "            masked_cols,\n",
    "             print_result=False,\n",
    "             return_vals = True\n",
    "           )\n",
    "\n",
    "_, mae2 = perform_qreg(train_dataframe, \n",
    "            test_dataframe, \n",
    "            masked_cols + new_cols,\n",
    "             print_result=False,\n",
    "             return_vals = True\n",
    "           )\n",
    "\n",
    "print(f'median baseline MAE: {median_baseline:2f}')\n",
    "print(f'qreg baseline MAE: {mae1:2f}')\n",
    "print(f'qreg baseline + {new_cols} MAE: {mae2:2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b9b5fd-2c8f-491b-be94-9ff7a2a082eb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment results will be saved to experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-14_17:36\n",
      "\n",
      "Performing 1 runs with parameters: {'train_start': '2023-07-01', 'train_end': '2023-09-25', 'test_start': '2023-09-25', 'test_end': '2023-09-29', 'train_size': 2435048, 'test_size': 216931, 'VALIDATION_SPLIT': 0.1, 'LEARNING_RATE': 0.0007, 'BATCH_SIZE': 10000, 'NUM_EPOCHS': 150, 'DROPOUT': 0.1, 'TRADE_SEQUENCE_LENGTH': 5, 'NUM_FEATURES': 6, 'target_variable': 'new_ys', 'features': ['callable', 'sinking', 'zerocoupon', 'is_non_transaction_based_compensation', 'is_general_obligation', 'callable_at_cav', 'extraordinary_make_whole_call', 'make_whole_call', 'has_unexpired_lines_of_credit', 'escrow_exists', 'quantity', 'days_to_maturity', 'days_to_call', 'coupon', 'issue_amount', 'last_seconds_ago', 'last_yield_spread', 'days_to_settle', 'days_to_par', 'maturity_amount', 'issue_price', 'orig_principal_amount', 'max_amount_outstanding', 'accrued_days', 'days_in_interest_payment', 'A/E', 'ficc_treasury_spread', 'max_ys_ys', 'max_ys_ago', 'max_ys_qdiff', 'min_ys_ys', 'min_ys_ago', 'min_ys_qdiff', 'max_qty_ys', 'max_qty_ago', 'max_qty_qdiff', 'min_ago_ys', 'min_ago_ago', 'min_ago_qdiff', 'D_min_ago_ys', 'D_min_ago_ago', 'D_min_ago_qdiff', 'P_min_ago_ys', 'P_min_ago_ago', 'P_min_ago_qdiff', 'S_min_ago_ys', 'S_min_ago_ago', 'S_min_ago_qdiff', 'ex_cusip_masked_series_average_1', 'ex_cusip_masked_series_average_2', 'ex_cusip_masked_series_average_5', 'ex_cusip_masked_series_average_10', 'ex_cusip_masked_series_average_15', 'ex_cusip_masked_series_average_20', 'ex_cusip_masked_series_average_30', 'ex_cusip_masked_series_average_50', 'series_max', 'series_min', 'series_std', 'series_vol', 'rating', 'incorporated_state_code', 'trade_type', 'purpose_class', 'max_ys_ttypes', 'min_ys_ttypes', 'max_qty_ttypes', 'min_ago_ttypes', 'D_min_ago_ttypes', 'P_min_ago_ttypes', 'S_min_ago_ttypes'], 'shuffle_buffer': 0.75, 'use_bottleneck_model': False}\n",
      "Epoch 1/150\n",
      "220/220 [==============================] - 139s 432ms/step - loss: 39.7705 - val_loss: 28.5912\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 28.7874 - val_loss: 31.5745\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 16.8781 - val_loss: 16.6245\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 13.0459 - val_loss: 12.9218\n",
      "Epoch 5/150\n",
      "220/220 [==============================] - 16s 75ms/step - loss: 12.5446 - val_loss: 12.1406\n",
      "Epoch 6/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 12.3017 - val_loss: 11.6357\n",
      "Epoch 7/150\n",
      "220/220 [==============================] - 19s 85ms/step - loss: 12.1326 - val_loss: 11.6499\n",
      "Epoch 8/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.9877 - val_loss: 11.3377\n",
      "Epoch 9/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.9041 - val_loss: 11.2938\n",
      "Epoch 10/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 11.7965 - val_loss: 11.3944\n",
      "Epoch 11/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 11.7176 - val_loss: 11.1539\n",
      "Epoch 12/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.6584 - val_loss: 11.1789\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 11.5770 - val_loss: 11.0242\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 11.5257 - val_loss: 10.9445\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.4736 - val_loss: 10.9210\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.4302 - val_loss: 10.9212\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 19s 86ms/step - loss: 11.3873 - val_loss: 10.8592\n",
      "Epoch 18/150\n",
      " 78/220 [=========>....................] - ETA: 10s - loss: 11.3464"
     ]
    }
   ],
   "source": [
    "shuffle_buffer = .75\n",
    "use_bottleneck_model = False\n",
    "NUM_EPOCHS = 150\n",
    "verbose = 1\n",
    "model_to_use='default'\n",
    "ytw_result = run_experiment(1, experiment_prefix='similar_trades_small_experiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "365f2660-9170-479d-8227-e8d3b1bfd070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.010530497111292"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_features_model = keras.models.load_model('experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-14_17:36/model/model_0')\n",
    "test_dataframe['prediction_newfeatures'] = new_features_model.predict(x_test, batch_size=10000)\n",
    "mean_absolute_error(test_dataframe['new_ys'] , test_dataframe['prediction_newfeatures'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "f5c01ca7-0369-4313-ab3c-708ce84ff559",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA: N = 2191544, MIN DATE = 2023-07-03 00:00:00, MAX DATE = 2023-09-22 00:00:00\n",
      "VALIDATION DATA: N = 243504, MIN DATE = 2023-07-03 00:00:00, MAX DATE = 2023-09-22 00:00:00\n",
      "TEST DATA: N = 216931, MIN DATE = 2023-09-25 00:00:00, MAX DATE = 2023-09-28 00:00:00\n",
      "Experiment results will be saved to experiments/similar_trades_small_experiment_baselines/new_ys_experiment_2023-11-14_22:07\n",
      "\n",
      "Performing 5 runs with parameters: {'train_start': '2023-07-01', 'train_end': '2023-09-25', 'test_start': '2023-09-25', 'test_end': '2023-09-29', 'train_size': 2435048, 'test_size': 216931, 'VALIDATION_SPLIT': 0.1, 'LEARNING_RATE': 0.0007, 'BATCH_SIZE': 10000, 'NUM_EPOCHS': 150, 'DROPOUT': 0.1, 'TRADE_SEQUENCE_LENGTH': 5, 'NUM_FEATURES': 6, 'target_variable': 'new_ys', 'features': ['callable', 'sinking', 'zerocoupon', 'is_non_transaction_based_compensation', 'is_general_obligation', 'callable_at_cav', 'extraordinary_make_whole_call', 'make_whole_call', 'has_unexpired_lines_of_credit', 'escrow_exists', 'quantity', 'days_to_maturity', 'days_to_call', 'coupon', 'issue_amount', 'last_seconds_ago', 'last_yield_spread', 'days_to_settle', 'days_to_par', 'maturity_amount', 'issue_price', 'orig_principal_amount', 'max_amount_outstanding', 'accrued_days', 'days_in_interest_payment', 'A/E', 'ficc_treasury_spread', 'max_ys_ys', 'max_ys_ago', 'max_ys_qdiff', 'min_ys_ys', 'min_ys_ago', 'min_ys_qdiff', 'max_qty_ys', 'max_qty_ago', 'max_qty_qdiff', 'min_ago_ys', 'min_ago_ago', 'min_ago_qdiff', 'D_min_ago_ys', 'D_min_ago_ago', 'D_min_ago_qdiff', 'P_min_ago_ys', 'P_min_ago_ago', 'P_min_ago_qdiff', 'S_min_ago_ys', 'S_min_ago_ago', 'S_min_ago_qdiff', 'rating', 'incorporated_state_code', 'trade_type', 'purpose_class', 'max_ys_ttypes', 'min_ys_ttypes', 'max_qty_ttypes', 'min_ago_ttypes', 'D_min_ago_ttypes', 'P_min_ago_ttypes', 'S_min_ago_ttypes'], 'shuffle_buffer': 0.75, 'use_bottleneck_model': False}\n",
      "Epoch 1/150\n",
      "220/220 [==============================] - 129s 372ms/step - loss: 39.7350 - val_loss: 28.4137\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 28.7935 - val_loss: 29.0581\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 16.8647 - val_loss: 15.5738\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 13.0216 - val_loss: 12.5610\n",
      "Epoch 5/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 12.5398 - val_loss: 11.9229\n",
      "Epoch 6/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 12.3159 - val_loss: 11.6527\n",
      "Epoch 7/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 12.1542 - val_loss: 11.6757\n",
      "Epoch 8/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 12.0169 - val_loss: 11.4114\n",
      "Epoch 9/150\n",
      "220/220 [==============================] - 19s 85ms/step - loss: 11.9157 - val_loss: 11.3633\n",
      "Epoch 10/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.8338 - val_loss: 11.2033\n",
      "Epoch 11/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.7558 - val_loss: 11.1059\n",
      "Epoch 12/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 11.7056 - val_loss: 11.1102\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 11.6218 - val_loss: 11.1582\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.5665 - val_loss: 11.0606\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.5126 - val_loss: 10.9717\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 19s 85ms/step - loss: 11.4723 - val_loss: 11.0616\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.4349 - val_loss: 10.9195\n",
      "Epoch 18/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.3846 - val_loss: 10.8385\n",
      "Epoch 19/150\n",
      " 24/220 [==>...........................] - ETA: 13s - loss: 11.1327"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 17s 77ms/step - loss: 10.5871 - val_loss: 10.3884\n",
      "Epoch 57/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.5681 - val_loss: 10.3558\n",
      "Epoch 58/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.5615 - val_loss: 10.4171\n",
      "Epoch 59/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.5563 - val_loss: 10.3284\n",
      "Epoch 60/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.5362 - val_loss: 10.3229\n",
      "Epoch 61/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.5127 - val_loss: 10.3605\n",
      "Epoch 62/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.5073 - val_loss: 10.3805\n",
      "Epoch 63/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.4880 - val_loss: 10.3582\n",
      "Epoch 64/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.4871 - val_loss: 10.3313\n",
      "Epoch 65/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.4742 - val_loss: 10.4395\n",
      "Epoch 66/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.4678 - val_loss: 10.3737\n",
      "Epoch 67/150\n",
      "220/220 [==============================] - 19s 85ms/step - loss: 10.4711 - val_loss: 10.3853\n",
      "Epoch 68/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.4629 - val_loss: 10.3840\n",
      "Epoch 69/150\n",
      "169/220 [======================>.......] - ETA: 3s - loss: 10.4237"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 18s 83ms/step - loss: 10.8993 - val_loss: 10.5874\n",
      "Epoch 37/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.8811 - val_loss: 10.5804\n",
      "Epoch 38/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.8660 - val_loss: 10.7199\n",
      "Epoch 39/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.8546 - val_loss: 10.6130\n",
      "Epoch 40/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.8174 - val_loss: 10.7784\n",
      "Epoch 41/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.8177 - val_loss: 10.5017\n",
      "Epoch 42/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.7853 - val_loss: 10.4939\n",
      "Epoch 43/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.7821 - val_loss: 10.6216\n",
      "Epoch 44/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7674 - val_loss: 10.5885\n",
      "Epoch 45/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7470 - val_loss: 10.5881\n",
      "Epoch 46/150\n",
      "220/220 [==============================] - 19s 85ms/step - loss: 10.7368 - val_loss: 10.4918\n",
      "Epoch 47/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7337 - val_loss: 10.5243\n",
      "Epoch 48/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7072 - val_loss: 10.4398\n",
      "Epoch 49/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.6861 - val_loss: 10.4913\n",
      "Epoch 50/150\n",
      " 37/220 [====>.........................] - ETA: 14s - loss: 10.5315"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3078 - val_loss: 10.4937\n",
      "Epoch 86/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.2939 - val_loss: 10.2497\n",
      "Epoch 87/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.2853 - val_loss: 10.2863\n",
      "Epoch 88/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.2797 - val_loss: 10.2906\n",
      "Epoch 89/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.2714 - val_loss: 10.2452\n",
      "Epoch 90/150\n",
      "220/220 [==============================] - 18s 84ms/step - loss: 10.2633 - val_loss: 10.3193\n",
      "Epoch 91/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.2795 - val_loss: 10.2371\n",
      "Epoch 92/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2644 - val_loss: 10.2676\n",
      "Epoch 93/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.2396 - val_loss: 10.2970\n",
      "Epoch 94/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.2408 - val_loss: 10.4011\n",
      "Epoch 95/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.2361 - val_loss: 10.2532\n",
      "Epoch 96/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.2308 - val_loss: 10.2895\n",
      "Epoch 97/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 10.2186 - val_loss: 10.2704\n",
      "Epoch 98/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.2038 - val_loss: 10.3070\n",
      "Epoch 99/150\n",
      "119/220 [===============>..............] - ETA: 7s - loss: 10.2178"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 17s 76ms/step - loss: 11.2469 - val_loss: 10.8951\n",
      "Epoch 23/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 11.2063 - val_loss: 10.7144\n",
      "Epoch 24/150\n",
      "220/220 [==============================] - 16s 75ms/step - loss: 11.1836 - val_loss: 10.7022\n",
      "Epoch 25/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.1377 - val_loss: 10.7078\n",
      "Epoch 26/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 11.1227 - val_loss: 10.6938\n",
      "Epoch 27/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 11.0983 - val_loss: 10.6335\n",
      "Epoch 28/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.0683 - val_loss: 10.5940\n",
      "Epoch 29/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 11.0405 - val_loss: 10.6096\n",
      "Epoch 30/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 11.0220 - val_loss: 10.6031\n",
      "Epoch 31/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.9950 - val_loss: 10.6240\n",
      "Epoch 32/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.9738 - val_loss: 10.6751\n",
      "Epoch 33/150\n",
      "220/220 [==============================] - 19s 87ms/step - loss: 10.9564 - val_loss: 10.5876\n",
      "Epoch 34/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.9358 - val_loss: 10.5764\n",
      "Epoch 35/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.9043 - val_loss: 10.5943\n",
      "Epoch 36/150\n",
      " 94/220 [===========>..................] - ETA: 8s - loss: 10.9069"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 19s 84ms/step - loss: 10.4696 - val_loss: 10.4150\n",
      "Epoch 68/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4587 - val_loss: 10.2847\n",
      "Epoch 69/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4404 - val_loss: 10.3326\n",
      "Epoch 70/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.4213 - val_loss: 10.3584\n",
      "Epoch 71/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.4209 - val_loss: 10.3499\n",
      "Epoch 72/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4107 - val_loss: 10.4153\n",
      "Epoch 73/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.4083 - val_loss: 10.2886\n",
      "Epoch 74/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.4064 - val_loss: 10.3013\n",
      "Epoch 75/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.3853 - val_loss: 10.2697\n",
      "Epoch 76/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3798 - val_loss: 10.2792\n",
      "Epoch 77/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 10.3599 - val_loss: 10.4507\n",
      "Epoch 78/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.3509 - val_loss: 10.2794\n",
      "Epoch 79/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3493 - val_loss: 10.3046\n",
      "Epoch 80/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.3277 - val_loss: 10.2916\n",
      "Epoch 81/150\n",
      "130/220 [================>.............] - ETA: 7s - loss: 10.2779"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 18s 80ms/step - loss: 10.2723 - val_loss: 10.2875\n",
      "Epoch 88/150\n",
      "220/220 [==============================] - 18s 79ms/step - loss: 10.2712 - val_loss: 10.2518\n",
      "Epoch 89/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2696 - val_loss: 10.2907\n",
      "Epoch 90/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.2493 - val_loss: 10.2216\n",
      "Epoch 91/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 10.2768 - val_loss: 10.2078\n",
      "Epoch 92/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2321 - val_loss: 10.2650\n",
      "Epoch 93/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2347 - val_loss: 10.2723\n",
      "Epoch 94/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.2234 - val_loss: 10.2153\n",
      "Epoch 95/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.2240 - val_loss: 10.4275\n",
      "Epoch 96/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2013 - val_loss: 10.2108\n",
      "========================= TRIAL 2, MAE: 15.03891836795422 =========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_49_layer_call_fn, lstm_cell_49_layer_call_and_return_conditional_losses, lstm_cell_50_layer_call_fn, lstm_cell_50_layer_call_and_return_conditional_losses, lstm_cell_52_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd91343a310> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fdb26a86850> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd910f15b10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd91747c150> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to experiments/similar_trades_small_experiment_baselines/new_ys_experiment_2023-11-14_22:07/model/model_2\n",
      "Epoch 1/150\n",
      "220/220 [==============================] - 125s 393ms/step - loss: 39.7360 - val_loss: 28.5582\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 28.7903 - val_loss: 29.3919\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 16.8656 - val_loss: 15.7483\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 13.0320 - val_loss: 13.0975\n",
      "Epoch 5/150\n",
      "  1/220 [..............................] - ETA: 14s - loss: 12.5240"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 16s 73ms/step - loss: 11.6831 - val_loss: 11.3354\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 11.6096 - val_loss: 11.0663\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 11.5653 - val_loss: 10.9584\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.5107 - val_loss: 11.0575\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.4725 - val_loss: 10.8757\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 11.4292 - val_loss: 10.9980\n",
      "Epoch 18/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.3804 - val_loss: 10.8624\n",
      "Epoch 19/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.3365 - val_loss: 10.8050\n",
      "Epoch 20/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 11.3113 - val_loss: 10.8352\n",
      "Epoch 21/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 11.2666 - val_loss: 10.7605\n",
      "Epoch 22/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.2437 - val_loss: 10.7352\n",
      "Epoch 23/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.2075 - val_loss: 10.7713\n",
      "Epoch 24/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 11.1718 - val_loss: 10.7710\n",
      "Epoch 25/150\n",
      "216/220 [============================>.] - ETA: 0s - loss: 11.1381"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 16s 74ms/step - loss: 10.9493 - val_loss: 10.5621\n",
      "Epoch 34/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.9439 - val_loss: 10.6895\n",
      "Epoch 35/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.9020 - val_loss: 10.6114\n",
      "Epoch 36/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.9005 - val_loss: 10.5678\n",
      "Epoch 37/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.8840 - val_loss: 10.5536\n",
      "Epoch 38/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.8543 - val_loss: 10.6306\n",
      "Epoch 39/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.8365 - val_loss: 10.5936\n",
      "Epoch 40/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.8027 - val_loss: 10.5988\n",
      "Epoch 41/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.7963 - val_loss: 10.6676\n",
      "Epoch 42/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7767 - val_loss: 10.4647\n",
      "Epoch 43/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.7581 - val_loss: 10.5442\n",
      "Epoch 44/150\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 10.7503 - val_loss: 10.5054\n",
      "Epoch 45/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.7268 - val_loss: 10.4514\n",
      "Epoch 46/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7131 - val_loss: 10.5239\n",
      "Epoch 47/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.6942 - val_loss: 10.4401\n",
      "Epoch 48/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.6751 - val_loss: 10.5623\n",
      "Epoch 49/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6771 - val_loss: 10.5550\n",
      "Epoch 50/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.6520 - val_loss: 10.4593\n",
      "Epoch 51/150\n",
      "220/220 [==============================] - 19s 83ms/step - loss: 10.6355 - val_loss: 10.4019\n",
      "Epoch 52/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.6378 - val_loss: 10.5674\n",
      "Epoch 53/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6109 - val_loss: 10.5191\n",
      "Epoch 54/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 10.6063 - val_loss: 10.4004\n",
      "Epoch 55/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.5928 - val_loss: 10.3888\n",
      "Epoch 56/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5887 - val_loss: 10.4449\n",
      "Epoch 57/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.5673 - val_loss: 10.4074\n",
      "Epoch 58/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.5615 - val_loss: 10.4116\n",
      "Epoch 59/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5320 - val_loss: 10.4209\n",
      "Epoch 60/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5370 - val_loss: 10.3712\n",
      "Epoch 61/150\n",
      "220/220 [==============================] - 19s 85ms/step - loss: 10.5116 - val_loss: 10.4328\n",
      "Epoch 62/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.5126 - val_loss: 10.4076\n",
      "Epoch 63/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4937 - val_loss: 10.4176\n",
      "Epoch 64/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.4917 - val_loss: 10.3533\n",
      "Epoch 65/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.4814 - val_loss: 10.3643\n",
      "Epoch 66/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.4710 - val_loss: 10.4956\n",
      "Epoch 67/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.4554 - val_loss: 10.4534\n",
      "Epoch 68/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.4604 - val_loss: 10.3446\n",
      "Epoch 69/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.4275 - val_loss: 10.3401\n",
      "Epoch 70/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.4323 - val_loss: 10.3915\n",
      "Epoch 71/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 10.4106 - val_loss: 10.2938\n",
      "Epoch 72/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.4057 - val_loss: 10.3667\n",
      "Epoch 73/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3794 - val_loss: 10.2972\n",
      "Epoch 74/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.3902 - val_loss: 10.3637\n",
      "Epoch 75/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.3690 - val_loss: 10.3094\n",
      "Epoch 76/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3570 - val_loss: 10.3221\n",
      "Epoch 77/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3598 - val_loss: 10.4739\n",
      "Epoch 78/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 10.3539 - val_loss: 10.3045\n",
      "Epoch 79/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3423 - val_loss: 10.3842\n",
      "Epoch 80/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3364 - val_loss: 10.3456\n",
      "Epoch 81/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.3237 - val_loss: 10.2351\n",
      "Epoch 82/150\n",
      "220/220 [==============================] - 18s 79ms/step - loss: 10.3166 - val_loss: 10.4143\n",
      "Epoch 83/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3128 - val_loss: 10.3611\n",
      "Epoch 84/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.3107 - val_loss: 10.2847\n",
      "Epoch 85/150\n",
      "220/220 [==============================] - 19s 84ms/step - loss: 10.2974 - val_loss: 10.2765\n",
      "Epoch 86/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.2691 - val_loss: 10.3556\n",
      "Epoch 87/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.2687 - val_loss: 10.2320\n",
      "Epoch 88/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.2513 - val_loss: 10.2788\n",
      "Epoch 89/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.2527 - val_loss: 10.3443\n",
      "Epoch 90/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2520 - val_loss: 10.2054\n",
      "Epoch 91/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.2634 - val_loss: 10.2071\n",
      "Epoch 92/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.2402 - val_loss: 10.2264\n",
      "Epoch 93/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.2359 - val_loss: 10.2474\n",
      "Epoch 94/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.2337 - val_loss: 10.2160\n",
      "Epoch 95/150\n",
      "220/220 [==============================] - 19s 86ms/step - loss: 10.2183 - val_loss: 10.3561\n",
      "Epoch 96/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.2021 - val_loss: 10.3814\n",
      "Epoch 97/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.2061 - val_loss: 10.2513\n",
      "Epoch 98/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.1815 - val_loss: 10.1977\n",
      "Epoch 99/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.1950 - val_loss: 10.2165\n",
      "Epoch 100/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.1811 - val_loss: 10.2940\n",
      "Epoch 101/150\n",
      "220/220 [==============================] - 17s 80ms/step - loss: 10.1750 - val_loss: 10.1987\n",
      "Epoch 102/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.1664 - val_loss: 10.1941\n",
      "Epoch 103/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.1497 - val_loss: 10.1809\n",
      "Epoch 104/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.1447 - val_loss: 10.1458\n",
      "Epoch 105/150\n",
      "220/220 [==============================] - 19s 85ms/step - loss: 10.1345 - val_loss: 10.1709\n",
      "Epoch 106/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.1332 - val_loss: 10.1963\n",
      "Epoch 107/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1450 - val_loss: 10.1727\n",
      "Epoch 108/150\n",
      "220/220 [==============================] - 19s 86ms/step - loss: 10.1233 - val_loss: 10.2012\n",
      "Epoch 109/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.1311 - val_loss: 10.2085\n",
      "Epoch 110/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1244 - val_loss: 10.2791\n",
      "Epoch 111/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.1032 - val_loss: 10.1743\n",
      "Epoch 112/150\n",
      "220/220 [==============================] - 19s 87ms/step - loss: 10.0922 - val_loss: 10.2510\n",
      "Epoch 113/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.0959 - val_loss: 10.3484\n",
      "Epoch 114/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.0825 - val_loss: 10.1427\n",
      "Epoch 115/150\n",
      "220/220 [==============================] - 19s 88ms/step - loss: 10.0703 - val_loss: 10.1846\n",
      "Epoch 116/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.0713 - val_loss: 10.1856\n",
      "Epoch 117/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.0712 - val_loss: 10.2224\n",
      "Epoch 118/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.0399 - val_loss: 10.1697\n",
      "Epoch 119/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.0520 - val_loss: 10.1504\n",
      "Epoch 120/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.0459 - val_loss: 10.2005\n",
      "Epoch 121/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.0497 - val_loss: 10.1053\n",
      "Epoch 122/150\n",
      "220/220 [==============================] - 19s 88ms/step - loss: 10.0350 - val_loss: 10.1934\n",
      "Epoch 123/150\n",
      "220/220 [==============================] - 17s 74ms/step - loss: 10.0196 - val_loss: 10.1568\n",
      "Epoch 124/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.0093 - val_loss: 10.1007\n",
      "Epoch 125/150\n",
      "220/220 [==============================] - 17s 80ms/step - loss: 10.0103 - val_loss: 10.1069\n",
      "Epoch 126/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.0082 - val_loss: 10.1017\n",
      "Epoch 127/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.0107 - val_loss: 10.1593\n",
      "Epoch 128/150\n",
      "220/220 [==============================] - 16s 75ms/step - loss: 9.9900 - val_loss: 10.1073\n",
      "Epoch 129/150\n",
      "220/220 [==============================] - 19s 85ms/step - loss: 9.9912 - val_loss: 10.1653\n",
      "Epoch 130/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 9.9686 - val_loss: 10.0854\n",
      "Epoch 131/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 9.9807 - val_loss: 10.1536\n",
      "Epoch 132/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 9.9579 - val_loss: 10.0938\n",
      "Epoch 133/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 9.9811 - val_loss: 10.1173\n",
      "Epoch 134/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 9.9526 - val_loss: 10.1714\n",
      "Epoch 135/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 9.9405 - val_loss: 10.1468\n",
      "Epoch 136/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 9.9454 - val_loss: 10.1332\n",
      "Epoch 137/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 9.9470 - val_loss: 10.0968\n",
      "Epoch 138/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 9.9307 - val_loss: 10.0651\n",
      "Epoch 139/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 9.9223 - val_loss: 10.0585\n",
      "Epoch 140/150\n",
      "220/220 [==============================] - 16s 75ms/step - loss: 9.9294 - val_loss: 10.0719\n",
      "Epoch 141/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 9.9182 - val_loss: 10.1024\n",
      "Epoch 142/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 9.9238 - val_loss: 10.2152\n",
      "Epoch 143/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 9.8929 - val_loss: 10.0994\n",
      "Epoch 144/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 9.8947 - val_loss: 10.0580\n",
      "Epoch 145/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 9.8926 - val_loss: 10.1006\n",
      "Epoch 146/150\n",
      "220/220 [==============================] - 19s 83ms/step - loss: 9.8835 - val_loss: 10.0990\n",
      "Epoch 147/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 9.8843 - val_loss: 10.1202\n",
      "Epoch 148/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 9.8573 - val_loss: 10.0724\n",
      "Epoch 149/150\n",
      "220/220 [==============================] - 19s 86ms/step - loss: 9.8639 - val_loss: 10.2050\n",
      "Epoch 150/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 9.8555 - val_loss: 10.0766\n",
      "========================= TRIAL 3, MAE: 15.371468678506515 =========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_55_layer_call_fn, lstm_cell_55_layer_call_and_return_conditional_losses, lstm_cell_56_layer_call_fn, lstm_cell_56_layer_call_and_return_conditional_losses, lstm_cell_58_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd90a2c4c50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd90c99c210> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd906953190> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd905279450> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to experiments/similar_trades_small_experiment_baselines/new_ys_experiment_2023-11-14_22:07/model/model_3\n",
      "Epoch 1/150\n",
      "220/220 [==============================] - 135s 406ms/step - loss: 39.7337 - val_loss: 28.3975\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 28.7934 - val_loss: 29.1590\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 19s 86ms/step - loss: 16.8673 - val_loss: 14.8854\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 13.0342 - val_loss: 12.2906\n",
      "Epoch 5/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 12.5544 - val_loss: 11.8798\n",
      "Epoch 6/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 12.3236 - val_loss: 11.6719\n",
      "Epoch 7/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 12.1580 - val_loss: 11.4713\n",
      "Epoch 8/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 12.0173 - val_loss: 11.4107\n",
      "Epoch 9/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.9156 - val_loss: 11.2938\n",
      "Epoch 10/150\n",
      "220/220 [==============================] - 19s 87ms/step - loss: 11.8258 - val_loss: 11.2557\n",
      "Epoch 11/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 11.7564 - val_loss: 11.1918\n",
      "Epoch 12/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.6843 - val_loss: 11.0782\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 11.6140 - val_loss: 11.1485\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 11.5629 - val_loss: 10.9830\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.5134 - val_loss: 10.9349\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 11.4655 - val_loss: 10.9684\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 19s 85ms/step - loss: 11.4345 - val_loss: 10.9006\n",
      "Epoch 18/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.3817 - val_loss: 10.8474\n",
      "Epoch 19/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.3368 - val_loss: 10.7969\n",
      "Epoch 20/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 11.3105 - val_loss: 10.8887\n",
      "Epoch 21/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 11.2775 - val_loss: 10.8028\n",
      "Epoch 22/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.2524 - val_loss: 10.8673\n",
      "Epoch 23/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 11.2079 - val_loss: 10.7690\n",
      "Epoch 24/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 11.1730 - val_loss: 10.7509\n",
      "Epoch 25/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.1482 - val_loss: 10.8030\n",
      "Epoch 26/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.1133 - val_loss: 10.6699\n",
      "Epoch 27/150\n",
      "220/220 [==============================] - 19s 85ms/step - loss: 11.0919 - val_loss: 10.6694\n",
      "Epoch 28/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.0681 - val_loss: 10.7344\n",
      "Epoch 29/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.0416 - val_loss: 10.6667\n",
      "Epoch 30/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 11.0204 - val_loss: 10.6630\n",
      "Epoch 31/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.9916 - val_loss: 10.6822\n",
      "Epoch 32/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.9752 - val_loss: 10.5619\n",
      "Epoch 33/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.9547 - val_loss: 10.5943\n",
      "Epoch 34/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.9372 - val_loss: 10.6126\n",
      "Epoch 35/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.8979 - val_loss: 10.7470\n",
      "Epoch 36/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.8942 - val_loss: 10.5368\n",
      "Epoch 37/150\n",
      "220/220 [==============================] - 18s 84ms/step - loss: 10.8742 - val_loss: 10.5607\n",
      "Epoch 38/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.8496 - val_loss: 10.5007\n",
      "Epoch 39/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.8332 - val_loss: 10.5301\n",
      "Epoch 40/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.8142 - val_loss: 10.5987\n",
      "Epoch 41/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.7911 - val_loss: 10.5069\n",
      "Epoch 42/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7935 - val_loss: 10.5197\n",
      "Epoch 43/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7589 - val_loss: 10.5457\n",
      "Epoch 44/150\n",
      "220/220 [==============================] - 20s 90ms/step - loss: 10.7442 - val_loss: 10.4915\n",
      "Epoch 45/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7306 - val_loss: 10.4690\n",
      "Epoch 46/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7213 - val_loss: 10.6494\n",
      "Epoch 47/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.7079 - val_loss: 10.6825\n",
      "Epoch 48/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.6837 - val_loss: 10.6849\n",
      "Epoch 49/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6688 - val_loss: 10.4144\n",
      "Epoch 50/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.6536 - val_loss: 10.4795\n",
      "Epoch 51/150\n",
      "220/220 [==============================] - 19s 86ms/step - loss: 10.6372 - val_loss: 10.5843\n",
      "Epoch 52/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6426 - val_loss: 10.4696\n",
      "Epoch 53/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6155 - val_loss: 10.4297\n",
      "Epoch 54/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.6096 - val_loss: 10.4376\n",
      "Epoch 55/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.5934 - val_loss: 10.4979\n",
      "Epoch 56/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5962 - val_loss: 10.4327\n",
      "Epoch 57/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.5650 - val_loss: 10.3932\n",
      "Epoch 58/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 10.5544 - val_loss: 10.4844\n",
      "Epoch 59/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5382 - val_loss: 10.4257\n",
      "Epoch 60/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5307 - val_loss: 10.3787\n",
      "Epoch 61/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.5192 - val_loss: 10.3879\n",
      "Epoch 62/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.5169 - val_loss: 10.3502\n",
      "Epoch 63/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4862 - val_loss: 10.3990\n",
      "Epoch 64/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.4918 - val_loss: 10.6165\n",
      "Epoch 65/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 10.4729 - val_loss: 10.3256\n",
      "Epoch 66/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4704 - val_loss: 10.3765\n",
      "Epoch 67/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.4674 - val_loss: 10.3265\n",
      "Epoch 68/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.4442 - val_loss: 10.3164\n",
      "Epoch 69/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4355 - val_loss: 10.2644\n",
      "Epoch 70/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4339 - val_loss: 10.3141\n",
      "Epoch 71/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.4284 - val_loss: 10.2830\n",
      "Epoch 72/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.4054 - val_loss: 10.2735\n",
      "Epoch 73/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3874 - val_loss: 10.2734\n",
      "Epoch 74/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.3873 - val_loss: 10.2845\n",
      "Epoch 75/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 10.3696 - val_loss: 10.2732\n",
      "Epoch 76/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3602 - val_loss: 10.2741\n",
      "Epoch 77/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3537 - val_loss: 10.3650\n",
      "Epoch 78/150\n",
      "220/220 [==============================] - 19s 85ms/step - loss: 10.3600 - val_loss: 10.3545\n",
      "Epoch 79/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.3195 - val_loss: 10.3603\n",
      "========================= TRIAL 4, MAE: 15.03453673737532 =========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_61_layer_call_fn, lstm_cell_61_layer_call_and_return_conditional_losses, lstm_cell_62_layer_call_fn, lstm_cell_62_layer_call_and_return_conditional_losses, lstm_cell_64_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd9000ae590> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd902435f90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd8fe81a310> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd901df6b10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to experiments/similar_trades_small_experiment_baselines/new_ys_experiment_2023-11-14_22:07/model/model_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_37_layer_call_fn, lstm_cell_37_layer_call_and_return_conditional_losses, lstm_cell_38_layer_call_fn, lstm_cell_38_layer_call_and_return_conditional_losses, lstm_cell_40_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fceb266f990> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fce94390710> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fce94084890> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fce940bed10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_43_layer_call_fn, lstm_cell_43_layer_call_and_return_conditional_losses, lstm_cell_44_layer_call_fn, lstm_cell_44_layer_call_and_return_conditional_losses, lstm_cell_46_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fceaea8d250> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fdb2767ae50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd8f1d04a90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fdb27c21c50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_49_layer_call_fn, lstm_cell_49_layer_call_and_return_conditional_losses, lstm_cell_50_layer_call_fn, lstm_cell_50_layer_call_and_return_conditional_losses, lstm_cell_52_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd91343a310> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fdb26a86850> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd910f15b10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd91747c150> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_55_layer_call_fn, lstm_cell_55_layer_call_and_return_conditional_losses, lstm_cell_56_layer_call_fn, lstm_cell_56_layer_call_and_return_conditional_losses, lstm_cell_58_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd90a2c4c50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd90c99c210> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd906953190> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd905279450> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_61_layer_call_fn, lstm_cell_61_layer_call_and_return_conditional_losses, lstm_cell_62_layer_call_fn, lstm_cell_62_layer_call_and_return_conditional_losses, lstm_cell_64_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd9000ae590> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd902435f90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd8fe81a310> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd901df6b10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "reset_model_features()\n",
    "params, normalizers, x_train, y_train, x_val, y_val, x_test, y_test, val_idx = create_data_set_and_model(train_dataframe, \n",
    "                                                                                                         test_dataframe, \n",
    "                                                                                                         trade_history_col)\n",
    "shuffle_buffer = .75\n",
    "use_bottleneck_model = False\n",
    "NUM_EPOCHS = 150\n",
    "verbose = 1\n",
    "model_to_use='default'\n",
    "ytw_result = run_experiment(5, experiment_prefix='similar_trades_small_experiment_baselines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "6ff366f3-1bb0-4576-99f2-38f60d535a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.060472582225009"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_model = keras.models.load_model('experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-14_19:33/model/model_0')\n",
    "test_dataframe['prediction_baseline'] = og_model.predict(x_test, batch_size=10000)\n",
    "mean_absolute_error(test_dataframe['new_ys'] , test_dataframe['prediction_baseline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "b378ab89-76b3-4edc-a05a-159899f5b0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.956470404621125\n",
      "15.070765638603984\n",
      "15.03891836795422\n",
      "15.371468678506515\n",
      "15.03453673737532\n"
     ]
    }
   ],
   "source": [
    "for _, model in ytw_result: \n",
    "    print(mean_absolute_error(test_dataframe['new_ys'] , model.predict(x_test, batch_size=10000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1448cf8-3021-4fd3-bca5-07f07db3c1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.094431965412232"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([14.956470404621125,\n",
    "15.070765638603984,\n",
    "15.03891836795422,\n",
    "15.371468678506515,\n",
    "15.03453673737532]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48c95291-0dc0-4ad6-a13d-c1db25a91511",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_cols_subset = ['ex_cusip_masked_series_average_1',\n",
    "         'ex_cusip_masked_series_average_10',\n",
    "         'ex_cusip_masked_series_average_50']\n",
    "\n",
    "new_cols_subset = ['series_max', 'series_min', 'series_vol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8811f324-0fba-463f-b727-290d8707fbde",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA: N = 2191544, MIN DATE = 2023-07-03 00:00:00, MAX DATE = 2023-09-22 00:00:00\n",
      "VALIDATION DATA: N = 243504, MIN DATE = 2023-07-03 00:00:00, MAX DATE = 2023-09-22 00:00:00\n",
      "TEST DATA: N = 216931, MIN DATE = 2023-09-25 00:00:00, MAX DATE = 2023-09-28 00:00:00\n",
      "Experiment results will be saved to experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-15_12:52\n",
      "\n",
      "Performing 5 runs with parameters: {'train_start': '2023-07-01', 'train_end': '2023-09-25', 'test_start': '2023-09-25', 'test_end': '2023-09-29', 'train_size': 2435048, 'test_size': 216931, 'VALIDATION_SPLIT': 0.1, 'LEARNING_RATE': 0.0007, 'BATCH_SIZE': 10000, 'NUM_EPOCHS': 150, 'DROPOUT': 0.1, 'TRADE_SEQUENCE_LENGTH': 5, 'NUM_FEATURES': 6, 'target_variable': 'new_ys', 'features': ['callable', 'sinking', 'zerocoupon', 'is_non_transaction_based_compensation', 'is_general_obligation', 'callable_at_cav', 'extraordinary_make_whole_call', 'make_whole_call', 'has_unexpired_lines_of_credit', 'escrow_exists', 'quantity', 'days_to_maturity', 'days_to_call', 'coupon', 'issue_amount', 'last_seconds_ago', 'last_yield_spread', 'days_to_settle', 'days_to_par', 'maturity_amount', 'issue_price', 'orig_principal_amount', 'max_amount_outstanding', 'accrued_days', 'days_in_interest_payment', 'A/E', 'ficc_treasury_spread', 'max_ys_ys', 'max_ys_ago', 'max_ys_qdiff', 'min_ys_ys', 'min_ys_ago', 'min_ys_qdiff', 'max_qty_ys', 'max_qty_ago', 'max_qty_qdiff', 'min_ago_ys', 'min_ago_ago', 'min_ago_qdiff', 'D_min_ago_ys', 'D_min_ago_ago', 'D_min_ago_qdiff', 'P_min_ago_ys', 'P_min_ago_ago', 'P_min_ago_qdiff', 'S_min_ago_ys', 'S_min_ago_ago', 'S_min_ago_qdiff', 'ex_cusip_masked_series_average_1', 'ex_cusip_masked_series_average_10', 'ex_cusip_masked_series_average_50', 'series_max', 'series_min', 'series_vol', 'rating', 'incorporated_state_code', 'trade_type', 'purpose_class', 'max_ys_ttypes', 'min_ys_ttypes', 'max_qty_ttypes', 'min_ago_ttypes', 'D_min_ago_ttypes', 'P_min_ago_ttypes', 'S_min_ago_ttypes'], 'shuffle_buffer': 0.75, 'use_bottleneck_model': False}\n",
      "Epoch 1/150\n",
      "220/220 [==============================] - 130s 407ms/step - loss: 39.7501 - val_loss: 28.0312\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 16s 71ms/step - loss: 28.7796 - val_loss: 29.7193\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 16.9026 - val_loss: 15.0801\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 13.0345 - val_loss: 12.8525\n",
      "Epoch 5/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 12.5468 - val_loss: 12.2410\n",
      "Epoch 6/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 12.2991 - val_loss: 11.7106\n",
      "Epoch 7/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 12.1243 - val_loss: 11.6175\n",
      "Epoch 8/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 11.9975 - val_loss: 11.3955\n",
      "Epoch 9/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.8889 - val_loss: 11.3102\n",
      "Epoch 10/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 11.7905 - val_loss: 11.1953\n",
      "Epoch 11/150\n",
      "220/220 [==============================] - 18s 79ms/step - loss: 11.7090 - val_loss: 11.2313\n",
      "Epoch 12/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.6588 - val_loss: 11.1301\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.5753 - val_loss: 11.0925\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 11.5279 - val_loss: 11.0647\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 11.4793 - val_loss: 10.9059\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.4288 - val_loss: 11.0150\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 11.3833 - val_loss: 10.9102\n",
      "Epoch 18/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.2724 - val_loss: 10.7887\n",
      "Epoch 21/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 11.2274 - val_loss: 10.7842\n",
      "Epoch 22/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 11.2183 - val_loss: 11.0202\n",
      "Epoch 23/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.1564 - val_loss: 10.7278\n",
      "Epoch 24/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 11.1269 - val_loss: 10.7149\n",
      "Epoch 25/150\n",
      "220/220 [==============================] - 18s 79ms/step - loss: 11.1015 - val_loss: 10.6440\n",
      "Epoch 26/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.0659 - val_loss: 10.9064\n",
      "Epoch 27/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.0528 - val_loss: 10.7000\n",
      "Epoch 28/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 11.0232 - val_loss: 10.6302\n",
      "Epoch 29/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 11.0014 - val_loss: 10.6076\n",
      "Epoch 30/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.9655 - val_loss: 10.6190\n",
      "Epoch 31/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.9546 - val_loss: 10.6079\n",
      "Epoch 32/150\n",
      "220/220 [==============================] - 18s 79ms/step - loss: 10.9257 - val_loss: 10.5864\n",
      "Epoch 33/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.9150 - val_loss: 10.5676\n",
      "Epoch 34/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.8950 - val_loss: 10.9042\n",
      "Epoch 35/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.8653 - val_loss: 10.5275\n",
      "Epoch 36/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.8412 - val_loss: 10.5913\n",
      "Epoch 37/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.8389 - val_loss: 10.6240\n",
      "Epoch 38/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.8136 - val_loss: 10.4856\n",
      "Epoch 39/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.7837 - val_loss: 10.5024\n",
      "Epoch 40/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.7682 - val_loss: 10.5543\n",
      "Epoch 41/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7614 - val_loss: 10.5074\n",
      "Epoch 42/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.7268 - val_loss: 10.5580\n",
      "Epoch 43/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7125 - val_loss: 10.5011\n",
      "Epoch 44/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.7108 - val_loss: 10.4626\n",
      "Epoch 45/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.6795 - val_loss: 10.7284\n",
      "Epoch 46/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.6871 - val_loss: 10.4970\n",
      "Epoch 47/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.6601 - val_loss: 10.5626\n",
      "Epoch 48/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.6714 - val_loss: 10.4419\n",
      "Epoch 49/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.6391 - val_loss: 10.3914\n",
      "Epoch 50/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6193 - val_loss: 10.4833\n",
      "Epoch 51/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5988 - val_loss: 10.4125\n",
      "Epoch 52/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.6017 - val_loss: 10.5474\n",
      "Epoch 53/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.5749 - val_loss: 10.4044\n",
      "Epoch 54/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5828 - val_loss: 10.3820\n",
      "Epoch 55/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.5432 - val_loss: 10.4004\n",
      "Epoch 56/150\n",
      "220/220 [==============================] - 18s 79ms/step - loss: 10.5352 - val_loss: 10.5267\n",
      "Epoch 57/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5280 - val_loss: 10.3369\n",
      "Epoch 58/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.5091 - val_loss: 10.4440\n",
      "Epoch 59/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.5151 - val_loss: 10.3455\n",
      "Epoch 60/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.4867 - val_loss: 10.3304\n",
      "Epoch 61/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.4349 - val_loss: 10.4021\n",
      "Epoch 68/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4067 - val_loss: 10.3563\n",
      "Epoch 69/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.3893 - val_loss: 10.3094\n",
      "Epoch 70/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.3911 - val_loss: 10.2869\n",
      "Epoch 71/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3701 - val_loss: 10.3105\n",
      "Epoch 72/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3618 - val_loss: 10.2061\n",
      "Epoch 73/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.3594 - val_loss: 10.2996\n",
      "Epoch 74/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.3581 - val_loss: 10.2620\n",
      "Epoch 75/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3344 - val_loss: 10.3046\n",
      "Epoch 76/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.3242 - val_loss: 10.2683\n",
      "Epoch 77/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.3191 - val_loss: 10.2895\n",
      "Epoch 78/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3135 - val_loss: 10.2642\n",
      "Epoch 79/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3068 - val_loss: 10.2864\n",
      "Epoch 80/150\n",
      "176/220 [=======================>......] - ETA: 3s - loss: 10.2871"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 121s 363ms/step - loss: 39.7491 - val_loss: 27.6856\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 28.7853 - val_loss: 29.8583\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 16.8827 - val_loss: 15.6737\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 13.0384 - val_loss: 12.7526\n",
      "Epoch 5/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 12.5490 - val_loss: 12.1725\n",
      "Epoch 6/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 12.2924 - val_loss: 11.7069\n",
      "Epoch 7/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 12.1268 - val_loss: 11.5121\n",
      "Epoch 8/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 11.9946 - val_loss: 11.3688\n",
      "Epoch 9/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.8952 - val_loss: 11.3493\n",
      "Epoch 10/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 11.7923 - val_loss: 11.3389\n",
      "Epoch 11/150\n",
      "220/220 [==============================] - 19s 84ms/step - loss: 11.7127 - val_loss: 11.2613\n",
      "Epoch 12/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.6630 - val_loss: 11.0530\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.5806 - val_loss: 11.6542\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 11.5355 - val_loss: 10.9540\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 11.4943 - val_loss: 10.9963\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.4515 - val_loss: 10.8946\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 11.3968 - val_loss: 10.9787\n",
      "Epoch 18/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 11.3576 - val_loss: 10.9172\n",
      "Epoch 19/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.3170 - val_loss: 10.8576\n",
      "Epoch 20/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.2714 - val_loss: 10.8929\n",
      "Epoch 21/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 11.2524 - val_loss: 10.9566\n",
      "Epoch 22/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 11.2081 - val_loss: 10.7726\n",
      "Epoch 23/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.1685 - val_loss: 10.7893\n",
      "Epoch 24/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 11.1384 - val_loss: 10.8700\n",
      "Epoch 25/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 11.1102 - val_loss: 10.7053\n",
      "Epoch 26/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.0815 - val_loss: 10.6378\n",
      "Epoch 27/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.0592 - val_loss: 10.8489\n",
      "Epoch 28/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 11.0297 - val_loss: 10.6444\n",
      "Epoch 29/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.9990 - val_loss: 10.8207\n",
      "Epoch 30/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.9978 - val_loss: 10.5929\n",
      "Epoch 31/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.9755 - val_loss: 10.6130\n",
      "Epoch 32/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.9485 - val_loss: 10.5875\n",
      "Epoch 33/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.9356 - val_loss: 10.5668\n",
      "Epoch 34/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.8946 - val_loss: 10.5712\n",
      "Epoch 35/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.8681 - val_loss: 10.6807\n",
      "Epoch 36/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.8696 - val_loss: 10.5139\n",
      "Epoch 37/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.8490 - val_loss: 10.5637\n",
      "Epoch 38/150\n",
      "220/220 [==============================] - 18s 84ms/step - loss: 10.8237 - val_loss: 10.5592\n",
      "Epoch 39/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.7888 - val_loss: 10.5803\n",
      "Epoch 40/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.7838 - val_loss: 10.5638\n",
      "Epoch 41/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.7744 - val_loss: 10.5473\n",
      "Epoch 42/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.7464 - val_loss: 10.4862\n",
      "Epoch 43/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.7169 - val_loss: 10.4826\n",
      "Epoch 44/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7131 - val_loss: 10.4806\n",
      "Epoch 45/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 10.6929 - val_loss: 10.4981\n",
      "Epoch 46/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6777 - val_loss: 10.4972\n",
      "Epoch 47/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6783 - val_loss: 10.4503\n",
      "Epoch 48/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.6531 - val_loss: 10.4487\n",
      "Epoch 49/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.6445 - val_loss: 10.3804\n",
      "Epoch 50/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6219 - val_loss: 10.3649\n",
      "Epoch 51/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.6143 - val_loss: 10.4542\n",
      "Epoch 52/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.5897 - val_loss: 10.5071\n",
      "Epoch 53/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5952 - val_loss: 10.5829\n",
      "Epoch 54/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5741 - val_loss: 10.3565\n",
      "Epoch 55/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.5605 - val_loss: 10.4385\n",
      "Epoch 56/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.5388 - val_loss: 10.3355\n",
      "Epoch 57/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5421 - val_loss: 10.3358\n",
      "Epoch 58/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.5179 - val_loss: 10.6140\n",
      "Epoch 59/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.5043 - val_loss: 10.4003\n",
      "Epoch 60/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4879 - val_loss: 10.3368\n",
      "Epoch 61/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4803 - val_loss: 10.3410\n",
      "Epoch 62/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.4830 - val_loss: 10.4197\n",
      "Epoch 63/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4542 - val_loss: 10.3239\n",
      "Epoch 64/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4499 - val_loss: 10.2682\n",
      "Epoch 65/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.4360 - val_loss: 10.2852\n",
      "Epoch 66/150\n",
      "220/220 [==============================] - 16s 75ms/step - loss: 10.4111 - val_loss: 10.2527\n",
      "Epoch 67/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4125 - val_loss: 10.3737\n",
      "Epoch 68/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.4016 - val_loss: 10.3807\n",
      "Epoch 69/150\n",
      "220/220 [==============================] - 18s 78ms/step - loss: 10.4014 - val_loss: 10.3005\n",
      "Epoch 70/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3894 - val_loss: 10.2604\n",
      "Epoch 71/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.3891 - val_loss: 10.2775\n",
      "Epoch 72/150\n",
      "220/220 [==============================] - 29s 131ms/step - loss: 10.3813 - val_loss: 10.3193\n",
      "Epoch 73/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.3664 - val_loss: 10.2565\n",
      "Epoch 74/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3527 - val_loss: 10.3006\n",
      "Epoch 75/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.3377 - val_loss: 10.2378\n",
      "Epoch 76/150\n",
      "220/220 [==============================] - 17s 74ms/step - loss: 10.3339 - val_loss: 10.2581\n",
      "Epoch 77/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3272 - val_loss: 10.3973\n",
      "Epoch 78/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.3088 - val_loss: 10.2524\n",
      "Epoch 79/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.2970 - val_loss: 10.2371\n",
      "Epoch 80/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3043 - val_loss: 10.2388\n",
      "Epoch 81/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.2997 - val_loss: 10.2598\n",
      "Epoch 82/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.2762 - val_loss: 10.2249\n",
      "Epoch 83/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2721 - val_loss: 10.2474\n",
      "Epoch 84/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2605 - val_loss: 10.2269\n",
      "Epoch 85/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.2649 - val_loss: 10.1989\n",
      "Epoch 86/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.2382 - val_loss: 10.2402\n",
      "Epoch 87/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2249 - val_loss: 10.3426\n",
      "Epoch 88/150\n",
      "220/220 [==============================] - 16s 75ms/step - loss: 10.2184 - val_loss: 10.1820\n",
      "Epoch 89/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.2202 - val_loss: 10.1985\n",
      "Epoch 90/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2039 - val_loss: 10.1874\n",
      "Epoch 91/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1938 - val_loss: 10.2254\n",
      "Epoch 92/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.1877 - val_loss: 10.2122\n",
      "Epoch 93/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.1743 - val_loss: 10.2070\n",
      "Epoch 94/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1789 - val_loss: 10.2199\n",
      "Epoch 95/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.1597 - val_loss: 10.1575\n",
      "Epoch 96/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.1539 - val_loss: 10.1034\n",
      "Epoch 97/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1674 - val_loss: 10.1430\n",
      "Epoch 98/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1577 - val_loss: 10.1043\n",
      "Epoch 99/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.1430 - val_loss: 10.2529\n",
      "Epoch 100/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.1275 - val_loss: 10.1616\n",
      "Epoch 101/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1314 - val_loss: 10.1698\n",
      "Epoch 102/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.1278 - val_loss: 10.1629\n",
      "Epoch 103/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.1066 - val_loss: 10.1171\n",
      "Epoch 104/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1059 - val_loss: 10.1304\n",
      "Epoch 105/150\n",
      "220/220 [==============================] - 16s 75ms/step - loss: 10.1018 - val_loss: 10.1560\n",
      "Epoch 106/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.0849 - val_loss: 10.1568\n",
      "========================= TRIAL 1, MAE: 14.972722803104359 =========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_7_layer_call_fn, lstm_cell_7_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn, lstm_cell_8_layer_call_and_return_conditional_losses, lstm_cell_10_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-15_12:52/model/model_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-15_12:52/model/model_1/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4cce42710> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4ccb2e290> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4d40cd590> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4cf426d10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-15_12:52/model/model_1\n",
      "Epoch 1/150\n",
      "220/220 [==============================] - 122s 353ms/step - loss: 39.7497 - val_loss: 27.7311\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 28.7854 - val_loss: 29.3701\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 16.8972 - val_loss: 15.6011\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 13.0439 - val_loss: 12.8881\n",
      "Epoch 5/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 12.5436 - val_loss: 12.6320\n",
      "Epoch 6/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 12.2921 - val_loss: 11.6768\n",
      "Epoch 7/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 12.1217 - val_loss: 11.7278\n",
      "Epoch 8/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.9907 - val_loss: 11.3997\n",
      "Epoch 9/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 11.8985 - val_loss: 11.2953\n",
      "Epoch 10/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.7987 - val_loss: 11.1516\n",
      "Epoch 11/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.7166 - val_loss: 11.2007\n",
      "Epoch 12/150\n",
      "220/220 [==============================] - 20s 92ms/step - loss: 11.6630 - val_loss: 11.1403\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 21s 97ms/step - loss: 11.5836 - val_loss: 11.0411\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.5332 - val_loss: 11.0037\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 11.4696 - val_loss: 10.9147\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 11.4309 - val_loss: 10.9048\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.3825 - val_loss: 10.8462\n",
      "Epoch 18/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.3368 - val_loss: 11.0051\n",
      "Epoch 19/150\n",
      "220/220 [==============================] - 19s 86ms/step - loss: 11.2822 - val_loss: 10.8266\n",
      "Epoch 20/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.2614 - val_loss: 10.8861\n",
      "Epoch 21/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.2247 - val_loss: 10.7986\n",
      "Epoch 22/150\n",
      "220/220 [==============================] - 28s 130ms/step - loss: 11.1934 - val_loss: 10.7691\n",
      "Epoch 23/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.1706 - val_loss: 10.6877\n",
      "Epoch 24/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.1228 - val_loss: 10.7481\n",
      "Epoch 25/150\n",
      "220/220 [==============================] - 26s 117ms/step - loss: 11.1056 - val_loss: 10.8621\n",
      "Epoch 26/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.0702 - val_loss: 10.7514\n",
      "Epoch 27/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.0484 - val_loss: 10.7396\n",
      "Epoch 28/150\n",
      "220/220 [==============================] - 21s 96ms/step - loss: 11.0122 - val_loss: 10.6098\n",
      "Epoch 29/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.9988 - val_loss: 10.6100\n",
      "Epoch 30/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.9590 - val_loss: 10.6097\n",
      "Epoch 31/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.9558 - val_loss: 10.6569\n",
      "Epoch 32/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.9321 - val_loss: 10.5354\n",
      "Epoch 33/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.8962 - val_loss: 10.5379\n",
      "Epoch 34/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.8816 - val_loss: 10.6566\n",
      "Epoch 35/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.8533 - val_loss: 10.5387\n",
      "Epoch 36/150\n",
      "220/220 [==============================] - 17s 74ms/step - loss: 10.8526 - val_loss: 10.5172\n",
      "Epoch 37/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.8330 - val_loss: 10.5395\n",
      "Epoch 38/150\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 10.8127 - val_loss: 10.5118\n",
      "Epoch 39/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.7730 - val_loss: 10.4898\n",
      "Epoch 40/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7669 - val_loss: 10.4842\n",
      "Epoch 41/150\n",
      "220/220 [==============================] - 22s 101ms/step - loss: 10.7568 - val_loss: 10.4339\n",
      "Epoch 42/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.7209 - val_loss: 10.5031\n",
      "Epoch 43/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.7059 - val_loss: 10.5559\n",
      "Epoch 44/150\n",
      "220/220 [==============================] - 25s 115ms/step - loss: 10.6872 - val_loss: 10.5371\n",
      "Epoch 45/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.6762 - val_loss: 10.4639\n",
      "Epoch 46/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.6584 - val_loss: 10.4185\n",
      "Epoch 47/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.6530 - val_loss: 10.7214\n",
      "Epoch 48/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6415 - val_loss: 10.4906\n",
      "Epoch 49/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6142 - val_loss: 10.4776\n",
      "Epoch 50/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.6134 - val_loss: 10.4149\n",
      "Epoch 51/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.5957 - val_loss: 10.4757\n",
      "Epoch 52/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6021 - val_loss: 10.5865\n",
      "Epoch 53/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.5678 - val_loss: 10.3677\n",
      "Epoch 54/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.5737 - val_loss: 10.4227\n",
      "Epoch 55/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5429 - val_loss: 10.4185\n",
      "Epoch 56/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5261 - val_loss: 10.3826\n",
      "Epoch 57/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.5029 - val_loss: 10.4520\n",
      "Epoch 58/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.5119 - val_loss: 10.3939\n",
      "Epoch 59/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4882 - val_loss: 10.3386\n",
      "Epoch 60/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.4692 - val_loss: 10.3160\n",
      "Epoch 61/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.4723 - val_loss: 10.3954\n",
      "Epoch 62/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4518 - val_loss: 10.2938\n",
      "Epoch 63/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4330 - val_loss: 10.3535\n",
      "Epoch 64/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.4206 - val_loss: 10.3417\n",
      "Epoch 65/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.4191 - val_loss: 10.3408\n",
      "Epoch 66/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4201 - val_loss: 10.2789\n",
      "Epoch 67/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.4172 - val_loss: 10.4151\n",
      "Epoch 68/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.3884 - val_loss: 10.2760\n",
      "Epoch 69/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3618 - val_loss: 10.2874\n",
      "Epoch 70/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3599 - val_loss: 10.2665\n",
      "Epoch 71/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.3683 - val_loss: 10.2945\n",
      "Epoch 72/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.3577 - val_loss: 10.3429\n",
      "Epoch 73/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3427 - val_loss: 10.2350\n",
      "Epoch 74/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.3438 - val_loss: 10.2668\n",
      "Epoch 75/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.3386 - val_loss: 10.2139\n",
      "Epoch 76/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3174 - val_loss: 10.2392\n",
      "Epoch 77/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3039 - val_loss: 10.2540\n",
      "Epoch 78/150\n",
      "220/220 [==============================] - 20s 91ms/step - loss: 10.3061 - val_loss: 10.3511\n",
      "Epoch 79/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.2932 - val_loss: 10.2121\n",
      "Epoch 80/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 10.2868 - val_loss: 10.1909\n",
      "Epoch 81/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.2797 - val_loss: 10.2389\n",
      "Epoch 82/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.2733 - val_loss: 10.3108\n",
      "Epoch 83/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.2569 - val_loss: 10.2518\n",
      "Epoch 84/150\n",
      "220/220 [==============================] - 18s 79ms/step - loss: 10.2539 - val_loss: 10.2238\n",
      "Epoch 85/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2407 - val_loss: 10.2697\n",
      "Epoch 86/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2240 - val_loss: 10.2194\n",
      "Epoch 87/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.2130 - val_loss: 10.1422\n",
      "Epoch 88/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.2081 - val_loss: 10.1939\n",
      "Epoch 89/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2197 - val_loss: 10.2383\n",
      "Epoch 90/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.2037 - val_loss: 10.2000\n",
      "Epoch 91/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.1753 - val_loss: 10.2196\n",
      "Epoch 92/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.1650 - val_loss: 10.2594\n",
      "Epoch 93/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.1790 - val_loss: 10.1689\n",
      "Epoch 94/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.1720 - val_loss: 10.3372\n",
      "Epoch 95/150\n",
      "220/220 [==============================] - 16s 75ms/step - loss: 10.1607 - val_loss: 10.4107\n",
      "Epoch 96/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1499 - val_loss: 10.1357\n",
      "Epoch 97/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.1498 - val_loss: 10.1451\n",
      "Epoch 98/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.1293 - val_loss: 10.1050\n",
      "Epoch 99/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1228 - val_loss: 10.2042\n",
      "Epoch 100/150\n",
      "220/220 [==============================] - 16s 75ms/step - loss: 10.1239 - val_loss: 10.1645\n",
      "Epoch 101/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.1310 - val_loss: 10.0989\n",
      "Epoch 102/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1026 - val_loss: 10.1198\n",
      "Epoch 103/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.0977 - val_loss: 10.3236\n",
      "Epoch 104/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.0868 - val_loss: 10.2292\n",
      "Epoch 105/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.0714 - val_loss: 10.2975\n",
      "Epoch 106/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.0791 - val_loss: 10.1902\n",
      "Epoch 107/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.0773 - val_loss: 10.0982\n",
      "Epoch 108/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.0515 - val_loss: 10.1803\n",
      "Epoch 109/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.0841 - val_loss: 10.1187\n",
      "Epoch 110/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.0606 - val_loss: 10.1376\n",
      "Epoch 111/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.0425 - val_loss: 10.1211\n",
      "Epoch 112/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.0382 - val_loss: 10.1068\n",
      "Epoch 113/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.0346 - val_loss: 10.2084\n",
      "Epoch 114/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.0123 - val_loss: 10.1457\n",
      "Epoch 115/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.0114 - val_loss: 10.1165\n",
      "Epoch 116/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.0047 - val_loss: 10.1244\n",
      "Epoch 117/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.0017 - val_loss: 10.1182\n",
      "========================= TRIAL 2, MAE: 14.91724899381621 =========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_13_layer_call_fn, lstm_cell_13_layer_call_and_return_conditional_losses, lstm_cell_14_layer_call_fn, lstm_cell_14_layer_call_and_return_conditional_losses, lstm_cell_16_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-15_12:52/model/model_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-15_12:52/model/model_2/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4d0ae6490> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c4e46fd0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c9923210> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c92b6c90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-15_12:52/model/model_2\n",
      "Epoch 1/150\n",
      "220/220 [==============================] - 127s 375ms/step - loss: 39.7498 - val_loss: 27.7610\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 28.7807 - val_loss: 29.0346\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 16.8930 - val_loss: 15.6726\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 13.0472 - val_loss: 12.9253\n",
      "Epoch 5/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 12.5450 - val_loss: 12.1758\n",
      "Epoch 6/150\n",
      "220/220 [==============================] - 19s 88ms/step - loss: 12.3012 - val_loss: 11.6971\n",
      "Epoch 7/150\n",
      "220/220 [==============================] - 19s 86ms/step - loss: 12.1321 - val_loss: 11.6336\n",
      "Epoch 8/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 12.0046 - val_loss: 11.4529\n",
      "Epoch 9/150\n",
      "220/220 [==============================] - 20s 93ms/step - loss: 11.8915 - val_loss: 11.2861\n",
      "Epoch 10/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.7945 - val_loss: 11.2046\n",
      "Epoch 11/150\n",
      "220/220 [==============================] - 20s 90ms/step - loss: 11.7194 - val_loss: 11.1981\n",
      "Epoch 12/150\n",
      "220/220 [==============================] - 23s 104ms/step - loss: 11.6652 - val_loss: 11.1436\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.5838 - val_loss: 11.1482\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 11.5238 - val_loss: 11.0989\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 19s 83ms/step - loss: 11.4774 - val_loss: 10.8916\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.4340 - val_loss: 11.0540\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.4001 - val_loss: 10.8501\n",
      "Epoch 18/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 11.3527 - val_loss: 10.9137\n",
      "Epoch 19/150\n",
      "220/220 [==============================] - 16s 75ms/step - loss: 11.2996 - val_loss: 10.8239\n",
      "Epoch 20/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.2647 - val_loss: 10.7739\n",
      "Epoch 21/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 11.2326 - val_loss: 10.7337\n",
      "Epoch 22/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 11.1999 - val_loss: 10.7616\n",
      "Epoch 23/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.1706 - val_loss: 10.7079\n",
      "Epoch 24/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.1439 - val_loss: 10.6381\n",
      "Epoch 25/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 11.1140 - val_loss: 10.6742\n",
      "Epoch 26/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.0855 - val_loss: 10.7122\n",
      "Epoch 27/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.0466 - val_loss: 10.6289\n",
      "Epoch 28/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 11.0281 - val_loss: 10.8113\n",
      "Epoch 29/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 11.0026 - val_loss: 10.6221\n",
      "Epoch 30/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.9809 - val_loss: 10.6821\n",
      "Epoch 31/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.9619 - val_loss: 10.6725\n",
      "Epoch 32/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.9359 - val_loss: 10.5859\n",
      "Epoch 33/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.9128 - val_loss: 10.5381\n",
      "Epoch 34/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.8883 - val_loss: 10.5713\n",
      "Epoch 35/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.8728 - val_loss: 10.6410\n",
      "Epoch 36/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.8515 - val_loss: 10.6510\n",
      "Epoch 37/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.8537 - val_loss: 10.5187\n",
      "Epoch 38/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.8143 - val_loss: 10.5936\n",
      "Epoch 39/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.7820 - val_loss: 10.5896\n",
      "Epoch 40/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.7855 - val_loss: 10.5614\n",
      "Epoch 41/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7522 - val_loss: 10.5158\n",
      "Epoch 42/150\n",
      "220/220 [==============================] - 21s 97ms/step - loss: 10.7402 - val_loss: 10.4848\n",
      "Epoch 43/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.7154 - val_loss: 10.6579\n",
      "Epoch 44/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.7095 - val_loss: 10.4800\n",
      "Epoch 45/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.6918 - val_loss: 10.4906\n",
      "Epoch 46/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.6832 - val_loss: 10.5510\n",
      "Epoch 47/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6679 - val_loss: 10.5261\n",
      "Epoch 48/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6616 - val_loss: 10.4595\n",
      "Epoch 49/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 10.6302 - val_loss: 10.4370\n",
      "Epoch 50/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.6184 - val_loss: 10.4571\n",
      "Epoch 51/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6021 - val_loss: 10.5466\n",
      "Epoch 52/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.6306 - val_loss: 10.4424\n",
      "Epoch 53/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.5913 - val_loss: 10.6102\n",
      "Epoch 54/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5838 - val_loss: 10.4355\n",
      "Epoch 55/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5595 - val_loss: 10.5311\n",
      "Epoch 56/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 10.5462 - val_loss: 10.4030\n",
      "Epoch 57/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5298 - val_loss: 10.3714\n",
      "Epoch 58/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5092 - val_loss: 10.4944\n",
      "Epoch 59/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.5215 - val_loss: 10.3502\n",
      "Epoch 60/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.4952 - val_loss: 10.3938\n",
      "Epoch 61/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4873 - val_loss: 10.3371\n",
      "Epoch 62/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4829 - val_loss: 10.2955\n",
      "Epoch 63/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.4670 - val_loss: 10.3553\n",
      "Epoch 64/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.4495 - val_loss: 10.3378\n",
      "Epoch 65/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4513 - val_loss: 10.2962\n",
      "Epoch 66/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.4246 - val_loss: 10.5414\n",
      "Epoch 67/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.4318 - val_loss: 10.3779\n",
      "Epoch 68/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.4206 - val_loss: 10.3048\n",
      "Epoch 69/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3889 - val_loss: 10.3312\n",
      "Epoch 70/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.3823 - val_loss: 10.3171\n",
      "Epoch 71/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.3860 - val_loss: 10.3591\n",
      "Epoch 72/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3790 - val_loss: 10.2694\n",
      "Epoch 73/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.3563 - val_loss: 10.3563\n",
      "Epoch 74/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.3609 - val_loss: 10.2867\n",
      "Epoch 75/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3420 - val_loss: 10.5406\n",
      "Epoch 76/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3245 - val_loss: 10.2781\n",
      "Epoch 77/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.3252 - val_loss: 10.3087\n",
      "Epoch 78/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.3135 - val_loss: 10.2956\n",
      "Epoch 79/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3146 - val_loss: 10.2639\n",
      "Epoch 80/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.3075 - val_loss: 10.3923\n",
      "Epoch 81/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.2995 - val_loss: 10.4818\n",
      "Epoch 82/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2922 - val_loss: 10.3003\n",
      "Epoch 83/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2695 - val_loss: 10.2275\n",
      "Epoch 84/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.2786 - val_loss: 10.1917\n",
      "Epoch 85/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2639 - val_loss: 10.5780\n",
      "Epoch 86/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2585 - val_loss: 10.2002\n",
      "Epoch 87/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.2432 - val_loss: 10.2335\n",
      "Epoch 88/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.2307 - val_loss: 10.3311\n",
      "Epoch 89/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.2307 - val_loss: 10.2790\n",
      "Epoch 90/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2200 - val_loss: 10.2920\n",
      "Epoch 91/150\n",
      "220/220 [==============================] - 22s 102ms/step - loss: 10.2205 - val_loss: 10.3086\n",
      "Epoch 92/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2031 - val_loss: 10.2831\n",
      "Epoch 93/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1801 - val_loss: 10.3772\n",
      "Epoch 94/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.1849 - val_loss: 10.2765\n",
      "========================= TRIAL 3, MAE: 14.861387048893556 =========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_19_layer_call_fn, lstm_cell_19_layer_call_and_return_conditional_losses, lstm_cell_20_layer_call_fn, lstm_cell_20_layer_call_and_return_conditional_losses, lstm_cell_22_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-15_12:52/model/model_3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-15_12:52/model/model_3/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c4d4f110> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c117f750> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c117f210> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c66e6c50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-15_12:52/model/model_3\n",
      "Epoch 1/150\n",
      "220/220 [==============================] - 126s 368ms/step - loss: 39.7500 - val_loss: 28.0079\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 28.7840 - val_loss: 28.6548\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 16.8936 - val_loss: 15.6373\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 13.0523 - val_loss: 13.2683\n",
      "Epoch 5/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 12.5566 - val_loss: 12.0428\n",
      "Epoch 6/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 12.3090 - val_loss: 11.7630\n",
      "Epoch 7/150\n",
      "220/220 [==============================] - 17s 74ms/step - loss: 12.1372 - val_loss: 11.5481\n",
      "Epoch 8/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.9953 - val_loss: 11.4387\n",
      "Epoch 9/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 11.8922 - val_loss: 11.3553\n",
      "Epoch 10/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 11.8106 - val_loss: 11.1611\n",
      "Epoch 11/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.7262 - val_loss: 11.3080\n",
      "Epoch 12/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.6619 - val_loss: 11.0549\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 11.5909 - val_loss: 11.2039\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.5299 - val_loss: 10.9305\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.4913 - val_loss: 11.0459\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 11.4488 - val_loss: 11.0006\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 11.4019 - val_loss: 10.8826\n",
      "Epoch 18/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 11.3550 - val_loss: 11.0030\n",
      "Epoch 19/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 11.3055 - val_loss: 10.9173\n",
      "Epoch 20/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 11.2869 - val_loss: 10.8456\n",
      "Epoch 21/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 11.2529 - val_loss: 10.8347\n",
      "Epoch 22/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.2024 - val_loss: 10.8049\n",
      "Epoch 23/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 11.1766 - val_loss: 10.7104\n",
      "Epoch 24/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 11.1471 - val_loss: 10.6540\n",
      "Epoch 25/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.1226 - val_loss: 10.6722\n",
      "Epoch 26/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.0932 - val_loss: 10.7069\n",
      "Epoch 27/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 11.0589 - val_loss: 10.7174\n",
      "Epoch 28/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.0381 - val_loss: 10.6427\n",
      "Epoch 29/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.0161 - val_loss: 10.6180\n",
      "Epoch 30/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.9997 - val_loss: 10.6067\n",
      "Epoch 31/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.9741 - val_loss: 10.6688\n",
      "Epoch 32/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.9403 - val_loss: 10.7416\n",
      "Epoch 33/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.9218 - val_loss: 10.5360\n",
      "Epoch 34/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.9095 - val_loss: 10.6495\n",
      "Epoch 35/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.8743 - val_loss: 10.6835\n",
      "Epoch 36/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.8685 - val_loss: 10.6067\n",
      "Epoch 37/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.8596 - val_loss: 10.5990\n",
      "Epoch 38/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.8243 - val_loss: 10.6071\n",
      "Epoch 39/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.7980 - val_loss: 10.6559\n",
      "Epoch 40/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7860 - val_loss: 10.6058\n",
      "Epoch 41/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.7762 - val_loss: 10.5211\n",
      "Epoch 42/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.7388 - val_loss: 10.4617\n",
      "Epoch 43/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.7266 - val_loss: 10.7014\n",
      "Epoch 44/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.7120 - val_loss: 10.5266\n",
      "Epoch 45/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.7183 - val_loss: 10.4642\n",
      "Epoch 46/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.7021 - val_loss: 10.4594\n",
      "Epoch 47/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.6641 - val_loss: 10.6906\n",
      "Epoch 48/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.6609 - val_loss: 10.4426\n",
      "Epoch 49/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6469 - val_loss: 10.4168\n",
      "Epoch 50/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.6131 - val_loss: 10.4211\n",
      "Epoch 51/150\n",
      "220/220 [==============================] - 17s 80ms/step - loss: 10.6219 - val_loss: 10.5649\n",
      "Epoch 52/150\n",
      "220/220 [==============================] - 17s 74ms/step - loss: 10.5943 - val_loss: 10.5049\n",
      "Epoch 53/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5933 - val_loss: 10.4916\n",
      "Epoch 54/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.5858 - val_loss: 10.3936\n",
      "Epoch 55/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.5752 - val_loss: 10.3674\n",
      "Epoch 56/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.5403 - val_loss: 10.3361\n",
      "Epoch 57/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5375 - val_loss: 10.3935\n",
      "Epoch 58/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.5184 - val_loss: 10.3740\n",
      "Epoch 59/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4992 - val_loss: 10.4476\n",
      "Epoch 60/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.4980 - val_loss: 10.4478\n",
      "Epoch 61/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.4936 - val_loss: 10.4600\n",
      "Epoch 62/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.4693 - val_loss: 10.3593\n",
      "Epoch 63/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.4635 - val_loss: 10.4488\n",
      "Epoch 64/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.4632 - val_loss: 10.4736\n",
      "Epoch 65/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.4337 - val_loss: 10.3004\n",
      "Epoch 66/150\n",
      "220/220 [==============================] - 17s 74ms/step - loss: 10.4294 - val_loss: 10.3309\n",
      "Epoch 67/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.4175 - val_loss: 10.3233\n",
      "Epoch 68/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.4100 - val_loss: 10.3571\n",
      "Epoch 69/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.4010 - val_loss: 10.2730\n",
      "Epoch 70/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3921 - val_loss: 10.2782\n",
      "Epoch 71/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3749 - val_loss: 10.2690\n",
      "Epoch 72/150\n",
      "220/220 [==============================] - 17s 80ms/step - loss: 10.3776 - val_loss: 10.3180\n",
      "Epoch 73/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.3630 - val_loss: 10.2893\n",
      "Epoch 74/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3479 - val_loss: 10.2933\n",
      "Epoch 75/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.3422 - val_loss: 10.3526\n",
      "Epoch 76/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.3249 - val_loss: 10.2306\n",
      "Epoch 77/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3229 - val_loss: 10.2334\n",
      "Epoch 78/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3258 - val_loss: 10.3328\n",
      "Epoch 79/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.3067 - val_loss: 10.3228\n",
      "Epoch 80/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.3111 - val_loss: 10.2882\n",
      "Epoch 81/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3004 - val_loss: 10.2079\n",
      "Epoch 82/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.2821 - val_loss: 10.2312\n",
      "Epoch 83/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.2792 - val_loss: 10.2714\n",
      "Epoch 84/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2697 - val_loss: 10.2930\n",
      "Epoch 85/150\n",
      "220/220 [==============================] - 16s 75ms/step - loss: 10.2689 - val_loss: 10.2996\n",
      "Epoch 86/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.2374 - val_loss: 10.2586\n",
      "Epoch 87/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.2442 - val_loss: 10.2605\n",
      "Epoch 88/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2460 - val_loss: 10.1987\n",
      "Epoch 89/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.2238 - val_loss: 10.1825\n",
      "Epoch 90/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.2023 - val_loss: 10.1794\n",
      "Epoch 91/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.2248 - val_loss: 10.1543\n",
      "Epoch 92/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.1960 - val_loss: 10.1815\n",
      "Epoch 93/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.1819 - val_loss: 10.2619\n",
      "Epoch 94/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1920 - val_loss: 10.1786\n",
      "Epoch 95/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1681 - val_loss: 10.1352\n",
      "Epoch 96/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.1751 - val_loss: 10.1072\n",
      "Epoch 97/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.1671 - val_loss: 10.1981\n",
      "Epoch 98/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.1574 - val_loss: 10.1601\n",
      "Epoch 99/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.1396 - val_loss: 10.2109\n",
      "Epoch 100/150\n",
      "220/220 [==============================] - 18s 79ms/step - loss: 10.1475 - val_loss: 10.2409\n",
      "Epoch 101/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.1209 - val_loss: 10.1736\n",
      "Epoch 102/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1321 - val_loss: 10.1448\n",
      "Epoch 103/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.1250 - val_loss: 10.1852\n",
      "Epoch 104/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.1215 - val_loss: 10.1201\n",
      "Epoch 105/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.1015 - val_loss: 10.2046\n",
      "Epoch 106/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.1004 - val_loss: 10.1621\n",
      "========================= TRIAL 4, MAE: 15.01059748064228 =========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_25_layer_call_fn, lstm_cell_25_layer_call_and_return_conditional_losses, lstm_cell_26_layer_call_fn, lstm_cell_26_layer_call_and_return_conditional_losses, lstm_cell_28_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-15_12:52/model/model_4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-15_12:52/model/model_4/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4be2c5350> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c11bee90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4cf41f250> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c11ec3d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-15_12:52/model/model_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://f1c15fa0-2199-4d4f-a7d4-5369e709cf1d/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://f1c15fa0-2199-4d4f-a7d4-5369e709cf1d/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4d47d2990> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4d48c8c10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4d48c4050> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4d49277d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_7_layer_call_fn, lstm_cell_7_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn, lstm_cell_8_layer_call_and_return_conditional_losses, lstm_cell_10_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://5864b169-9733-4120-b5c5-199d8e5ac9fc/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://5864b169-9733-4120-b5c5-199d8e5ac9fc/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4cce42710> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4ccb2e290> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4d40cd590> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4cf426d10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_13_layer_call_fn, lstm_cell_13_layer_call_and_return_conditional_losses, lstm_cell_14_layer_call_fn, lstm_cell_14_layer_call_and_return_conditional_losses, lstm_cell_16_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://2e9f8c89-0699-4d89-855f-9ee01b983448/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://2e9f8c89-0699-4d89-855f-9ee01b983448/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4d0ae6490> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c4e46fd0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c9923210> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c92b6c90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_19_layer_call_fn, lstm_cell_19_layer_call_and_return_conditional_losses, lstm_cell_20_layer_call_fn, lstm_cell_20_layer_call_and_return_conditional_losses, lstm_cell_22_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://4e3bd59b-da21-49fa-8e8c-7ea5b08930a1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://4e3bd59b-da21-49fa-8e8c-7ea5b08930a1/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c4d4f110> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c117f750> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c117f210> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c66e6c50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_25_layer_call_fn, lstm_cell_25_layer_call_and_return_conditional_losses, lstm_cell_26_layer_call_fn, lstm_cell_26_layer_call_and_return_conditional_losses, lstm_cell_28_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://22cd7105-dcf2-41ef-92f8-495e627fd651/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://22cd7105-dcf2-41ef-92f8-495e627fd651/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4be2c5350> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c11bee90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4cf41f250> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c11ec3d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "reset_model_features()\n",
    "modify_features(masked_cols_subset + new_cols_subset, 'add', 'numeric')\n",
    "params, normalizers, x_train, y_train, x_val, y_val, x_test, y_test, val_idx = create_data_set_and_model(train_dataframe, \n",
    "                                                                                                         test_dataframe, \n",
    "                                                                                                         trade_history_col)\n",
    "shuffle_buffer = .75\n",
    "use_bottleneck_model = False\n",
    "NUM_EPOCHS = 150\n",
    "verbose = 1\n",
    "model_to_use='default'\n",
    "ytw_result = run_experiment(5, experiment_prefix='similar_trades_small_experiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ccc491d-b734-4025-be75-d2bd41d5b8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.880043125378567\n",
      "14.972722803104359\n",
      "14.91724899381621\n",
      "14.861387048893556\n",
      "15.01059748064228\n"
     ]
    }
   ],
   "source": [
    "for _, model in ytw_result: \n",
    "    print(mean_absolute_error(test_dataframe['new_ys'] , model.predict(x_test, batch_size=10000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "269cc09a-6e2f-4af6-b1a6-749d5e630209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.928399890366993"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([14.880043125378567,\n",
    "14.972722803104359,\n",
    "14.91724899381621,\n",
    "14.861387048893556,\n",
    "15.01059748064228]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f881833-fa48-4574-ae9f-da658e12a990",
   "metadata": {},
   "source": [
    "# Full Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a69b5932-41ed-416e-b77b-297112b6b405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_data = pd.read_pickle('working_dataset.pkl')\n",
    "processed_data = processed_data.sort_values(by='trade_datetime', ascending=True)\n",
    "processed_data['cusip_series'] = processed_data['cusip'].str[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c019875-7287-4af3-9ce2-2b4d5304c1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = '2023-02-01'\n",
    "train_end = '2023-09-01'\n",
    "test_start = '2023-09-01'\n",
    "test_end = '2023-09-29'\n",
    "\n",
    "VALIDATION_SPLIT = 0.1\n",
    "LEARNING_RATE = 0.0007\n",
    "BATCH_SIZE = 10000\n",
    "NUM_EPOCHS = 75 \n",
    "DROPOUT = 0.1 \n",
    "TRADE_SEQUENCE_LENGTH = 5\n",
    "NUM_FEATURES = 6\n",
    "target_variable = 'new_ys' \n",
    "trade_history_col = 'trade_history'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d77c8a-381b-467c-83ab-03a8097c130e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 412, in _handle_workers\n",
      "    pool._maintain_pool()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 248, in _maintain_pool\n",
      "    self._repopulate_pool()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 241, in _repopulate_pool\n",
      "    w.start()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 112, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/context.py\", line 277, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/popen_fork.py\", line 20, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/popen_fork.py\", line 70, in _launch\n",
      "    self.pid = os.fork()\n",
      "OSError: [Errno 12] Cannot allocate memory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "temp2 = calculate_masked_average_2(processed_data[['trade_datetime','cusip','cusip_series','new_ys']], \n",
    "                                         windows = [1, 2, 10, 15, 20, 30, 50], \n",
    "                                         mask_cusip=True, \n",
    "                                         mp = multiprocessing.cpu_count()//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe59249e-a79b-41ca-8bec-51a6a8dd7a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_cols = [f'ex_cusip_masked_series_average_{i}' for i in [1, 2, 10, 15, 20, 30, 50]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d9849dc-04ab-446c-9046-d866ec5a3e05",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7306/1701464896.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocessed_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmasked_cols\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'to_df' is not defined"
     ]
    }
   ],
   "source": [
    "processed_data[masked_cols] = to_df(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0b39e9-c311-4992-b5b8-a0e946195725",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = multiprocessing.cpu_count()-1\n",
    "series_features = {'series_max': lambda data: calculate_masked_func(data, lambda y: y.max(), ex_cusip = True, max_sequence_length = 10, mp = mp),\n",
    "                    'series_min': lambda data: calculate_masked_func(data, lambda y: y.min(), ex_cusip = True, max_sequence_length = 10, mp = mp),\n",
    "                    'series_std': lambda data: calculate_masked_func(data, lambda y: y.median(), ex_cusip = True, max_sequence_length = 10, mp = mp),\n",
    "                    'series_vol': lambda data: calculate_masked_func(data, lambda y: y.std(), ex_cusip = True, max_sequence_length = 10, mp = mp)}\n",
    "\n",
    "combined_features = lambda data: calculate_masked_func_2(data, lambda y: (y.max(), y.min(), y.median(), y.std()), windows = [0], mask_cusip = True, mp = mp)\n",
    "new_cols = list(series_features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a97aa-6cbf-4ae5-aa2a-8f6654fcc3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time ttemp = combined_features(processed_data[['trade_datetime', 'cusip', 'cusip_series','new_ys']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f6ca59-1445-4639-b9b5-6b063468a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data[new_cols] = to_df(ttemp.apply(lambda x: x[0] if x else None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86b51ff-8c32-4f27-8e1d-606964f68027",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data['series_vol'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80d4775-3bd4-41d6-846b-6c118a069adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_data.to_pickle('similar_trades_3month.pkl')\n",
    "# df = pd.read_pickle('similar_trades_3month.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12714dff-0944-4e17-abcb-b2daaa82080f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = '2023-07-01'\n",
    "train_end = '2023-09-25'\n",
    "test_start = '2023-09-25'\n",
    "test_end = '2023-09-29'\n",
    "\n",
    "VALIDATION_SPLIT = 0.1\n",
    "LEARNING_RATE = 0.0007\n",
    "BATCH_SIZE = 10000\n",
    "NUM_EPOCHS = 75 \n",
    "DROPOUT = 0.1 \n",
    "TRADE_SEQUENCE_LENGTH = 5\n",
    "NUM_FEATURES = 6\n",
    "target_variable = 'new_ys' \n",
    "trade_history_col = 'trade_history'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91feb4d6-4d2c-4781-a399-e12fe182bd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filter = (df.trade_date < train_end) & (df.trade_date >= train_start)\n",
    "test_filter = (df.trade_date >= test_start) & (df.trade_date <test_end)\n",
    "\n",
    "train_dataframe = df[train_filter]\\\n",
    ".sort_values(by='trade_date', ascending=True)\\\n",
    ".reset_index(drop=True).copy()\n",
    "\n",
    "test_dataframe = df[test_filter]\\\n",
    ".sort_values(by='trade_date', ascending=True)\\\n",
    ".reset_index(drop=True).copy()\n",
    "\n",
    "train_dataframe['last_seconds_ago'] = train_dataframe['last_seconds_ago'].fillna(0)\n",
    "train_dataframe['last_yield_spread'] = train_dataframe['last_yield_spread'].fillna(0)\n",
    "\n",
    "test_dataframe['last_seconds_ago'] = test_dataframe['last_seconds_ago'].fillna(0)\n",
    "test_dataframe['last_yield_spread'] = test_dataframe['last_yield_spread'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "a3f901eb-b064-41d5-aa2e-e0afcd670408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA: N = 2191544, MIN DATE = 2023-07-03 00:00:00, MAX DATE = 2023-09-22 00:00:00\n",
      "VALIDATION DATA: N = 243504, MIN DATE = 2023-07-03 00:00:00, MAX DATE = 2023-09-22 00:00:00\n",
      "TEST DATA: N = 216931, MIN DATE = 2023-09-25 00:00:00, MAX DATE = 2023-09-28 00:00:00\n"
     ]
    }
   ],
   "source": [
    "reset_model_features()\n",
    "modify_features(masked_cols + new_cols, 'add', 'numeric')\n",
    "params, normalizers, x_train, y_train, x_val, y_val, x_test, y_test, val_idx = create_data_set_and_model(train_dataframe, \n",
    "                                                                                                         test_dataframe, \n",
    "                                                                                                         trade_history_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "e6869b67-ea79-4a56-a776-c9d495dd9c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median baseline MAE: 45.694182\n",
      "qreg baseline MAE: 40.851194\n",
      "qreg baseline + ['series_max', 'series_min', 'series_std', 'series_vol'] MAE: 39.623604\n"
     ]
    }
   ],
   "source": [
    "median_baseline = mean_absolute_error(test_dataframe[\"new_ys\"], np.repeat(train_dataframe[\"new_ys\"].median(), len(test_dataframe)))\n",
    "_, mae1 = perform_qreg(train_dataframe, \n",
    "            test_dataframe, \n",
    "            masked_cols,\n",
    "             print_result=False,\n",
    "             return_vals = True\n",
    "           )\n",
    "\n",
    "_, mae2 = perform_qreg(train_dataframe, \n",
    "            test_dataframe, \n",
    "            masked_cols + new_cols,\n",
    "             print_result=False,\n",
    "             return_vals = True\n",
    "           )\n",
    "\n",
    "print(f'median baseline MAE: {median_baseline:2f}')\n",
    "print(f'qreg baseline MAE: {mae1:2f}')\n",
    "print(f'qreg baseline + {new_cols} MAE: {mae2:2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5ed32a-af0e-427e-a293-ab2535f224b9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment results will be saved to experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-14_17:36\n",
      "\n",
      "Performing 1 runs with parameters: {'train_start': '2023-07-01', 'train_end': '2023-09-25', 'test_start': '2023-09-25', 'test_end': '2023-09-29', 'train_size': 2435048, 'test_size': 216931, 'VALIDATION_SPLIT': 0.1, 'LEARNING_RATE': 0.0007, 'BATCH_SIZE': 10000, 'NUM_EPOCHS': 150, 'DROPOUT': 0.1, 'TRADE_SEQUENCE_LENGTH': 5, 'NUM_FEATURES': 6, 'target_variable': 'new_ys', 'features': ['callable', 'sinking', 'zerocoupon', 'is_non_transaction_based_compensation', 'is_general_obligation', 'callable_at_cav', 'extraordinary_make_whole_call', 'make_whole_call', 'has_unexpired_lines_of_credit', 'escrow_exists', 'quantity', 'days_to_maturity', 'days_to_call', 'coupon', 'issue_amount', 'last_seconds_ago', 'last_yield_spread', 'days_to_settle', 'days_to_par', 'maturity_amount', 'issue_price', 'orig_principal_amount', 'max_amount_outstanding', 'accrued_days', 'days_in_interest_payment', 'A/E', 'ficc_treasury_spread', 'max_ys_ys', 'max_ys_ago', 'max_ys_qdiff', 'min_ys_ys', 'min_ys_ago', 'min_ys_qdiff', 'max_qty_ys', 'max_qty_ago', 'max_qty_qdiff', 'min_ago_ys', 'min_ago_ago', 'min_ago_qdiff', 'D_min_ago_ys', 'D_min_ago_ago', 'D_min_ago_qdiff', 'P_min_ago_ys', 'P_min_ago_ago', 'P_min_ago_qdiff', 'S_min_ago_ys', 'S_min_ago_ago', 'S_min_ago_qdiff', 'ex_cusip_masked_series_average_1', 'ex_cusip_masked_series_average_2', 'ex_cusip_masked_series_average_5', 'ex_cusip_masked_series_average_10', 'ex_cusip_masked_series_average_15', 'ex_cusip_masked_series_average_20', 'ex_cusip_masked_series_average_30', 'ex_cusip_masked_series_average_50', 'series_max', 'series_min', 'series_std', 'series_vol', 'rating', 'incorporated_state_code', 'trade_type', 'purpose_class', 'max_ys_ttypes', 'min_ys_ttypes', 'max_qty_ttypes', 'min_ago_ttypes', 'D_min_ago_ttypes', 'P_min_ago_ttypes', 'S_min_ago_ttypes'], 'shuffle_buffer': 0.75, 'use_bottleneck_model': False}\n",
      "Epoch 1/150\n",
      "220/220 [==============================] - 139s 432ms/step - loss: 39.7705 - val_loss: 28.5912\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 28.7874 - val_loss: 31.5745\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 16.8781 - val_loss: 16.6245\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 13.0459 - val_loss: 12.9218\n",
      "Epoch 5/150\n",
      "220/220 [==============================] - 16s 75ms/step - loss: 12.5446 - val_loss: 12.1406\n",
      "Epoch 6/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 12.3017 - val_loss: 11.6357\n",
      "Epoch 7/150\n",
      "220/220 [==============================] - 19s 85ms/step - loss: 12.1326 - val_loss: 11.6499\n",
      "Epoch 8/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.9877 - val_loss: 11.3377\n",
      "Epoch 9/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.9041 - val_loss: 11.2938\n",
      "Epoch 10/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 11.7965 - val_loss: 11.3944\n",
      "Epoch 11/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 11.7176 - val_loss: 11.1539\n",
      "Epoch 12/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.6584 - val_loss: 11.1789\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 11.5770 - val_loss: 11.0242\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 11.5257 - val_loss: 10.9445\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.4736 - val_loss: 10.9210\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.4302 - val_loss: 10.9212\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 19s 86ms/step - loss: 11.3873 - val_loss: 10.8592\n",
      "Epoch 18/150\n",
      " 78/220 [=========>....................] - ETA: 10s - loss: 11.3464"
     ]
    }
   ],
   "source": [
    "shuffle_buffer = .75\n",
    "use_bottleneck_model = False\n",
    "NUM_EPOCHS = 150\n",
    "verbose = 1\n",
    "model_to_use='default'\n",
    "ytw_result = run_experiment(1, experiment_prefix='similar_trades_small_experiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "2779fb5d-bf05-45d4-8e84-e3df049e059c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.010530497111292"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_features_model = keras.models.load_model('experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-14_17:36/model/model_0')\n",
    "test_dataframe['prediction_newfeatures'] = new_features_model.predict(x_test, batch_size=10000)\n",
    "mean_absolute_error(test_dataframe['new_ys'] , test_dataframe['prediction_newfeatures'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "d0c314d3-8af9-4bb5-a4f2-feb0a5e0d944",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA: N = 2191544, MIN DATE = 2023-07-03 00:00:00, MAX DATE = 2023-09-22 00:00:00\n",
      "VALIDATION DATA: N = 243504, MIN DATE = 2023-07-03 00:00:00, MAX DATE = 2023-09-22 00:00:00\n",
      "TEST DATA: N = 216931, MIN DATE = 2023-09-25 00:00:00, MAX DATE = 2023-09-28 00:00:00\n",
      "Experiment results will be saved to experiments/similar_trades_small_experiment_baselines/new_ys_experiment_2023-11-14_22:07\n",
      "\n",
      "Performing 5 runs with parameters: {'train_start': '2023-07-01', 'train_end': '2023-09-25', 'test_start': '2023-09-25', 'test_end': '2023-09-29', 'train_size': 2435048, 'test_size': 216931, 'VALIDATION_SPLIT': 0.1, 'LEARNING_RATE': 0.0007, 'BATCH_SIZE': 10000, 'NUM_EPOCHS': 150, 'DROPOUT': 0.1, 'TRADE_SEQUENCE_LENGTH': 5, 'NUM_FEATURES': 6, 'target_variable': 'new_ys', 'features': ['callable', 'sinking', 'zerocoupon', 'is_non_transaction_based_compensation', 'is_general_obligation', 'callable_at_cav', 'extraordinary_make_whole_call', 'make_whole_call', 'has_unexpired_lines_of_credit', 'escrow_exists', 'quantity', 'days_to_maturity', 'days_to_call', 'coupon', 'issue_amount', 'last_seconds_ago', 'last_yield_spread', 'days_to_settle', 'days_to_par', 'maturity_amount', 'issue_price', 'orig_principal_amount', 'max_amount_outstanding', 'accrued_days', 'days_in_interest_payment', 'A/E', 'ficc_treasury_spread', 'max_ys_ys', 'max_ys_ago', 'max_ys_qdiff', 'min_ys_ys', 'min_ys_ago', 'min_ys_qdiff', 'max_qty_ys', 'max_qty_ago', 'max_qty_qdiff', 'min_ago_ys', 'min_ago_ago', 'min_ago_qdiff', 'D_min_ago_ys', 'D_min_ago_ago', 'D_min_ago_qdiff', 'P_min_ago_ys', 'P_min_ago_ago', 'P_min_ago_qdiff', 'S_min_ago_ys', 'S_min_ago_ago', 'S_min_ago_qdiff', 'rating', 'incorporated_state_code', 'trade_type', 'purpose_class', 'max_ys_ttypes', 'min_ys_ttypes', 'max_qty_ttypes', 'min_ago_ttypes', 'D_min_ago_ttypes', 'P_min_ago_ttypes', 'S_min_ago_ttypes'], 'shuffle_buffer': 0.75, 'use_bottleneck_model': False}\n",
      "Epoch 1/150\n",
      "220/220 [==============================] - 129s 372ms/step - loss: 39.7350 - val_loss: 28.4137\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 28.7935 - val_loss: 29.0581\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 16.8647 - val_loss: 15.5738\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 13.0216 - val_loss: 12.5610\n",
      "Epoch 5/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 12.5398 - val_loss: 11.9229\n",
      "Epoch 6/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 12.3159 - val_loss: 11.6527\n",
      "Epoch 7/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 12.1542 - val_loss: 11.6757\n",
      "Epoch 8/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 12.0169 - val_loss: 11.4114\n",
      "Epoch 9/150\n",
      "220/220 [==============================] - 19s 85ms/step - loss: 11.9157 - val_loss: 11.3633\n",
      "Epoch 10/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.8338 - val_loss: 11.2033\n",
      "Epoch 11/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.7558 - val_loss: 11.1059\n",
      "Epoch 12/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 11.7056 - val_loss: 11.1102\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 11.6218 - val_loss: 11.1582\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.5665 - val_loss: 11.0606\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.5126 - val_loss: 10.9717\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 19s 85ms/step - loss: 11.4723 - val_loss: 11.0616\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.4349 - val_loss: 10.9195\n",
      "Epoch 18/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.3846 - val_loss: 10.8385\n",
      "Epoch 19/150\n",
      " 24/220 [==>...........................] - ETA: 13s - loss: 11.1327"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 17s 77ms/step - loss: 10.5871 - val_loss: 10.3884\n",
      "Epoch 57/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.5681 - val_loss: 10.3558\n",
      "Epoch 58/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.5615 - val_loss: 10.4171\n",
      "Epoch 59/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.5563 - val_loss: 10.3284\n",
      "Epoch 60/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.5362 - val_loss: 10.3229\n",
      "Epoch 61/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.5127 - val_loss: 10.3605\n",
      "Epoch 62/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.5073 - val_loss: 10.3805\n",
      "Epoch 63/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.4880 - val_loss: 10.3582\n",
      "Epoch 64/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.4871 - val_loss: 10.3313\n",
      "Epoch 65/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.4742 - val_loss: 10.4395\n",
      "Epoch 66/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.4678 - val_loss: 10.3737\n",
      "Epoch 67/150\n",
      "220/220 [==============================] - 19s 85ms/step - loss: 10.4711 - val_loss: 10.3853\n",
      "Epoch 68/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.4629 - val_loss: 10.3840\n",
      "Epoch 69/150\n",
      "169/220 [======================>.......] - ETA: 3s - loss: 10.4237"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 18s 83ms/step - loss: 10.8993 - val_loss: 10.5874\n",
      "Epoch 37/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.8811 - val_loss: 10.5804\n",
      "Epoch 38/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.8660 - val_loss: 10.7199\n",
      "Epoch 39/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.8546 - val_loss: 10.6130\n",
      "Epoch 40/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.8174 - val_loss: 10.7784\n",
      "Epoch 41/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.8177 - val_loss: 10.5017\n",
      "Epoch 42/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.7853 - val_loss: 10.4939\n",
      "Epoch 43/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.7821 - val_loss: 10.6216\n",
      "Epoch 44/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7674 - val_loss: 10.5885\n",
      "Epoch 45/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7470 - val_loss: 10.5881\n",
      "Epoch 46/150\n",
      "220/220 [==============================] - 19s 85ms/step - loss: 10.7368 - val_loss: 10.4918\n",
      "Epoch 47/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7337 - val_loss: 10.5243\n",
      "Epoch 48/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7072 - val_loss: 10.4398\n",
      "Epoch 49/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.6861 - val_loss: 10.4913\n",
      "Epoch 50/150\n",
      " 37/220 [====>.........................] - ETA: 14s - loss: 10.5315"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3078 - val_loss: 10.4937\n",
      "Epoch 86/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.2939 - val_loss: 10.2497\n",
      "Epoch 87/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.2853 - val_loss: 10.2863\n",
      "Epoch 88/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.2797 - val_loss: 10.2906\n",
      "Epoch 89/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.2714 - val_loss: 10.2452\n",
      "Epoch 90/150\n",
      "220/220 [==============================] - 18s 84ms/step - loss: 10.2633 - val_loss: 10.3193\n",
      "Epoch 91/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.2795 - val_loss: 10.2371\n",
      "Epoch 92/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2644 - val_loss: 10.2676\n",
      "Epoch 93/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.2396 - val_loss: 10.2970\n",
      "Epoch 94/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.2408 - val_loss: 10.4011\n",
      "Epoch 95/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.2361 - val_loss: 10.2532\n",
      "Epoch 96/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.2308 - val_loss: 10.2895\n",
      "Epoch 97/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 10.2186 - val_loss: 10.2704\n",
      "Epoch 98/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.2038 - val_loss: 10.3070\n",
      "Epoch 99/150\n",
      "119/220 [===============>..............] - ETA: 7s - loss: 10.2178"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 17s 76ms/step - loss: 11.2469 - val_loss: 10.8951\n",
      "Epoch 23/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 11.2063 - val_loss: 10.7144\n",
      "Epoch 24/150\n",
      "220/220 [==============================] - 16s 75ms/step - loss: 11.1836 - val_loss: 10.7022\n",
      "Epoch 25/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.1377 - val_loss: 10.7078\n",
      "Epoch 26/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 11.1227 - val_loss: 10.6938\n",
      "Epoch 27/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 11.0983 - val_loss: 10.6335\n",
      "Epoch 28/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.0683 - val_loss: 10.5940\n",
      "Epoch 29/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 11.0405 - val_loss: 10.6096\n",
      "Epoch 30/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 11.0220 - val_loss: 10.6031\n",
      "Epoch 31/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.9950 - val_loss: 10.6240\n",
      "Epoch 32/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.9738 - val_loss: 10.6751\n",
      "Epoch 33/150\n",
      "220/220 [==============================] - 19s 87ms/step - loss: 10.9564 - val_loss: 10.5876\n",
      "Epoch 34/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.9358 - val_loss: 10.5764\n",
      "Epoch 35/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.9043 - val_loss: 10.5943\n",
      "Epoch 36/150\n",
      " 94/220 [===========>..................] - ETA: 8s - loss: 10.9069"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 19s 84ms/step - loss: 10.4696 - val_loss: 10.4150\n",
      "Epoch 68/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4587 - val_loss: 10.2847\n",
      "Epoch 69/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4404 - val_loss: 10.3326\n",
      "Epoch 70/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.4213 - val_loss: 10.3584\n",
      "Epoch 71/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.4209 - val_loss: 10.3499\n",
      "Epoch 72/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4107 - val_loss: 10.4153\n",
      "Epoch 73/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.4083 - val_loss: 10.2886\n",
      "Epoch 74/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.4064 - val_loss: 10.3013\n",
      "Epoch 75/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.3853 - val_loss: 10.2697\n",
      "Epoch 76/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3798 - val_loss: 10.2792\n",
      "Epoch 77/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 10.3599 - val_loss: 10.4507\n",
      "Epoch 78/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.3509 - val_loss: 10.2794\n",
      "Epoch 79/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3493 - val_loss: 10.3046\n",
      "Epoch 80/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.3277 - val_loss: 10.2916\n",
      "Epoch 81/150\n",
      "130/220 [================>.............] - ETA: 7s - loss: 10.2779"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 18s 80ms/step - loss: 10.2723 - val_loss: 10.2875\n",
      "Epoch 88/150\n",
      "220/220 [==============================] - 18s 79ms/step - loss: 10.2712 - val_loss: 10.2518\n",
      "Epoch 89/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2696 - val_loss: 10.2907\n",
      "Epoch 90/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.2493 - val_loss: 10.2216\n",
      "Epoch 91/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 10.2768 - val_loss: 10.2078\n",
      "Epoch 92/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2321 - val_loss: 10.2650\n",
      "Epoch 93/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2347 - val_loss: 10.2723\n",
      "Epoch 94/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.2234 - val_loss: 10.2153\n",
      "Epoch 95/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.2240 - val_loss: 10.4275\n",
      "Epoch 96/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2013 - val_loss: 10.2108\n",
      "========================= TRIAL 2, MAE: 15.03891836795422 =========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_49_layer_call_fn, lstm_cell_49_layer_call_and_return_conditional_losses, lstm_cell_50_layer_call_fn, lstm_cell_50_layer_call_and_return_conditional_losses, lstm_cell_52_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd91343a310> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fdb26a86850> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd910f15b10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd91747c150> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to experiments/similar_trades_small_experiment_baselines/new_ys_experiment_2023-11-14_22:07/model/model_2\n",
      "Epoch 1/150\n",
      "220/220 [==============================] - 125s 393ms/step - loss: 39.7360 - val_loss: 28.5582\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 28.7903 - val_loss: 29.3919\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 16.8656 - val_loss: 15.7483\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 13.0320 - val_loss: 13.0975\n",
      "Epoch 5/150\n",
      "  1/220 [..............................] - ETA: 14s - loss: 12.5240"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 16s 73ms/step - loss: 11.6831 - val_loss: 11.3354\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 11.6096 - val_loss: 11.0663\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 11.5653 - val_loss: 10.9584\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.5107 - val_loss: 11.0575\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.4725 - val_loss: 10.8757\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 11.4292 - val_loss: 10.9980\n",
      "Epoch 18/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.3804 - val_loss: 10.8624\n",
      "Epoch 19/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.3365 - val_loss: 10.8050\n",
      "Epoch 20/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 11.3113 - val_loss: 10.8352\n",
      "Epoch 21/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 11.2666 - val_loss: 10.7605\n",
      "Epoch 22/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.2437 - val_loss: 10.7352\n",
      "Epoch 23/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.2075 - val_loss: 10.7713\n",
      "Epoch 24/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 11.1718 - val_loss: 10.7710\n",
      "Epoch 25/150\n",
      "216/220 [============================>.] - ETA: 0s - loss: 11.1381"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 16s 74ms/step - loss: 10.9493 - val_loss: 10.5621\n",
      "Epoch 34/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.9439 - val_loss: 10.6895\n",
      "Epoch 35/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.9020 - val_loss: 10.6114\n",
      "Epoch 36/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.9005 - val_loss: 10.5678\n",
      "Epoch 37/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.8840 - val_loss: 10.5536\n",
      "Epoch 38/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.8543 - val_loss: 10.6306\n",
      "Epoch 39/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.8365 - val_loss: 10.5936\n",
      "Epoch 40/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.8027 - val_loss: 10.5988\n",
      "Epoch 41/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.7963 - val_loss: 10.6676\n",
      "Epoch 42/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7767 - val_loss: 10.4647\n",
      "Epoch 43/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.7581 - val_loss: 10.5442\n",
      "Epoch 44/150\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 10.7503 - val_loss: 10.5054\n",
      "Epoch 45/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.7268 - val_loss: 10.4514\n",
      "Epoch 46/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7131 - val_loss: 10.5239\n",
      "Epoch 47/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.6942 - val_loss: 10.4401\n",
      "Epoch 48/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.6751 - val_loss: 10.5623\n",
      "Epoch 49/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6771 - val_loss: 10.5550\n",
      "Epoch 50/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.6520 - val_loss: 10.4593\n",
      "Epoch 51/150\n",
      "220/220 [==============================] - 19s 83ms/step - loss: 10.6355 - val_loss: 10.4019\n",
      "Epoch 52/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.6378 - val_loss: 10.5674\n",
      "Epoch 53/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6109 - val_loss: 10.5191\n",
      "Epoch 54/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 10.6063 - val_loss: 10.4004\n",
      "Epoch 55/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.5928 - val_loss: 10.3888\n",
      "Epoch 56/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5887 - val_loss: 10.4449\n",
      "Epoch 57/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.5673 - val_loss: 10.4074\n",
      "Epoch 58/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.5615 - val_loss: 10.4116\n",
      "Epoch 59/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5320 - val_loss: 10.4209\n",
      "Epoch 60/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5370 - val_loss: 10.3712\n",
      "Epoch 61/150\n",
      "220/220 [==============================] - 19s 85ms/step - loss: 10.5116 - val_loss: 10.4328\n",
      "Epoch 62/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.5126 - val_loss: 10.4076\n",
      "Epoch 63/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4937 - val_loss: 10.4176\n",
      "Epoch 64/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.4917 - val_loss: 10.3533\n",
      "Epoch 65/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.4814 - val_loss: 10.3643\n",
      "Epoch 66/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.4710 - val_loss: 10.4956\n",
      "Epoch 67/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.4554 - val_loss: 10.4534\n",
      "Epoch 68/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.4604 - val_loss: 10.3446\n",
      "Epoch 69/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.4275 - val_loss: 10.3401\n",
      "Epoch 70/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.4323 - val_loss: 10.3915\n",
      "Epoch 71/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 10.4106 - val_loss: 10.2938\n",
      "Epoch 72/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.4057 - val_loss: 10.3667\n",
      "Epoch 73/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3794 - val_loss: 10.2972\n",
      "Epoch 74/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.3902 - val_loss: 10.3637\n",
      "Epoch 75/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.3690 - val_loss: 10.3094\n",
      "Epoch 76/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3570 - val_loss: 10.3221\n",
      "Epoch 77/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3598 - val_loss: 10.4739\n",
      "Epoch 78/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 10.3539 - val_loss: 10.3045\n",
      "Epoch 79/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3423 - val_loss: 10.3842\n",
      "Epoch 80/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3364 - val_loss: 10.3456\n",
      "Epoch 81/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.3237 - val_loss: 10.2351\n",
      "Epoch 82/150\n",
      "220/220 [==============================] - 18s 79ms/step - loss: 10.3166 - val_loss: 10.4143\n",
      "Epoch 83/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3128 - val_loss: 10.3611\n",
      "Epoch 84/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.3107 - val_loss: 10.2847\n",
      "Epoch 85/150\n",
      "220/220 [==============================] - 19s 84ms/step - loss: 10.2974 - val_loss: 10.2765\n",
      "Epoch 86/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.2691 - val_loss: 10.3556\n",
      "Epoch 87/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.2687 - val_loss: 10.2320\n",
      "Epoch 88/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.2513 - val_loss: 10.2788\n",
      "Epoch 89/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.2527 - val_loss: 10.3443\n",
      "Epoch 90/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2520 - val_loss: 10.2054\n",
      "Epoch 91/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.2634 - val_loss: 10.2071\n",
      "Epoch 92/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.2402 - val_loss: 10.2264\n",
      "Epoch 93/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.2359 - val_loss: 10.2474\n",
      "Epoch 94/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.2337 - val_loss: 10.2160\n",
      "Epoch 95/150\n",
      "220/220 [==============================] - 19s 86ms/step - loss: 10.2183 - val_loss: 10.3561\n",
      "Epoch 96/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.2021 - val_loss: 10.3814\n",
      "Epoch 97/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.2061 - val_loss: 10.2513\n",
      "Epoch 98/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.1815 - val_loss: 10.1977\n",
      "Epoch 99/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.1950 - val_loss: 10.2165\n",
      "Epoch 100/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.1811 - val_loss: 10.2940\n",
      "Epoch 101/150\n",
      "220/220 [==============================] - 17s 80ms/step - loss: 10.1750 - val_loss: 10.1987\n",
      "Epoch 102/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.1664 - val_loss: 10.1941\n",
      "Epoch 103/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.1497 - val_loss: 10.1809\n",
      "Epoch 104/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.1447 - val_loss: 10.1458\n",
      "Epoch 105/150\n",
      "220/220 [==============================] - 19s 85ms/step - loss: 10.1345 - val_loss: 10.1709\n",
      "Epoch 106/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.1332 - val_loss: 10.1963\n",
      "Epoch 107/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1450 - val_loss: 10.1727\n",
      "Epoch 108/150\n",
      "220/220 [==============================] - 19s 86ms/step - loss: 10.1233 - val_loss: 10.2012\n",
      "Epoch 109/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.1311 - val_loss: 10.2085\n",
      "Epoch 110/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1244 - val_loss: 10.2791\n",
      "Epoch 111/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.1032 - val_loss: 10.1743\n",
      "Epoch 112/150\n",
      "220/220 [==============================] - 19s 87ms/step - loss: 10.0922 - val_loss: 10.2510\n",
      "Epoch 113/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.0959 - val_loss: 10.3484\n",
      "Epoch 114/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.0825 - val_loss: 10.1427\n",
      "Epoch 115/150\n",
      "220/220 [==============================] - 19s 88ms/step - loss: 10.0703 - val_loss: 10.1846\n",
      "Epoch 116/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.0713 - val_loss: 10.1856\n",
      "Epoch 117/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.0712 - val_loss: 10.2224\n",
      "Epoch 118/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.0399 - val_loss: 10.1697\n",
      "Epoch 119/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.0520 - val_loss: 10.1504\n",
      "Epoch 120/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.0459 - val_loss: 10.2005\n",
      "Epoch 121/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.0497 - val_loss: 10.1053\n",
      "Epoch 122/150\n",
      "220/220 [==============================] - 19s 88ms/step - loss: 10.0350 - val_loss: 10.1934\n",
      "Epoch 123/150\n",
      "220/220 [==============================] - 17s 74ms/step - loss: 10.0196 - val_loss: 10.1568\n",
      "Epoch 124/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.0093 - val_loss: 10.1007\n",
      "Epoch 125/150\n",
      "220/220 [==============================] - 17s 80ms/step - loss: 10.0103 - val_loss: 10.1069\n",
      "Epoch 126/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.0082 - val_loss: 10.1017\n",
      "Epoch 127/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.0107 - val_loss: 10.1593\n",
      "Epoch 128/150\n",
      "220/220 [==============================] - 16s 75ms/step - loss: 9.9900 - val_loss: 10.1073\n",
      "Epoch 129/150\n",
      "220/220 [==============================] - 19s 85ms/step - loss: 9.9912 - val_loss: 10.1653\n",
      "Epoch 130/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 9.9686 - val_loss: 10.0854\n",
      "Epoch 131/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 9.9807 - val_loss: 10.1536\n",
      "Epoch 132/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 9.9579 - val_loss: 10.0938\n",
      "Epoch 133/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 9.9811 - val_loss: 10.1173\n",
      "Epoch 134/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 9.9526 - val_loss: 10.1714\n",
      "Epoch 135/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 9.9405 - val_loss: 10.1468\n",
      "Epoch 136/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 9.9454 - val_loss: 10.1332\n",
      "Epoch 137/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 9.9470 - val_loss: 10.0968\n",
      "Epoch 138/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 9.9307 - val_loss: 10.0651\n",
      "Epoch 139/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 9.9223 - val_loss: 10.0585\n",
      "Epoch 140/150\n",
      "220/220 [==============================] - 16s 75ms/step - loss: 9.9294 - val_loss: 10.0719\n",
      "Epoch 141/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 9.9182 - val_loss: 10.1024\n",
      "Epoch 142/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 9.9238 - val_loss: 10.2152\n",
      "Epoch 143/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 9.8929 - val_loss: 10.0994\n",
      "Epoch 144/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 9.8947 - val_loss: 10.0580\n",
      "Epoch 145/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 9.8926 - val_loss: 10.1006\n",
      "Epoch 146/150\n",
      "220/220 [==============================] - 19s 83ms/step - loss: 9.8835 - val_loss: 10.0990\n",
      "Epoch 147/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 9.8843 - val_loss: 10.1202\n",
      "Epoch 148/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 9.8573 - val_loss: 10.0724\n",
      "Epoch 149/150\n",
      "220/220 [==============================] - 19s 86ms/step - loss: 9.8639 - val_loss: 10.2050\n",
      "Epoch 150/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 9.8555 - val_loss: 10.0766\n",
      "========================= TRIAL 3, MAE: 15.371468678506515 =========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_55_layer_call_fn, lstm_cell_55_layer_call_and_return_conditional_losses, lstm_cell_56_layer_call_fn, lstm_cell_56_layer_call_and_return_conditional_losses, lstm_cell_58_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd90a2c4c50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd90c99c210> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd906953190> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd905279450> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to experiments/similar_trades_small_experiment_baselines/new_ys_experiment_2023-11-14_22:07/model/model_3\n",
      "Epoch 1/150\n",
      "220/220 [==============================] - 135s 406ms/step - loss: 39.7337 - val_loss: 28.3975\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 28.7934 - val_loss: 29.1590\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 19s 86ms/step - loss: 16.8673 - val_loss: 14.8854\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 13.0342 - val_loss: 12.2906\n",
      "Epoch 5/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 12.5544 - val_loss: 11.8798\n",
      "Epoch 6/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 12.3236 - val_loss: 11.6719\n",
      "Epoch 7/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 12.1580 - val_loss: 11.4713\n",
      "Epoch 8/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 12.0173 - val_loss: 11.4107\n",
      "Epoch 9/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.9156 - val_loss: 11.2938\n",
      "Epoch 10/150\n",
      "220/220 [==============================] - 19s 87ms/step - loss: 11.8258 - val_loss: 11.2557\n",
      "Epoch 11/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 11.7564 - val_loss: 11.1918\n",
      "Epoch 12/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.6843 - val_loss: 11.0782\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 11.6140 - val_loss: 11.1485\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 11.5629 - val_loss: 10.9830\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.5134 - val_loss: 10.9349\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 11.4655 - val_loss: 10.9684\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 19s 85ms/step - loss: 11.4345 - val_loss: 10.9006\n",
      "Epoch 18/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.3817 - val_loss: 10.8474\n",
      "Epoch 19/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.3368 - val_loss: 10.7969\n",
      "Epoch 20/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 11.3105 - val_loss: 10.8887\n",
      "Epoch 21/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 11.2775 - val_loss: 10.8028\n",
      "Epoch 22/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.2524 - val_loss: 10.8673\n",
      "Epoch 23/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 11.2079 - val_loss: 10.7690\n",
      "Epoch 24/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 11.1730 - val_loss: 10.7509\n",
      "Epoch 25/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.1482 - val_loss: 10.8030\n",
      "Epoch 26/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.1133 - val_loss: 10.6699\n",
      "Epoch 27/150\n",
      "220/220 [==============================] - 19s 85ms/step - loss: 11.0919 - val_loss: 10.6694\n",
      "Epoch 28/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.0681 - val_loss: 10.7344\n",
      "Epoch 29/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.0416 - val_loss: 10.6667\n",
      "Epoch 30/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 11.0204 - val_loss: 10.6630\n",
      "Epoch 31/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.9916 - val_loss: 10.6822\n",
      "Epoch 32/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.9752 - val_loss: 10.5619\n",
      "Epoch 33/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.9547 - val_loss: 10.5943\n",
      "Epoch 34/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.9372 - val_loss: 10.6126\n",
      "Epoch 35/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.8979 - val_loss: 10.7470\n",
      "Epoch 36/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.8942 - val_loss: 10.5368\n",
      "Epoch 37/150\n",
      "220/220 [==============================] - 18s 84ms/step - loss: 10.8742 - val_loss: 10.5607\n",
      "Epoch 38/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.8496 - val_loss: 10.5007\n",
      "Epoch 39/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.8332 - val_loss: 10.5301\n",
      "Epoch 40/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.8142 - val_loss: 10.5987\n",
      "Epoch 41/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.7911 - val_loss: 10.5069\n",
      "Epoch 42/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7935 - val_loss: 10.5197\n",
      "Epoch 43/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7589 - val_loss: 10.5457\n",
      "Epoch 44/150\n",
      "220/220 [==============================] - 20s 90ms/step - loss: 10.7442 - val_loss: 10.4915\n",
      "Epoch 45/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7306 - val_loss: 10.4690\n",
      "Epoch 46/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7213 - val_loss: 10.6494\n",
      "Epoch 47/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.7079 - val_loss: 10.6825\n",
      "Epoch 48/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.6837 - val_loss: 10.6849\n",
      "Epoch 49/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6688 - val_loss: 10.4144\n",
      "Epoch 50/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.6536 - val_loss: 10.4795\n",
      "Epoch 51/150\n",
      "220/220 [==============================] - 19s 86ms/step - loss: 10.6372 - val_loss: 10.5843\n",
      "Epoch 52/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6426 - val_loss: 10.4696\n",
      "Epoch 53/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6155 - val_loss: 10.4297\n",
      "Epoch 54/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.6096 - val_loss: 10.4376\n",
      "Epoch 55/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.5934 - val_loss: 10.4979\n",
      "Epoch 56/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5962 - val_loss: 10.4327\n",
      "Epoch 57/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.5650 - val_loss: 10.3932\n",
      "Epoch 58/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 10.5544 - val_loss: 10.4844\n",
      "Epoch 59/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5382 - val_loss: 10.4257\n",
      "Epoch 60/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5307 - val_loss: 10.3787\n",
      "Epoch 61/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.5192 - val_loss: 10.3879\n",
      "Epoch 62/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.5169 - val_loss: 10.3502\n",
      "Epoch 63/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4862 - val_loss: 10.3990\n",
      "Epoch 64/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.4918 - val_loss: 10.6165\n",
      "Epoch 65/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 10.4729 - val_loss: 10.3256\n",
      "Epoch 66/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4704 - val_loss: 10.3765\n",
      "Epoch 67/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.4674 - val_loss: 10.3265\n",
      "Epoch 68/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.4442 - val_loss: 10.3164\n",
      "Epoch 69/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4355 - val_loss: 10.2644\n",
      "Epoch 70/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4339 - val_loss: 10.3141\n",
      "Epoch 71/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.4284 - val_loss: 10.2830\n",
      "Epoch 72/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.4054 - val_loss: 10.2735\n",
      "Epoch 73/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3874 - val_loss: 10.2734\n",
      "Epoch 74/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.3873 - val_loss: 10.2845\n",
      "Epoch 75/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 10.3696 - val_loss: 10.2732\n",
      "Epoch 76/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3602 - val_loss: 10.2741\n",
      "Epoch 77/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3537 - val_loss: 10.3650\n",
      "Epoch 78/150\n",
      "220/220 [==============================] - 19s 85ms/step - loss: 10.3600 - val_loss: 10.3545\n",
      "Epoch 79/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.3195 - val_loss: 10.3603\n",
      "========================= TRIAL 4, MAE: 15.03453673737532 =========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_61_layer_call_fn, lstm_cell_61_layer_call_and_return_conditional_losses, lstm_cell_62_layer_call_fn, lstm_cell_62_layer_call_and_return_conditional_losses, lstm_cell_64_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd9000ae590> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd902435f90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd8fe81a310> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd901df6b10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to experiments/similar_trades_small_experiment_baselines/new_ys_experiment_2023-11-14_22:07/model/model_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_37_layer_call_fn, lstm_cell_37_layer_call_and_return_conditional_losses, lstm_cell_38_layer_call_fn, lstm_cell_38_layer_call_and_return_conditional_losses, lstm_cell_40_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fceb266f990> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fce94390710> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fce94084890> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fce940bed10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_43_layer_call_fn, lstm_cell_43_layer_call_and_return_conditional_losses, lstm_cell_44_layer_call_fn, lstm_cell_44_layer_call_and_return_conditional_losses, lstm_cell_46_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fceaea8d250> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fdb2767ae50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd8f1d04a90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fdb27c21c50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_49_layer_call_fn, lstm_cell_49_layer_call_and_return_conditional_losses, lstm_cell_50_layer_call_fn, lstm_cell_50_layer_call_and_return_conditional_losses, lstm_cell_52_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd91343a310> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fdb26a86850> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd910f15b10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd91747c150> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_55_layer_call_fn, lstm_cell_55_layer_call_and_return_conditional_losses, lstm_cell_56_layer_call_fn, lstm_cell_56_layer_call_and_return_conditional_losses, lstm_cell_58_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd90a2c4c50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd90c99c210> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd906953190> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd905279450> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_61_layer_call_fn, lstm_cell_61_layer_call_and_return_conditional_losses, lstm_cell_62_layer_call_fn, lstm_cell_62_layer_call_and_return_conditional_losses, lstm_cell_64_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd9000ae590> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd902435f90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd8fe81a310> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd901df6b10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "reset_model_features()\n",
    "params, normalizers, x_train, y_train, x_val, y_val, x_test, y_test, val_idx = create_data_set_and_model(train_dataframe, \n",
    "                                                                                                         test_dataframe, \n",
    "                                                                                                         trade_history_col)\n",
    "shuffle_buffer = .75\n",
    "use_bottleneck_model = False\n",
    "NUM_EPOCHS = 150\n",
    "verbose = 1\n",
    "model_to_use='default'\n",
    "ytw_result = run_experiment(5, experiment_prefix='similar_trades_small_experiment_baselines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "8e065d92-67d7-48d3-9e50-859c4bebd6c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.060472582225009"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_model = keras.models.load_model('experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-14_19:33/model/model_0')\n",
    "test_dataframe['prediction_baseline'] = og_model.predict(x_test, batch_size=10000)\n",
    "mean_absolute_error(test_dataframe['new_ys'] , test_dataframe['prediction_baseline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "894479ed-bec2-406d-9c65-952c4e62b9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.956470404621125\n",
      "15.070765638603984\n",
      "15.03891836795422\n",
      "15.371468678506515\n",
      "15.03453673737532\n"
     ]
    }
   ],
   "source": [
    "for _, model in ytw_result: \n",
    "    print(mean_absolute_error(test_dataframe['new_ys'] , model.predict(x_test, batch_size=10000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ef341b3-6f6e-47e4-88d6-77fe33f2c63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.094431965412232"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([14.956470404621125,\n",
    "15.070765638603984,\n",
    "15.03891836795422,\n",
    "15.371468678506515,\n",
    "15.03453673737532]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8beff7d5-0335-40d2-8ab4-840a0312c41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_cols_subset = ['ex_cusip_masked_series_average_1',\n",
    "         'ex_cusip_masked_series_average_10',\n",
    "         'ex_cusip_masked_series_average_50']\n",
    "\n",
    "new_cols_subset = ['series_max', 'series_min', 'series_vol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cdb2ab8-06e1-4013-abfd-e9527f94e60d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA: N = 2191544, MIN DATE = 2023-07-03 00:00:00, MAX DATE = 2023-09-22 00:00:00\n",
      "VALIDATION DATA: N = 243504, MIN DATE = 2023-07-03 00:00:00, MAX DATE = 2023-09-22 00:00:00\n",
      "TEST DATA: N = 216931, MIN DATE = 2023-09-25 00:00:00, MAX DATE = 2023-09-28 00:00:00\n",
      "Experiment results will be saved to experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-15_12:52\n",
      "\n",
      "Performing 5 runs with parameters: {'train_start': '2023-07-01', 'train_end': '2023-09-25', 'test_start': '2023-09-25', 'test_end': '2023-09-29', 'train_size': 2435048, 'test_size': 216931, 'VALIDATION_SPLIT': 0.1, 'LEARNING_RATE': 0.0007, 'BATCH_SIZE': 10000, 'NUM_EPOCHS': 150, 'DROPOUT': 0.1, 'TRADE_SEQUENCE_LENGTH': 5, 'NUM_FEATURES': 6, 'target_variable': 'new_ys', 'features': ['callable', 'sinking', 'zerocoupon', 'is_non_transaction_based_compensation', 'is_general_obligation', 'callable_at_cav', 'extraordinary_make_whole_call', 'make_whole_call', 'has_unexpired_lines_of_credit', 'escrow_exists', 'quantity', 'days_to_maturity', 'days_to_call', 'coupon', 'issue_amount', 'last_seconds_ago', 'last_yield_spread', 'days_to_settle', 'days_to_par', 'maturity_amount', 'issue_price', 'orig_principal_amount', 'max_amount_outstanding', 'accrued_days', 'days_in_interest_payment', 'A/E', 'ficc_treasury_spread', 'max_ys_ys', 'max_ys_ago', 'max_ys_qdiff', 'min_ys_ys', 'min_ys_ago', 'min_ys_qdiff', 'max_qty_ys', 'max_qty_ago', 'max_qty_qdiff', 'min_ago_ys', 'min_ago_ago', 'min_ago_qdiff', 'D_min_ago_ys', 'D_min_ago_ago', 'D_min_ago_qdiff', 'P_min_ago_ys', 'P_min_ago_ago', 'P_min_ago_qdiff', 'S_min_ago_ys', 'S_min_ago_ago', 'S_min_ago_qdiff', 'ex_cusip_masked_series_average_1', 'ex_cusip_masked_series_average_10', 'ex_cusip_masked_series_average_50', 'series_max', 'series_min', 'series_vol', 'rating', 'incorporated_state_code', 'trade_type', 'purpose_class', 'max_ys_ttypes', 'min_ys_ttypes', 'max_qty_ttypes', 'min_ago_ttypes', 'D_min_ago_ttypes', 'P_min_ago_ttypes', 'S_min_ago_ttypes'], 'shuffle_buffer': 0.75, 'use_bottleneck_model': False}\n",
      "Epoch 1/150\n",
      "220/220 [==============================] - 130s 407ms/step - loss: 39.7501 - val_loss: 28.0312\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 16s 71ms/step - loss: 28.7796 - val_loss: 29.7193\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 16.9026 - val_loss: 15.0801\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 13.0345 - val_loss: 12.8525\n",
      "Epoch 5/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 12.5468 - val_loss: 12.2410\n",
      "Epoch 6/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 12.2991 - val_loss: 11.7106\n",
      "Epoch 7/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 12.1243 - val_loss: 11.6175\n",
      "Epoch 8/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 11.9975 - val_loss: 11.3955\n",
      "Epoch 9/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.8889 - val_loss: 11.3102\n",
      "Epoch 10/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 11.7905 - val_loss: 11.1953\n",
      "Epoch 11/150\n",
      "220/220 [==============================] - 18s 79ms/step - loss: 11.7090 - val_loss: 11.2313\n",
      "Epoch 12/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.6588 - val_loss: 11.1301\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.5753 - val_loss: 11.0925\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 11.5279 - val_loss: 11.0647\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 11.4793 - val_loss: 10.9059\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.4288 - val_loss: 11.0150\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 11.3833 - val_loss: 10.9102\n",
      "Epoch 18/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.2724 - val_loss: 10.7887\n",
      "Epoch 21/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 11.2274 - val_loss: 10.7842\n",
      "Epoch 22/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 11.2183 - val_loss: 11.0202\n",
      "Epoch 23/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.1564 - val_loss: 10.7278\n",
      "Epoch 24/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 11.1269 - val_loss: 10.7149\n",
      "Epoch 25/150\n",
      "220/220 [==============================] - 18s 79ms/step - loss: 11.1015 - val_loss: 10.6440\n",
      "Epoch 26/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.0659 - val_loss: 10.9064\n",
      "Epoch 27/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.0528 - val_loss: 10.7000\n",
      "Epoch 28/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 11.0232 - val_loss: 10.6302\n",
      "Epoch 29/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 11.0014 - val_loss: 10.6076\n",
      "Epoch 30/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.9655 - val_loss: 10.6190\n",
      "Epoch 31/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.9546 - val_loss: 10.6079\n",
      "Epoch 32/150\n",
      "220/220 [==============================] - 18s 79ms/step - loss: 10.9257 - val_loss: 10.5864\n",
      "Epoch 33/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.9150 - val_loss: 10.5676\n",
      "Epoch 34/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.8950 - val_loss: 10.9042\n",
      "Epoch 35/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.8653 - val_loss: 10.5275\n",
      "Epoch 36/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.8412 - val_loss: 10.5913\n",
      "Epoch 37/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.8389 - val_loss: 10.6240\n",
      "Epoch 38/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.8136 - val_loss: 10.4856\n",
      "Epoch 39/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.7837 - val_loss: 10.5024\n",
      "Epoch 40/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.7682 - val_loss: 10.5543\n",
      "Epoch 41/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7614 - val_loss: 10.5074\n",
      "Epoch 42/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.7268 - val_loss: 10.5580\n",
      "Epoch 43/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7125 - val_loss: 10.5011\n",
      "Epoch 44/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.7108 - val_loss: 10.4626\n",
      "Epoch 45/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.6795 - val_loss: 10.7284\n",
      "Epoch 46/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.6871 - val_loss: 10.4970\n",
      "Epoch 47/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.6601 - val_loss: 10.5626\n",
      "Epoch 48/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.6714 - val_loss: 10.4419\n",
      "Epoch 49/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.6391 - val_loss: 10.3914\n",
      "Epoch 50/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6193 - val_loss: 10.4833\n",
      "Epoch 51/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5988 - val_loss: 10.4125\n",
      "Epoch 52/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.6017 - val_loss: 10.5474\n",
      "Epoch 53/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.5749 - val_loss: 10.4044\n",
      "Epoch 54/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5828 - val_loss: 10.3820\n",
      "Epoch 55/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.5432 - val_loss: 10.4004\n",
      "Epoch 56/150\n",
      "220/220 [==============================] - 18s 79ms/step - loss: 10.5352 - val_loss: 10.5267\n",
      "Epoch 57/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5280 - val_loss: 10.3369\n",
      "Epoch 58/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.5091 - val_loss: 10.4440\n",
      "Epoch 59/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.5151 - val_loss: 10.3455\n",
      "Epoch 60/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.4867 - val_loss: 10.3304\n",
      "Epoch 61/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.4349 - val_loss: 10.4021\n",
      "Epoch 68/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4067 - val_loss: 10.3563\n",
      "Epoch 69/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.3893 - val_loss: 10.3094\n",
      "Epoch 70/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.3911 - val_loss: 10.2869\n",
      "Epoch 71/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3701 - val_loss: 10.3105\n",
      "Epoch 72/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3618 - val_loss: 10.2061\n",
      "Epoch 73/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.3594 - val_loss: 10.2996\n",
      "Epoch 74/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.3581 - val_loss: 10.2620\n",
      "Epoch 75/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3344 - val_loss: 10.3046\n",
      "Epoch 76/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.3242 - val_loss: 10.2683\n",
      "Epoch 77/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.3191 - val_loss: 10.2895\n",
      "Epoch 78/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3135 - val_loss: 10.2642\n",
      "Epoch 79/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3068 - val_loss: 10.2864\n",
      "Epoch 80/150\n",
      "176/220 [=======================>......] - ETA: 3s - loss: 10.2871"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 121s 363ms/step - loss: 39.7491 - val_loss: 27.6856\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 28.7853 - val_loss: 29.8583\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 16.8827 - val_loss: 15.6737\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 13.0384 - val_loss: 12.7526\n",
      "Epoch 5/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 12.5490 - val_loss: 12.1725\n",
      "Epoch 6/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 12.2924 - val_loss: 11.7069\n",
      "Epoch 7/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 12.1268 - val_loss: 11.5121\n",
      "Epoch 8/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 11.9946 - val_loss: 11.3688\n",
      "Epoch 9/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.8952 - val_loss: 11.3493\n",
      "Epoch 10/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 11.7923 - val_loss: 11.3389\n",
      "Epoch 11/150\n",
      "220/220 [==============================] - 19s 84ms/step - loss: 11.7127 - val_loss: 11.2613\n",
      "Epoch 12/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.6630 - val_loss: 11.0530\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.5806 - val_loss: 11.6542\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 11.5355 - val_loss: 10.9540\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 11.4943 - val_loss: 10.9963\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.4515 - val_loss: 10.8946\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 11.3968 - val_loss: 10.9787\n",
      "Epoch 18/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 11.3576 - val_loss: 10.9172\n",
      "Epoch 19/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.3170 - val_loss: 10.8576\n",
      "Epoch 20/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.2714 - val_loss: 10.8929\n",
      "Epoch 21/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 11.2524 - val_loss: 10.9566\n",
      "Epoch 22/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 11.2081 - val_loss: 10.7726\n",
      "Epoch 23/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.1685 - val_loss: 10.7893\n",
      "Epoch 24/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 11.1384 - val_loss: 10.8700\n",
      "Epoch 25/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 11.1102 - val_loss: 10.7053\n",
      "Epoch 26/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.0815 - val_loss: 10.6378\n",
      "Epoch 27/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.0592 - val_loss: 10.8489\n",
      "Epoch 28/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 11.0297 - val_loss: 10.6444\n",
      "Epoch 29/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.9990 - val_loss: 10.8207\n",
      "Epoch 30/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.9978 - val_loss: 10.5929\n",
      "Epoch 31/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.9755 - val_loss: 10.6130\n",
      "Epoch 32/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.9485 - val_loss: 10.5875\n",
      "Epoch 33/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.9356 - val_loss: 10.5668\n",
      "Epoch 34/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.8946 - val_loss: 10.5712\n",
      "Epoch 35/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.8681 - val_loss: 10.6807\n",
      "Epoch 36/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.8696 - val_loss: 10.5139\n",
      "Epoch 37/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.8490 - val_loss: 10.5637\n",
      "Epoch 38/150\n",
      "220/220 [==============================] - 18s 84ms/step - loss: 10.8237 - val_loss: 10.5592\n",
      "Epoch 39/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.7888 - val_loss: 10.5803\n",
      "Epoch 40/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.7838 - val_loss: 10.5638\n",
      "Epoch 41/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.7744 - val_loss: 10.5473\n",
      "Epoch 42/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.7464 - val_loss: 10.4862\n",
      "Epoch 43/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.7169 - val_loss: 10.4826\n",
      "Epoch 44/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7131 - val_loss: 10.4806\n",
      "Epoch 45/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 10.6929 - val_loss: 10.4981\n",
      "Epoch 46/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6777 - val_loss: 10.4972\n",
      "Epoch 47/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6783 - val_loss: 10.4503\n",
      "Epoch 48/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.6531 - val_loss: 10.4487\n",
      "Epoch 49/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.6445 - val_loss: 10.3804\n",
      "Epoch 50/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6219 - val_loss: 10.3649\n",
      "Epoch 51/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.6143 - val_loss: 10.4542\n",
      "Epoch 52/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.5897 - val_loss: 10.5071\n",
      "Epoch 53/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5952 - val_loss: 10.5829\n",
      "Epoch 54/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5741 - val_loss: 10.3565\n",
      "Epoch 55/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.5605 - val_loss: 10.4385\n",
      "Epoch 56/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.5388 - val_loss: 10.3355\n",
      "Epoch 57/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5421 - val_loss: 10.3358\n",
      "Epoch 58/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.5179 - val_loss: 10.6140\n",
      "Epoch 59/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.5043 - val_loss: 10.4003\n",
      "Epoch 60/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4879 - val_loss: 10.3368\n",
      "Epoch 61/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4803 - val_loss: 10.3410\n",
      "Epoch 62/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.4830 - val_loss: 10.4197\n",
      "Epoch 63/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4542 - val_loss: 10.3239\n",
      "Epoch 64/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4499 - val_loss: 10.2682\n",
      "Epoch 65/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.4360 - val_loss: 10.2852\n",
      "Epoch 66/150\n",
      "220/220 [==============================] - 16s 75ms/step - loss: 10.4111 - val_loss: 10.2527\n",
      "Epoch 67/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4125 - val_loss: 10.3737\n",
      "Epoch 68/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.4016 - val_loss: 10.3807\n",
      "Epoch 69/150\n",
      "220/220 [==============================] - 18s 78ms/step - loss: 10.4014 - val_loss: 10.3005\n",
      "Epoch 70/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3894 - val_loss: 10.2604\n",
      "Epoch 71/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.3891 - val_loss: 10.2775\n",
      "Epoch 72/150\n",
      "220/220 [==============================] - 29s 131ms/step - loss: 10.3813 - val_loss: 10.3193\n",
      "Epoch 73/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.3664 - val_loss: 10.2565\n",
      "Epoch 74/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3527 - val_loss: 10.3006\n",
      "Epoch 75/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.3377 - val_loss: 10.2378\n",
      "Epoch 76/150\n",
      "220/220 [==============================] - 17s 74ms/step - loss: 10.3339 - val_loss: 10.2581\n",
      "Epoch 77/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3272 - val_loss: 10.3973\n",
      "Epoch 78/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.3088 - val_loss: 10.2524\n",
      "Epoch 79/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.2970 - val_loss: 10.2371\n",
      "Epoch 80/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3043 - val_loss: 10.2388\n",
      "Epoch 81/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.2997 - val_loss: 10.2598\n",
      "Epoch 82/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.2762 - val_loss: 10.2249\n",
      "Epoch 83/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2721 - val_loss: 10.2474\n",
      "Epoch 84/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2605 - val_loss: 10.2269\n",
      "Epoch 85/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.2649 - val_loss: 10.1989\n",
      "Epoch 86/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.2382 - val_loss: 10.2402\n",
      "Epoch 87/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2249 - val_loss: 10.3426\n",
      "Epoch 88/150\n",
      "220/220 [==============================] - 16s 75ms/step - loss: 10.2184 - val_loss: 10.1820\n",
      "Epoch 89/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.2202 - val_loss: 10.1985\n",
      "Epoch 90/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2039 - val_loss: 10.1874\n",
      "Epoch 91/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1938 - val_loss: 10.2254\n",
      "Epoch 92/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.1877 - val_loss: 10.2122\n",
      "Epoch 93/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.1743 - val_loss: 10.2070\n",
      "Epoch 94/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1789 - val_loss: 10.2199\n",
      "Epoch 95/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.1597 - val_loss: 10.1575\n",
      "Epoch 96/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.1539 - val_loss: 10.1034\n",
      "Epoch 97/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1674 - val_loss: 10.1430\n",
      "Epoch 98/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1577 - val_loss: 10.1043\n",
      "Epoch 99/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.1430 - val_loss: 10.2529\n",
      "Epoch 100/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.1275 - val_loss: 10.1616\n",
      "Epoch 101/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1314 - val_loss: 10.1698\n",
      "Epoch 102/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.1278 - val_loss: 10.1629\n",
      "Epoch 103/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.1066 - val_loss: 10.1171\n",
      "Epoch 104/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1059 - val_loss: 10.1304\n",
      "Epoch 105/150\n",
      "220/220 [==============================] - 16s 75ms/step - loss: 10.1018 - val_loss: 10.1560\n",
      "Epoch 106/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.0849 - val_loss: 10.1568\n",
      "========================= TRIAL 1, MAE: 14.972722803104359 =========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_7_layer_call_fn, lstm_cell_7_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn, lstm_cell_8_layer_call_and_return_conditional_losses, lstm_cell_10_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-15_12:52/model/model_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-15_12:52/model/model_1/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4cce42710> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4ccb2e290> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4d40cd590> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4cf426d10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-15_12:52/model/model_1\n",
      "Epoch 1/150\n",
      "220/220 [==============================] - 122s 353ms/step - loss: 39.7497 - val_loss: 27.7311\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 28.7854 - val_loss: 29.3701\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 16.8972 - val_loss: 15.6011\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 13.0439 - val_loss: 12.8881\n",
      "Epoch 5/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 12.5436 - val_loss: 12.6320\n",
      "Epoch 6/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 12.2921 - val_loss: 11.6768\n",
      "Epoch 7/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 12.1217 - val_loss: 11.7278\n",
      "Epoch 8/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.9907 - val_loss: 11.3997\n",
      "Epoch 9/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 11.8985 - val_loss: 11.2953\n",
      "Epoch 10/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.7987 - val_loss: 11.1516\n",
      "Epoch 11/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.7166 - val_loss: 11.2007\n",
      "Epoch 12/150\n",
      "220/220 [==============================] - 20s 92ms/step - loss: 11.6630 - val_loss: 11.1403\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 21s 97ms/step - loss: 11.5836 - val_loss: 11.0411\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.5332 - val_loss: 11.0037\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 11.4696 - val_loss: 10.9147\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 11.4309 - val_loss: 10.9048\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.3825 - val_loss: 10.8462\n",
      "Epoch 18/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.3368 - val_loss: 11.0051\n",
      "Epoch 19/150\n",
      "220/220 [==============================] - 19s 86ms/step - loss: 11.2822 - val_loss: 10.8266\n",
      "Epoch 20/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.2614 - val_loss: 10.8861\n",
      "Epoch 21/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.2247 - val_loss: 10.7986\n",
      "Epoch 22/150\n",
      "220/220 [==============================] - 28s 130ms/step - loss: 11.1934 - val_loss: 10.7691\n",
      "Epoch 23/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.1706 - val_loss: 10.6877\n",
      "Epoch 24/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.1228 - val_loss: 10.7481\n",
      "Epoch 25/150\n",
      "220/220 [==============================] - 26s 117ms/step - loss: 11.1056 - val_loss: 10.8621\n",
      "Epoch 26/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.0702 - val_loss: 10.7514\n",
      "Epoch 27/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.0484 - val_loss: 10.7396\n",
      "Epoch 28/150\n",
      "220/220 [==============================] - 21s 96ms/step - loss: 11.0122 - val_loss: 10.6098\n",
      "Epoch 29/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.9988 - val_loss: 10.6100\n",
      "Epoch 30/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.9590 - val_loss: 10.6097\n",
      "Epoch 31/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.9558 - val_loss: 10.6569\n",
      "Epoch 32/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.9321 - val_loss: 10.5354\n",
      "Epoch 33/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.8962 - val_loss: 10.5379\n",
      "Epoch 34/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.8816 - val_loss: 10.6566\n",
      "Epoch 35/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.8533 - val_loss: 10.5387\n",
      "Epoch 36/150\n",
      "220/220 [==============================] - 17s 74ms/step - loss: 10.8526 - val_loss: 10.5172\n",
      "Epoch 37/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.8330 - val_loss: 10.5395\n",
      "Epoch 38/150\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 10.8127 - val_loss: 10.5118\n",
      "Epoch 39/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.7730 - val_loss: 10.4898\n",
      "Epoch 40/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7669 - val_loss: 10.4842\n",
      "Epoch 41/150\n",
      "220/220 [==============================] - 22s 101ms/step - loss: 10.7568 - val_loss: 10.4339\n",
      "Epoch 42/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.7209 - val_loss: 10.5031\n",
      "Epoch 43/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.7059 - val_loss: 10.5559\n",
      "Epoch 44/150\n",
      "220/220 [==============================] - 25s 115ms/step - loss: 10.6872 - val_loss: 10.5371\n",
      "Epoch 45/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.6762 - val_loss: 10.4639\n",
      "Epoch 46/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.6584 - val_loss: 10.4185\n",
      "Epoch 47/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.6530 - val_loss: 10.7214\n",
      "Epoch 48/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6415 - val_loss: 10.4906\n",
      "Epoch 49/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6142 - val_loss: 10.4776\n",
      "Epoch 50/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.6134 - val_loss: 10.4149\n",
      "Epoch 51/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.5957 - val_loss: 10.4757\n",
      "Epoch 52/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6021 - val_loss: 10.5865\n",
      "Epoch 53/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.5678 - val_loss: 10.3677\n",
      "Epoch 54/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.5737 - val_loss: 10.4227\n",
      "Epoch 55/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5429 - val_loss: 10.4185\n",
      "Epoch 56/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5261 - val_loss: 10.3826\n",
      "Epoch 57/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.5029 - val_loss: 10.4520\n",
      "Epoch 58/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.5119 - val_loss: 10.3939\n",
      "Epoch 59/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4882 - val_loss: 10.3386\n",
      "Epoch 60/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.4692 - val_loss: 10.3160\n",
      "Epoch 61/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.4723 - val_loss: 10.3954\n",
      "Epoch 62/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4518 - val_loss: 10.2938\n",
      "Epoch 63/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4330 - val_loss: 10.3535\n",
      "Epoch 64/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.4206 - val_loss: 10.3417\n",
      "Epoch 65/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.4191 - val_loss: 10.3408\n",
      "Epoch 66/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4201 - val_loss: 10.2789\n",
      "Epoch 67/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.4172 - val_loss: 10.4151\n",
      "Epoch 68/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.3884 - val_loss: 10.2760\n",
      "Epoch 69/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3618 - val_loss: 10.2874\n",
      "Epoch 70/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3599 - val_loss: 10.2665\n",
      "Epoch 71/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.3683 - val_loss: 10.2945\n",
      "Epoch 72/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.3577 - val_loss: 10.3429\n",
      "Epoch 73/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3427 - val_loss: 10.2350\n",
      "Epoch 74/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.3438 - val_loss: 10.2668\n",
      "Epoch 75/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.3386 - val_loss: 10.2139\n",
      "Epoch 76/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3174 - val_loss: 10.2392\n",
      "Epoch 77/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3039 - val_loss: 10.2540\n",
      "Epoch 78/150\n",
      "220/220 [==============================] - 20s 91ms/step - loss: 10.3061 - val_loss: 10.3511\n",
      "Epoch 79/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.2932 - val_loss: 10.2121\n",
      "Epoch 80/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 10.2868 - val_loss: 10.1909\n",
      "Epoch 81/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.2797 - val_loss: 10.2389\n",
      "Epoch 82/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.2733 - val_loss: 10.3108\n",
      "Epoch 83/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.2569 - val_loss: 10.2518\n",
      "Epoch 84/150\n",
      "220/220 [==============================] - 18s 79ms/step - loss: 10.2539 - val_loss: 10.2238\n",
      "Epoch 85/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2407 - val_loss: 10.2697\n",
      "Epoch 86/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2240 - val_loss: 10.2194\n",
      "Epoch 87/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.2130 - val_loss: 10.1422\n",
      "Epoch 88/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.2081 - val_loss: 10.1939\n",
      "Epoch 89/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2197 - val_loss: 10.2383\n",
      "Epoch 90/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.2037 - val_loss: 10.2000\n",
      "Epoch 91/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.1753 - val_loss: 10.2196\n",
      "Epoch 92/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.1650 - val_loss: 10.2594\n",
      "Epoch 93/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.1790 - val_loss: 10.1689\n",
      "Epoch 94/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.1720 - val_loss: 10.3372\n",
      "Epoch 95/150\n",
      "220/220 [==============================] - 16s 75ms/step - loss: 10.1607 - val_loss: 10.4107\n",
      "Epoch 96/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1499 - val_loss: 10.1357\n",
      "Epoch 97/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.1498 - val_loss: 10.1451\n",
      "Epoch 98/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.1293 - val_loss: 10.1050\n",
      "Epoch 99/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1228 - val_loss: 10.2042\n",
      "Epoch 100/150\n",
      "220/220 [==============================] - 16s 75ms/step - loss: 10.1239 - val_loss: 10.1645\n",
      "Epoch 101/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.1310 - val_loss: 10.0989\n",
      "Epoch 102/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1026 - val_loss: 10.1198\n",
      "Epoch 103/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.0977 - val_loss: 10.3236\n",
      "Epoch 104/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.0868 - val_loss: 10.2292\n",
      "Epoch 105/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.0714 - val_loss: 10.2975\n",
      "Epoch 106/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.0791 - val_loss: 10.1902\n",
      "Epoch 107/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.0773 - val_loss: 10.0982\n",
      "Epoch 108/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.0515 - val_loss: 10.1803\n",
      "Epoch 109/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.0841 - val_loss: 10.1187\n",
      "Epoch 110/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.0606 - val_loss: 10.1376\n",
      "Epoch 111/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.0425 - val_loss: 10.1211\n",
      "Epoch 112/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.0382 - val_loss: 10.1068\n",
      "Epoch 113/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.0346 - val_loss: 10.2084\n",
      "Epoch 114/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.0123 - val_loss: 10.1457\n",
      "Epoch 115/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.0114 - val_loss: 10.1165\n",
      "Epoch 116/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.0047 - val_loss: 10.1244\n",
      "Epoch 117/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.0017 - val_loss: 10.1182\n",
      "========================= TRIAL 2, MAE: 14.91724899381621 =========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_13_layer_call_fn, lstm_cell_13_layer_call_and_return_conditional_losses, lstm_cell_14_layer_call_fn, lstm_cell_14_layer_call_and_return_conditional_losses, lstm_cell_16_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-15_12:52/model/model_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-15_12:52/model/model_2/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4d0ae6490> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c4e46fd0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c9923210> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c92b6c90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-15_12:52/model/model_2\n",
      "Epoch 1/150\n",
      "220/220 [==============================] - 127s 375ms/step - loss: 39.7498 - val_loss: 27.7610\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 28.7807 - val_loss: 29.0346\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 16.8930 - val_loss: 15.6726\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 13.0472 - val_loss: 12.9253\n",
      "Epoch 5/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 12.5450 - val_loss: 12.1758\n",
      "Epoch 6/150\n",
      "220/220 [==============================] - 19s 88ms/step - loss: 12.3012 - val_loss: 11.6971\n",
      "Epoch 7/150\n",
      "220/220 [==============================] - 19s 86ms/step - loss: 12.1321 - val_loss: 11.6336\n",
      "Epoch 8/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 12.0046 - val_loss: 11.4529\n",
      "Epoch 9/150\n",
      "220/220 [==============================] - 20s 93ms/step - loss: 11.8915 - val_loss: 11.2861\n",
      "Epoch 10/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.7945 - val_loss: 11.2046\n",
      "Epoch 11/150\n",
      "220/220 [==============================] - 20s 90ms/step - loss: 11.7194 - val_loss: 11.1981\n",
      "Epoch 12/150\n",
      "220/220 [==============================] - 23s 104ms/step - loss: 11.6652 - val_loss: 11.1436\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.5838 - val_loss: 11.1482\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 11.5238 - val_loss: 11.0989\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 19s 83ms/step - loss: 11.4774 - val_loss: 10.8916\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.4340 - val_loss: 11.0540\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.4001 - val_loss: 10.8501\n",
      "Epoch 18/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 11.3527 - val_loss: 10.9137\n",
      "Epoch 19/150\n",
      "220/220 [==============================] - 16s 75ms/step - loss: 11.2996 - val_loss: 10.8239\n",
      "Epoch 20/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.2647 - val_loss: 10.7739\n",
      "Epoch 21/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 11.2326 - val_loss: 10.7337\n",
      "Epoch 22/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 11.1999 - val_loss: 10.7616\n",
      "Epoch 23/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.1706 - val_loss: 10.7079\n",
      "Epoch 24/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.1439 - val_loss: 10.6381\n",
      "Epoch 25/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 11.1140 - val_loss: 10.6742\n",
      "Epoch 26/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.0855 - val_loss: 10.7122\n",
      "Epoch 27/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.0466 - val_loss: 10.6289\n",
      "Epoch 28/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 11.0281 - val_loss: 10.8113\n",
      "Epoch 29/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 11.0026 - val_loss: 10.6221\n",
      "Epoch 30/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.9809 - val_loss: 10.6821\n",
      "Epoch 31/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.9619 - val_loss: 10.6725\n",
      "Epoch 32/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.9359 - val_loss: 10.5859\n",
      "Epoch 33/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.9128 - val_loss: 10.5381\n",
      "Epoch 34/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.8883 - val_loss: 10.5713\n",
      "Epoch 35/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.8728 - val_loss: 10.6410\n",
      "Epoch 36/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.8515 - val_loss: 10.6510\n",
      "Epoch 37/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.8537 - val_loss: 10.5187\n",
      "Epoch 38/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.8143 - val_loss: 10.5936\n",
      "Epoch 39/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 10.7820 - val_loss: 10.5896\n",
      "Epoch 40/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.7855 - val_loss: 10.5614\n",
      "Epoch 41/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7522 - val_loss: 10.5158\n",
      "Epoch 42/150\n",
      "220/220 [==============================] - 21s 97ms/step - loss: 10.7402 - val_loss: 10.4848\n",
      "Epoch 43/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.7154 - val_loss: 10.6579\n",
      "Epoch 44/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.7095 - val_loss: 10.4800\n",
      "Epoch 45/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.6918 - val_loss: 10.4906\n",
      "Epoch 46/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.6832 - val_loss: 10.5510\n",
      "Epoch 47/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6679 - val_loss: 10.5261\n",
      "Epoch 48/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6616 - val_loss: 10.4595\n",
      "Epoch 49/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 10.6302 - val_loss: 10.4370\n",
      "Epoch 50/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.6184 - val_loss: 10.4571\n",
      "Epoch 51/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6021 - val_loss: 10.5466\n",
      "Epoch 52/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.6306 - val_loss: 10.4424\n",
      "Epoch 53/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.5913 - val_loss: 10.6102\n",
      "Epoch 54/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5838 - val_loss: 10.4355\n",
      "Epoch 55/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5595 - val_loss: 10.5311\n",
      "Epoch 56/150\n",
      "220/220 [==============================] - 18s 83ms/step - loss: 10.5462 - val_loss: 10.4030\n",
      "Epoch 57/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5298 - val_loss: 10.3714\n",
      "Epoch 58/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5092 - val_loss: 10.4944\n",
      "Epoch 59/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.5215 - val_loss: 10.3502\n",
      "Epoch 60/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.4952 - val_loss: 10.3938\n",
      "Epoch 61/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4873 - val_loss: 10.3371\n",
      "Epoch 62/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4829 - val_loss: 10.2955\n",
      "Epoch 63/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.4670 - val_loss: 10.3553\n",
      "Epoch 64/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.4495 - val_loss: 10.3378\n",
      "Epoch 65/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4513 - val_loss: 10.2962\n",
      "Epoch 66/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.4246 - val_loss: 10.5414\n",
      "Epoch 67/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.4318 - val_loss: 10.3779\n",
      "Epoch 68/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.4206 - val_loss: 10.3048\n",
      "Epoch 69/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3889 - val_loss: 10.3312\n",
      "Epoch 70/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.3823 - val_loss: 10.3171\n",
      "Epoch 71/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.3860 - val_loss: 10.3591\n",
      "Epoch 72/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3790 - val_loss: 10.2694\n",
      "Epoch 73/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.3563 - val_loss: 10.3563\n",
      "Epoch 74/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.3609 - val_loss: 10.2867\n",
      "Epoch 75/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3420 - val_loss: 10.5406\n",
      "Epoch 76/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3245 - val_loss: 10.2781\n",
      "Epoch 77/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.3252 - val_loss: 10.3087\n",
      "Epoch 78/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.3135 - val_loss: 10.2956\n",
      "Epoch 79/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3146 - val_loss: 10.2639\n",
      "Epoch 80/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.3075 - val_loss: 10.3923\n",
      "Epoch 81/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.2995 - val_loss: 10.4818\n",
      "Epoch 82/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2922 - val_loss: 10.3003\n",
      "Epoch 83/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2695 - val_loss: 10.2275\n",
      "Epoch 84/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.2786 - val_loss: 10.1917\n",
      "Epoch 85/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2639 - val_loss: 10.5780\n",
      "Epoch 86/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2585 - val_loss: 10.2002\n",
      "Epoch 87/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.2432 - val_loss: 10.2335\n",
      "Epoch 88/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.2307 - val_loss: 10.3311\n",
      "Epoch 89/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.2307 - val_loss: 10.2790\n",
      "Epoch 90/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2200 - val_loss: 10.2920\n",
      "Epoch 91/150\n",
      "220/220 [==============================] - 22s 102ms/step - loss: 10.2205 - val_loss: 10.3086\n",
      "Epoch 92/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2031 - val_loss: 10.2831\n",
      "Epoch 93/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1801 - val_loss: 10.3772\n",
      "Epoch 94/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 10.1849 - val_loss: 10.2765\n",
      "========================= TRIAL 3, MAE: 14.861387048893556 =========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_19_layer_call_fn, lstm_cell_19_layer_call_and_return_conditional_losses, lstm_cell_20_layer_call_fn, lstm_cell_20_layer_call_and_return_conditional_losses, lstm_cell_22_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-15_12:52/model/model_3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-15_12:52/model/model_3/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c4d4f110> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c117f750> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c117f210> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c66e6c50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-15_12:52/model/model_3\n",
      "Epoch 1/150\n",
      "220/220 [==============================] - 126s 368ms/step - loss: 39.7500 - val_loss: 28.0079\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 28.7840 - val_loss: 28.6548\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 16.8936 - val_loss: 15.6373\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 13.0523 - val_loss: 13.2683\n",
      "Epoch 5/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 12.5566 - val_loss: 12.0428\n",
      "Epoch 6/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 12.3090 - val_loss: 11.7630\n",
      "Epoch 7/150\n",
      "220/220 [==============================] - 17s 74ms/step - loss: 12.1372 - val_loss: 11.5481\n",
      "Epoch 8/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.9953 - val_loss: 11.4387\n",
      "Epoch 9/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 11.8922 - val_loss: 11.3553\n",
      "Epoch 10/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 11.8106 - val_loss: 11.1611\n",
      "Epoch 11/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.7262 - val_loss: 11.3080\n",
      "Epoch 12/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.6619 - val_loss: 11.0549\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 18s 82ms/step - loss: 11.5909 - val_loss: 11.2039\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.5299 - val_loss: 10.9305\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.4913 - val_loss: 11.0459\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 11.4488 - val_loss: 11.0006\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 11.4019 - val_loss: 10.8826\n",
      "Epoch 18/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 11.3550 - val_loss: 11.0030\n",
      "Epoch 19/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 11.3055 - val_loss: 10.9173\n",
      "Epoch 20/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 11.2869 - val_loss: 10.8456\n",
      "Epoch 21/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 11.2529 - val_loss: 10.8347\n",
      "Epoch 22/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.2024 - val_loss: 10.8049\n",
      "Epoch 23/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 11.1766 - val_loss: 10.7104\n",
      "Epoch 24/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 11.1471 - val_loss: 10.6540\n",
      "Epoch 25/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.1226 - val_loss: 10.6722\n",
      "Epoch 26/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.0932 - val_loss: 10.7069\n",
      "Epoch 27/150\n",
      "220/220 [==============================] - 18s 81ms/step - loss: 11.0589 - val_loss: 10.7174\n",
      "Epoch 28/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 11.0381 - val_loss: 10.6427\n",
      "Epoch 29/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 11.0161 - val_loss: 10.6180\n",
      "Epoch 30/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.9997 - val_loss: 10.6067\n",
      "Epoch 31/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.9741 - val_loss: 10.6688\n",
      "Epoch 32/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.9403 - val_loss: 10.7416\n",
      "Epoch 33/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.9218 - val_loss: 10.5360\n",
      "Epoch 34/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.9095 - val_loss: 10.6495\n",
      "Epoch 35/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.8743 - val_loss: 10.6835\n",
      "Epoch 36/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.8685 - val_loss: 10.6067\n",
      "Epoch 37/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.8596 - val_loss: 10.5990\n",
      "Epoch 38/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.8243 - val_loss: 10.6071\n",
      "Epoch 39/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.7980 - val_loss: 10.6559\n",
      "Epoch 40/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.7860 - val_loss: 10.6058\n",
      "Epoch 41/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.7762 - val_loss: 10.5211\n",
      "Epoch 42/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.7388 - val_loss: 10.4617\n",
      "Epoch 43/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.7266 - val_loss: 10.7014\n",
      "Epoch 44/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.7120 - val_loss: 10.5266\n",
      "Epoch 45/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.7183 - val_loss: 10.4642\n",
      "Epoch 46/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.7021 - val_loss: 10.4594\n",
      "Epoch 47/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.6641 - val_loss: 10.6906\n",
      "Epoch 48/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.6609 - val_loss: 10.4426\n",
      "Epoch 49/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.6469 - val_loss: 10.4168\n",
      "Epoch 50/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.6131 - val_loss: 10.4211\n",
      "Epoch 51/150\n",
      "220/220 [==============================] - 17s 80ms/step - loss: 10.6219 - val_loss: 10.5649\n",
      "Epoch 52/150\n",
      "220/220 [==============================] - 17s 74ms/step - loss: 10.5943 - val_loss: 10.5049\n",
      "Epoch 53/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5933 - val_loss: 10.4916\n",
      "Epoch 54/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.5858 - val_loss: 10.3936\n",
      "Epoch 55/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.5752 - val_loss: 10.3674\n",
      "Epoch 56/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.5403 - val_loss: 10.3361\n",
      "Epoch 57/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.5375 - val_loss: 10.3935\n",
      "Epoch 58/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.5184 - val_loss: 10.3740\n",
      "Epoch 59/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.4992 - val_loss: 10.4476\n",
      "Epoch 60/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.4980 - val_loss: 10.4478\n",
      "Epoch 61/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.4936 - val_loss: 10.4600\n",
      "Epoch 62/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.4693 - val_loss: 10.3593\n",
      "Epoch 63/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.4635 - val_loss: 10.4488\n",
      "Epoch 64/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.4632 - val_loss: 10.4736\n",
      "Epoch 65/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.4337 - val_loss: 10.3004\n",
      "Epoch 66/150\n",
      "220/220 [==============================] - 17s 74ms/step - loss: 10.4294 - val_loss: 10.3309\n",
      "Epoch 67/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.4175 - val_loss: 10.3233\n",
      "Epoch 68/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.4100 - val_loss: 10.3571\n",
      "Epoch 69/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.4010 - val_loss: 10.2730\n",
      "Epoch 70/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3921 - val_loss: 10.2782\n",
      "Epoch 71/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3749 - val_loss: 10.2690\n",
      "Epoch 72/150\n",
      "220/220 [==============================] - 17s 80ms/step - loss: 10.3776 - val_loss: 10.3180\n",
      "Epoch 73/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.3630 - val_loss: 10.2893\n",
      "Epoch 74/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3479 - val_loss: 10.2933\n",
      "Epoch 75/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.3422 - val_loss: 10.3526\n",
      "Epoch 76/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.3249 - val_loss: 10.2306\n",
      "Epoch 77/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3229 - val_loss: 10.2334\n",
      "Epoch 78/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3258 - val_loss: 10.3328\n",
      "Epoch 79/150\n",
      "220/220 [==============================] - 18s 80ms/step - loss: 10.3067 - val_loss: 10.3228\n",
      "Epoch 80/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.3111 - val_loss: 10.2882\n",
      "Epoch 81/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.3004 - val_loss: 10.2079\n",
      "Epoch 82/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.2821 - val_loss: 10.2312\n",
      "Epoch 83/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.2792 - val_loss: 10.2714\n",
      "Epoch 84/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2697 - val_loss: 10.2930\n",
      "Epoch 85/150\n",
      "220/220 [==============================] - 16s 75ms/step - loss: 10.2689 - val_loss: 10.2996\n",
      "Epoch 86/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.2374 - val_loss: 10.2586\n",
      "Epoch 87/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.2442 - val_loss: 10.2605\n",
      "Epoch 88/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.2460 - val_loss: 10.1987\n",
      "Epoch 89/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.2238 - val_loss: 10.1825\n",
      "Epoch 90/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 10.2023 - val_loss: 10.1794\n",
      "Epoch 91/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.2248 - val_loss: 10.1543\n",
      "Epoch 92/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.1960 - val_loss: 10.1815\n",
      "Epoch 93/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.1819 - val_loss: 10.2619\n",
      "Epoch 94/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1920 - val_loss: 10.1786\n",
      "Epoch 95/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1681 - val_loss: 10.1352\n",
      "Epoch 96/150\n",
      "220/220 [==============================] - 17s 79ms/step - loss: 10.1751 - val_loss: 10.1072\n",
      "Epoch 97/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.1671 - val_loss: 10.1981\n",
      "Epoch 98/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.1574 - val_loss: 10.1601\n",
      "Epoch 99/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 10.1396 - val_loss: 10.2109\n",
      "Epoch 100/150\n",
      "220/220 [==============================] - 18s 79ms/step - loss: 10.1475 - val_loss: 10.2409\n",
      "Epoch 101/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.1209 - val_loss: 10.1736\n",
      "Epoch 102/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 10.1321 - val_loss: 10.1448\n",
      "Epoch 103/150\n",
      "220/220 [==============================] - 17s 78ms/step - loss: 10.1250 - val_loss: 10.1852\n",
      "Epoch 104/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 10.1215 - val_loss: 10.1201\n",
      "Epoch 105/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 10.1015 - val_loss: 10.2046\n",
      "Epoch 106/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 10.1004 - val_loss: 10.1621\n",
      "========================= TRIAL 4, MAE: 15.01059748064228 =========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_25_layer_call_fn, lstm_cell_25_layer_call_and_return_conditional_losses, lstm_cell_26_layer_call_fn, lstm_cell_26_layer_call_and_return_conditional_losses, lstm_cell_28_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-15_12:52/model/model_4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-15_12:52/model/model_4/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4be2c5350> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c11bee90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4cf41f250> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c11ec3d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to experiments/similar_trades_small_experiment/new_ys_experiment_2023-11-15_12:52/model/model_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://f1c15fa0-2199-4d4f-a7d4-5369e709cf1d/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://f1c15fa0-2199-4d4f-a7d4-5369e709cf1d/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4d47d2990> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4d48c8c10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4d48c4050> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4d49277d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_7_layer_call_fn, lstm_cell_7_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn, lstm_cell_8_layer_call_and_return_conditional_losses, lstm_cell_10_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://5864b169-9733-4120-b5c5-199d8e5ac9fc/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://5864b169-9733-4120-b5c5-199d8e5ac9fc/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4cce42710> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4ccb2e290> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4d40cd590> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4cf426d10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_13_layer_call_fn, lstm_cell_13_layer_call_and_return_conditional_losses, lstm_cell_14_layer_call_fn, lstm_cell_14_layer_call_and_return_conditional_losses, lstm_cell_16_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://2e9f8c89-0699-4d89-855f-9ee01b983448/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://2e9f8c89-0699-4d89-855f-9ee01b983448/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4d0ae6490> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c4e46fd0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c9923210> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c92b6c90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_19_layer_call_fn, lstm_cell_19_layer_call_and_return_conditional_losses, lstm_cell_20_layer_call_fn, lstm_cell_20_layer_call_and_return_conditional_losses, lstm_cell_22_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://4e3bd59b-da21-49fa-8e8c-7ea5b08930a1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://4e3bd59b-da21-49fa-8e8c-7ea5b08930a1/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c4d4f110> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c117f750> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c117f210> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c66e6c50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NON_CAT_AND_BINARY_FEATURES, D_min_ago_ttypes, P_min_ago_ttypes, S_min_ago_ttypes with unsupported characters which will be renamed to non_cat_and_binary_features, d_min_ago_ttypes, p_min_ago_ttypes, s_min_ago_ttypes in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_25_layer_call_fn, lstm_cell_25_layer_call_and_return_conditional_losses, lstm_cell_26_layer_call_fn, lstm_cell_26_layer_call_and_return_conditional_losses, lstm_cell_28_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://22cd7105-dcf2-41ef-92f8-495e627fd651/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://22cd7105-dcf2-41ef-92f8-495e627fd651/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4be2c5350> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c11bee90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4cf41f250> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd4c11ec3d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "reset_model_features()\n",
    "modify_features(masked_cols_subset + new_cols_subset, 'add', 'numeric')\n",
    "params, normalizers, x_train, y_train, x_val, y_val, x_test, y_test, val_idx = create_data_set_and_model(train_dataframe, \n",
    "                                                                                                         test_dataframe, \n",
    "                                                                                                         trade_history_col)\n",
    "shuffle_buffer = .75\n",
    "use_bottleneck_model = False\n",
    "NUM_EPOCHS = 150\n",
    "verbose = 1\n",
    "model_to_use='default'\n",
    "ytw_result = run_experiment(5, experiment_prefix='similar_trades_small_experiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "821611d0-cc3e-4ca6-a33e-b9569bf55686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.880043125378567\n",
      "14.972722803104359\n",
      "14.91724899381621\n",
      "14.861387048893556\n",
      "15.01059748064228\n"
     ]
    }
   ],
   "source": [
    "for _, model in ytw_result: \n",
    "    print(mean_absolute_error(test_dataframe['new_ys'] , model.predict(x_test, batch_size=10000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5018f1fe-480d-45ef-b13b-4ee62b676684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.928399890366993"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([14.880043125378567,\n",
    "14.972722803104359,\n",
    "14.91724899381621,\n",
    "14.861387048893556,\n",
    "15.01059748064228]).mean()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
