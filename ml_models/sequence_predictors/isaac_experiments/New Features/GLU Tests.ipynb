{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f64a01a-fc76-4427-9c17-90e0c6170c88",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GLU Units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9573053a-638e-4425-ac52-71d6d97b43d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load packages, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae9770dd-3c8a-4aa7-9aac-b2e9f43ea216",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 20:20:25.918932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-07 20:20:25.931599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-07 20:20:25.933255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing pandarallel with 16.0 cores\n",
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "TF Version: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle5 as pickle\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import gcsfs\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from ficc.utils.auxiliary_functions import sqltodf\n",
    "from ficc.utils.auxiliary_variables import PREDICTORS, NON_CAT_FEATURES, BINARY, CATEGORICAL_FEATURES, IDENTIFIERS, PURPOSE_CLASS_DICT, NUM_OF_DAYS_IN_YEAR\n",
    "from ficc.utils.gcp_storage_functions import upload_data, download_data\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from ficc_keras_utils import *\n",
    "import ficc_keras_utils\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "print(f'TF Version: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f400a88d-5a00-4df9-b6f0-8ae32511f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/home/jupyter/ficc/isaac_creds.json\"\n",
    "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "bq_client = bigquery.Client()\n",
    "storage_client = storage.Client()\n",
    "\n",
    "##COMMON VARIABLES\n",
    "#DATA WINDOW\n",
    "train_start = '2023-02-01'\n",
    "train_end = '2023-07-01'\n",
    "test_start = '2023-07-01'\n",
    "test_end = '2023-08-01'\n",
    "#MODEL PARAMETERS \n",
    "VALIDATION_SPLIT = 0.1 #ficc_keras_utils.VALIDATION_SPLIT\n",
    "LEARNING_RATE = ficc_keras_utils.LEARNING_RATE\n",
    "BATCH_SIZE = 1000 #ficc_keras_utils.BATCH_SIZE\n",
    "NUM_EPOCHS = 75 #ficc_keras_utils.NUM_EPOCHS\n",
    "DROPOUT = 0.1 #ficc_keras_utils.DROPOUT\n",
    "\n",
    "##NOTEBOOK SPECIFIC VARIABLES \n",
    "TRADE_SEQUENCE_LENGTH = 5\n",
    "NUM_FEATURES = 6\n",
    "target_variable = 'new_ys' \n",
    "trade_history_col = 'trade_history_shortened'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43044bf8-1700-410d-8c2f-bfd90061ef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_pickle(path, bucket = 'isaac_data'):\n",
    "    if os.path.isfile(path):\n",
    "        print('File available, loading pickle')\n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "    else:\n",
    "        print(f'File not available, downloading from cloud storage and saving to {path}')\n",
    "        fs = gcsfs.GCSFileSystem(project='eng-reactor-287421')\n",
    "        gc_path = os.path.join(bucket, path)\n",
    "        print(gc_path)\n",
    "        with fs.open(gc_path) as gf:\n",
    "            data = pd.read_pickle(gf)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86182ec4-0214-4471-ae0e-bc030a643a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "--bucket=\n",
    "--file= "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb0e5b4c-9f62-4bbb-a4f8-8b774195943f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not available, downloading from cloud storage and saving to data_jan23_aug_23.pkl\n",
      "custom-train-job-test/data_jan23_aug_23.pkl\n",
      "CPU times: user 3min 8s, sys: 29.4 s, total: 3min 37s\n",
      "Wall time: 5min 44s\n"
     ]
    }
   ],
   "source": [
    "%time processed_data = load_data_from_pickle('data_jan23_aug_23.pkl', bucket = 'custom-train-job-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ae88e69-3297-4ba7-b10a-69d4c5a412d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2023-01-03 00:00:00'), Timestamp('2023-08-15 00:00:00'))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.trade_date.min(), processed_data.trade_date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bea3dfdf-c387-43b7-88b3-dbf3af6234d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.reset_index(drop=True, inplace=True)\n",
    "processed_data.sort_values('trade_datetime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7976648d-51a1-42e9-af08-8ef03301673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data): \n",
    "    # data['ted-rate'] = (data['t_rate_10'] - data['t_rate_2']) * 100\n",
    "    \n",
    "    data = data[(data.days_to_call == 0) | (data.days_to_call > np.log10(400))]\n",
    "    data = data[(data.days_to_refund == 0) | (data.days_to_refund > np.log10(400))]\n",
    "    data = data[(data.days_to_maturity == 0) | (data.days_to_maturity > np.log10(400))]\n",
    "    data = data[data.days_to_maturity < np.log10(30000)]\n",
    "    data['trade_history_sum'] = data.trade_history.parallel_apply(lambda x: np.sum(x))\n",
    "    data.issue_amount = data.issue_amount.replace([np.inf, -np.inf], np.nan)\n",
    "    data.dropna(inplace=True, subset=PREDICTORS+['trade_history_sum'])\n",
    "    data.purpose_sub_class.fillna(0, inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81f644cc-1140-455d-8425-dc5334d30499",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttype_dict = { (0,0):'D', (0,1):'S', (1,0):'P' }\n",
    "\n",
    "ys_variants = [\"max_ys\", \"min_ys\", \"max_qty\", \"min_ago\", \"D_min_ago\", \"P_min_ago\", \"S_min_ago\"]\n",
    "ys_feats = [\"_ys\", \"_ttypes\", \"_ago\", \"_qdiff\"]\n",
    "D_prev = dict()\n",
    "P_prev = dict()\n",
    "S_prev = dict()\n",
    "\n",
    "def get_trade_history_columns():\n",
    "    '''\n",
    "    This function is used to create a list of columns\n",
    "    '''\n",
    "    YS_COLS = []\n",
    "    for prefix in ys_variants:\n",
    "        for suffix in ys_feats:\n",
    "            YS_COLS.append(prefix + suffix)\n",
    "    return YS_COLS\n",
    "\n",
    "def extract_feature_from_trade(row, name, trade):\n",
    "    yield_spread = trade[0]\n",
    "    ttypes = ttype_dict[(trade[3],trade[4])] + row.trade_type\n",
    "    seconds_ago = trade[5]\n",
    "    diff = 10**trade[2] - 10**row.quantity\n",
    "    quantity_diff = np.sign(diff) * np.log10(1 + np.abs(diff))\n",
    "    return [yield_spread, ttypes,  seconds_ago, quantity_diff]\n",
    "\n",
    "def trade_history_derived_features(row):\n",
    "    trade_history = row.trade_history\n",
    "    trade = trade_history[0]\n",
    "    \n",
    "    D_min_ago_t = D_prev.get(row.cusip,trade)\n",
    "    D_min_ago = 9        \n",
    "\n",
    "    P_min_ago_t = P_prev.get(row.cusip,trade)\n",
    "    P_min_ago = 9\n",
    "    \n",
    "    S_min_ago_t = S_prev.get(row.cusip,trade)\n",
    "    S_min_ago = 9\n",
    "    \n",
    "    max_ys_t = trade; max_ys = trade[0]\n",
    "    min_ys_t = trade; min_ys = trade[0]\n",
    "    max_qty_t = trade; max_qty = trade[2]\n",
    "    min_ago_t = trade; min_ago = trade[5]\n",
    "    \n",
    "    for trade in trade_history[0:]:\n",
    "        #Checking if the first trade in the history is from the same block\n",
    "        if trade[5] == 0: \n",
    "            continue\n",
    " \n",
    "        if trade[0] > max_ys: \n",
    "            max_ys_t = trade\n",
    "            max_ys = trade[0]\n",
    "        elif trade[0] < min_ys: \n",
    "            min_ys_t = trade; \n",
    "            min_ys = trade[0]\n",
    "\n",
    "        if trade[2] > max_qty: \n",
    "            max_qty_t = trade \n",
    "            max_qty = trade[2]\n",
    "        if trade[5] < min_ago: \n",
    "            min_ago_t = trade; \n",
    "            min_ago = trade[5]\n",
    "            \n",
    "        side = ttype_dict[(trade[3],trade[4])]\n",
    "        if side == \"D\":\n",
    "            if trade[5] < D_min_ago: \n",
    "                D_min_ago_t = trade; D_min_ago = trade[5]\n",
    "                D_prev[row.cusip] = trade\n",
    "        elif side == \"P\":\n",
    "            if trade[5] < P_min_ago: \n",
    "                P_min_ago_t = trade; P_min_ago = trade[5]\n",
    "                P_prev[row.cusip] = trade\n",
    "        elif side == \"S\":\n",
    "            if trade[5] < S_min_ago: \n",
    "                S_min_ago_t = trade; S_min_ago = trade[5]\n",
    "                S_prev[row.cusip] = trade\n",
    "        else: \n",
    "            print(\"invalid side\", trade)\n",
    "    \n",
    "    trade_history_dict = {\"max_ys\":max_ys_t,\n",
    "                          \"min_ys\":min_ys_t,\n",
    "                          \"max_qty\":max_qty_t,\n",
    "                          \"min_ago\":min_ago_t,\n",
    "                          \"D_min_ago\":D_min_ago_t,\n",
    "                          \"P_min_ago\":P_min_ago_t,\n",
    "                          \"S_min_ago\":S_min_ago_t}\n",
    "\n",
    "    return_list = []\n",
    "    for variant in ys_variants:\n",
    "        feature_list = extract_feature_from_trade(row,variant,trade_history_dict[variant])\n",
    "        return_list += feature_list\n",
    "    \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "41f57df8-a39e-48ca-9ab7-85aae1c1883c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 53s, sys: 23.8 s, total: 3min 17s\n",
      "Wall time: 5min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "temp = processed_data[['cusip','trade_history','quantity','trade_type']].parallel_apply(trade_history_derived_features, axis=1)\n",
    "YS_COLS = get_trade_history_columns()\n",
    "processed_data[YS_COLS] = pd.DataFrame(temp.tolist(), index=processed_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be46d633-1b20-497b-b362-f0c7450e9b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.55 s, sys: 117 ms, total: 1.67 s\n",
      "Wall time: 1.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "processed_data['ttypes'] = (processed_data.last_trade_type.astype(str) + processed_data.trade_type.astype(str)).astype('category')\n",
    "processed_data['diff_size'] = (processed_data.par_traded.astype(float) - processed_data.last_size).astype(np.float32)\n",
    "processed_data['abs_last_yield_spread'] = np.abs(processed_data['last_yield_spread'])\n",
    "processed_data['abs_diff_size'] = np.abs(processed_data['diff_size'])\n",
    "processed_data['days_duration'] = (processed_data.last_calc_date - processed_data.last_settlement_date).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71cbdfd6-fefc-4bde-b8bd-7b5a46045132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.8 s, sys: 15.2 s, total: 1min\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "processed_data['trade_history_sum'] = processed_data.trade_history.parallel_apply(lambda x: np.sum(x))\n",
    "processed_data = processed_data.dropna(subset=['trade_history_sum'])\n",
    "processed_data.purpose_sub_class.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b23e0d2-a140-422c-bfb8-a1ac3a3043cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['new_ficc_ycl', 'yield_spread']:\n",
    "    if col in NON_CAT_FEATURES:\n",
    "        NON_CAT_FEATURES.remove(col)\n",
    "    if col in PREDICTORS:\n",
    "        PREDICTORS.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf73d079-bbeb-4caf-ad70-eaa627cf2944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 11s, sys: 19.6 s, total: 1min 30s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "processed_data = process_data(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "75845e84-eb9e-4d1f-8d21-d676f119b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data['moodys_rating'] = processed_data.moodys_long.fillna('NR')\n",
    "processed_data['ratings_concat'] = processed_data['rating'] + processed_data['moodys_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f750a58c-f443-4585-8713-0a2a12fca45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_features(cols, how, where=None):\n",
    "    #function to conveniently add or subtract features from the global feature lists \n",
    "    \n",
    "    global CATEGORICAL_FEATURES, PREDICTORS, NON_CAT_FEATURES, BINARY\n",
    "    \n",
    "    if how not in ['add','remove']:\n",
    "        raise ValueError(f\"'how' argument must be one off the following: {['add','remove']}\")\n",
    "        \n",
    "    if where and where not in ['categorical','binary', 'numeric']:\n",
    "        raise ValueError(f\"'where' argument must be one off the following: {['categorical','binary', 'numeric']}\")\n",
    "    \n",
    "    if how == 'remove':\n",
    "        for col in cols: \n",
    "            if col in CATEGORICAL_FEATURES:\n",
    "                CATEGORICAL_FEATURES.remove(col)\n",
    "            if col in BINARY:\n",
    "                BINARY.remove(col)\n",
    "            if col in NON_CAT_FEATURES:\n",
    "                NON_CAT_FEATURES.remove(col)\n",
    "            if col in PREDICTORS:\n",
    "                PREDICTORS.remove(col)\n",
    "            \n",
    "    if how == 'add':\n",
    "        for col in cols: \n",
    "            if col not in CATEGORICAL_FEATURES and where=='categorical':\n",
    "                CATEGORICAL_FEATURES.append(col)\n",
    "            if col not in BINARY and where=='binary':\n",
    "                BINARY.append(col)\n",
    "            if col not in NON_CAT_FEATURES and where=='numeric':\n",
    "                NON_CAT_FEATURES.append(col)\n",
    "            if col not in PREDICTORS:\n",
    "                PREDICTORS.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ba82a9c-cec4-49f8-91c9-85fee70df319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating\n",
      "incorporated_state_code\n",
      "trade_type\n",
      "purpose_class\n",
      "max_ys_ttypes\n",
      "min_ys_ttypes\n",
      "max_qty_ttypes\n",
      "min_ago_ttypes\n",
      "D_min_ago_ttypes\n",
      "P_min_ago_ttypes\n",
      "S_min_ago_ttypes\n",
      "moodys_rating\n",
      "ratings_concat\n"
     ]
    }
   ],
   "source": [
    "additional_features = ['ttypes', 'diff_size', 'abs_last_yield_spread', 'abs_diff_size', 'days_duration']\n",
    "YS_COLS = ['max_ys_ys', 'max_ys_ttypes', 'max_ys_ago', 'max_ys_qdiff', 'min_ys_ys', 'min_ys_ttypes', 'min_ys_ago', 'min_ys_qdiff', 'max_qty_ys', 'max_qty_ttypes', \\\n",
    "           'max_qty_ago', 'max_qty_qdiff', 'min_ago_ys', 'min_ago_ttypes', 'min_ago_ago', 'min_ago_qdiff', 'D_min_ago_ys', 'D_min_ago_ttypes', 'D_min_ago_ago', 'D_min_ago_qdiff',\\\n",
    "           'P_min_ago_ys', 'P_min_ago_ttypes', 'P_min_ago_ago', 'P_min_ago_qdiff', 'S_min_ago_ys', 'S_min_ago_ttypes', 'S_min_ago_ago', 'S_min_ago_qdiff']\n",
    "    \n",
    "for col in YS_COLS:\n",
    "    if 'ttypes' in col and col not in PREDICTORS:\n",
    "        PREDICTORS.append(col)\n",
    "        CATEGORICAL_FEATURES.append(col)\n",
    "    elif col not in PREDICTORS:\n",
    "        NON_CAT_FEATURES.append(col)\n",
    "        PREDICTORS.append(col)\n",
    "        \n",
    "if 'target_attention_features' not in PREDICTORS:\n",
    "    PREDICTORS.append('target_attention_features')\n",
    "    \n",
    "if 'ficc_treasury_spread' not in PREDICTORS:\n",
    "    PREDICTORS.append('ficc_treasury_spread')\n",
    "    NON_CAT_FEATURES.append('ficc_treasury_spread')\n",
    "\n",
    "#add for the encoders, then remove later if needed \n",
    "modify_features(['rating','moodys_rating', 'ratings_concat'], how = 'add', where='categorical')   \n",
    "    \n",
    "encoders = {}\n",
    "fmax = {}\n",
    "for f in CATEGORICAL_FEATURES:\n",
    "    print(f)\n",
    "    if f in ['rating', 'incorporated_state_code', 'trade_type', 'purpose_class']:\n",
    "        fprep = preprocessing.LabelEncoder().fit(categorical_feature_values[f])\n",
    "    else:\n",
    "        fprep = preprocessing.LabelEncoder().fit(processed_data[f].drop_duplicates())\n",
    "    fmax[f] = np.max(fprep.transform(fprep.classes_))\n",
    "    encoders[f] = fprep\n",
    "    \n",
    "# with open('encoders.pkl','wb') as file:\n",
    "#     pickle.dump(encoders,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "69e34f7f-f26f-442a-8122-567a9710dd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.23 s, sys: 360 ms, total: 7.59 s\n",
      "Wall time: 7.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TRADE_SEQUENCE_LENGTH = 5 \n",
    "processed_data['trade_history_shortened'] = processed_data['trade_history'].apply(lambda x: x[:TRADE_SEQUENCE_LENGTH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "61236328-8a6e-4021-8dae-b8f21c341c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.sort_values('trade_datetime',ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a301ce8f-b141-4bb9-bd32-5e80eadba832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data start: 2023-02-01 00:00:00, end: 2023-06-30 00:00:00\n",
      "Test data start: 2023-07-03 00:00:00, end: 2023-07-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "train_filter = (processed_data.trade_date < train_end) & (processed_data.trade_date >= train_start)\n",
    "test_filter = (processed_data.trade_date >= test_start) & (processed_data.trade_date <test_end)\n",
    "                                                            \n",
    "train_dataframe = processed_data[train_filter]\\\n",
    ".sort_values(by='trade_date', ascending=True)\\\n",
    ".reset_index(drop=True)\n",
    "\n",
    "test_dataframe = processed_data[test_filter]\\\n",
    ".sort_values(by='trade_date', ascending=True)\\\n",
    ".reset_index(drop=True)\n",
    "\n",
    "print('Training data start: {}, end: {}'.format(train_dataframe.trade_date.min(),train_dataframe.trade_date.max()))\n",
    "print('Test data start: {}, end: {}'.format(test_dataframe.trade_date.min(),test_dataframe.trade_date.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4146e5ed-09ea-426e-a8b3-00d50ff481e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3387004, 643018)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataframe), len(test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7d58b814-46ad-4391-aee4-4f77ef3c99f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_new(df, trade_history_col, yield_history_cols):\n",
    "    global encoders\n",
    "    datalist = []\n",
    "    \n",
    "    datalist.append(np.stack(df[trade_history_col].to_numpy()))\n",
    "    datalist.append(np.stack(df['target_attention_features'].to_numpy()))\n",
    "\n",
    "    noncat_and_binary = []\n",
    "    for f in NON_CAT_FEATURES + BINARY:\n",
    "        noncat_and_binary.append(np.expand_dims(df[f].to_numpy().astype('float32'), axis=1))\n",
    "    datalist.append(np.concatenate(noncat_and_binary, axis=-1))\n",
    "    \n",
    "    for f in CATEGORICAL_FEATURES:\n",
    "        encoded = encoders[f].transform(df[f])\n",
    "        datalist.append(encoded.astype('float32'))\n",
    "    \n",
    "    return datalist\n",
    "\n",
    "def create_data_set_and_model(train_dataframe, test_dataframe, trade_history_col, yield_history_cols):\n",
    "    \n",
    "    if not isinstance(trade_history_col, str):\n",
    "        raise ValueError('trade_history_col must be a string')\n",
    "    \n",
    "    if isinstance(yield_history_cols, str):\n",
    "        num_yield_history = 1\n",
    "        yield_history_cols = [yield_history_cols]\n",
    "    else:\n",
    "        num_yield_history = len(yield_history_cols)\n",
    "    \n",
    "    yield_history_lengths = [train_dataframe[x][0].shape[0] for x in yield_history_cols]\n",
    "    \n",
    "    TRADE_SEQUENCE_LENGTH = train_dataframe[trade_history_col][0].shape[0] \n",
    "    \n",
    "    params = {'TRADE_SEQUENCE_LENGTH':TRADE_SEQUENCE_LENGTH, \n",
    "           'yield_history_cols':yield_history_cols, \n",
    "           'yield_history_lengths':yield_history_lengths, \n",
    "           'num_yield_history':num_yield_history }\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    val_idx = np.random.choice(range(len(train_dataframe)), \n",
    "                     size = int(VALIDATION_SPLIT*len(train_dataframe)),\n",
    "                     replace=False)\n",
    "\n",
    "    print(f'TRAINING DATA: N = {len(train_dataframe)-len(val_idx)}, MIN DATE = {train_dataframe.drop(val_idx, axis=0).trade_date.min()}, MAX DATE = {train_dataframe.drop(val_idx, axis=0).trade_date.max()}')\n",
    "    print(f'VALIDATION DATA: N = {len(val_idx)}, MIN DATE = {train_dataframe.iloc[val_idx].trade_date.min()}, MAX DATE = {train_dataframe.iloc[val_idx].trade_date.max()}')\n",
    "    print(f'TEST DATA: N = {len(test_dataframe)}, MIN DATE = {test_dataframe.trade_date.min()}, MAX DATE = {test_dataframe.trade_date.max()}')\n",
    "\n",
    "    x_train = create_input_new(train_dataframe.drop(val_idx, axis=0), trade_history_col, yield_history_cols)\n",
    "    y_train = train_dataframe.drop(val_idx, axis=0)[target_variable]\n",
    "\n",
    "    x_val = create_input_new(train_dataframe.iloc[val_idx], trade_history_col, yield_history_cols)\n",
    "    y_val = train_dataframe.iloc[val_idx][target_variable]\n",
    "\n",
    "    x_test = create_input_new(test_dataframe, trade_history_col, yield_history_cols)\n",
    "    y_test = test_dataframe[target_variable]    \n",
    "    \n",
    "    # Normalization layer for the yield history\n",
    "    yield_history_normalizers = []\n",
    "    for i in range(num_yield_history):\n",
    "        yield_history_normalizers.append(Normalization(name=f'Yield_history_normalizer_{yield_history_cols[i]}'))\n",
    "        yield_history_normalizers[i].adapt(x_train[i],batch_size=BATCH_SIZE)\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        # Normalization layer for the trade history\n",
    "        trade_history_normalizer = Normalization(name='Trade_history_normalizer')\n",
    "        trade_history_normalizer.adapt(x_train[0],batch_size=BATCH_SIZE)\n",
    "\n",
    "        # Normalization layer for the non-categorical and binary features\n",
    "        noncat_binary_normalizer = Normalization(name='Numerical_binary_normalizer')\n",
    "        noncat_binary_normalizer.adapt(x_train[2], batch_size = BATCH_SIZE)\n",
    "\n",
    "    normalizers = {'yield_history_normalizers': yield_history_normalizers,\n",
    "                  'trade_history_normalizer': trade_history_normalizer,\n",
    "                  'noncat_binary_normalizer': noncat_binary_normalizer}\n",
    "\n",
    "    return  params, normalizers, x_train, y_train, x_val, y_val, x_test, y_test, val_idx\n",
    "\n",
    "def create_tf_data(x_train, y_train, shuffle=False, shuffle_buffer=1):\n",
    "                     \n",
    "    X=()\n",
    "    for x in x_train:\n",
    "        X += (tf.data.Dataset.from_tensor_slices(x),)\n",
    "        \n",
    "\n",
    "    temp = tf.data.Dataset.zip((X))\n",
    "    del X\n",
    "    dataset = tf.data.Dataset.zip((temp,\n",
    "                        tf.data.Dataset.from_tensor_slices(y_train)))\n",
    "    del temp\n",
    "    if shuffle:\n",
    "        shuffle_buffer = int(len(x_train[0])*shuffle_buffer)\n",
    "        dataset = dataset.shuffle(shuffle_buffer)\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "\n",
    "#!pip install gpustat\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import regex as r\n",
    "import gpustat\n",
    "\n",
    "class LossCallback(Callback):\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        super().on_epoch_begin(batch)\n",
    "        self.gpu_usage = []\n",
    "        \n",
    "    def on_train_batch_end(self, batch, logs={}):  \n",
    "        if batch % 25 == 0:\n",
    "            s = str(gpustat.new_query()[0])\n",
    "            gpu_usage = int(r.findall('[0-9]+ %', s)[0].split('%')[0])\n",
    "            self.gpu_usage.append(gpu_usage)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        s = str(gpustat.new_query()[0])\n",
    "        gpu_mem = r.findall('[0-9]+ / [0-9]+', s)[0].replace(' ', '')\n",
    "        \n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "            \n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        \n",
    "        \n",
    "        train_loss_cols = [x for x in logs.keys() if 'val' not in x]\n",
    "        val_loss_cols = [x for x in logs.keys() if 'val' in x]\n",
    "        \n",
    "        for loss in train_loss_cols:\n",
    "            \n",
    "                train_loss.append(logs.get(loss, None))\n",
    "                \n",
    "        for loss in val_loss_cols:\n",
    "            \n",
    "                val_loss.append(logs.get(loss, None))\n",
    "        \n",
    "        if train_loss and val_loss:\n",
    "            # print(f'Epoch {epoch + 1}')\n",
    "            df = pd.DataFrame([train_loss, val_loss], \n",
    "                              index = ['Train loss', 'Val loss'],\n",
    "                              # columns = [f'Ensemble_model_{i+1}' for i in range(ENSEMBLE_SIZE)],\n",
    "                             columns = train_loss_cols)\n",
    "            \n",
    "            df['GPU MEMORY'] = gpu_mem\n",
    "            df['MAX GPU USAGE'] = str(max(self.gpu_usage))+'%' if self.gpu_usage else None\n",
    "            display(df)\n",
    "            print()\n",
    "            \n",
    "loss_callback = LossCallback()\n",
    "\n",
    "fit_callbacks = [\n",
    "keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=10,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    restore_best_weights=True),\n",
    "    # CSVLoggerTimeHistory(timestamp+'_training_logs_yield_history.csv', separator=\",\", append=False),\n",
    "    loss_callback\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4746138a-370b-442f-b1fb-b19cc84aca13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA: N = 3048304, MIN DATE = 2023-02-01 00:00:00, MAX DATE = 2023-06-30 00:00:00\n",
      "VALIDATION DATA: N = 338700, MIN DATE = 2023-02-01 00:00:00, MAX DATE = 2023-06-30 00:00:00\n",
      "TEST DATA: N = 643018, MIN DATE = 2023-07-03 00:00:00, MAX DATE = 2023-07-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "trade_history_col = 'trade_history_shortened'\n",
    "yield_history_cols = []\n",
    "modify_features(['moodys_rating', 'ratings_concat'], how = 'remove')   \n",
    "\n",
    "params, normalizers, x_train, y_train, x_val, y_val, x_test, y_test, val_idx = create_data_set_and_model(train_dataframe,                                                                                                         \n",
    "                                                                                                         test_dataframe, \n",
    "                                                                                                         trade_history_col, \n",
    "                                                                                                         yield_history_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "591d6658-3336-404e-ac64-c5c26cb9085a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "(3048304, 5, 6)\n",
      "(3048304, 1, 3)\n",
      "(3048304, 48)\n",
      "(3048304,)\n",
      "(3048304,)\n",
      "(3048304,)\n",
      "(3048304,)\n",
      "(3048304,)\n",
      "(3048304,)\n",
      "(3048304,)\n",
      "(3048304,)\n",
      "(3048304,)\n",
      "(3048304,)\n",
      "(3048304,)\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))\n",
    "for i in range(len(x_train)):\n",
    "    print(x_train[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a07e7fa2-41f6-47e0-9e8c-627541a9a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = True\n",
    "shuffle_buffer = 0.75\n",
    "\n",
    "TRADE_SEQUENCE_LENGTH = params.get('TRADE_SEQUENCE_LENGTH')\n",
    "yield_history_cols = params.get('yield_history_cols')\n",
    "yield_history_lengths = params.get('yield_history_lengths')\n",
    "num_yield_history = params.get('num_yield_history')\n",
    "\n",
    "yield_history_normalizers = normalizers.get('yield_history_normalizers')\n",
    "trade_history_normalizer = normalizers.get('trade_history_normalizer')\n",
    "noncat_binary_normalizer = normalizers.get('noncat_binary_normalizer')\n",
    "\n",
    "tf.keras.utils.set_random_seed(10)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d %H-%M')\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    train_ds = create_tf_data(x_train, y_train, shuffle, shuffle_buffer)\n",
    "    train_ds = train_ds.batch(BATCH_SIZE).prefetch(2).cache()\n",
    "    val_ds = create_tf_data(x_val, y_val, shuffle = False)\n",
    "    val_ds = val_ds.batch(5000).prefetch(2).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cc3598cb-15a3-4dce-aaeb-38d412b2297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedLinearUnit(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True,\n",
    "                                name = 'W')\n",
    "        \n",
    "        self.bias1 = self.add_weight(\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "                                name = 'b1')\n",
    "        \n",
    "        self.V = self.add_weight(shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True,\n",
    "                                 name = 'V')\n",
    "        self.bias2 = self.add_weight(\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "        name='b2')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        sigmoid = tf.sigmoid(inputs @ self.W + self.bias1)\n",
    "        output = tf.multiply(inputs @ self.V + self.bias2, sigmoid)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e604bc6a-0b82-4ab7-b336-7303bea1a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(TRADE_SEQUENCE_LENGTH, yield_history_cols, yield_history_lengths, num_yield_history,\n",
    "                      yield_history_normalizers, trade_history_normalizer, noncat_binary_normalizer):\n",
    "    inputs = []\n",
    "    layer = []\n",
    "    \n",
    "    trade_history_input = layers.Input(name=\"trade_history_input\", \n",
    "                                       shape=(TRADE_SEQUENCE_LENGTH, NUM_FEATURES), \n",
    "                                       dtype = tf.float32) \n",
    "\n",
    "    target_attention_input = layers.Input(name=\"target_attention_input\", \n",
    "                                       shape=(1, 3), \n",
    "                                       dtype = tf.float32) \n",
    "    inputs.append(trade_history_input)\n",
    "    inputs.append(target_attention_input)\n",
    "\n",
    "    inputs.append(layers.Input(\n",
    "        name=\"NON_CAT_AND_BINARY_FEATURES\",\n",
    "        shape=(len(NON_CAT_FEATURES + BINARY),)\n",
    "    ))\n",
    "\n",
    "\n",
    "    layer.append(noncat_binary_normalizer(inputs[num_yield_history+2]))\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "    ############## TRADE HISTORY MODEL #################\n",
    "\n",
    "    lstm_layer = layers.Bidirectional(layers.LSTM(50, \n",
    "                             activation='tanh',\n",
    "                             input_shape=(TRADE_SEQUENCE_LENGTH,NUM_FEATURES),\n",
    "                             return_sequences = True,\n",
    "                             name='LSTM'))\n",
    "\n",
    "    lstm_layer_2 = layers.Bidirectional(layers.LSTM(100, \n",
    "                                                    activation='tanh',\n",
    "                                                    input_shape=(TRADE_SEQUENCE_LENGTH, 50),\n",
    "                                                    return_sequences = True,\n",
    "                                                    name='LSTM_2'))\n",
    "\n",
    "\n",
    "\n",
    "    features = lstm_layer(trade_history_normalizer(inputs[num_yield_history]))\n",
    "    features = lstm_layer_2(features)  \n",
    "    \n",
    "    \n",
    "    attention_sequence = layers.Dense(200, activation='relu', name='attention_dense')(target_attention_input)\n",
    "    attention = layers.Dot(axes=[2, 2])([features, attention_sequence])\n",
    "    attention = layers.Activation('softmax')(attention)\n",
    "\n",
    "    context_vector = layers.Dot(axes=[1, 1])([features, attention])\n",
    "    context_vector = layers.Flatten(name='context_vector_flatten')(context_vector)\n",
    "\n",
    "\n",
    "    trade_history_output = layers.Dense(100, \n",
    "                                        activation='relu')(context_vector)\n",
    " \n",
    "    ####################################################\n",
    "\n",
    "    ############## REFERENCE DATA MODEL ################\n",
    "    global encoders\n",
    "    for f in CATEGORICAL_FEATURES:\n",
    "        fin = layers.Input(shape=(1,), name = f)\n",
    "        inputs.append(fin)\n",
    "        embedded = layers.Flatten(name = f + \"_flat\")( layers.Embedding(input_dim = fmax[f]+1,\n",
    "                                                                        output_dim = max(30,int(np.sqrt(fmax[f]))),\n",
    "                                                                        input_length= 1,\n",
    "                                                                        name = f + \"_embed\")(fin))\n",
    "        layer.append(embedded)\n",
    "    \n",
    "    \n",
    "    ### CONCATENATE EMBEDDINGS\n",
    "    reference_hidden = layers.Dropout(DROPOUT)(layers.concatenate(layer, axis=-1))\n",
    "    reference_hidden = layers.Dense(400,\n",
    "                                    activation='relu',\n",
    "                                    name='reference_hidden_1')(reference_hidden)\n",
    "    reference_hidden = layers.BatchNormalization()(reference_hidden)\n",
    "    reference_GLU = GatedLinearUnit()(reference_hidden)\n",
    "    \n",
    "\n",
    "    reference_hidden2 = layers.Dropout(DROPOUT)(reference_GLU)\n",
    "    reference_hidden2 = layers.Dense(200,activation='relu',name='reference_hidden_2')(reference_hidden2)\n",
    "    reference_hidden2 = layers.BatchNormalization()(reference_hidden2)\n",
    "    reference_GLU2 = GatedLinearUnit()(reference_hidden2)\n",
    "    \n",
    "    \n",
    "    reference_hidden3 = layers.Dropout(DROPOUT)(reference_GLU2)\n",
    "    reference_hidden3 = layers.Dense(100,activation='relu',name='reference_hidden_3')(reference_hidden3)\n",
    "    reference_hidden3 = layers.BatchNormalization()(reference_hidden3)\n",
    "    reference_output = GatedLinearUnit()(reference_hidden3)\n",
    "   \n",
    "\n",
    "    ####################################################\n",
    "\n",
    "    feed_forward_input = layers.concatenate([reference_output, trade_history_output])\n",
    "    \n",
    "    hidden = layers.Dropout(DROPOUT)(feed_forward_input)\n",
    "    hidden = layers.Dense(300,activation='relu')(feed_forward_input)\n",
    "    hidden = layers.BatchNormalization()(hidden)\n",
    "    hidden = GatedLinearUnit()(hidden)\n",
    "\n",
    "    hidden2 = layers.Dropout(DROPOUT)(hidden)\n",
    "    hidden2 = layers.Dense(100,activation='relu')(hidden)\n",
    "    hidden2 = layers.BatchNormalization()(hidden2)\n",
    "    hidden2 = GatedLinearUnit()(hidden2)\n",
    "\n",
    "    final = layers.Dense(1)(hidden2)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=final)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_data_set_and_model(train_dataframe, test_dataframe, trade_history_col, yield_history_cols):\n",
    "    \n",
    "    if not isinstance(trade_history_col, str):\n",
    "        raise ValueError('trade_history_col must be a string')\n",
    "    \n",
    "    if isinstance(yield_history_cols, str):\n",
    "        num_yield_history = 1\n",
    "        yield_history_cols = [yield_history_cols]\n",
    "    else:\n",
    "        num_yield_history = len(yield_history_cols)\n",
    "    \n",
    "    yield_history_lengths = [train_dataframe[x][0].shape[0] for x in yield_history_cols]\n",
    "    \n",
    "    TRADE_SEQUENCE_LENGTH = train_dataframe[trade_history_col][0].shape[0] \n",
    "    \n",
    "    params = {'TRADE_SEQUENCE_LENGTH':TRADE_SEQUENCE_LENGTH, \n",
    "           'yield_history_cols':yield_history_cols, \n",
    "           'yield_history_lengths':yield_history_lengths, \n",
    "           'num_yield_history':num_yield_history }\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    val_idx = np.random.choice(range(len(train_dataframe)), \n",
    "                     size = int(VALIDATION_SPLIT*len(train_dataframe)),\n",
    "                     replace=False)\n",
    "\n",
    "    print(f'TRAINING DATA: N = {len(train_dataframe)-len(val_idx)}, MIN DATE = {train_dataframe.drop(val_idx, axis=0).trade_date.min()}, MAX DATE = {train_dataframe.drop(val_idx, axis=0).trade_date.max()}')\n",
    "    print(f'VALIDATION DATA: N = {len(val_idx)}, MIN DATE = {train_dataframe.iloc[val_idx].trade_date.min()}, MAX DATE = {train_dataframe.iloc[val_idx].trade_date.max()}')\n",
    "    print(f'TEST DATA: N = {len(test_dataframe)}, MIN DATE = {test_dataframe.trade_date.min()}, MAX DATE = {test_dataframe.trade_date.max()}')\n",
    "\n",
    "    x_train = create_input_new(train_dataframe.drop(val_idx, axis=0), trade_history_col, yield_history_cols)\n",
    "    y_train = train_dataframe.drop(val_idx, axis=0)[target_variable]\n",
    "\n",
    "    x_val = create_input_new(train_dataframe.iloc[val_idx], trade_history_col, yield_history_cols)\n",
    "    y_val = train_dataframe.iloc[val_idx][target_variable]\n",
    "\n",
    "    x_test = create_input_new(test_dataframe, trade_history_col, yield_history_cols)\n",
    "    y_test = test_dataframe[target_variable]    \n",
    "    \n",
    "    # Normalization layer for the yield history\n",
    "    yield_history_normalizers = []\n",
    "    for i in range(num_yield_history):\n",
    "        yield_history_normalizers.append(Normalization(name=f'Yield_history_normalizer_{yield_history_cols[i]}'))\n",
    "        yield_history_normalizers[i].adapt(x_train[i],batch_size=BATCH_SIZE)\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        # Normalization layer for the trade history\n",
    "        trade_history_normalizer = Normalization(name='Trade_history_normalizer')\n",
    "        trade_history_normalizer.adapt(x_train[0],batch_size=BATCH_SIZE)\n",
    "\n",
    "        # Normalization layer for the non-categorical and binary features\n",
    "        noncat_binary_normalizer = Normalization(name='Numerical_binary_normalizer')\n",
    "        noncat_binary_normalizer.adapt(x_train[2], batch_size = BATCH_SIZE)\n",
    "\n",
    "    normalizers = {'yield_history_normalizers': yield_history_normalizers,\n",
    "                  'trade_history_normalizer': trade_history_normalizer,\n",
    "                  'noncat_binary_normalizer': noncat_binary_normalizer}\n",
    "\n",
    "    return  params, normalizers, x_train, y_train, x_val, y_val, x_test, y_test, val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "33bc7c40-030f-4e8e-9db6-b4ea615b5e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_new(params, normalizers, x_train, y_train, x_val, y_val, shuffle, shuffle_buffer=1):\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    TRADE_SEQUENCE_LENGTH = params.get('TRADE_SEQUENCE_LENGTH')\n",
    "    yield_history_cols = params.get('yield_history_cols')\n",
    "    yield_history_lengths = params.get('yield_history_lengths')\n",
    "    num_yield_history = params.get('num_yield_history')\n",
    "      \n",
    "    yield_history_normalizers = normalizers.get('yield_history_normalizers')\n",
    "    trade_history_normalizer = normalizers.get('trade_history_normalizer')\n",
    "    noncat_binary_normalizer = normalizers.get('noncat_binary_normalizer')\n",
    "       \n",
    "    tf.keras.utils.set_random_seed(10)\n",
    "    model = generate_model(TRADE_SEQUENCE_LENGTH, yield_history_cols, yield_history_lengths, num_yield_history, \n",
    "                               yield_history_normalizers, trade_history_normalizer, noncat_binary_normalizer)\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H-%M')\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        train_ds = create_tf_data(x_train, y_train, shuffle, shuffle_buffer)\n",
    "        train_ds = train_ds.batch(BATCH_SIZE).prefetch(2).cache()\n",
    "        val_ds = create_tf_data(x_val, y_val, shuffle = False)\n",
    "        val_ds = val_ds.batch(BATCH_SIZE).prefetch(2).cache()\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "          loss=keras.losses.MeanAbsoluteError())\n",
    "\n",
    "    history= model.fit(train_ds,\n",
    "                                      validation_data=val_ds,\n",
    "                                        epochs=NUM_EPOCHS,     \n",
    "                                        verbose=1, \n",
    "                                        callbacks=fit_callbacks,\n",
    "                                        use_multiprocessing=True,\n",
    "                                        workers=8)\n",
    "    \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9bf4c685-7cfd-4e3e-9940-b56c1a702e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3ca067-5abb-48a6-aa72-e2406eaf86e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-05 22:46:57.268644: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14254866432 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-09-05 22:47:09.029135: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 346170 of 2286228\n",
      "2023-09-05 22:47:19.029150: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 648405 of 2286228\n",
      "2023-09-05 22:47:29.029136: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 1196238 of 2286228\n",
      "2023-09-05 22:47:39.029132: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 1737284 of 2286228\n",
      "2023-09-05 22:47:49.029125: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 2285575 of 2286228\n",
      "2023-09-05 22:47:49.041631: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:405] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6/3049 [..............................] - ETA: 1:17 - loss: 57.2690  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0229s vs `on_train_batch_end` time: 0.0230s). Check your callbacks.\n",
      "3049/3049 [==============================] - ETA: 0s - loss: 14.3787"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-05 22:50:09.820693: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14254866432 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>GPU MEMORY</th>\n",
       "      <th>MAX GPU USAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train loss</th>\n",
       "      <td>14.379</td>\n",
       "      <td>7070/15360</td>\n",
       "      <td>92%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Val loss</th>\n",
       "      <td>10.244</td>\n",
       "      <td>7070/15360</td>\n",
       "      <td>92%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             loss  GPU MEMORY MAX GPU USAGE\n",
       "Train loss 14.379  7070/15360           92%\n",
       "Val loss   10.244  7070/15360           92%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3049/3049 [==============================] - 213s 50ms/step - loss: 14.3787 - val_loss: 10.2439\n",
      "Epoch 2/125\n",
      "3049/3049 [==============================] - ETA: 0s - loss: 10.5259"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>GPU MEMORY</th>\n",
       "      <th>MAX GPU USAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train loss</th>\n",
       "      <td>10.526</td>\n",
       "      <td>7070/15360</td>\n",
       "      <td>94%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Val loss</th>\n",
       "      <td>9.970</td>\n",
       "      <td>7070/15360</td>\n",
       "      <td>94%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             loss  GPU MEMORY MAX GPU USAGE\n",
       "Train loss 10.526  7070/15360           94%\n",
       "Val loss    9.970  7070/15360           94%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3049/3049 [==============================] - 115s 38ms/step - loss: 10.5259 - val_loss: 9.9697\n",
      "Epoch 3/125\n",
      "3049/3049 [==============================] - ETA: 0s - loss: 10.1487"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>GPU MEMORY</th>\n",
       "      <th>MAX GPU USAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train loss</th>\n",
       "      <td>10.149</td>\n",
       "      <td>7070/15360</td>\n",
       "      <td>93%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Val loss</th>\n",
       "      <td>9.682</td>\n",
       "      <td>7070/15360</td>\n",
       "      <td>93%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             loss  GPU MEMORY MAX GPU USAGE\n",
       "Train loss 10.149  7070/15360           93%\n",
       "Val loss    9.682  7070/15360           93%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3049/3049 [==============================] - 102s 33ms/step - loss: 10.1487 - val_loss: 9.6824\n",
      "Epoch 4/125\n",
      "3047/3049 [============================>.] - ETA: 0s - loss: 9.8831"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>GPU MEMORY</th>\n",
       "      <th>MAX GPU USAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train loss</th>\n",
       "      <td>9.883</td>\n",
       "      <td>7070/15360</td>\n",
       "      <td>94%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Val loss</th>\n",
       "      <td>9.677</td>\n",
       "      <td>7070/15360</td>\n",
       "      <td>94%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss  GPU MEMORY MAX GPU USAGE\n",
       "Train loss 9.883  7070/15360           94%\n",
       "Val loss   9.677  7070/15360           94%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3049/3049 [==============================] - 114s 37ms/step - loss: 9.8832 - val_loss: 9.6767\n",
      "Epoch 5/125\n",
      "3047/3049 [============================>.] - ETA: 0s - loss: 9.7205"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>GPU MEMORY</th>\n",
       "      <th>MAX GPU USAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train loss</th>\n",
       "      <td>9.721</td>\n",
       "      <td>7070/15360</td>\n",
       "      <td>94%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Val loss</th>\n",
       "      <td>9.518</td>\n",
       "      <td>7070/15360</td>\n",
       "      <td>94%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss  GPU MEMORY MAX GPU USAGE\n",
       "Train loss 9.721  7070/15360           94%\n",
       "Val loss   9.518  7070/15360           94%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3049/3049 [==============================] - 115s 38ms/step - loss: 9.7206 - val_loss: 9.5179\n",
      "Epoch 6/125\n",
      "3049/3049 [==============================] - ETA: 0s - loss: 9.6046"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>GPU MEMORY</th>\n",
       "      <th>MAX GPU USAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train loss</th>\n",
       "      <td>9.605</td>\n",
       "      <td>7070/15360</td>\n",
       "      <td>95%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Val loss</th>\n",
       "      <td>9.381</td>\n",
       "      <td>7070/15360</td>\n",
       "      <td>95%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss  GPU MEMORY MAX GPU USAGE\n",
       "Train loss 9.605  7070/15360           95%\n",
       "Val loss   9.381  7070/15360           95%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3049/3049 [==============================] - 115s 38ms/step - loss: 9.6046 - val_loss: 9.3813\n",
      "Epoch 7/125\n",
      "3048/3049 [============================>.] - ETA: 0s - loss: 9.5106"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>GPU MEMORY</th>\n",
       "      <th>MAX GPU USAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train loss</th>\n",
       "      <td>9.511</td>\n",
       "      <td>7070/15360</td>\n",
       "      <td>93%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Val loss</th>\n",
       "      <td>9.272</td>\n",
       "      <td>7070/15360</td>\n",
       "      <td>93%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss  GPU MEMORY MAX GPU USAGE\n",
       "Train loss 9.511  7070/15360           93%\n",
       "Val loss   9.272  7070/15360           93%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3049/3049 [==============================] - 113s 37ms/step - loss: 9.5106 - val_loss: 9.2723\n",
      "Epoch 8/125\n",
      "3048/3049 [============================>.] - ETA: 0s - loss: 9.4353"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>GPU MEMORY</th>\n",
       "      <th>MAX GPU USAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train loss</th>\n",
       "      <td>9.435</td>\n",
       "      <td>7070/15360</td>\n",
       "      <td>94%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Val loss</th>\n",
       "      <td>9.321</td>\n",
       "      <td>7070/15360</td>\n",
       "      <td>94%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss  GPU MEMORY MAX GPU USAGE\n",
       "Train loss 9.435  7070/15360           94%\n",
       "Val loss   9.321  7070/15360           94%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3049/3049 [==============================] - 113s 37ms/step - loss: 9.4353 - val_loss: 9.3210\n",
      "Epoch 9/125\n",
      "3047/3049 [============================>.] - ETA: 0s - loss: 9.3692"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>GPU MEMORY</th>\n",
       "      <th>MAX GPU USAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train loss</th>\n",
       "      <td>9.369</td>\n",
       "      <td>7070/15360</td>\n",
       "      <td>94%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Val loss</th>\n",
       "      <td>9.166</td>\n",
       "      <td>7070/15360</td>\n",
       "      <td>94%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss  GPU MEMORY MAX GPU USAGE\n",
       "Train loss 9.369  7070/15360           94%\n",
       "Val loss   9.166  7070/15360           94%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3049/3049 [==============================] - 113s 37ms/step - loss: 9.3692 - val_loss: 9.1663\n",
      "Epoch 10/125\n",
      "3048/3049 [============================>.] - ETA: 0s - loss: 9.3064"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>GPU MEMORY</th>\n",
       "      <th>MAX GPU USAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train loss</th>\n",
       "      <td>9.306</td>\n",
       "      <td>7070/15360</td>\n",
       "      <td>94%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Val loss</th>\n",
       "      <td>9.064</td>\n",
       "      <td>7070/15360</td>\n",
       "      <td>94%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss  GPU MEMORY MAX GPU USAGE\n",
       "Train loss 9.306  7070/15360           94%\n",
       "Val loss   9.064  7070/15360           94%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3049/3049 [==============================] - 114s 37ms/step - loss: 9.3064 - val_loss: 9.0645\n",
      "Epoch 11/125\n",
      "1422/3049 [============>.................] - ETA: 47s - loss: 9.3202"
     ]
    }
   ],
   "source": [
    "# results = []\n",
    "# predictions = []\n",
    "NUM_EPOCHS = 125\n",
    "\n",
    "for i in range(4):\n",
    "    tf.keras.utils.set_random_seed(10)\n",
    "    start = time.time()\n",
    "    history, model = train_model_new(params, normalizers, x_train, y_train, x_val, y_val, True, shuffle_buffer = .75)\n",
    "\n",
    "    pred = model.predict(x_test)\n",
    "    predictions.append(pred)\n",
    "    print('='*25+f' TRIAL {i}, MAE: {mean_absolute_error(pred,y_test)} '+'='*25)\n",
    "    results.append([history, model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "55d638a8-3dfa-4db0-b010-25b4459ed17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1: 7.89 (76 epochs)\n",
      "Trial 2: 8.02 (125 epochs)\n",
      "Trial 3: 8.04 (125 epochs)\n",
      "Trial 4: 7.94 (92 epochs)\n",
      "Trial 5: 8.07 (125 epochs)\n"
     ]
    }
   ],
   "source": [
    "pred_filter = (-5000 < test_dataframe['prediction']) & (test_dataframe['prediction'] < 5000)\n",
    "\n",
    "for i, pred in enumerate(predictions): \n",
    "    print(f'Trial {i+1}: {mean_absolute_error(pred[pred_filter], y_test[pred_filter]):.2f} ({len(results[i][0].history[\"val_loss\"])} epochs)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78df66cb-40c6-4a7a-bd4a-0a2c776cbdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_test, batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef274b-5198-4f3b-bd69-54d4cf132be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataframe['prediction'] = pred\n",
    "test_dataframe['error'] = test_dataframe.new_ys - test_dataframe['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7778e500-5c8f-4cd9-9b58-cb018fec8faa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.886971382603239"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(pred[pred_filter], y_test[pred_filter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bd1996-60b6-4924-9edf-c997446b5bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantity: [4.30103]\n",
      "days_to_maturity: [3.96553094]\n",
      "days_to_call: [3.14050804]\n",
      "coupon: [3.875]\n",
      "issue_amount: [7.9439397]\n",
      "last_seconds_ago: [19942088.]\n",
      "last_yield_spread: [55.16586272]\n",
      "days_to_settle: [2]\n",
      "days_to_par: [3.14050804]\n",
      "maturity_amount: [6.59328618]\n",
      "issue_price: [100.]\n",
      "orig_principal_amount: [6.59328618]\n",
      "max_amount_outstanding: [6.59328618]\n",
      "accrued_days: [1891]\n",
      "days_in_interest_payment: [180.]\n",
      "A/E: [0.44444444]\n",
      "max_ys_ys: [78.37144734]\n",
      "max_ys_ago: [7.3017921]\n",
      "max_ys_qdiff: [0.]\n",
      "min_ys_ys: [40.86586272]\n",
      "min_ys_ago: [7.29977065]\n",
      "min_ys_qdiff: [0.]\n",
      "max_qty_ys: [55.16586272]\n",
      "max_qty_ago: [7.29977065]\n",
      "max_qty_qdiff: [0.]\n",
      "min_ago_ys: [55.16586272]\n",
      "min_ago_ago: [7.29977065]\n",
      "min_ago_qdiff: [0.]\n",
      "D_min_ago_ys: [55.16586272]\n",
      "D_min_ago_ago: [7.29977065]\n",
      "D_min_ago_qdiff: [0.]\n",
      "P_min_ago_ys: [78.37144734]\n",
      "P_min_ago_ago: [7.3017921]\n",
      "P_min_ago_qdiff: [0.]\n",
      "S_min_ago_ys: [40.86586272]\n",
      "S_min_ago_ago: [7.29977065]\n",
      "S_min_ago_qdiff: [0.]\n",
      "ficc_treasury_spread: [12.91293769]\n"
     ]
    }
   ],
   "source": [
    "for col in NON_CAT_FEATURES:\n",
    "    print(f'{col}: {test_dataframe[test_dataframe.rtrs_control_number == 2023071804421200][col].values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46a9656-1306-472a-ac81-d886adf9c90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rtrs_control_number</th>\n",
       "      <th>trade_datetime</th>\n",
       "      <th>last_trade_datetime</th>\n",
       "      <th>cusip</th>\n",
       "      <th>new_ys</th>\n",
       "      <th>last_yield</th>\n",
       "      <th>new_real_time_ficc_ycl</th>\n",
       "      <th>prediction</th>\n",
       "      <th>error</th>\n",
       "      <th>trade_history_shortened</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>348245</th>\n",
       "      <td>2023071804421200</td>\n",
       "      <td>2023-07-18 12:13:07</td>\n",
       "      <td>2022-11-29 16:44:59</td>\n",
       "      <td>64987DJC9</td>\n",
       "      <td>61.706</td>\n",
       "      <td>480.500</td>\n",
       "      <td>411.293</td>\n",
       "      <td>-10102020.000</td>\n",
       "      <td>10102081.706</td>\n",
       "      <td>[[55.16586271742739, 99.5, 4.301030158996582, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348247</th>\n",
       "      <td>2023071804420700</td>\n",
       "      <td>2023-07-18 12:13:07</td>\n",
       "      <td>2022-11-29 16:44:59</td>\n",
       "      <td>64987DJC9</td>\n",
       "      <td>66.606</td>\n",
       "      <td>480.500</td>\n",
       "      <td>411.293</td>\n",
       "      <td>-10102021.000</td>\n",
       "      <td>10102087.606</td>\n",
       "      <td>[[55.16586271742739, 99.5, 4.301030158996582, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348615</th>\n",
       "      <td>2023071804568200</td>\n",
       "      <td>2023-07-18 12:16:36</td>\n",
       "      <td>2023-07-18 12:13:07</td>\n",
       "      <td>64987DJC9</td>\n",
       "      <td>40.687</td>\n",
       "      <td>468.400</td>\n",
       "      <td>408.313</td>\n",
       "      <td>-10102006.000</td>\n",
       "      <td>10102046.687</td>\n",
       "      <td>[[64.4870623097533, 77.4, 4.301030158996582, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348622</th>\n",
       "      <td>2023071804532000</td>\n",
       "      <td>2023-07-18 12:16:30</td>\n",
       "      <td>2023-07-18 12:13:07</td>\n",
       "      <td>64987DJC9</td>\n",
       "      <td>40.087</td>\n",
       "      <td>468.400</td>\n",
       "      <td>408.313</td>\n",
       "      <td>-10102010.000</td>\n",
       "      <td>10102050.087</td>\n",
       "      <td>[[64.4870623097533, 77.4, 4.301030158996582, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379199</th>\n",
       "      <td>2023071900295300</td>\n",
       "      <td>2023-07-19 08:37:40</td>\n",
       "      <td>2023-07-18 12:16:36</td>\n",
       "      <td>64987DJC9</td>\n",
       "      <td>34.489</td>\n",
       "      <td>444.600</td>\n",
       "      <td>405.571</td>\n",
       "      <td>-10102019.000</td>\n",
       "      <td>10102053.489</td>\n",
       "      <td>[[40.68706230975323, 53.6, 4.301030158996582, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379418</th>\n",
       "      <td>2023071900319500</td>\n",
       "      <td>2023-07-19 08:40:02</td>\n",
       "      <td>2023-07-19 08:37:40</td>\n",
       "      <td>64987DJC9</td>\n",
       "      <td>36.202</td>\n",
       "      <td>435.000</td>\n",
       "      <td>405.558</td>\n",
       "      <td>-10102022.000</td>\n",
       "      <td>10102058.202</td>\n",
       "      <td>[[34.50169568948314, 51.0, 4.301030158996582, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399923</th>\n",
       "      <td>2023072011145200</td>\n",
       "      <td>2023-07-20 16:05:01</td>\n",
       "      <td>2023-07-19 08:40:02</td>\n",
       "      <td>64987DJC9</td>\n",
       "      <td>17.738</td>\n",
       "      <td>436.700</td>\n",
       "      <td>404.827</td>\n",
       "      <td>-10102017.000</td>\n",
       "      <td>10102034.738</td>\n",
       "      <td>[[36.20169568948319, 52.7, 4.301030158996582, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        rtrs_control_number      trade_datetime last_trade_datetime  \\\n",
       "348245     2023071804421200 2023-07-18 12:13:07 2022-11-29 16:44:59   \n",
       "348247     2023071804420700 2023-07-18 12:13:07 2022-11-29 16:44:59   \n",
       "348615     2023071804568200 2023-07-18 12:16:36 2023-07-18 12:13:07   \n",
       "348622     2023071804532000 2023-07-18 12:16:30 2023-07-18 12:13:07   \n",
       "379199     2023071900295300 2023-07-19 08:37:40 2023-07-18 12:16:36   \n",
       "379418     2023071900319500 2023-07-19 08:40:02 2023-07-19 08:37:40   \n",
       "399923     2023072011145200 2023-07-20 16:05:01 2023-07-19 08:40:02   \n",
       "\n",
       "            cusip  new_ys  last_yield  new_real_time_ficc_ycl    prediction  \\\n",
       "348245  64987DJC9  61.706     480.500                 411.293 -10102020.000   \n",
       "348247  64987DJC9  66.606     480.500                 411.293 -10102021.000   \n",
       "348615  64987DJC9  40.687     468.400                 408.313 -10102006.000   \n",
       "348622  64987DJC9  40.087     468.400                 408.313 -10102010.000   \n",
       "379199  64987DJC9  34.489     444.600                 405.571 -10102019.000   \n",
       "379418  64987DJC9  36.202     435.000                 405.558 -10102022.000   \n",
       "399923  64987DJC9  17.738     436.700                 404.827 -10102017.000   \n",
       "\n",
       "              error                            trade_history_shortened  \n",
       "348245 10102081.706  [[55.16586271742739, 99.5, 4.301030158996582, ...  \n",
       "348247 10102087.606  [[55.16586271742739, 99.5, 4.301030158996582, ...  \n",
       "348615 10102046.687  [[64.4870623097533, 77.4, 4.301030158996582, 0...  \n",
       "348622 10102050.087  [[64.4870623097533, 77.4, 4.301030158996582, 0...  \n",
       "379199 10102053.489  [[40.68706230975323, 53.6, 4.301030158996582, ...  \n",
       "379418 10102058.202  [[34.50169568948314, 51.0, 4.301030158996582, ...  \n",
       "399923 10102034.738  [[36.20169568948319, 52.7, 4.301030158996582, ...  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataframe[test_dataframe.cusip == '64987DJC9'][['rtrs_control_number', 'trade_datetime', 'last_trade_datetime', 'cusip','new_ys', 'last_yield', 'new_real_time_ficc_ycl', 'prediction', 'error', trade_history_col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a9d4a-edf2-4926-9f9b-c9e374c7d08e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cusip</th>\n",
       "      <th>new_ys</th>\n",
       "      <th>last_yield</th>\n",
       "      <th>prediction</th>\n",
       "      <th>error</th>\n",
       "      <th>coupon</th>\n",
       "      <th>days_to_maturity</th>\n",
       "      <th>default_indicator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>545293</th>\n",
       "      <td>236887AW5</td>\n",
       "      <td>852.580</td>\n",
       "      <td>421.700</td>\n",
       "      <td>267.265</td>\n",
       "      <td>585.315</td>\n",
       "      <td>4.000</td>\n",
       "      <td>3.184</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545291</th>\n",
       "      <td>236887AW5</td>\n",
       "      <td>852.380</td>\n",
       "      <td>421.700</td>\n",
       "      <td>262.808</td>\n",
       "      <td>589.572</td>\n",
       "      <td>4.000</td>\n",
       "      <td>3.184</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375668</th>\n",
       "      <td>132047BX8</td>\n",
       "      <td>1502.105</td>\n",
       "      <td>1440.200</td>\n",
       "      <td>886.736</td>\n",
       "      <td>615.369</td>\n",
       "      <td>4.500</td>\n",
       "      <td>3.476</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375651</th>\n",
       "      <td>132047BX8</td>\n",
       "      <td>1512.405</td>\n",
       "      <td>1440.200</td>\n",
       "      <td>886.736</td>\n",
       "      <td>625.669</td>\n",
       "      <td>4.500</td>\n",
       "      <td>3.476</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375653</th>\n",
       "      <td>132047BX8</td>\n",
       "      <td>1512.405</td>\n",
       "      <td>1440.200</td>\n",
       "      <td>886.736</td>\n",
       "      <td>625.669</td>\n",
       "      <td>4.500</td>\n",
       "      <td>3.476</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375650</th>\n",
       "      <td>132047BX8</td>\n",
       "      <td>1516.505</td>\n",
       "      <td>1440.200</td>\n",
       "      <td>877.017</td>\n",
       "      <td>639.487</td>\n",
       "      <td>4.500</td>\n",
       "      <td>3.476</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497919</th>\n",
       "      <td>132047BX8</td>\n",
       "      <td>1604.270</td>\n",
       "      <td>1806.000</td>\n",
       "      <td>888.239</td>\n",
       "      <td>716.031</td>\n",
       "      <td>4.500</td>\n",
       "      <td>3.476</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579588</th>\n",
       "      <td>227235AT5</td>\n",
       "      <td>1003.795</td>\n",
       "      <td>400.000</td>\n",
       "      <td>198.692</td>\n",
       "      <td>805.103</td>\n",
       "      <td>3.600</td>\n",
       "      <td>3.137</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579589</th>\n",
       "      <td>227235AT5</td>\n",
       "      <td>999.395</td>\n",
       "      <td>400.000</td>\n",
       "      <td>181.047</td>\n",
       "      <td>818.347</td>\n",
       "      <td>3.600</td>\n",
       "      <td>3.137</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121726</th>\n",
       "      <td>261333ET1</td>\n",
       "      <td>432.622</td>\n",
       "      <td>702.400</td>\n",
       "      <td>1350.019</td>\n",
       "      <td>-917.397</td>\n",
       "      <td>5.000</td>\n",
       "      <td>3.036</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121767</th>\n",
       "      <td>261333ET1</td>\n",
       "      <td>432.622</td>\n",
       "      <td>702.400</td>\n",
       "      <td>1354.186</td>\n",
       "      <td>-921.564</td>\n",
       "      <td>5.000</td>\n",
       "      <td>3.036</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121727</th>\n",
       "      <td>261333ET1</td>\n",
       "      <td>428.722</td>\n",
       "      <td>702.400</td>\n",
       "      <td>1354.186</td>\n",
       "      <td>-925.464</td>\n",
       "      <td>5.000</td>\n",
       "      <td>3.036</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121725</th>\n",
       "      <td>261333ET1</td>\n",
       "      <td>423.722</td>\n",
       "      <td>702.400</td>\n",
       "      <td>1354.186</td>\n",
       "      <td>-930.464</td>\n",
       "      <td>5.000</td>\n",
       "      <td>3.036</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418193</th>\n",
       "      <td>64987DHP2</td>\n",
       "      <td>4.025</td>\n",
       "      <td>360.000</td>\n",
       "      <td>-10102006.000</td>\n",
       "      <td>10102010.025</td>\n",
       "      <td>2.800</td>\n",
       "      <td>3.007</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399923</th>\n",
       "      <td>64987DJC9</td>\n",
       "      <td>17.738</td>\n",
       "      <td>436.700</td>\n",
       "      <td>-10102017.000</td>\n",
       "      <td>10102034.738</td>\n",
       "      <td>3.875</td>\n",
       "      <td>3.965</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348615</th>\n",
       "      <td>64987DJC9</td>\n",
       "      <td>40.687</td>\n",
       "      <td>468.400</td>\n",
       "      <td>-10102006.000</td>\n",
       "      <td>10102046.687</td>\n",
       "      <td>3.875</td>\n",
       "      <td>3.966</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348622</th>\n",
       "      <td>64987DJC9</td>\n",
       "      <td>40.087</td>\n",
       "      <td>468.400</td>\n",
       "      <td>-10102010.000</td>\n",
       "      <td>10102050.087</td>\n",
       "      <td>3.875</td>\n",
       "      <td>3.966</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379199</th>\n",
       "      <td>64987DJC9</td>\n",
       "      <td>34.489</td>\n",
       "      <td>444.600</td>\n",
       "      <td>-10102019.000</td>\n",
       "      <td>10102053.489</td>\n",
       "      <td>3.875</td>\n",
       "      <td>3.965</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460564</th>\n",
       "      <td>64987DHZ0</td>\n",
       "      <td>42.662</td>\n",
       "      <td>372.700</td>\n",
       "      <td>-10102015.000</td>\n",
       "      <td>10102057.662</td>\n",
       "      <td>3.450</td>\n",
       "      <td>3.574</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379418</th>\n",
       "      <td>64987DJC9</td>\n",
       "      <td>36.202</td>\n",
       "      <td>435.000</td>\n",
       "      <td>-10102022.000</td>\n",
       "      <td>10102058.202</td>\n",
       "      <td>3.875</td>\n",
       "      <td>3.965</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376391</th>\n",
       "      <td>64987DHP2</td>\n",
       "      <td>65.378</td>\n",
       "      <td>393.000</td>\n",
       "      <td>-10102001.000</td>\n",
       "      <td>10102066.378</td>\n",
       "      <td>2.800</td>\n",
       "      <td>3.007</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460576</th>\n",
       "      <td>64987DHZ0</td>\n",
       "      <td>55.262</td>\n",
       "      <td>372.700</td>\n",
       "      <td>-10102013.000</td>\n",
       "      <td>10102068.262</td>\n",
       "      <td>3.450</td>\n",
       "      <td>3.574</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348245</th>\n",
       "      <td>64987DJC9</td>\n",
       "      <td>61.706</td>\n",
       "      <td>480.500</td>\n",
       "      <td>-10102020.000</td>\n",
       "      <td>10102081.706</td>\n",
       "      <td>3.875</td>\n",
       "      <td>3.966</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348247</th>\n",
       "      <td>64987DJC9</td>\n",
       "      <td>66.606</td>\n",
       "      <td>480.500</td>\n",
       "      <td>-10102021.000</td>\n",
       "      <td>10102087.606</td>\n",
       "      <td>3.875</td>\n",
       "      <td>3.966</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207763</th>\n",
       "      <td>64987DHP2</td>\n",
       "      <td>95.700</td>\n",
       "      <td>110.800</td>\n",
       "      <td>-10102008.000</td>\n",
       "      <td>10102103.700</td>\n",
       "      <td>2.800</td>\n",
       "      <td>3.010</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            cusip   new_ys  last_yield    prediction        error  coupon  \\\n",
       "545293  236887AW5  852.580     421.700       267.265      585.315   4.000   \n",
       "545291  236887AW5  852.380     421.700       262.808      589.572   4.000   \n",
       "375668  132047BX8 1502.105    1440.200       886.736      615.369   4.500   \n",
       "375651  132047BX8 1512.405    1440.200       886.736      625.669   4.500   \n",
       "375653  132047BX8 1512.405    1440.200       886.736      625.669   4.500   \n",
       "375650  132047BX8 1516.505    1440.200       877.017      639.487   4.500   \n",
       "497919  132047BX8 1604.270    1806.000       888.239      716.031   4.500   \n",
       "579588  227235AT5 1003.795     400.000       198.692      805.103   3.600   \n",
       "579589  227235AT5  999.395     400.000       181.047      818.347   3.600   \n",
       "121726  261333ET1  432.622     702.400      1350.019     -917.397   5.000   \n",
       "121767  261333ET1  432.622     702.400      1354.186     -921.564   5.000   \n",
       "121727  261333ET1  428.722     702.400      1354.186     -925.464   5.000   \n",
       "121725  261333ET1  423.722     702.400      1354.186     -930.464   5.000   \n",
       "418193  64987DHP2    4.025     360.000 -10102006.000 10102010.025   2.800   \n",
       "399923  64987DJC9   17.738     436.700 -10102017.000 10102034.738   3.875   \n",
       "348615  64987DJC9   40.687     468.400 -10102006.000 10102046.687   3.875   \n",
       "348622  64987DJC9   40.087     468.400 -10102010.000 10102050.087   3.875   \n",
       "379199  64987DJC9   34.489     444.600 -10102019.000 10102053.489   3.875   \n",
       "460564  64987DHZ0   42.662     372.700 -10102015.000 10102057.662   3.450   \n",
       "379418  64987DJC9   36.202     435.000 -10102022.000 10102058.202   3.875   \n",
       "376391  64987DHP2   65.378     393.000 -10102001.000 10102066.378   2.800   \n",
       "460576  64987DHZ0   55.262     372.700 -10102013.000 10102068.262   3.450   \n",
       "348245  64987DJC9   61.706     480.500 -10102020.000 10102081.706   3.875   \n",
       "348247  64987DJC9   66.606     480.500 -10102021.000 10102087.606   3.875   \n",
       "207763  64987DHP2   95.700     110.800 -10102008.000 10102103.700   2.800   \n",
       "\n",
       "        days_to_maturity  default_indicator  \n",
       "545293             3.184              False  \n",
       "545291             3.184              False  \n",
       "375668             3.476              False  \n",
       "375651             3.476              False  \n",
       "375653             3.476              False  \n",
       "375650             3.476              False  \n",
       "497919             3.476              False  \n",
       "579588             3.137              False  \n",
       "579589             3.137              False  \n",
       "121726             3.036              False  \n",
       "121767             3.036              False  \n",
       "121727             3.036              False  \n",
       "121725             3.036              False  \n",
       "418193             3.007              False  \n",
       "399923             3.965              False  \n",
       "348615             3.966              False  \n",
       "348622             3.966              False  \n",
       "379199             3.965              False  \n",
       "460564             3.574              False  \n",
       "379418             3.965              False  \n",
       "376391             3.007              False  \n",
       "460576             3.574              False  \n",
       "348245             3.966              False  \n",
       "348247             3.966              False  \n",
       "207763             3.010              False  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataframe[['cusip','new_ys', 'last_yield', 'prediction', 'error', 'coupon', 'days_to_maturity', 'default_indicator']].sort_values(by='error', key=abs).tail(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0436a1de-13f1-47d2-b3ec-630fd1a4464b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "problem_cusips = ['64987DHP2', '64987DHZ0', '64987DJC9']\n",
    "problem_cusips_df = test_dataframe[test_dataframe.cusip.isin(problem_cusips)]\n",
    "same_series = test_dataframe[~test_dataframe.cusip.isin(problem_cusips)][test_dataframe.cusip.str[:6] == '64987D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd6b0ef-e55d-404c-89b2-3b9b53ebabcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_cat_comp = pd.concat([same_series[NON_CAT_FEATURES].describe().loc[['mean']],\n",
    "          problem_cusips_df[NON_CAT_FEATURES].describe().loc[['mean']]]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "0905f995-0e93-4e10-aae1-03f70cc883c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quantity</th>\n",
       "      <th>days_to_maturity</th>\n",
       "      <th>days_to_call</th>\n",
       "      <th>coupon</th>\n",
       "      <th>issue_amount</th>\n",
       "      <th>last_seconds_ago</th>\n",
       "      <th>last_yield_spread</th>\n",
       "      <th>days_to_settle</th>\n",
       "      <th>days_to_par</th>\n",
       "      <th>maturity_amount</th>\n",
       "      <th>issue_price</th>\n",
       "      <th>orig_principal_amount</th>\n",
       "      <th>max_amount_outstanding</th>\n",
       "      <th>accrued_days</th>\n",
       "      <th>days_in_interest_payment</th>\n",
       "      <th>A/E</th>\n",
       "      <th>max_ys_ys</th>\n",
       "      <th>max_ys_ago</th>\n",
       "      <th>max_ys_qdiff</th>\n",
       "      <th>min_ys_ys</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.598</td>\n",
       "      <td>3.443</td>\n",
       "      <td>1.688</td>\n",
       "      <td>2.525</td>\n",
       "      <td>8.202</td>\n",
       "      <td>2188757.143</td>\n",
       "      <td>84.966</td>\n",
       "      <td>2.950</td>\n",
       "      <td>1.688</td>\n",
       "      <td>6.744</td>\n",
       "      <td>100.000</td>\n",
       "      <td>6.744</td>\n",
       "      <td>6.744</td>\n",
       "      <td>1430.283</td>\n",
       "      <td>180.000</td>\n",
       "      <td>0.442</td>\n",
       "      <td>99.190</td>\n",
       "      <td>5.111</td>\n",
       "      <td>0.671</td>\n",
       "      <td>69.592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.766</td>\n",
       "      <td>4.302</td>\n",
       "      <td>3.444</td>\n",
       "      <td>4.215</td>\n",
       "      <td>7.458</td>\n",
       "      <td>12060410.105</td>\n",
       "      <td>46.040</td>\n",
       "      <td>2.903</td>\n",
       "      <td>3.444</td>\n",
       "      <td>6.178</td>\n",
       "      <td>76.500</td>\n",
       "      <td>6.178</td>\n",
       "      <td>6.178</td>\n",
       "      <td>1420.497</td>\n",
       "      <td>136.500</td>\n",
       "      <td>1.837</td>\n",
       "      <td>57.856</td>\n",
       "      <td>6.131</td>\n",
       "      <td>2.852</td>\n",
       "      <td>38.996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      quantity  days_to_maturity  days_to_call  coupon  issue_amount  \\\n",
       "mean     4.598             3.443         1.688   2.525         8.202   \n",
       "mean     4.766             4.302         3.444   4.215         7.458   \n",
       "\n",
       "      last_seconds_ago  last_yield_spread  days_to_settle  days_to_par  \\\n",
       "mean       2188757.143             84.966           2.950        1.688   \n",
       "mean      12060410.105             46.040           2.903        3.444   \n",
       "\n",
       "      maturity_amount  issue_price  orig_principal_amount  \\\n",
       "mean            6.744      100.000                  6.744   \n",
       "mean            6.178       76.500                  6.178   \n",
       "\n",
       "      max_amount_outstanding  accrued_days  days_in_interest_payment   A/E  \\\n",
       "mean                   6.744      1430.283                   180.000 0.442   \n",
       "mean                   6.178      1420.497                   136.500 1.837   \n",
       "\n",
       "      max_ys_ys  max_ys_ago  max_ys_qdiff  min_ys_ys  \n",
       "mean     99.190       5.111         0.671     69.592  \n",
       "mean     57.856       6.131         2.852     38.996  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_ys_ago</th>\n",
       "      <th>min_ys_qdiff</th>\n",
       "      <th>max_qty_ys</th>\n",
       "      <th>max_qty_ago</th>\n",
       "      <th>max_qty_qdiff</th>\n",
       "      <th>min_ago_ys</th>\n",
       "      <th>min_ago_ago</th>\n",
       "      <th>min_ago_qdiff</th>\n",
       "      <th>D_min_ago_ys</th>\n",
       "      <th>D_min_ago_ago</th>\n",
       "      <th>D_min_ago_qdiff</th>\n",
       "      <th>P_min_ago_ys</th>\n",
       "      <th>P_min_ago_ago</th>\n",
       "      <th>P_min_ago_qdiff</th>\n",
       "      <th>S_min_ago_ys</th>\n",
       "      <th>S_min_ago_ago</th>\n",
       "      <th>S_min_ago_qdiff</th>\n",
       "      <th>ficc_treasury_spread</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.393</td>\n",
       "      <td>0.372</td>\n",
       "      <td>82.086</td>\n",
       "      <td>5.080</td>\n",
       "      <td>2.254</td>\n",
       "      <td>84.966</td>\n",
       "      <td>4.402</td>\n",
       "      <td>0.649</td>\n",
       "      <td>85.703</td>\n",
       "      <td>4.616</td>\n",
       "      <td>0.530</td>\n",
       "      <td>99.213</td>\n",
       "      <td>5.462</td>\n",
       "      <td>1.965</td>\n",
       "      <td>74.073</td>\n",
       "      <td>5.391</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-90.305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.156</td>\n",
       "      <td>2.449</td>\n",
       "      <td>44.177</td>\n",
       "      <td>6.034</td>\n",
       "      <td>2.852</td>\n",
       "      <td>46.040</td>\n",
       "      <td>5.723</td>\n",
       "      <td>2.268</td>\n",
       "      <td>46.040</td>\n",
       "      <td>5.723</td>\n",
       "      <td>2.268</td>\n",
       "      <td>55.332</td>\n",
       "      <td>5.878</td>\n",
       "      <td>2.268</td>\n",
       "      <td>39.657</td>\n",
       "      <td>6.825</td>\n",
       "      <td>2.852</td>\n",
       "      <td>-19.571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      min_ys_ago  min_ys_qdiff  max_qty_ys  max_qty_ago  max_qty_qdiff  \\\n",
       "mean       5.393         0.372      82.086        5.080          2.254   \n",
       "mean       6.156         2.449      44.177        6.034          2.852   \n",
       "\n",
       "      min_ago_ys  min_ago_ago  min_ago_qdiff  D_min_ago_ys  D_min_ago_ago  \\\n",
       "mean      84.966        4.402          0.649        85.703          4.616   \n",
       "mean      46.040        5.723          2.268        46.040          5.723   \n",
       "\n",
       "      D_min_ago_qdiff  P_min_ago_ys  P_min_ago_ago  P_min_ago_qdiff  \\\n",
       "mean            0.530        99.213          5.462            1.965   \n",
       "mean            2.268        55.332          5.878            2.268   \n",
       "\n",
       "      S_min_ago_ys  S_min_ago_ago  S_min_ago_qdiff  ficc_treasury_spread  \n",
       "mean        74.073          5.391            0.173               -90.305  \n",
       "mean        39.657          6.825            2.852               -19.571  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(non_cat_comp.iloc[:, :20])\n",
    "\n",
    "display(non_cat_comp.iloc[:, 20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "3755c99d-88ee-45b3-90b7-7d857b615458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>callable</th>\n",
       "      <th>sinking</th>\n",
       "      <th>zerocoupon</th>\n",
       "      <th>is_non_transaction_based_compensation</th>\n",
       "      <th>is_general_obligation</th>\n",
       "      <th>callable_at_cav</th>\n",
       "      <th>extraordinary_make_whole_call</th>\n",
       "      <th>make_whole_call</th>\n",
       "      <th>has_unexpired_lines_of_credit</th>\n",
       "      <th>escrow_exists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    callable sinking zerocoupon is_non_transaction_based_compensation  \\\n",
       "top     True   False      False                                 False   \n",
       "top     True    True      False                                 False   \n",
       "\n",
       "    is_general_obligation callable_at_cav extraordinary_make_whole_call  \\\n",
       "top                 False           False                         False   \n",
       "top                 False           False                         False   \n",
       "\n",
       "    make_whole_call has_unexpired_lines_of_credit escrow_exists  \n",
       "top           False                         False         False  \n",
       "top           False                          True         False  "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([same_series[BINARY].describe().loc[['top']],\n",
    "          problem_cusips_df[BINARY].describe().loc[['top']]]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "18a7a0fe-bdd8-4bc3-94f7-b942a5065c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>incorporated_state_code</th>\n",
       "      <th>trade_type</th>\n",
       "      <th>purpose_class</th>\n",
       "      <th>max_ys_ttypes</th>\n",
       "      <th>min_ys_ttypes</th>\n",
       "      <th>max_qty_ttypes</th>\n",
       "      <th>min_ago_ttypes</th>\n",
       "      <th>D_min_ago_ttypes</th>\n",
       "      <th>P_min_ago_ttypes</th>\n",
       "      <th>S_min_ago_ttypes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>MR</td>\n",
       "      <td>NY</td>\n",
       "      <td>D</td>\n",
       "      <td>18.000</td>\n",
       "      <td>DD</td>\n",
       "      <td>SD</td>\n",
       "      <td>DD</td>\n",
       "      <td>DD</td>\n",
       "      <td>DD</td>\n",
       "      <td>PD</td>\n",
       "      <td>SD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>MR</td>\n",
       "      <td>NY</td>\n",
       "      <td>D</td>\n",
       "      <td>18.000</td>\n",
       "      <td>PD</td>\n",
       "      <td>SD</td>\n",
       "      <td>DD</td>\n",
       "      <td>DD</td>\n",
       "      <td>DD</td>\n",
       "      <td>PD</td>\n",
       "      <td>SD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rating incorporated_state_code trade_type  purpose_class max_ys_ttypes  \\\n",
       "top     MR                      NY          D         18.000            DD   \n",
       "top     MR                      NY          D         18.000            PD   \n",
       "\n",
       "    min_ys_ttypes max_qty_ttypes min_ago_ttypes D_min_ago_ttypes  \\\n",
       "top            SD             DD             DD               DD   \n",
       "top            SD             DD             DD               DD   \n",
       "\n",
       "    P_min_ago_ttypes S_min_ago_ttypes  \n",
       "top               PD               SD  \n",
       "top               PD               SD  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([same_series[CATEGORICAL_FEATURES].describe().loc[['top']],\n",
    "          problem_cusips_df[CATEGORICAL_FEATURES].describe().loc[['top']]]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "9ac82dca-d6c5-41bc-9b0b-1cf4956cc7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothetical_df = problem_cusips_df.copy()\n",
    "hypothetical_df['sinking'] = False\n",
    "hypothetical_df['has_unexpired_lines_of_credit'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "5a654ece-0fb6-4313-897e-2e592c68291d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10102059.711738383"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(model.predict(create_input_new(hypothetical_df, trade_history_col, yield_history_cols)),\n",
    "                    hypothetical_df.new_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "37779c56-dfde-4e2f-9160-9e611b1820ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10102060.128405051"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(model.predict(create_input_new(problem_cusips_df, trade_history_col, yield_history_cols)),\n",
    "                    hypothetical_df.new_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "5fce3b46-68a4-447f-bfed-29a4b295222b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rtrs_control_number</th>\n",
       "      <th>cusip</th>\n",
       "      <th>yield</th>\n",
       "      <th>is_callable</th>\n",
       "      <th>refund_date</th>\n",
       "      <th>accrual_date</th>\n",
       "      <th>dated_date</th>\n",
       "      <th>next_sink_date</th>\n",
       "      <th>coupon</th>\n",
       "      <th>delivery_date</th>\n",
       "      <th>...</th>\n",
       "      <th>S_min_ago_qdiff</th>\n",
       "      <th>ttypes</th>\n",
       "      <th>diff_size</th>\n",
       "      <th>abs_last_yield_spread</th>\n",
       "      <th>abs_diff_size</th>\n",
       "      <th>days_duration</th>\n",
       "      <th>trade_history_sum</th>\n",
       "      <th>moodys_rating</th>\n",
       "      <th>ratings_concat</th>\n",
       "      <th>trade_history_shortened</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows  151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [rtrs_control_number, cusip, yield, is_callable, refund_date, accrual_date, dated_date, next_sink_date, coupon, delivery_date, trade_date, trade_datetime, par_call_date, interest_payment_frequency, is_called, is_non_transaction_based_compensation, is_general_obligation, callable_at_cav, extraordinary_make_whole_call, make_whole_call, has_unexpired_lines_of_credit, escrow_exists, incorporated_state_code, trade_type, par_traded, maturity_date, settlement_date, next_call_date, issue_amount, maturity_amount, issue_price, orig_principal_amount, max_amount_outstanding, dollar_price, calc_date, purpose_sub_class, called_redemption_type, calc_day_cat, previous_coupon_payment_date, instrument_primary_name, purpose_class, call_timing, call_timing_in_part, sink_frequency, sink_amount_type, issue_text, state_tax_status, series_name, transaction_type, next_call_price, par_call_price, when_issued, min_amount_outstanding, original_yield, par_price, default_indicator, sp_stand_alone, sp_long, moodys_long, coupon_type, federal_tax_status, use_of_proceeds, muni_security_type, muni_issue_type, capital_type, other_enhancement_type, next_coupon_payment_date, first_coupon_date, last_period_accrues_from_date, rating, trade_history, last_yield_spread, last_rtrs_control_number, last_yield, last_dollar_price, last_seconds_ago, last_size, last_calc_date, last_maturity_date, last_next_call_date, last_par_call_date, last_refund_date, last_trade_datetime, last_calc_day_cat, last_settlement_date, last_trade_type, treasury_rate, ficc_treasury_spread, quantity, callable, called, zerocoupon, whenissued, sinking, deferred, days_to_settle, days_to_maturity, days_to_call, days_to_refund, days_to_par, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 151 columns]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataframe[train_dataframe.has_unexpired_lines_of_credit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "9af36e4e-40cf-40aa-b686-08acd6420445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cusip\n",
       "64987D2Y9                      [-24.0, 21.0, 1.0]\n",
       "64987D3V4                      [12.0, 21.0, 15.0]\n",
       "64987D5T7                    [-19.0, -4.0, -12.0]\n",
       "64987D5W0                   [-41.0, -41.0, -41.0]\n",
       "64987D5X8                    [-86.0, 20.0, -23.0]\n",
       "64987D5Y6                   [-20.0, -20.0, -20.0]\n",
       "64987D6B5                      [-13.0, 9.0, -0.0]\n",
       "64987D6D1                       [-2.0, 26.0, 4.0]\n",
       "64987D6G4                      [24.0, 24.0, 24.0]\n",
       "64987DAW4                     [-18.0, 10.0, -0.0]\n",
       "64987DEC4                       [-6.0, 24.0, 2.0]\n",
       "64987DGL2                         [2.0, 2.0, 2.0]\n",
       "64987DGY4                      [-17.0, 31.0, 5.0]\n",
       "64987DHP2    [10102010.0, 10102104.0, 10102060.0]\n",
       "64987DHZ0    [10102058.0, 10102068.0, 10102063.0]\n",
       "64987DJC9    [10102035.0, 10102088.0, 10102059.0]\n",
       "64987DJP0                        [-0.0, 5.0, 2.0]\n",
       "64987DJW5                      [-7.0, -5.0, -7.0]\n",
       "64987DK45                      [-0.0, 28.0, 11.0]\n",
       "64987DKC7                      [17.0, 19.0, 18.0]\n",
       "64987DL51                       [-1.0, 14.0, 7.0]\n",
       "64987DL69                       [5.0, 14.0, 10.0]\n",
       "64987DL77                      [-9.0, -1.0, -3.0]\n",
       "64987DL85                      [21.0, 26.0, 24.0]\n",
       "64987DM27                      [-9.0, 29.0, 16.0]\n",
       "64987DMU5                        [-2.0, 4.0, 1.0]\n",
       "64987DMX9                        [-5.0, 5.0, 1.0]\n",
       "64987DPG3                       [-4.0, 3.0, -1.0]\n",
       "64987DQ56                       [-3.0, 13.0, 7.0]\n",
       "64987DRG1                        [-5.0, 3.0, 1.0]\n",
       "64987DRH9                      [-24.0, 7.0, -7.0]\n",
       "64987DRJ5                        [-8.0, 9.0, 1.0]\n",
       "64987DRK2                   [-15.0, -12.0, -14.0]\n",
       "64987DRL0                      [-5.0, -2.0, -4.0]\n",
       "64987DSU9                      [-0.0, 26.0, 11.0]\n",
       "64987DSV7                      [-3.0, -1.0, -2.0]\n",
       "64987DSX3                     [-28.0, 46.0, -1.0]\n",
       "64987DUZ5                       [7.0, 48.0, 24.0]\n",
       "64987DVC5                    [-22.0, -8.0, -15.0]\n",
       "64987DVD3                      [-9.0, 46.0, 12.0]\n",
       "64987DVP6                       [-10.0, 8.0, 0.0]\n",
       "64987DWJ9                      [29.0, 41.0, 35.0]\n",
       "64987DWM2                        [1.0, 17.0, 9.0]\n",
       "64987DXC3                     [-13.0, 50.0, 14.0]\n",
       "64987DXL3                         [1.0, 8.0, 4.0]\n",
       "Name: error, dtype: object"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataframe[test_dataframe.cusip.str[:6] == '64987D'][['cusip','new_ys', 'last_yield', 'prediction', 'error', 'coupon', 'days_to_maturity', 'default_indicator']].sort_values(by='error', key=abs)\\\n",
    ".groupby('cusip')['error']\\\n",
    ".apply(lambda x: (min(x), max(x), x.mean()))\\\n",
    ".apply(np.round, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94297422-6a5f-486b-b517-adac45f2b43f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2817b575-ac99-4ef1-82f8-796eaf239bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1: 8.114\n",
      "Trial 2: 8.234\n",
      "Trial 3: 8.186\n",
      "Trial 4: 8.220\n",
      "Average MAE: 8.188\n",
      "Epochs [98, 125, 125, 125]\n"
     ]
    }
   ],
   "source": [
    "#CURRENT TRIAl \n",
    "\n",
    "#75 EPOCHS \n",
    "#no RESIDUAL \n",
    "#DROPOUT 0.1\n",
    "#GLU for ref model, dropout after GLU\n",
    "\n",
    "t = 0\n",
    "for i, prediction in enumerate(predictions):\n",
    "    print(f'Trial {i+1}: {mean_absolute_error(prediction, y_test):.3f}')\n",
    "    t += mean_absolute_error(prediction, y_test)\n",
    "print(f'Average MAE: {t/len(predictions):.3f}')\n",
    "print(f'Epochs {[len(hist.history[\"loss\"]) for hist, _ in results]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f20c3de-efb3-4bb7-a57b-4e868efa592c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1: 8.489\n",
      "Trial 2: 8.276\n",
      "Trial 3: 8.338\n",
      "Trial 4: 8.403\n",
      "Average MAE: 8.376\n",
      "Epochs [75, 67, 75, 75]\n"
     ]
    }
   ],
   "source": [
    "#CURRENT TRIAl \n",
    "\n",
    "#75 EPOCHS \n",
    "#no RESIDUAL \n",
    "#DROPOUT 0.1\n",
    "#GLU for ref model, dropout after GLU\n",
    "\n",
    "t = 0\n",
    "for i, prediction in enumerate(predictions):\n",
    "    print(f'Trial {i+1}: {mean_absolute_error(prediction, y_test):.3f}')\n",
    "    t += mean_absolute_error(prediction, y_test)\n",
    "print(f'Average MAE: {t/len(predictions):.3f}')\n",
    "print(f'Epochs {[len(hist.history[\"loss\"]) for hist, _ in results]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "429d17c9-ed62-42c3-ae2c-5a6a928cedba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1: 8.301\n",
      "Trial 2: 8.174\n",
      "Average MAE: 8.237\n",
      "Epochs [68, 46]\n"
     ]
    }
   ],
   "source": [
    "#CURRENT TRIAl \n",
    "\n",
    "#75 EPOCHS \n",
    "#no RESIDUAL \n",
    "#DROPOUT 0.1\n",
    "#GLU for both ref and hist model, no residual\n",
    "\n",
    "t = 0\n",
    "for i, prediction in enumerate(predictions):\n",
    "    print(f'Trial {i+1}: {mean_absolute_error(prediction, y_test):.3f}')\n",
    "    t += mean_absolute_error(prediction, y_test)\n",
    "print(f'Average MAE: {t/len(predictions):.3f}')\n",
    "print(f'Epochs {[len(hist.history[\"loss\"]) for hist, _ in results]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5fa8db38-62a2-426c-a3f6-d9e4f5caa0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1: 8.346\n",
      "Trial 2: 8.198\n",
      "Trial 3: 8.327\n",
      "Trial 4: 8.215\n",
      "Average MAE: 8.271\n",
      "Epochs [75, 70, 75, 71]\n"
     ]
    }
   ],
   "source": [
    "#CURRENT TRIAl \n",
    "\n",
    "#75 EPOCHS \n",
    "#no RESIDUAL \n",
    "#DROPOUT 0.1\n",
    "#GLU for both ref and hist model, no residual\n",
    "\n",
    "t = 0\n",
    "for i, prediction in enumerate(predictions):\n",
    "    print(f'Trial {i+1}: {mean_absolute_error(prediction, y_test):.3f}')\n",
    "    t += mean_absolute_error(prediction, y_test)\n",
    "print(f'Average MAE: {t/len(predictions):.3f}')\n",
    "print(f'Epochs {[len(hist.history[\"loss\"]) for hist, _ in results]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00a3468d-4862-46de-8b9b-64579dccf59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1: 8.159\n",
      "Trial 2: 8.135\n",
      "Trial 3: 8.235\n",
      "Trial 4: 8.279\n",
      "Average MAE: 8.202\n",
      "Epochs [49, 49, 97, 125]\n"
     ]
    }
   ],
   "source": [
    "#CURRENT TRIAl \n",
    "\n",
    "#75 EPOCHS \n",
    "#no RESIDUAL \n",
    "#DROPOUT 0.1\n",
    "#GLU for ref model only with residuals ADDED\n",
    "\n",
    "t = 0\n",
    "for i, prediction in enumerate(predictions):\n",
    "    print(f'Trial {i+1}: {mean_absolute_error(prediction, y_test):.3f}')\n",
    "    t += mean_absolute_error(prediction, y_test)\n",
    "print(f'Average MAE: {t/len(predictions):.3f}')\n",
    "print(f'Epochs {[len(hist.history[\"loss\"]) for hist, _ in results]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1088,
   "id": "7fd8b23d-c2e9-4ecb-a135-8077f9d74122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1: 8.048\n",
      "Trial 2: 8.082\n",
      "Trial 3: 8.067\n",
      "Trial 4: 8.061\n",
      "Average MAE: 8.065\n",
      "Epochs [70, 74, 68, 86]\n"
     ]
    }
   ],
   "source": [
    "#CURRENT TRIAl \n",
    "\n",
    "#75 EPOCHS \n",
    "#no RESIDUAL \n",
    "#DROPOUT 0.1\n",
    "#GLU for ref model only with no residuals\n",
    "\n",
    "t = 0\n",
    "for i, prediction in enumerate(predictions):\n",
    "    print(f'Trial {i+1}: {mean_absolute_error(prediction, y_test):.3f}')\n",
    "    t += mean_absolute_error(prediction, y_test)\n",
    "print(f'Average MAE: {t/len(predictions):.3f}')\n",
    "print(f'Epochs {[len(hist.history[\"loss\"]) for hist, _ in results]}')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
