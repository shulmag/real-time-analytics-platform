{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c407d3f-44f9-44c7-b20c-aacf5d9dd5e0",
   "metadata": {},
   "source": [
    "TRADE HISTORY LENGTH 2, CORRECTLY ORDERED \n",
    "\n",
    "YIELD CURVE HISTORY 5MIN_12_AVERAGE , CORRECTLY ORDERED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58de4fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 20:34:31.701112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-18 20:34:31.714075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-18 20:34:31.715630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing pandarallel with 16.0 cores\n",
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "TF Version: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import seaborn as sns\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle5 as pickle\n",
    "\n",
    "\n",
    "from ficc.utils.nelson_siegel_model import *\n",
    "from ficc.utils.diff_in_days import *\n",
    "from ficc.utils.auxiliary_functions import sqltodf\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "\n",
    "\n",
    "from ficc.data.process_data import process_data\n",
    "from ficc.utils.auxiliary_variables import PREDICTORS, NON_CAT_FEATURES, BINARY, CATEGORICAL_FEATURES, IDENTIFIERS, PURPOSE_CLASS_DICT, NUM_OF_DAYS_IN_YEAR\n",
    "from ficc.utils.gcp_storage_functions import upload_data, download_data\n",
    "from ficc.utils.auxiliary_variables import RELATED_TRADE_BINARY_FEATURES, RELATED_TRADE_NON_CAT_FEATURES, RELATED_TRADE_CATEGORICAL_FEATURES\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from ficc_keras_utils import *\n",
    "import ficc_keras_utils\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "print(f'TF Version: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c11246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/home/jupyter/ficc/isaac_creds.json\"\n",
    "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "bq_client = bigquery.Client()\n",
    "storage_client = storage.Client()\n",
    "\n",
    "##COMMON VARIABLES\n",
    "#DATA WINDOW\n",
    "train_start = ficc_keras_utils.train_start\n",
    "train_end = ficc_keras_utils.train_end\n",
    "test_start = ficc_keras_utils.test_start\n",
    "test_end = ficc_keras_utils.test_end\n",
    "#MODEL PARAMETERS \n",
    "VALIDATION_SPLIT = ficc_keras_utils.VALIDATION_SPLIT\n",
    "LEARNING_RATE = ficc_keras_utils.LEARNING_RATE\n",
    "BATCH_SIZE = ficc_keras_utils.BATCH_SIZE\n",
    "NUM_EPOCHS = 150 #ficc_keras_utils.NUM_EPOCHS\n",
    "DROPOUT = ficc_keras_utils.DROPOUT\n",
    "\n",
    "##NOTEBOOK SPECIFIC VARIABLES \n",
    "TRADE_SEQUENCE_LENGTH = 2\n",
    "YIELD_SEQUENCE_LENGTH = 11\n",
    "NUM_FEATURES = 6\n",
    "# target_variable = 'new_ys_diff'\n",
    "target_variable = 'new_ys'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca18089-58e8-4a2e-9c50-11898bfde236",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_start ,\n",
    "train_end ,\n",
    "test_start ,\n",
    "test_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d7cee41-fd7e-48ae-bf29-9ae07e10b13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = pd.read_pickle('../../processed_data-2023-05-12.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc47f4e4-9a1a-4ffa-932c-5413827d0731",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_features = ['ttypes', 'diff_size', 'abs_last_yield_spread', 'abs_diff_size', 'days_duration']\n",
    "YS_COLS = ['max_ys_ys', 'max_ys_ttypes', 'max_ys_ago', 'max_ys_qdiff', 'min_ys_ys', 'min_ys_ttypes', 'min_ys_ago', 'min_ys_qdiff', 'max_qty_ys', 'max_qty_ttypes', \\\n",
    "           'max_qty_ago', 'max_qty_qdiff', 'min_ago_ys', 'min_ago_ttypes', 'min_ago_ago', 'min_ago_qdiff', 'D_min_ago_ys', 'D_min_ago_ttypes', 'D_min_ago_ago', 'D_min_ago_qdiff',\\\n",
    "           'P_min_ago_ys', 'P_min_ago_ttypes', 'P_min_ago_ago', 'P_min_ago_qdiff', 'S_min_ago_ys', 'S_min_ago_ttypes', 'S_min_ago_ago', 'S_min_ago_qdiff']\n",
    "encoders = {}\n",
    "fmax = {}\n",
    "for f in CATEGORICAL_FEATURES:\n",
    "    print(f)\n",
    "    if f in ['rating', 'incorporated_state_code', 'trade_type', 'purpose_class']:\n",
    "        fprep = preprocessing.LabelEncoder().fit(categorical_feature_values[f])\n",
    "    else:\n",
    "        fprep = preprocessing.LabelEncoder().fit(processed_data[f].drop_duplicates())\n",
    "    fmax[f] = np.max(fprep.transform(fprep.classes_))\n",
    "    encoders[f] = fprep\n",
    "    \n",
    "with open('encoders.pkl','wb') as file:\n",
    "    pickle.dump(encoders,file)\n",
    "    \n",
    "for col in YS_COLS:\n",
    "    if 'ttypes' in col and col not in PREDICTORS:\n",
    "        PREDICTORS.append(col)\n",
    "        CATEGORICAL_FEATURES.append(col)\n",
    "    elif col not in PREDICTORS:\n",
    "        NON_CAT_FEATURES.append(col)\n",
    "        PREDICTORS.append(col)\n",
    "        \n",
    "if 'target_attention_features' not in PREDICTORS:\n",
    "    PREDICTORS.append('target_attention_features')\n",
    "    \n",
    "if 'ficc_treasury_spread' not in PREDICTORS:\n",
    "    PREDICTORS.append('ficc_treasury_spread')\n",
    "    NON_CAT_FEATURES.append('ficc_treasury_spread')\n",
    "    \n",
    "for col in ['new_ficc_ycl', 'new_real_time_ficc_ycl']:     \n",
    "    if col not in PREDICTORS:\n",
    "        PREDICTORS.append(col)\n",
    "        NON_CAT_FEATURES.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8e9cb49-8e29-46da-baef-b009a27580d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29 s, sys: 486 ms, total: 29.5 s\n",
      "Wall time: 29.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "processed_data['yield_curve_history_5min_12_averaged_first_differenced'] = processed_data['yield_curve_history_5min_12_averaged']\\\n",
    ".apply(lambda x: -np.diff(x))\\\n",
    ".apply(lambda x: x[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b92bf3e9-bd78-4aa1-a031-892a653b3524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 s, sys: 498 ms, total: 11.5 s\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "processed_data['trade_history_shortened'] = processed_data['trade_history'].apply(lambda x: x[:TRADE_SEQUENCE_LENGTH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb0d6e5b-5e69-454e-9d53-348fc11ad2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.8 s, sys: 329 ms, total: 11.1 s\n",
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "processed_data['trade_history_fixed'] = processed_data['trade_history_shortened'].apply(lambda x: x[::-1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c8c7d1-9f2c-4d50-9a0c-08070f4cf0f5",
   "metadata": {},
   "source": [
    "Add Overnight Movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c61464b0-bbb5-4c04-a053-4d297e513b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ficc.yield_curve.YieldCurve import *\n",
    "from ficc.yield_curve.time_series_functions import *\n",
    "from ficc.yield_curve.auxiliary_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0966697-f6ec-4678-b69a-bad9eaf31ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting start date to 2022-12-25 00:00:00-05:00\n",
      "Setting end date to 2023-04-01 00:00:00-04:00\n",
      "Loading real time yield curve values from BigQuery\n",
      "Real time yield curve values loaded, data spans 2022-12-28 09:30:00 to 2023-03-31 15:59:00\n",
      "Estimating yield curves for maturity T = 0.1 to T = 30.0\n",
      "Estimating and saving yield curves for maturity T = 0.1 to T = 30.0\n"
     ]
    }
   ],
   "source": [
    "rtyc = RealTimeYieldCurve(start_date='2022-12-25', end_date='2023-04-01')\n",
    "rtyc.initialize_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d0c4ddf5-c964-4d46-9edd-bd70b4f3498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtyc_values = rtyc.yield_curves.copy()\n",
    "overnight_moves = between_day_overnight_move(rtyc_values)\n",
    "overnight_moves.index = pd.to_datetime(overnight_moves.index)\n",
    "overnight_moves_dict = overnight_moves[key_maturities].loc[train_start:].T.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9daf580",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filter = (processed_data.trade_date < train_end) & (processed_data.trade_date >= train_start)\n",
    "test_filter = (processed_data.trade_date >= test_start) & (processed_data.trade_date <test_end)\n",
    "                                                            \n",
    "train_dataframe = processed_data[train_filter]\\\n",
    ".sort_values(by='trade_date', ascending=True)\\\n",
    ".reset_index(drop=True)\n",
    "\n",
    "test_dataframe = processed_data[test_filter]\\\n",
    ".sort_values(by='trade_date', ascending=True)\\\n",
    ".reset_index(drop=True)\n",
    "\n",
    "print('Training data start: {}, end: {}'.format(train_dataframe.trade_date.min(),train_dataframe.trade_date.max()))\n",
    "print('Test data start: {}, end: {}'.format(test_dataframe.trade_date.min(),test_dataframe.trade_date.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "143612cc-ac30-4e90-b2cd-2a6fca0b3b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_overnight_cols(trade_dates):\n",
    "    default = dict(zip(range(6), np.zeros(6)))\n",
    "    return pd.DataFrame(trade_dates.apply(lambda x: overnight_moves_dict.get(x, default).values()).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c6750048-0a01-46f0-a34f-9c97c5352566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.4 s, sys: 49.6 ms, total: 10.4 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "overnight_cols_train = return_overnight_cols(train_dataframe.trade_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0fb6fe68-7bec-43ca-9c3c-993c6fc50a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.8 s, sys: 2.32 ms, total: 4.81 s\n",
      "Wall time: 4.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "overnight_cols_test = return_overnight_cols(test_dataframe.trade_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "81eb9f10-af88-4c63-97ed-615eba67fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "overnight_col_names = ['ficc_ycl_overnight_'+str(x) for x in key_maturities]\n",
    "train_dataframe[overnight_col_names] = overnight_cols_train\n",
    "test_dataframe[overnight_col_names] = overnight_cols_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "52349a9b-3c19-43bd-9a2f-d0d5e04b0682",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in overnight_col_names:\n",
    "    if col not in PREDICTORS:\n",
    "        PREDICTORS.append(col) \n",
    "    if col not in NON_CAT_FEATURES:\n",
    "        NON_CAT_FEATURES.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9306cd8-2d23-4d15-b7b8-7ac8b6c6d6b6",
   "metadata": {},
   "source": [
    "Using Charles'LightGBM Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd48a287-a27c-4aa8-978a-c462ce7429ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31095/4074403927.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCATEGORICAL_FEATURES\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mBINARY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_dataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtest_dataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "for col in CATEGORICAL_FEATURES + BINARY:\n",
    "    train_dataframe[col] = train_dataframe[col].astype('category')\n",
    "    test_dataframe[col] = test_dataframe[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135478fa-7ea7-4b10-8b24-cab25b887839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "import lightgbm\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "defaultdepth = 8\n",
    "defaultseed = 881\n",
    "defaultloss = 'mae' # 'huber' # 'fair'\n",
    "huberdelta = 30\n",
    "default_n_jobs = 4\n",
    "\n",
    "def gbmprep(df, only=[], plus=[], minus=[]):\n",
    "    if only != []: \n",
    "        USECOLS = only\n",
    "    else:\n",
    "        USECOLS = df.select_dtypes(include=['category','bool','number']).columns\n",
    "    USECOLS = ( set(USECOLS) - mkleakers(TARGETS) - TOOMANY | set(plus) ) - set(minus)\n",
    "    return df[ list(USECOLS) ]\n",
    "\n",
    "\n",
    "def myLGBM(seed = defaultseed, depth = defaultdepth, loss = defaultloss):\n",
    "    return LGBMRegressor(max_depth=depth, num_leaves=depth*10,  objective=loss, verbosity=-1, alpha = huberdelta,\n",
    "                         n_estimators=depth*30, subsample = 0.5, subsample_freq = 10, # linear_tree = True,\n",
    "                         random_state = seed, n_jobs = default_n_jobs, device_type = \"cpu\") \n",
    "\n",
    "def mkensemble(n = 4, seed = defaultseed, depth = defaultdepth, loss = defaultloss):\n",
    "    regressors = []\n",
    "    for j in range(0,n):\n",
    "        regressors = regressors + [( 'm'+str(j), myLGBM(seed+j, depth, loss) )]\n",
    "    return VotingRegressor( regressors, n_jobs = default_n_jobs, verbose = False )\n",
    "\n",
    "# def trtestrows(df,target):\n",
    "#     global first_train_date, first_test_date\n",
    "#     train_rows = (df.trade_date >= first_train_date) & (df.trade_date < first_test_date) & df[target].notnull()\n",
    "#     test_rows = (df.trade_date >= first_test_date) & df[target].notnull()\n",
    "#     return train_rows, test_rows\n",
    "    \n",
    "def ess(weights):\n",
    "    return weights.sum()**2 / (weights**2).sum()\n",
    "\n",
    "def traintest(train_dataframe, test_dataframe, target, n = 4, seed = defaultseed, depth = defaultdepth, loss = defaultloss, sample_weight=None,\n",
    "              only=[], plus=[], minus=[], showplot=5, evaltrain=False):\n",
    "    global TARGETS\n",
    "    TARGETS = TARGETS | { target }\n",
    "    \n",
    "    # train_rows, test_rows = trtestrows(df,target)\n",
    "\n",
    "    ens = mkensemble(n, seed, depth, loss)\n",
    "    if n == 1:\n",
    "        text = \"Training one model \"\n",
    "    else:\n",
    "        text = f\"Training {n} models\" \n",
    "    print(text + f\"with {len(mydf.columns)} columns on {np.sum(train_rows)} examples starting {first_train_date:%Y-%m-%d}\")\n",
    "    if sample_weight is not None:\n",
    "        print( f\"Weighted samples with effective sample size = {ess(sample_weight):8.0f}\" )\n",
    "    print(f\"Evaluating {target} predictions on {np.sum(test_rows)} examples starting {first_test_date:%Y-%m-%d}\")\n",
    "    ens.fit( train_dataframe[PREDICTORS], train_dataframe[target], sample_weight )\n",
    "    uss()\n",
    "\n",
    "    if evaltrain:\n",
    "        maeval(train_dataframe, ens, target, only, plus, minus)\n",
    "    maeval(test_dataframe, ens, target, only, plus, minus)\n",
    "    drawpoints(df, test_rows, target, showplot)     \n",
    "    return ens\n",
    "\n",
    "def extend(df, name, vals):\n",
    "    # assert len(df) == len(rows), \"*** len(df) != len(rows)\"\n",
    "    # if len(vals) == len(df): \n",
    "    #     vals = vals[rows]\n",
    "    # assert len(vals) == np.sum(rows), \"*** len(vals) != np.sum(rows)\"\n",
    "    \n",
    "    if not name in df.columns: df[name] = 0.0\n",
    "    df.loc[:, name] = vals\n",
    "    return\n",
    "\n",
    "def maeval(df, model, label, only=[], plus=[], minus=[]):\n",
    "    dfp = model.predict(df[PREDICTORS])\n",
    "    extend(df, label + \"_preds\",  dfp )\n",
    "\n",
    "    delta = df[label] - dfp\n",
    "    extend(df, label + \"_err\", delta )\n",
    "    \n",
    "    da = delta.abs()\n",
    "    extend(df, label + \"_ae\", da )\n",
    "\n",
    "    n = len(da)\n",
    "    base = f\"\\n{label} n = {n}  bias = {delta.mean():5.2f}  MAE={da.mean():5.2f} +/- {da.std():.2f}\"\n",
    "    print( base + f\" ({da.std()/np.sqrt(n):.2f})\\t median {da.median():5.2f}\" )\n",
    "    return df\n",
    "\n",
    "\n",
    "def myplotimportance(model, nfeatures = 30):\n",
    "    if isinstance( model, LGBMRegressor ): \n",
    "        m = model\n",
    "    elif isinstance( model.estimators_[0], LGBMRegressor ): \n",
    "        m = model.estimators_[-1]\n",
    "    else: \n",
    "        print(\"*** not a valid model\")\n",
    "    lightgbm.plot_importance(m, importance_type=\"gain\", precision=0, ignore_zero=False,\\\n",
    "                             max_num_features=nfeatures, figsize=(8,np.ceil(nfeatures/7)) )\n",
    "    \n",
    "def imptdf(model):\n",
    "    m = model.estimators_[-1]\n",
    "    imps = pd.DataFrame([m.feature_name_, m.booster_.feature_importance(importance_type='gain'), m.feature_importances_]).T\n",
    "    imps.columns = ['name', 'gain', 'splits']\n",
    "    imps = imps.sort_values(by='gain', ascending=False)\n",
    "    # imps['gain'] = (imps.gain/imps.gain.mean()) / (imps.splits/imps.splits.mean())\n",
    "    imps['gain'] = imps.gain/1e6\n",
    "    return imps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e52282e-93b4-42b1-9353-5f6f8ce771d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'new_ys'\n",
    "%time nysmodel = traintest(data, TARGET, n = 1)\n",
    "myplotimportance(nysmodel,30)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
