{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "094cd8ea",
   "metadata": {},
   "source": [
    "## Set Transformer Model\n",
    "\n",
    "\n",
    "This notebook implements the [set transformer](https://arxiv.org/abs/1810.00825) model to predict the yield spreads. The model takes a sequence of trades as input and predicts the spreads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b292f710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data_preparation import process_data\n",
    "\n",
    "\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import LayerNormalization, Dense\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow import repeat\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cef8764",
   "metadata": {},
   "source": [
    "Setting the seed for keras layer initializer. This removes the randomness from the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d98d0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_initializer = initializers.RandomNormal(mean=0.0, stddev=0.1, seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f28422d",
   "metadata": {},
   "source": [
    "Initializing big query client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f6b692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a003cdd3",
   "metadata": {},
   "source": [
    "### Checking if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0d3d262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f0dea3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79989115",
   "metadata": {},
   "source": [
    "#### Hyper-parameters for the model\n",
    "The batch size and learning rate have an impact on the smoothness of convergence of the model.\\\n",
    "Larger the batch size the smoother the convergence. For a larger batch size we need a higher learning rate and vice-versa\n",
    "\n",
    "\n",
    "The SECONDS_AGO_FEATURE decide the scale of the seconds ago feature. The feature can be either on a logarithmic scale or a square root scale. The parameters can be set to None to remove the features from input to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb191d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "TRAIN_TEST_SPLIT = 0.85\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "\n",
    "# Model Parameters\n",
    "SEQUENCE_LENGTH = 5\n",
    "NUM_FEATURES= 8\n",
    "EMBED_DIM = 100\n",
    "SEED_DIM = 1\n",
    "FF_DIM = 300\n",
    "DROPOUT_RATE = 0.0\n",
    "NUM_HEADS = 1\n",
    "BATCH_SIZE = 1000\n",
    "INDUCED_POINTS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afef13ba",
   "metadata": {},
   "source": [
    "#### Query to fetch data from BigQuery\n",
    "\n",
    "The SQL query uses the trade history with reference data view. The recent field is an aggregated array of 32 recent previous trades in the same cusip. The array contains the yield spreads, size, trade direction, and the seconds elapsed from the most recent trade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "002aa558",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_QUERY = \"\"\"SELECT\n",
    "                    rtrs_control_number,\n",
    "                    yield_spread,\n",
    "                    par_traded,\n",
    "                    trade_type,\n",
    "                    recent\n",
    "                FROM\n",
    "                    `eng-reactor-287421.primary_views.trade_history_with_reference_data`\n",
    "                WHERE\n",
    "                    trade_date >= '2021-01-01'\n",
    "                AND \n",
    "                    trade_date < '2021-04-01'\n",
    "                AND \n",
    "                    yield_spread IS NOT NULL\n",
    "                LIMIT\n",
    "                    100000\n",
    "                \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f1e96b",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "We grab the data from BigQuery and converts it into a format suitable for input to the model. The driver function of this is process_data. All the data processing functions have been moved to the data_preparation.py file. \n",
    "\n",
    "The data query returns a table with a nested column, called recent, which contains the  32 recent previous trades in the same CUSIP. The **fetch_data** function executes the query and grabs data from BigQuery as a data frame. \n",
    "\n",
    "The aggregated arrays are stored as a list of dictionaries. The **tradeDictToList**  extracts the yield spreads, the size, the type, and seconds elapsed from the dictionary and stores them as a list. In the process of extraction, we perform a few blunt normalizations. The yield spreads are converted from percentage points to basis points. The size of the trade is scaled down by dividing it by 1000. The seconds ago are converted into a logarithmic scale. If the seconds ago are negative (due to trade time after publishing time) the function adds a zero to the list. \n",
    "\n",
    "The trades which do not have sufficient history are padded with zeros. The **pad_trade_history** function pads the beginning of trade history with zeros to ensure that the length of the list is equal to trade history. \n",
    "\n",
    "| RTRS Control Number | trade_history                                          | yield_spread    |\n",
    "|:-------------------:|---------------------------------------------------|-----------|\n",
    "| 2021031700698100    | [[5.997612444822997, 50.0, 0.0, 0.0, 71.458333... | 20.281352 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b551fe2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative seconds ago\n",
      "Negative seconds ago\n",
      "Negative seconds ago\n",
      "Negative seconds ago\n",
      "Number of training Samples 60735\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rtrs_control_number</th>\n",
       "      <th>yield_spread</th>\n",
       "      <th>par_traded</th>\n",
       "      <th>trade_type</th>\n",
       "      <th>trade_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021031905029000</td>\n",
       "      <td>-85.964339</td>\n",
       "      <td>10000.000000000</td>\n",
       "      <td>D</td>\n",
       "      <td>[[-105.964338945754, 4000.0, 0.0, 1.0, 6.51471...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021022408792800</td>\n",
       "      <td>-110.901862</td>\n",
       "      <td>20000.000000000</td>\n",
       "      <td>S</td>\n",
       "      <td>[[-106.70186192700702, 50.0, 0.0, 0.0, 8.85537...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021010803866300</td>\n",
       "      <td>131.576567</td>\n",
       "      <td>250000.000000000</td>\n",
       "      <td>S</td>\n",
       "      <td>[[131.57656694241902, 50.0, 0.0, 0.0, 7.638679...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021012107657000</td>\n",
       "      <td>23.645575</td>\n",
       "      <td>5000.000000000</td>\n",
       "      <td>D</td>\n",
       "      <td>[[19.945575447686004, 60.0, 0.0, 1.0, 9.595534...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021020403211200</td>\n",
       "      <td>33.796133</td>\n",
       "      <td>500000.000000000</td>\n",
       "      <td>S</td>\n",
       "      <td>[[40.59613290938899, 500.0, 0.0, 0.0, 6.553933...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rtrs_control_number  yield_spread        par_traded trade_type  \\\n",
       "0     2021031905029000    -85.964339   10000.000000000          D   \n",
       "1     2021022408792800   -110.901862   20000.000000000          S   \n",
       "3     2021010803866300    131.576567  250000.000000000          S   \n",
       "4     2021012107657000     23.645575    5000.000000000          D   \n",
       "5     2021020403211200     33.796133  500000.000000000          S   \n",
       "\n",
       "                                       trade_history  \n",
       "0  [[-105.964338945754, 4000.0, 0.0, 1.0, 6.51471...  \n",
       "1  [[-106.70186192700702, 50.0, 0.0, 0.0, 8.85537...  \n",
       "3  [[131.57656694241902, 50.0, 0.0, 0.0, 7.638679...  \n",
       "4  [[19.945575447686004, 60.0, 0.0, 1.0, 9.595534...  \n",
       "5  [[40.59613290938899, 500.0, 0.0, 0.0, 6.553933...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.1 s, sys: 1.49 s, total: 31.5 s\n",
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_dataframe, test_dataframe = process_data(DATA_QUERY,bq_client, SEQUENCE_LENGTH, NUM_FEATURES, TRAIN_TEST_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0c1acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe.to_pickle('temp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d51f7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rtrs_control_number</th>\n",
       "      <th>yield_spread</th>\n",
       "      <th>par_traded</th>\n",
       "      <th>trade_type</th>\n",
       "      <th>trade_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50968</th>\n",
       "      <td>2021030304195600</td>\n",
       "      <td>-1.158774</td>\n",
       "      <td>15000.000000000</td>\n",
       "      <td>P</td>\n",
       "      <td>[[-23.276521795774997, 45.0, 1.0, 0.0, 16.2097...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50009</th>\n",
       "      <td>2021021706957400</td>\n",
       "      <td>-73.736782</td>\n",
       "      <td>100000.000000000</td>\n",
       "      <td>S</td>\n",
       "      <td>[[40.89679316399801, 5.0, 0.0, 0.0, 15.8116282...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60830</th>\n",
       "      <td>2021011200212700</td>\n",
       "      <td>-78.169731</td>\n",
       "      <td>30000.000000000</td>\n",
       "      <td>S</td>\n",
       "      <td>[[-56.597830429389006, 50.0, 1.0, 0.0, 15.3530...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64373</th>\n",
       "      <td>2021021004647000</td>\n",
       "      <td>-68.236276</td>\n",
       "      <td>120000.000000000</td>\n",
       "      <td>P</td>\n",
       "      <td>[[-51.591652363448006, 10.0, 0.0, 0.0, 15.5271...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93723</th>\n",
       "      <td>2021012102923100</td>\n",
       "      <td>507.245575</td>\n",
       "      <td>5000.000000000</td>\n",
       "      <td>S</td>\n",
       "      <td>[[-60.94004651733801, 10.0, 0.0, 1.0, 15.52422...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95957</th>\n",
       "      <td>2021032501092400</td>\n",
       "      <td>-49.538834</td>\n",
       "      <td>25000.000000000</td>\n",
       "      <td>D</td>\n",
       "      <td>[[-75.228787439416, 230.0, 0.0, 0.0, 16.029777...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34279</th>\n",
       "      <td>2021010504015800</td>\n",
       "      <td>-48.002388</td>\n",
       "      <td>40000.000000000</td>\n",
       "      <td>D</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 40.0, 0.0, 0.0], [0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40383</th>\n",
       "      <td>2021022307154600</td>\n",
       "      <td>-99.171869</td>\n",
       "      <td>10000.000000000</td>\n",
       "      <td>D</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 10.0, 0.0, 0.0], [-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42090</th>\n",
       "      <td>2021032307080100</td>\n",
       "      <td>-105.009387</td>\n",
       "      <td>50000.000000000</td>\n",
       "      <td>S</td>\n",
       "      <td>[[-85.628787439416, 100.0, 1.0, 0.0, 16.013239...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82776</th>\n",
       "      <td>2021030501522800</td>\n",
       "      <td>150.649147</td>\n",
       "      <td>50000.000000000</td>\n",
       "      <td>D</td>\n",
       "      <td>[[143.03565674364702, 150.0, 0.0, 1.0, 16.4540...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60735 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rtrs_control_number  yield_spread        par_traded trade_type  \\\n",
       "50968     2021030304195600     -1.158774   15000.000000000          P   \n",
       "50009     2021021706957400    -73.736782  100000.000000000          S   \n",
       "60830     2021011200212700    -78.169731   30000.000000000          S   \n",
       "64373     2021021004647000    -68.236276  120000.000000000          P   \n",
       "93723     2021012102923100    507.245575    5000.000000000          S   \n",
       "...                    ...           ...               ...        ...   \n",
       "95957     2021032501092400    -49.538834   25000.000000000          D   \n",
       "34279     2021010504015800    -48.002388   40000.000000000          D   \n",
       "40383     2021022307154600    -99.171869   10000.000000000          D   \n",
       "42090     2021032307080100   -105.009387   50000.000000000          S   \n",
       "82776     2021030501522800    150.649147   50000.000000000          D   \n",
       "\n",
       "                                           trade_history  \n",
       "50968  [[-23.276521795774997, 45.0, 1.0, 0.0, 16.2097...  \n",
       "50009  [[40.89679316399801, 5.0, 0.0, 0.0, 15.8116282...  \n",
       "60830  [[-56.597830429389006, 50.0, 1.0, 0.0, 15.3530...  \n",
       "64373  [[-51.591652363448006, 10.0, 0.0, 0.0, 15.5271...  \n",
       "93723  [[-60.94004651733801, 10.0, 0.0, 1.0, 15.52422...  \n",
       "...                                                  ...  \n",
       "95957  [[-75.228787439416, 230.0, 0.0, 0.0, 16.029777...  \n",
       "34279  [[0.0, 0.0, 0.0, 0.0, 0.0, 40.0, 0.0, 0.0], [0...  \n",
       "40383  [[0.0, 0.0, 0.0, 0.0, 0.0, 10.0, 0.0, 0.0], [-...  \n",
       "42090  [[-85.628787439416, 100.0, 1.0, 0.0, 16.013239...  \n",
       "82776  [[143.03565674364702, 150.0, 0.0, 1.0, 16.4540...  \n",
       "\n",
       "[60735 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Shuffling the train set\n",
    "train_dataframe = train_dataframe.sample(frac=1)\n",
    "display(train_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab4c765",
   "metadata": {},
   "source": [
    "We just want the most recent trades from the trade history. By default the views have 32 trades in the trade history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9fef184",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe.trade_history = train_dataframe.trade_history.apply(lambda x: x[-SEQUENCE_LENGTH:])\n",
    "test_dataframe.trade_history = test_dataframe.trade_history.apply(lambda x:x[-SEQUENCE_LENGTH:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1707b6c7",
   "metadata": {},
   "source": [
    "Changing the dataframe to numpy array so that data can be fed into the input layer of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a475506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.stack(train_dataframe.trade_history.to_numpy())\n",
    "target = train_dataframe.yield_spread.to_numpy()\n",
    "\n",
    "test_data = np.stack(test_dataframe.trade_history.to_numpy())\n",
    "test_target =  test_dataframe.yield_spread.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11778b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60735, 5, 8)\n",
      "(60735,)\n",
      "(10523, 5, 8)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(target.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c977c4e",
   "metadata": {},
   "source": [
    "# Set Transformers\n",
    "\n",
    "Set Transformers are attention-based neural networks that are designed to model interactions among elements in the input sets. The model consists of an encoder and decoder, both of which rely on attention mechanisms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8249bbbf",
   "metadata": {},
   "source": [
    "### MultiHead Attention\n",
    "\n",
    "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. The multiheaded attention layer linearly projects the queries, keys, and values, number of heads times with different learned linear projections to $d_k$, $d_k$, and $d_v$ dimensions, respectively. On each of these projected versions of queries, keys, and values the layer then performs the attention functions, yielding $d_v$-dimensional output values. These are concatenated and once again projected, resulting in the final values.\n",
    "\n",
    "The particular attention that the multihead attention calculates is called scaled dot product attention. The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$. We then compute the dot products of the\n",
    "query with all keys, divide each by $\\sqrt{d_k}$ and apply a softmax function to obtain the weights on the values. \n",
    "\n",
    "\\begin{equation}\n",
    "    Attention(Q,K,V) = SoftMax\\Bigg(\\frac{Q\\times K}{\\sqrt{d_k}}\\Bigg) \\times V\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d86a1e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    '''\n",
    "    The mask is simply to ensure that the encoder doesn't pay any attention to padding tokens. \n",
    "    Here is the formula for the masked scaled dot product attention:\n",
    "    '''\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    \n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) \n",
    "    output = tf.matmul(attention_weights, v)  \n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, embed_dim).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, embed_dim)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, q, k, v, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  \n",
    "        k = self.wk(k)  \n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size) \n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention,\n",
    "                                        perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2280bf4b",
   "metadata": {},
   "source": [
    "RFF is a class that implements a feed forward network. RFF is a simple row-wise feedforward layer i.e., it processes each instance independently and identically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc153d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RFF(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Row-wise FeedForward layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d):\n",
    "        super(RFF, self).__init__()\n",
    "\n",
    "        self.linear_1 = Dense(d, activation='relu')\n",
    "        self.linear_2 = Dense(d, activation='relu')\n",
    "        self.linear_3 = Dense(d, activation='relu')\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [b, n, d].\n",
    "        Returns:\n",
    "            a float tensor with shape [b, n, d].\n",
    "        \"\"\"\n",
    "        return self.linear_3(self.linear_2(self.linear_1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e36a379",
   "metadata": {},
   "source": [
    "### Multiheaded Attention Block\n",
    "Multiheaded attention block (MAB) is an adaption of the encoder block of the Transformer without positional encoding and dropout. The implementation of the block is derived from the Keras implementation of the encoder block for the transformer. MAB's are the building blocks of the set attention model. We define the Set Attention Block and Pooling Multiheaded attention block using the MAB\n",
    "\n",
    "MABs can be defined as follows\n",
    "\n",
    "\\begin{equation}\n",
    "MAB(X,Y) = LayerNorm(H+FF(H))\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "H = LayerNorm(X + MultiheadAttention(X,Y)\n",
    "\\end{equation}\n",
    "\n",
    "Since the model is based on self-attention. The Key (X) and Value(Y) are the same. FF in the above equation is a feed forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9996b66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedz_dim: int, num_heads: int, rff: RFF):\n",
    "        super(MultiHeadAttentionBlock, self).__init__()\n",
    "        self.multihead = MultiHeadAttention(embedz_dim, num_heads)\n",
    "        self.layer_norm1 = LayerNormalization(epsilon=1e-6, dtype='float32')\n",
    "        self.layer_norm2 = LayerNormalization(epsilon=1e-6, dtype='float32')\n",
    "        self.rff = rff\n",
    "\n",
    "    def call(self, x, y):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor \n",
    "            y: a float tensor \n",
    "        Returns:\n",
    "            a float tensor \n",
    "        \"\"\"\n",
    "\n",
    "        h = self.layer_norm1(x + self.multihead(x, y, y))\n",
    "        return self.layer_norm2(h + self.rff(h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec0157d",
   "metadata": {},
   "source": [
    "### Set Attention Block\n",
    "A Set attention block (SAB) takes a set and performs self attention between the elements in the set, resulting in a set of equal size. The output of SAB contains pairwise interactions between the elements of the set. We can stack multiple SABs to encode higher order interactions.\n",
    "\n",
    "SABs can be defined as\n",
    "\n",
    "\\begin{equation}\n",
    "SAB(X) := MAB(X,X)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55a595c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetAttentionBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d: int, h: int, rff: RFF):\n",
    "        super(SetAttentionBlock, self).__init__()\n",
    "        self.mab = MultiHeadAttentionBlock(d, h, rff)\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [b, n, d].\n",
    "        Returns:\n",
    "            a float tensor with shape [b, n, d].\n",
    "        \"\"\"\n",
    "        return self.mab(x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bee43e",
   "metadata": {},
   "source": [
    "## Induced Set Attention Block\n",
    "\n",
    "We can use SABs for the Set encoder block but a potential problem can be that they will be very slow to train. The forward pass on a SAB is $O(n^2)$. Thus we use Induced Set Attention Block (ISAB), which bypasses this problem. Along with the set $X \\epsilon R^{n\\times d}$, ISAB defines m d-dimensional vectors $I \\epsilon R^{m\\times d}$, which the authors call inducing points. Inducing points are part of the ISAB itself, and they are trainable parameters that are trained along with other parameters of the network. An ISAB with m inducing points I are defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "ISAB_{m}(X) = MAB(X,H) \\epsilon R^{n\\times d}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "H = MAB(I,X) \\epsilon R^{m\\times d}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3f845a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InducedSetAttentionBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d: int, m: int, h: int, rff1: RFF, rff2: RFF):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            d: an integer, input dimension.\n",
    "            m: an integer, number of inducing points.\n",
    "            h: an integer, number of heads.\n",
    "            rff1, rff2: modules, row-wise feedforward layers.\n",
    "                It takes a float tensor with shape [b, n, d] and\n",
    "                returns a float tensor with the same shape.\n",
    "        \"\"\"\n",
    "        super(InducedSetAttentionBlock, self).__init__()\n",
    "        self.mab1 = MultiHeadAttentionBlock(d, h, rff1)\n",
    "        self.mab2 = MultiHeadAttentionBlock(d, h, rff2)\n",
    "        self.inducing_points = tf.random.normal(shape=(1, m, d))\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [b, n, d].\n",
    "        Returns:\n",
    "            a float tensor with shape [b, n, d].\n",
    "        \"\"\"\n",
    "        b = tf.shape(x)[0]\n",
    "        p = self.inducing_points\n",
    "        p = repeat(p, (b), axis=0)  # shape [b, m, d]\n",
    "\n",
    "        h = self.mab1(p, x)  # shape [b, m, d]\n",
    "        return self.mab2(x, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c99776",
   "metadata": {},
   "source": [
    "### Pooling Multihead Attention\n",
    "\n",
    "Set Transformers aggregate features by applying multihead attention on a learnable set of k seed vectors. Let $S \\epsilon R^{k\\times d}$ be the seed vectors and $Z \\epsilon R^{n \\times d}$ be the set of features constructed from the encoder. The a PMA is defined as.\n",
    "\n",
    "\\begin{equation}\n",
    "PMA_k(Z) = MAB(S,ff(Z))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "172697b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolingMultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, embed_dim: int, num_seeds: int, num_heads: int, rff: RFF, rff_s: RFF):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            embed_dims: an integer, input dimension.\n",
    "            num_seeds: an integer, number of seed vectors.\n",
    "            num_heads: an integer, number of heads.\n",
    "            rff: a module, row-wise feedforward layers.\n",
    "                It takes a float tensor with shape [b, n, d] and\n",
    "                returns a float tensor with the same shape.\n",
    "        \"\"\"\n",
    "        super(PoolingMultiHeadAttention, self).__init__()\n",
    "        self.mab = MultiHeadAttentionBlock(embed_dim, num_heads, rff)\n",
    "        self.seed_vectors = tf.random.normal(shape=(1, num_seeds, embed_dim))\n",
    "        self.rff_s = rff_s\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, z):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            z: a float tensor with shape [batch, seq_length, embed_dim].\n",
    "        Returns:\n",
    "            a float tensor with shape [batch, num_seeds, embed_dim]\n",
    "        \"\"\"\n",
    "        b = tf.shape(z)[0]\n",
    "        s = self.seed_vectors\n",
    "        s = repeat(s, (b), axis=0)  # shape [b, k, d]\n",
    "        return self.mab(s, self.rff_s(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796e4fc6",
   "metadata": {},
   "source": [
    "### Combining the blocks \n",
    "Encoder(X) = SAB(SAB(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "461ceb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim=12, num_induct=6, num_heads=6):\n",
    "        super(STEncoder, self).__init__()\n",
    "\n",
    "        # Embedding part\n",
    "        self.linear_1 = Dense(embed_dim, activation='relu')\n",
    "\n",
    "        # Encoding part\n",
    "        self.isab_1 = InducedSetAttentionBlock(embed_dim, num_induct, num_heads, RFF(embed_dim), RFF(embed_dim))\n",
    "        self.isab_2 = InducedSetAttentionBlock(embed_dim, num_induct, num_heads, RFF(embed_dim), RFF(embed_dim))\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.isab_2(self.isab_1(self.linear_1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb941615",
   "metadata": {},
   "source": [
    "Decoder Decoder(Z) = FF(SAB(PMA(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d425fa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, out_dim, embed_dim=12, num_heads=2, num_seeds=8):\n",
    "        super(STDecoder, self).__init__()\n",
    "\n",
    "        self.PMA = PoolingMultiHeadAttention(embed_dim,\n",
    "                                             num_seeds,\n",
    "                                             num_heads,\n",
    "                                             RFF(embed_dim),\n",
    "                                             RFF(embed_dim))\n",
    "        \n",
    "        self.SAB = SetAttentionBlock(embed_dim,\n",
    "                                     num_heads,\n",
    "                                     RFF(embed_dim))\n",
    "        \n",
    "        self.output_mapper = Dense(out_dim)\n",
    "        self.num_seeds, self.embed_dim= num_seeds, embed_dim\n",
    "\n",
    "    def call(self, x):\n",
    "        decoded_vec = self.SAB(self.PMA(x))\n",
    "        decoded_vec = tf.reshape(decoded_vec, [-1, self.num_seeds * self.embed_dim])\n",
    "        return tf.reshape(self.output_mapper(decoded_vec), (tf.shape(decoded_vec)[0],))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04759c4e",
   "metadata": {},
   "source": [
    "The SetTransformer class combines the encoder and deocder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dcd49c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetTransformer(tf.keras.Model):\n",
    "    def __init__(self, encoder_d=4, num_induct=3, encoder_h=2, out_dim=1, decoder_d=4, decoder_h=2, num_seeds=1):\n",
    "        super(SetTransformer, self).__init__()\n",
    "        self.basic_encoder = STEncoder(embed_dim=encoder_d, num_induct=num_induct, num_heads=encoder_h)\n",
    "        self.basic_decoder = STDecoder(out_dim=out_dim, embed_dim=decoder_d, num_heads=decoder_h, num_seeds=num_seeds)\n",
    "\n",
    "    def call(self, x):\n",
    "        enc_output = self.basic_encoder(x)  # (batch_size, set_len, d_model)\n",
    "        return self.basic_decoder(enc_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d86ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SetTransformer(encoder_d= EMBED_DIM,\n",
    "                            num_induct = INDUCED_POINTS,\n",
    "                            encoder_h=NUM_HEADS,\n",
    "                            out_dim=1,\n",
    "                            decoder_d=EMBED_DIM,\n",
    "                            decoder_h=NUM_HEADS,\n",
    "                            num_seeds=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0517448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "              loss=keras.losses.MeanSquaredError(),\n",
    "              metrics=[keras.metrics.MeanAbsoluteError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2410173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "49/49 - 10s - loss: 85626.4375 - mean_absolute_error: 79.9746 - val_loss: 59929.3906 - val_mean_absolute_error: 73.2783\n",
      "Epoch 2/200\n",
      "49/49 - 1s - loss: 84529.1953 - mean_absolute_error: 74.6564 - val_loss: 59188.8711 - val_mean_absolute_error: 69.2395\n",
      "Epoch 3/200\n",
      "49/49 - 1s - loss: 83728.6016 - mean_absolute_error: 70.5281 - val_loss: 58427.7852 - val_mean_absolute_error: 64.9037\n",
      "Epoch 4/200\n",
      "49/49 - 1s - loss: 82913.8281 - mean_absolute_error: 66.1963 - val_loss: 57746.1367 - val_mean_absolute_error: 61.7698\n",
      "Epoch 5/200\n",
      "49/49 - 1s - loss: 82208.9766 - mean_absolute_error: 62.8082 - val_loss: 57138.0000 - val_mean_absolute_error: 60.3688\n",
      "Epoch 6/200\n",
      "49/49 - 1s - loss: 81882.6406 - mean_absolute_error: 60.2832 - val_loss: 56513.3711 - val_mean_absolute_error: 56.3256\n",
      "Epoch 7/200\n",
      "49/49 - 1s - loss: 80885.9453 - mean_absolute_error: 57.4510 - val_loss: 56051.9648 - val_mean_absolute_error: 54.0676\n",
      "Epoch 8/200\n",
      "49/49 - 1s - loss: 80295.9141 - mean_absolute_error: 55.9921 - val_loss: 55387.0820 - val_mean_absolute_error: 51.4189\n",
      "Epoch 9/200\n",
      "49/49 - 1s - loss: 79738.4219 - mean_absolute_error: 54.5140 - val_loss: 54767.7930 - val_mean_absolute_error: 49.0532\n",
      "Epoch 10/200\n",
      "49/49 - 1s - loss: 78995.7266 - mean_absolute_error: 52.2447 - val_loss: 54359.1602 - val_mean_absolute_error: 49.0243\n",
      "Epoch 11/200\n",
      "49/49 - 1s - loss: 78412.3047 - mean_absolute_error: 52.2135 - val_loss: 53815.3594 - val_mean_absolute_error: 48.6056\n",
      "Epoch 12/200\n",
      "49/49 - 1s - loss: 77866.5312 - mean_absolute_error: 52.6129 - val_loss: 53276.8398 - val_mean_absolute_error: 48.1373\n",
      "Epoch 13/200\n",
      "49/49 - 1s - loss: 77160.3203 - mean_absolute_error: 51.3052 - val_loss: 52668.8984 - val_mean_absolute_error: 46.5010\n",
      "Epoch 14/200\n",
      "49/49 - 1s - loss: 76398.3984 - mean_absolute_error: 49.6270 - val_loss: 51984.7812 - val_mean_absolute_error: 44.9780\n",
      "Epoch 15/200\n",
      "49/49 - 1s - loss: 75650.3438 - mean_absolute_error: 48.8223 - val_loss: 51400.6836 - val_mean_absolute_error: 44.4363\n",
      "Epoch 16/200\n",
      "49/49 - 1s - loss: 75148.8906 - mean_absolute_error: 49.9287 - val_loss: 50924.9688 - val_mean_absolute_error: 44.7238\n",
      "Epoch 17/200\n",
      "49/49 - 1s - loss: 74288.0078 - mean_absolute_error: 48.1567 - val_loss: 50306.5820 - val_mean_absolute_error: 45.2233\n",
      "Epoch 18/200\n",
      "49/49 - 1s - loss: 73777.6719 - mean_absolute_error: 50.1370 - val_loss: 50135.2031 - val_mean_absolute_error: 47.8476\n",
      "Epoch 19/200\n",
      "49/49 - 1s - loss: 73100.2891 - mean_absolute_error: 49.7469 - val_loss: 49253.0391 - val_mean_absolute_error: 43.5337\n",
      "Epoch 20/200\n",
      "49/49 - 1s - loss: 72248.7500 - mean_absolute_error: 47.4128 - val_loss: 48466.5312 - val_mean_absolute_error: 44.8266\n",
      "Epoch 21/200\n",
      "49/49 - 1s - loss: 71661.9922 - mean_absolute_error: 49.0692 - val_loss: 48139.4023 - val_mean_absolute_error: 44.7903\n",
      "Epoch 22/200\n",
      "49/49 - 1s - loss: 70865.9297 - mean_absolute_error: 48.5617 - val_loss: 47209.2500 - val_mean_absolute_error: 43.1844\n",
      "Epoch 23/200\n",
      "49/49 - 1s - loss: 69925.6484 - mean_absolute_error: 45.8994 - val_loss: 46659.9922 - val_mean_absolute_error: 43.8678\n",
      "Epoch 24/200\n",
      "49/49 - 1s - loss: 69282.2656 - mean_absolute_error: 45.8833 - val_loss: 46055.1641 - val_mean_absolute_error: 42.8067\n",
      "Epoch 25/200\n",
      "49/49 - 1s - loss: 68470.5391 - mean_absolute_error: 45.4483 - val_loss: 45249.3008 - val_mean_absolute_error: 40.6503\n",
      "Epoch 26/200\n",
      "49/49 - 1s - loss: 67670.8203 - mean_absolute_error: 45.0987 - val_loss: 44723.0898 - val_mean_absolute_error: 41.6247\n",
      "Epoch 27/200\n",
      "49/49 - 1s - loss: 67051.2266 - mean_absolute_error: 45.6266 - val_loss: 44132.1289 - val_mean_absolute_error: 40.9611\n",
      "Epoch 28/200\n",
      "49/49 - 1s - loss: 66437.5547 - mean_absolute_error: 46.1863 - val_loss: 43786.0508 - val_mean_absolute_error: 42.7908\n",
      "Epoch 29/200\n",
      "49/49 - 1s - loss: 65615.8047 - mean_absolute_error: 45.3083 - val_loss: 43064.0117 - val_mean_absolute_error: 41.6438\n",
      "Epoch 30/200\n",
      "49/49 - 1s - loss: 64792.5117 - mean_absolute_error: 44.0371 - val_loss: 42692.5938 - val_mean_absolute_error: 41.4178\n",
      "Epoch 31/200\n",
      "49/49 - 1s - loss: 64210.3125 - mean_absolute_error: 44.2285 - val_loss: 41994.2344 - val_mean_absolute_error: 39.8847\n",
      "Epoch 32/200\n",
      "49/49 - 1s - loss: 63545.8672 - mean_absolute_error: 42.9318 - val_loss: 41562.7930 - val_mean_absolute_error: 40.0513\n",
      "Epoch 33/200\n",
      "49/49 - 1s - loss: 62858.5156 - mean_absolute_error: 43.5035 - val_loss: 41168.7617 - val_mean_absolute_error: 42.3647\n",
      "Epoch 34/200\n",
      "49/49 - 1s - loss: 62371.1094 - mean_absolute_error: 44.9110 - val_loss: 40711.8242 - val_mean_absolute_error: 42.3062\n",
      "Epoch 35/200\n",
      "49/49 - 1s - loss: 61688.4492 - mean_absolute_error: 43.1935 - val_loss: 40178.7969 - val_mean_absolute_error: 41.8356\n",
      "Epoch 36/200\n",
      "49/49 - 1s - loss: 62210.1758 - mean_absolute_error: 50.3498 - val_loss: 39704.9648 - val_mean_absolute_error: 40.8413\n",
      "Epoch 37/200\n",
      "49/49 - 1s - loss: 60986.1406 - mean_absolute_error: 47.0410 - val_loss: 39946.0234 - val_mean_absolute_error: 44.8306\n",
      "Epoch 38/200\n",
      "49/49 - 1s - loss: 60329.1406 - mean_absolute_error: 44.0334 - val_loss: 39379.9062 - val_mean_absolute_error: 41.3239\n",
      "Epoch 39/200\n",
      "49/49 - 1s - loss: 59856.0430 - mean_absolute_error: 43.8244 - val_loss: 38741.8594 - val_mean_absolute_error: 42.4719\n",
      "Epoch 40/200\n",
      "49/49 - 1s - loss: 59794.7656 - mean_absolute_error: 46.4975 - val_loss: 43903.7461 - val_mean_absolute_error: 47.0138\n",
      "Epoch 41/200\n",
      "49/49 - 1s - loss: 60801.2422 - mean_absolute_error: 55.0049 - val_loss: 38469.9375 - val_mean_absolute_error: 43.2841\n",
      "Epoch 42/200\n",
      "49/49 - 1s - loss: 60083.6133 - mean_absolute_error: 50.6823 - val_loss: 38426.1055 - val_mean_absolute_error: 42.5561\n",
      "Epoch 43/200\n",
      "49/49 - 1s - loss: 58813.3906 - mean_absolute_error: 47.2401 - val_loss: 37414.5195 - val_mean_absolute_error: 41.3365\n",
      "Epoch 44/200\n",
      "49/49 - 1s - loss: 58095.3711 - mean_absolute_error: 46.1887 - val_loss: 37736.2656 - val_mean_absolute_error: 43.4199\n",
      "Epoch 45/200\n",
      "49/49 - 1s - loss: 58303.3359 - mean_absolute_error: 47.7808 - val_loss: 37144.6016 - val_mean_absolute_error: 48.4860\n",
      "Epoch 46/200\n",
      "49/49 - 1s - loss: 57556.2695 - mean_absolute_error: 49.4433 - val_loss: 36264.2344 - val_mean_absolute_error: 42.1925\n",
      "Epoch 47/200\n",
      "49/49 - 1s - loss: 56705.9062 - mean_absolute_error: 47.2723 - val_loss: 37445.9922 - val_mean_absolute_error: 55.2475\n",
      "Epoch 48/200\n",
      "49/49 - 1s - loss: 56903.1094 - mean_absolute_error: 47.5261 - val_loss: 36239.1055 - val_mean_absolute_error: 42.4739\n",
      "Epoch 49/200\n",
      "49/49 - 1s - loss: 56298.8789 - mean_absolute_error: 45.8919 - val_loss: 35958.6797 - val_mean_absolute_error: 41.4185\n",
      "Epoch 50/200\n",
      "49/49 - 1s - loss: 55735.2539 - mean_absolute_error: 45.0201 - val_loss: 35317.1484 - val_mean_absolute_error: 42.2257\n",
      "Epoch 51/200\n",
      "49/49 - 1s - loss: 54857.0781 - mean_absolute_error: 46.5089 - val_loss: 38300.0742 - val_mean_absolute_error: 78.4363\n",
      "Epoch 52/200\n",
      "49/49 - 1s - loss: 55493.9648 - mean_absolute_error: 51.1220 - val_loss: 35086.9492 - val_mean_absolute_error: 41.8707\n",
      "Epoch 53/200\n",
      "49/49 - 1s - loss: 53796.3906 - mean_absolute_error: 45.0256 - val_loss: 33566.1406 - val_mean_absolute_error: 39.9102\n",
      "Epoch 54/200\n",
      "49/49 - 1s - loss: 53379.0508 - mean_absolute_error: 47.3484 - val_loss: 33794.9336 - val_mean_absolute_error: 40.5913\n",
      "Epoch 55/200\n",
      "49/49 - 1s - loss: 52342.7891 - mean_absolute_error: 44.3493 - val_loss: 34398.3281 - val_mean_absolute_error: 58.3125\n",
      "Epoch 56/200\n",
      "49/49 - 1s - loss: 52398.9336 - mean_absolute_error: 47.0076 - val_loss: 31947.1426 - val_mean_absolute_error: 38.8754\n",
      "Epoch 57/200\n",
      "49/49 - 1s - loss: 51927.2578 - mean_absolute_error: 45.3008 - val_loss: 31734.4336 - val_mean_absolute_error: 38.7616\n",
      "Epoch 58/200\n",
      "49/49 - 1s - loss: 57758.3945 - mean_absolute_error: 60.0140 - val_loss: 41374.1367 - val_mean_absolute_error: 90.6058\n",
      "Epoch 59/200\n",
      "49/49 - 1s - loss: 56468.6055 - mean_absolute_error: 64.2031 - val_loss: 33835.6719 - val_mean_absolute_error: 45.7477\n",
      "Epoch 60/200\n",
      "49/49 - 1s - loss: 52650.2930 - mean_absolute_error: 47.0265 - val_loss: 35289.3242 - val_mean_absolute_error: 47.1342\n",
      "Epoch 61/200\n",
      "49/49 - 1s - loss: 50906.2031 - mean_absolute_error: 45.7121 - val_loss: 31385.8457 - val_mean_absolute_error: 41.5193\n",
      "Epoch 62/200\n",
      "49/49 - 1s - loss: 49953.3242 - mean_absolute_error: 46.0290 - val_loss: 31085.0156 - val_mean_absolute_error: 40.2522\n",
      "Epoch 63/200\n",
      "49/49 - 1s - loss: 49746.3516 - mean_absolute_error: 43.8072 - val_loss: 30983.2051 - val_mean_absolute_error: 38.6349\n",
      "Epoch 64/200\n",
      "49/49 - 1s - loss: 48904.7578 - mean_absolute_error: 42.8777 - val_loss: 30362.3066 - val_mean_absolute_error: 37.2870\n",
      "Epoch 65/200\n",
      "49/49 - 1s - loss: 49033.5781 - mean_absolute_error: 42.5128 - val_loss: 30502.2324 - val_mean_absolute_error: 37.9799\n",
      "Epoch 66/200\n",
      "49/49 - 1s - loss: 48677.2031 - mean_absolute_error: 42.1705 - val_loss: 30495.6211 - val_mean_absolute_error: 38.1782\n",
      "Epoch 67/200\n",
      "49/49 - 1s - loss: 49045.1758 - mean_absolute_error: 44.8521 - val_loss: 31903.4688 - val_mean_absolute_error: 40.5829\n",
      "Epoch 68/200\n",
      "49/49 - 1s - loss: 49085.3828 - mean_absolute_error: 48.6735 - val_loss: 29854.3379 - val_mean_absolute_error: 39.4364\n",
      "Epoch 69/200\n",
      "49/49 - 1s - loss: 47142.8047 - mean_absolute_error: 43.0472 - val_loss: 29881.0039 - val_mean_absolute_error: 41.1924\n",
      "Epoch 70/200\n",
      "49/49 - 1s - loss: 47494.0625 - mean_absolute_error: 43.5448 - val_loss: 33010.8867 - val_mean_absolute_error: 46.6807\n",
      "Epoch 71/200\n",
      "49/49 - 1s - loss: 46914.1953 - mean_absolute_error: 46.4877 - val_loss: 31480.6992 - val_mean_absolute_error: 41.9350\n",
      "Epoch 72/200\n",
      "49/49 - 1s - loss: 46426.2266 - mean_absolute_error: 42.7924 - val_loss: 28708.0176 - val_mean_absolute_error: 37.4389\n",
      "Epoch 73/200\n",
      "49/49 - 1s - loss: 45798.2148 - mean_absolute_error: 46.4662 - val_loss: 29938.9668 - val_mean_absolute_error: 42.4793\n",
      "Epoch 74/200\n",
      "49/49 - 1s - loss: 45379.9531 - mean_absolute_error: 43.2080 - val_loss: 29036.1387 - val_mean_absolute_error: 38.4661\n",
      "Epoch 75/200\n",
      "49/49 - 1s - loss: 44772.3750 - mean_absolute_error: 43.5140 - val_loss: 30002.2949 - val_mean_absolute_error: 40.5032\n",
      "Epoch 76/200\n",
      "49/49 - 1s - loss: 44904.0469 - mean_absolute_error: 45.8913 - val_loss: 29166.2285 - val_mean_absolute_error: 41.5917\n",
      "Epoch 77/200\n",
      "49/49 - 1s - loss: 43774.2891 - mean_absolute_error: 43.2654 - val_loss: 29448.0938 - val_mean_absolute_error: 42.1784\n",
      "Epoch 78/200\n",
      "49/49 - 1s - loss: 43908.2773 - mean_absolute_error: 44.2456 - val_loss: 28467.1504 - val_mean_absolute_error: 40.2516\n",
      "Epoch 79/200\n",
      "49/49 - 1s - loss: 43212.8945 - mean_absolute_error: 43.1488 - val_loss: 28085.0332 - val_mean_absolute_error: 38.1809\n",
      "Epoch 80/200\n",
      "49/49 - 1s - loss: 43171.2500 - mean_absolute_error: 41.2861 - val_loss: 28951.3770 - val_mean_absolute_error: 46.3429\n",
      "Epoch 81/200\n",
      "49/49 - 1s - loss: 45131.4219 - mean_absolute_error: 52.9065 - val_loss: 33917.7305 - val_mean_absolute_error: 66.2534\n",
      "Epoch 82/200\n",
      "49/49 - 1s - loss: 54723.0312 - mean_absolute_error: 64.9299 - val_loss: 32115.9336 - val_mean_absolute_error: 73.7507\n",
      "Epoch 83/200\n",
      "49/49 - 1s - loss: 47918.5742 - mean_absolute_error: 63.8465 - val_loss: 34604.4727 - val_mean_absolute_error: 54.5296\n",
      "Epoch 84/200\n",
      "49/49 - 1s - loss: 43616.8594 - mean_absolute_error: 51.1241 - val_loss: 28270.8340 - val_mean_absolute_error: 38.9455\n",
      "Epoch 85/200\n",
      "49/49 - 1s - loss: 41973.7812 - mean_absolute_error: 42.8758 - val_loss: 28056.4395 - val_mean_absolute_error: 44.3315\n",
      "Epoch 86/200\n",
      "49/49 - 1s - loss: 41499.9180 - mean_absolute_error: 41.5686 - val_loss: 28066.0254 - val_mean_absolute_error: 39.2160\n",
      "Epoch 87/200\n",
      "49/49 - 1s - loss: 42634.8906 - mean_absolute_error: 47.3210 - val_loss: 29075.3203 - val_mean_absolute_error: 47.2438\n",
      "Epoch 88/200\n",
      "49/49 - 1s - loss: 42116.3164 - mean_absolute_error: 46.7164 - val_loss: 28034.3027 - val_mean_absolute_error: 38.8266\n",
      "Epoch 89/200\n",
      "49/49 - 1s - loss: 41934.0000 - mean_absolute_error: 41.6535 - val_loss: 28169.4004 - val_mean_absolute_error: 37.1695\n",
      "Epoch 90/200\n",
      "49/49 - 1s - loss: 41091.1094 - mean_absolute_error: 40.0792 - val_loss: 28194.8027 - val_mean_absolute_error: 39.5087\n",
      "Epoch 91/200\n",
      "49/49 - 1s - loss: 41421.8008 - mean_absolute_error: 43.3989 - val_loss: 30063.9043 - val_mean_absolute_error: 42.0331\n",
      "Epoch 92/200\n",
      "49/49 - 1s - loss: 42316.4844 - mean_absolute_error: 45.7577 - val_loss: 27864.5898 - val_mean_absolute_error: 49.4644\n",
      "Epoch 93/200\n",
      "49/49 - 1s - loss: 40838.1953 - mean_absolute_error: 42.5754 - val_loss: 27051.6289 - val_mean_absolute_error: 44.5549\n",
      "Epoch 94/200\n",
      "49/49 - 1s - loss: 40475.8867 - mean_absolute_error: 47.0069 - val_loss: 27478.4551 - val_mean_absolute_error: 54.5271\n",
      "Epoch 95/200\n",
      "49/49 - 1s - loss: 39127.9648 - mean_absolute_error: 41.4155 - val_loss: 27042.8750 - val_mean_absolute_error: 50.2095\n",
      "Epoch 96/200\n",
      "49/49 - 1s - loss: 38694.8438 - mean_absolute_error: 43.4952 - val_loss: 26344.1855 - val_mean_absolute_error: 48.2164\n",
      "Epoch 97/200\n",
      "49/49 - 1s - loss: 38495.7109 - mean_absolute_error: 43.9609 - val_loss: 26066.2012 - val_mean_absolute_error: 38.2995\n",
      "Epoch 98/200\n",
      "49/49 - 1s - loss: 37635.9375 - mean_absolute_error: 42.3714 - val_loss: 26730.9668 - val_mean_absolute_error: 36.3239\n",
      "Epoch 99/200\n",
      "49/49 - 1s - loss: 37433.3203 - mean_absolute_error: 39.9739 - val_loss: 27648.6016 - val_mean_absolute_error: 51.2952\n",
      "Epoch 100/200\n",
      "49/49 - 1s - loss: 37623.0898 - mean_absolute_error: 41.2710 - val_loss: 25886.2617 - val_mean_absolute_error: 36.9655\n",
      "Epoch 101/200\n",
      "49/49 - 1s - loss: 36821.4609 - mean_absolute_error: 41.6058 - val_loss: 26065.6406 - val_mean_absolute_error: 41.7201\n",
      "Epoch 102/200\n",
      "49/49 - 1s - loss: 36296.8906 - mean_absolute_error: 41.7014 - val_loss: 25864.2988 - val_mean_absolute_error: 39.6996\n",
      "Epoch 103/200\n",
      "49/49 - 1s - loss: 36684.0039 - mean_absolute_error: 39.3981 - val_loss: 25277.2051 - val_mean_absolute_error: 37.1200\n",
      "Epoch 104/200\n",
      "49/49 - 1s - loss: 35973.5625 - mean_absolute_error: 40.2138 - val_loss: 26325.5723 - val_mean_absolute_error: 37.8131\n",
      "Epoch 105/200\n",
      "49/49 - 1s - loss: 35375.1914 - mean_absolute_error: 40.0478 - val_loss: 26495.4961 - val_mean_absolute_error: 39.7428\n",
      "Epoch 106/200\n",
      "49/49 - 1s - loss: 35832.0430 - mean_absolute_error: 40.7306 - val_loss: 25762.2656 - val_mean_absolute_error: 41.0131\n",
      "Epoch 107/200\n",
      "49/49 - 1s - loss: 36107.4219 - mean_absolute_error: 43.4128 - val_loss: 27185.2656 - val_mean_absolute_error: 46.0654\n",
      "Epoch 108/200\n",
      "49/49 - 1s - loss: 35628.3555 - mean_absolute_error: 43.5710 - val_loss: 25183.7832 - val_mean_absolute_error: 36.3690\n",
      "Epoch 109/200\n",
      "49/49 - 1s - loss: 34543.2617 - mean_absolute_error: 41.7400 - val_loss: 26348.3242 - val_mean_absolute_error: 37.8887\n",
      "Epoch 110/200\n",
      "49/49 - 1s - loss: 33939.9062 - mean_absolute_error: 39.6308 - val_loss: 27970.6309 - val_mean_absolute_error: 40.3941\n",
      "Epoch 111/200\n",
      "49/49 - 1s - loss: 34816.4492 - mean_absolute_error: 41.2813 - val_loss: 25616.4551 - val_mean_absolute_error: 41.8882\n",
      "Epoch 112/200\n",
      "49/49 - 1s - loss: 33855.4414 - mean_absolute_error: 40.7801 - val_loss: 26457.5586 - val_mean_absolute_error: 38.9741\n",
      "Epoch 113/200\n",
      "49/49 - 1s - loss: 33698.8594 - mean_absolute_error: 40.7758 - val_loss: 26177.2676 - val_mean_absolute_error: 45.1781\n",
      "Epoch 114/200\n",
      "49/49 - 1s - loss: 35189.5195 - mean_absolute_error: 46.3532 - val_loss: 26251.0410 - val_mean_absolute_error: 41.7189\n",
      "Epoch 115/200\n",
      "49/49 - 1s - loss: 33904.5625 - mean_absolute_error: 42.5633 - val_loss: 25655.6621 - val_mean_absolute_error: 38.5046\n",
      "Epoch 116/200\n",
      "49/49 - 1s - loss: 33133.5078 - mean_absolute_error: 38.9678 - val_loss: 25449.5410 - val_mean_absolute_error: 38.1755\n",
      "Epoch 117/200\n",
      "49/49 - 1s - loss: 32856.1758 - mean_absolute_error: 39.8001 - val_loss: 26185.7344 - val_mean_absolute_error: 39.6037\n",
      "Epoch 118/200\n",
      "49/49 - 1s - loss: 33178.4805 - mean_absolute_error: 40.7119 - val_loss: 27403.4297 - val_mean_absolute_error: 45.8946\n",
      "Epoch 119/200\n",
      "49/49 - 1s - loss: 33364.8086 - mean_absolute_error: 43.4537 - val_loss: 26267.8770 - val_mean_absolute_error: 38.9817\n",
      "Epoch 120/200\n",
      "49/49 - 1s - loss: 32735.2988 - mean_absolute_error: 40.0271 - val_loss: 25858.9395 - val_mean_absolute_error: 41.0326\n",
      "Epoch 121/200\n",
      "49/49 - 1s - loss: 33075.4531 - mean_absolute_error: 40.6292 - val_loss: 29628.5176 - val_mean_absolute_error: 39.7227\n",
      "Epoch 122/200\n",
      "49/49 - 1s - loss: 33002.3398 - mean_absolute_error: 39.6205 - val_loss: 26264.4629 - val_mean_absolute_error: 39.7336\n",
      "Epoch 123/200\n",
      "49/49 - 1s - loss: 32201.0996 - mean_absolute_error: 39.9058 - val_loss: 25393.3926 - val_mean_absolute_error: 41.6623\n",
      "Epoch 124/200\n",
      "49/49 - 1s - loss: 31683.5195 - mean_absolute_error: 39.0409 - val_loss: 25947.5137 - val_mean_absolute_error: 35.6052\n",
      "Epoch 125/200\n",
      "49/49 - 1s - loss: 31452.5586 - mean_absolute_error: 38.6110 - val_loss: 26482.7578 - val_mean_absolute_error: 35.7999\n",
      "Epoch 126/200\n",
      "49/49 - 1s - loss: 30949.1641 - mean_absolute_error: 38.4387 - val_loss: 25887.4551 - val_mean_absolute_error: 38.8253\n",
      "Epoch 127/200\n",
      "49/49 - 1s - loss: 32856.1172 - mean_absolute_error: 41.1301 - val_loss: 24881.2031 - val_mean_absolute_error: 38.5605\n",
      "Epoch 128/200\n",
      "49/49 - 1s - loss: 31849.9746 - mean_absolute_error: 42.2959 - val_loss: 26392.4922 - val_mean_absolute_error: 37.7446\n",
      "Epoch 129/200\n",
      "49/49 - 1s - loss: 32096.5820 - mean_absolute_error: 42.0594 - val_loss: 24089.7090 - val_mean_absolute_error: 36.7118\n",
      "Epoch 130/200\n",
      "49/49 - 1s - loss: 31196.8027 - mean_absolute_error: 40.3032 - val_loss: 24798.6094 - val_mean_absolute_error: 38.0036\n",
      "Epoch 131/200\n",
      "49/49 - 1s - loss: 30693.4219 - mean_absolute_error: 40.0567 - val_loss: 27737.5293 - val_mean_absolute_error: 38.8256\n",
      "Epoch 132/200\n",
      "49/49 - 1s - loss: 31694.5234 - mean_absolute_error: 39.6335 - val_loss: 24982.2402 - val_mean_absolute_error: 49.2445\n",
      "Epoch 133/200\n",
      "49/49 - 1s - loss: 30744.0469 - mean_absolute_error: 42.1677 - val_loss: 25554.1562 - val_mean_absolute_error: 36.1777\n",
      "Epoch 134/200\n",
      "49/49 - 1s - loss: 30602.3867 - mean_absolute_error: 41.9254 - val_loss: 25715.7637 - val_mean_absolute_error: 37.9958\n",
      "Epoch 135/200\n",
      "49/49 - 1s - loss: 31333.9277 - mean_absolute_error: 38.8183 - val_loss: 27400.2383 - val_mean_absolute_error: 37.6870\n",
      "Epoch 136/200\n",
      "49/49 - 1s - loss: 30579.7812 - mean_absolute_error: 39.4677 - val_loss: 25665.9121 - val_mean_absolute_error: 42.5145\n",
      "Epoch 137/200\n",
      "49/49 - 1s - loss: 32419.8535 - mean_absolute_error: 45.4192 - val_loss: 28696.3594 - val_mean_absolute_error: 45.7734\n",
      "Epoch 138/200\n",
      "49/49 - 1s - loss: 33129.6094 - mean_absolute_error: 45.7379 - val_loss: 26336.0977 - val_mean_absolute_error: 37.1686\n",
      "Epoch 139/200\n",
      "49/49 - 1s - loss: 30999.4434 - mean_absolute_error: 39.2551 - val_loss: 24154.4844 - val_mean_absolute_error: 37.4822\n",
      "Epoch 140/200\n",
      "49/49 - 1s - loss: 29807.2891 - mean_absolute_error: 38.6168 - val_loss: 25360.5703 - val_mean_absolute_error: 41.7594\n",
      "Epoch 141/200\n",
      "49/49 - 1s - loss: 29227.7754 - mean_absolute_error: 40.0810 - val_loss: 23222.7715 - val_mean_absolute_error: 36.8194\n",
      "Epoch 142/200\n",
      "49/49 - 1s - loss: 29031.1309 - mean_absolute_error: 39.7628 - val_loss: 24535.8184 - val_mean_absolute_error: 35.5227\n",
      "Epoch 143/200\n",
      "49/49 - 1s - loss: 30529.6719 - mean_absolute_error: 40.6903 - val_loss: 25399.4746 - val_mean_absolute_error: 45.8011\n",
      "Epoch 144/200\n",
      "49/49 - 1s - loss: 30179.6270 - mean_absolute_error: 40.4948 - val_loss: 24131.2773 - val_mean_absolute_error: 37.4782\n",
      "Epoch 145/200\n",
      "49/49 - 1s - loss: 28957.1133 - mean_absolute_error: 38.2279 - val_loss: 25233.0508 - val_mean_absolute_error: 36.4764\n",
      "Epoch 146/200\n",
      "49/49 - 1s - loss: 30869.6660 - mean_absolute_error: 40.3153 - val_loss: 26084.7227 - val_mean_absolute_error: 43.1194\n",
      "Epoch 147/200\n",
      "49/49 - 1s - loss: 31005.2793 - mean_absolute_error: 42.2686 - val_loss: 22825.1992 - val_mean_absolute_error: 35.9813\n",
      "Epoch 148/200\n",
      "49/49 - 1s - loss: 29256.4863 - mean_absolute_error: 41.0520 - val_loss: 23267.0039 - val_mean_absolute_error: 35.2776\n",
      "Epoch 149/200\n",
      "49/49 - 1s - loss: 28471.9316 - mean_absolute_error: 39.2256 - val_loss: 25937.8594 - val_mean_absolute_error: 37.2516\n",
      "Epoch 150/200\n",
      "49/49 - 1s - loss: 28634.9336 - mean_absolute_error: 38.8211 - val_loss: 24890.7129 - val_mean_absolute_error: 35.0936\n",
      "Epoch 151/200\n",
      "49/49 - 1s - loss: 28337.2324 - mean_absolute_error: 37.5852 - val_loss: 24089.4297 - val_mean_absolute_error: 37.2079\n",
      "Epoch 152/200\n",
      "49/49 - 1s - loss: 28187.0176 - mean_absolute_error: 36.9936 - val_loss: 24814.4727 - val_mean_absolute_error: 40.4545\n",
      "Epoch 153/200\n",
      "49/49 - 1s - loss: 28340.2617 - mean_absolute_error: 39.9251 - val_loss: 23995.6641 - val_mean_absolute_error: 39.0432\n",
      "Epoch 154/200\n",
      "49/49 - 1s - loss: 27731.8086 - mean_absolute_error: 37.0798 - val_loss: 24639.2949 - val_mean_absolute_error: 35.3526\n",
      "Epoch 155/200\n",
      "49/49 - 1s - loss: 28105.6289 - mean_absolute_error: 39.3885 - val_loss: 22526.7988 - val_mean_absolute_error: 34.5974\n",
      "Epoch 156/200\n",
      "49/49 - 1s - loss: 27380.9570 - mean_absolute_error: 37.3607 - val_loss: 22832.3066 - val_mean_absolute_error: 36.9320\n",
      "Epoch 157/200\n",
      "49/49 - 1s - loss: 27308.7949 - mean_absolute_error: 36.7075 - val_loss: 22721.1406 - val_mean_absolute_error: 33.5135\n",
      "Epoch 158/200\n",
      "49/49 - 1s - loss: 26995.4766 - mean_absolute_error: 41.5480 - val_loss: 24116.1504 - val_mean_absolute_error: 46.1793\n",
      "Epoch 159/200\n",
      "49/49 - 1s - loss: 26399.2070 - mean_absolute_error: 36.3912 - val_loss: 23169.7773 - val_mean_absolute_error: 39.7090\n",
      "Epoch 160/200\n",
      "49/49 - 1s - loss: 26581.7910 - mean_absolute_error: 38.5476 - val_loss: 23149.6875 - val_mean_absolute_error: 34.7325\n",
      "Epoch 161/200\n",
      "49/49 - 1s - loss: 27136.8613 - mean_absolute_error: 37.0013 - val_loss: 24946.2441 - val_mean_absolute_error: 34.0451\n",
      "Epoch 162/200\n",
      "49/49 - 1s - loss: 27301.0605 - mean_absolute_error: 37.7379 - val_loss: 23850.9512 - val_mean_absolute_error: 36.4690\n",
      "Epoch 163/200\n",
      "49/49 - 1s - loss: 27162.2598 - mean_absolute_error: 37.8552 - val_loss: 23414.0469 - val_mean_absolute_error: 36.1322\n",
      "Epoch 164/200\n",
      "49/49 - 1s - loss: 27471.8887 - mean_absolute_error: 37.5604 - val_loss: 26807.3828 - val_mean_absolute_error: 35.6753\n",
      "Epoch 165/200\n",
      "49/49 - 1s - loss: 28110.8203 - mean_absolute_error: 38.8120 - val_loss: 23743.9004 - val_mean_absolute_error: 34.8663\n",
      "Epoch 166/200\n",
      "49/49 - 1s - loss: 26544.1328 - mean_absolute_error: 35.9768 - val_loss: 23027.0000 - val_mean_absolute_error: 37.8313\n",
      "Epoch 167/200\n",
      "49/49 - 1s - loss: 25591.8105 - mean_absolute_error: 36.6954 - val_loss: 22693.8105 - val_mean_absolute_error: 32.9638\n",
      "Epoch 168/200\n",
      "49/49 - 1s - loss: 25693.6152 - mean_absolute_error: 36.6024 - val_loss: 24632.6191 - val_mean_absolute_error: 48.3900\n",
      "Epoch 169/200\n",
      "49/49 - 1s - loss: 25764.6230 - mean_absolute_error: 36.8809 - val_loss: 22698.8809 - val_mean_absolute_error: 37.2924\n",
      "Epoch 170/200\n",
      "49/49 - 1s - loss: 26868.6914 - mean_absolute_error: 42.0230 - val_loss: 25547.4336 - val_mean_absolute_error: 39.8155\n",
      "Epoch 171/200\n",
      "49/49 - 1s - loss: 26149.1270 - mean_absolute_error: 37.6711 - val_loss: 24760.5430 - val_mean_absolute_error: 39.3774\n",
      "Epoch 172/200\n",
      "49/49 - 1s - loss: 24833.9727 - mean_absolute_error: 35.7023 - val_loss: 25561.2227 - val_mean_absolute_error: 34.9026\n",
      "Epoch 173/200\n",
      "49/49 - 1s - loss: 26612.7520 - mean_absolute_error: 36.8706 - val_loss: 25266.5547 - val_mean_absolute_error: 40.8647\n",
      "Epoch 174/200\n",
      "49/49 - 1s - loss: 26808.8711 - mean_absolute_error: 37.8961 - val_loss: 23518.9141 - val_mean_absolute_error: 35.7691\n",
      "Epoch 175/200\n",
      "49/49 - 1s - loss: 25191.7852 - mean_absolute_error: 37.3044 - val_loss: 24639.7832 - val_mean_absolute_error: 40.7379\n",
      "Epoch 176/200\n",
      "49/49 - 1s - loss: 25932.5273 - mean_absolute_error: 36.3970 - val_loss: 23443.5078 - val_mean_absolute_error: 35.7206\n",
      "Epoch 177/200\n",
      "49/49 - 1s - loss: 24744.8594 - mean_absolute_error: 35.8573 - val_loss: 24562.0801 - val_mean_absolute_error: 35.8991\n",
      "Epoch 178/200\n",
      "49/49 - 1s - loss: 27574.8711 - mean_absolute_error: 36.3381 - val_loss: 22789.3418 - val_mean_absolute_error: 35.5090\n",
      "Epoch 179/200\n",
      "49/49 - 1s - loss: 25359.6621 - mean_absolute_error: 35.9986 - val_loss: 22850.8496 - val_mean_absolute_error: 34.8814\n",
      "Epoch 180/200\n",
      "49/49 - 1s - loss: 24510.9258 - mean_absolute_error: 38.1596 - val_loss: 22915.3594 - val_mean_absolute_error: 33.6397\n",
      "Epoch 181/200\n",
      "49/49 - 1s - loss: 25564.8672 - mean_absolute_error: 38.2931 - val_loss: 24769.5957 - val_mean_absolute_error: 34.6528\n",
      "Epoch 182/200\n",
      "49/49 - 1s - loss: 23856.5156 - mean_absolute_error: 34.6430 - val_loss: 26275.3223 - val_mean_absolute_error: 40.9487\n",
      "Epoch 183/200\n",
      "49/49 - 1s - loss: 26014.4004 - mean_absolute_error: 38.9813 - val_loss: 29968.5332 - val_mean_absolute_error: 34.7779\n",
      "Epoch 184/200\n",
      "49/49 - 1s - loss: 29061.4102 - mean_absolute_error: 37.9216 - val_loss: 24849.6152 - val_mean_absolute_error: 40.2937\n",
      "Epoch 185/200\n",
      "49/49 - 1s - loss: 25406.0918 - mean_absolute_error: 37.3458 - val_loss: 24876.1953 - val_mean_absolute_error: 34.8737\n",
      "Epoch 186/200\n",
      "49/49 - 1s - loss: 24934.2988 - mean_absolute_error: 36.9569 - val_loss: 24501.8496 - val_mean_absolute_error: 33.9567\n",
      "Epoch 187/200\n",
      "49/49 - 1s - loss: 25083.8301 - mean_absolute_error: 36.9777 - val_loss: 27278.6270 - val_mean_absolute_error: 36.0373\n",
      "Epoch 188/200\n",
      "49/49 - 1s - loss: 25110.5801 - mean_absolute_error: 40.4854 - val_loss: 26701.1211 - val_mean_absolute_error: 39.4678\n",
      "Epoch 189/200\n",
      "49/49 - 1s - loss: 25804.4082 - mean_absolute_error: 38.6555 - val_loss: 26082.8867 - val_mean_absolute_error: 34.9952\n",
      "Epoch 190/200\n",
      "49/49 - 1s - loss: 24057.8770 - mean_absolute_error: 37.6970 - val_loss: 24791.8965 - val_mean_absolute_error: 32.8548\n",
      "Epoch 191/200\n",
      "49/49 - 1s - loss: 23588.2246 - mean_absolute_error: 35.2783 - val_loss: 25955.0938 - val_mean_absolute_error: 38.3070\n",
      "Epoch 192/200\n",
      "49/49 - 1s - loss: 24612.4277 - mean_absolute_error: 40.0504 - val_loss: 25332.1484 - val_mean_absolute_error: 37.4591\n",
      "Epoch 193/200\n",
      "49/49 - 1s - loss: 23370.7773 - mean_absolute_error: 35.6730 - val_loss: 25459.9102 - val_mean_absolute_error: 35.1290\n",
      "Epoch 194/200\n",
      "49/49 - 1s - loss: 23278.3027 - mean_absolute_error: 35.7838 - val_loss: 24765.7422 - val_mean_absolute_error: 32.7091\n",
      "Epoch 195/200\n",
      "49/49 - 1s - loss: 22729.3477 - mean_absolute_error: 36.9179 - val_loss: 25417.2793 - val_mean_absolute_error: 44.3687\n",
      "Epoch 196/200\n",
      "49/49 - 1s - loss: 24524.3359 - mean_absolute_error: 40.1292 - val_loss: 25804.2500 - val_mean_absolute_error: 38.8848\n",
      "Epoch 197/200\n",
      "49/49 - 1s - loss: 24522.0898 - mean_absolute_error: 38.7464 - val_loss: 24234.1055 - val_mean_absolute_error: 44.0700\n",
      "Epoch 198/200\n",
      "49/49 - 1s - loss: 24234.8984 - mean_absolute_error: 39.7441 - val_loss: 27038.6582 - val_mean_absolute_error: 35.7188\n",
      "Epoch 199/200\n",
      "49/49 - 1s - loss: 23654.2246 - mean_absolute_error: 38.4504 - val_loss: 25438.7383 - val_mean_absolute_error: 44.8754\n",
      "Epoch 200/200\n",
      "49/49 - 1s - loss: 22793.2832 - mean_absolute_error: 36.4153 - val_loss: 25154.8574 - val_mean_absolute_error: 34.4228\n",
      "CPU times: user 7min 3s, sys: 1min 6s, total: 8min 10s\n",
      "Wall time: 4min 32s\n"
     ]
    }
   ],
   "source": [
    "%time history = model.fit(train_data, target, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,  verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8921050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329/329 [==============================] - 4s 13ms/step - loss: 22743.1562 - mean_absolute_error: 31.4702\n",
      "Test MAE: 31.47\n"
     ]
    }
   ],
   "source": [
    "_, mae = model.evaluate(test_data, test_target, verbose=1)\n",
    "print(f\"Test MAE: {round(mae, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91acfb59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
