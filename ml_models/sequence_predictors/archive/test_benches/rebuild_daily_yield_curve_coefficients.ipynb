{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook corresponds to the cloud function: `train_daily_yield_curve`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae316da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'creds.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51c60b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import redis\n",
    "import pickle\n",
    "\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e8e740",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUSINESS_DAY = CustomBusinessDay(calendar=USFederalHolidayCalendar())    # used to skip over holidays when adding or subtracting business days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613c1302",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_DATE = '2023-06-16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fc5258",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'eng-reactor-287421'\n",
    "DATASET_NAME = 'yield_curves_v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a94a0afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_ID_MODEL = f'{PROJECT_ID}.{DATASET_NAME}.nelson_siegel_coef_daily'\n",
    "TABLE_ID_SCALER = f'{PROJECT_ID}.{DATASET_NAME}.standardscaler_parameters_daily' \n",
    "\n",
    "sp_index_tables = ['sp_12_22_year_national_amt_free_index',\n",
    "                   'sp_15plus_year_national_amt_free_index',\n",
    "                   'sp_7_12_year_national_amt_free_municipal_bond_index_yield',\n",
    "                   'sp_muni_high_quality_index_yield',\n",
    "                   'sp_high_quality_intermediate_managed_amt_free_municipal_bond_index_yield',\n",
    "                   'sp_high_quality_short_intermediate_municipal_bond_index_yield',\n",
    "                   'sp_high_quality_short_municipal_bond_index_yield',\n",
    "                   'sp_long_term_national_amt_free_municipal_bond_index_yield']\n",
    "\n",
    "sp_maturity_tables = ['sp_12_22_year_national_amt_free_index',\n",
    "                      'sp_15plus_year_national_amt_free_index',\n",
    "                      'sp_7_12_year_national_amt_free_index',\n",
    "                      'sp_high_quality_index',\n",
    "                      'sp_high_quality_intermediate_managed_amt_free_index',\n",
    "                      'sp_high_quality_short_intermediate_index',\n",
    "                      'sp_high_quality_short_index',\n",
    "                      'sp_long_term_national_amt_free_municipal_bond_index_yield']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aabeb8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index_data() -> pd.DataFrame:\n",
    "    '''This function load the S&P index data into a single dataframe. The output of the function is a \n",
    "    dataframe containing the yield to worst of all the indices in a single dataframe.'''\n",
    "    index_data = [] \n",
    "    for table in sp_index_tables:\n",
    "        query = f'''SELECT * FROM `eng-reactor-287421.spBondIndex.{table}` order by date desc limit 1'''    # takes the most recent `date` which refers to the date on which we grabbed the value from S&P\n",
    "        df = pd.read_gbq(query, project_id=PROJECT_ID, dialect='standard')\n",
    "        df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "        df['ytw'] = df['ytw'] * 100    # convert to basis points\n",
    "        df = df.drop_duplicates('date')    # TODO: does this line do anything? perhaps not because we set the limit to 1 in `query`\n",
    "        df.set_index('date', inplace=True, drop=True)    # `drop=True` removes the column that is to be used as the index\n",
    "        index_data.append(df)\n",
    "    \n",
    "    df = pd.concat(index_data, axis=1)\n",
    "    df.columns = sp_maturity_tables    # the reason that this is `sp_maturity_tables` instead of `sp_index_tables` is because the returned `df` from `load_index_data()` needs to correspond with that from `load_maturity_data()`\n",
    "    df.ffill(inplace=True, axis=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b61452c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_maturity_data() -> pd.DataFrame:\n",
    "    '''This function loads the S&P maturity data into a single dataframe. The output of the function is a dataframe containing the \n",
    "    weighted average maturities of all the indices in a single dataframe.'''\n",
    "    maturity_data  = []\n",
    "    for table in sp_maturity_tables:\n",
    "        query = f'SELECT * FROM `eng-reactor-287421.spBondIndexMaturities.{table}` order by effectivedate desc limit 1;'    # takes the most recent `effectivedate` which refers to the date on which we grabbed the value from S&P\n",
    "        df = pd.read_gbq(query, project_id=PROJECT_ID, dialect='standard')        \n",
    "        df['effectivedate'] = pd.to_datetime(df['effectivedate'], format='%Y-%m-%d')\n",
    "        df = df.drop_duplicates('effectivedate')    # TODO: does this line do anything? perhaps not because we set the limit to 1 in `query`\n",
    "        df.set_index('effectivedate', inplace=True, drop=True)    # `drop=True` removes the column that is to be used as the index\n",
    "        df = df[['weightedAverageMaturity']]\n",
    "        maturity_data.append(df) \n",
    "        \n",
    "    df = pd.concat(maturity_data, axis=1)\n",
    "    df.columns = sp_maturity_tables\n",
    "    df.ffill(inplace=True, axis=0)    # fills NaN values; TODO: is this necessary? shouldn't it always be the case that we do not haev NaN values?\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b93f986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maturity_dict(maturity_df: pd.DataFrame, date:str) -> dict:\n",
    "    '''This function creates a dictonary with the index name being the key and the weighted average maturities as the values.'''\n",
    "    temp_df = maturity_df.loc[date].T\n",
    "    temp_dict = dict(zip(temp_df.index, temp_df.values))\n",
    "    return temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdffade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yield_curve_maturity_df(index_data: pd.DataFrame, date: str, maturity_dict: dict) -> pd.DataFrame:\n",
    "    '''This function creates a dataframe that contains the yield to worst and weighted average maturity for a specific date.'''\n",
    "    df = pd.DataFrame(index_data.loc[date])\n",
    "    df.columns = ['ytw']\n",
    "    df['Weighted_Maturity'] = df.index.map(maturity_dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfaaab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_transformation(t: np.array, L: float):\n",
    "    '''This function takes a numpy array of maturities and a shape parameter. \n",
    "    It returns the exponential function calculated from those values.'''\n",
    "    return L*(1-np.exp(-t/L))/t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "793ef580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laguerre_transformation(t: np.array, L: float):\n",
    "    '''This function takes a numpy array of maturities and a shape parameter. \n",
    "    It returns the laguerre function calculated from those values.'''\n",
    "    return (L*(1-np.exp(-t/L))/t) -np.exp(-t/L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52305c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_inputs(yield_curve_maturity_df: pd.DataFrame, L: int):\n",
    "    '''This function creates the inputs for the regression model.\n",
    "    The inputs are created using the exponential and laguerre transform.'''\n",
    "    temp_df = yield_curve_maturity_df.copy()\n",
    "    temp_df['X1'] = decay_transformation(temp_df['Weighted_Maturity'], L)\n",
    "    temp_df['X2'] = laguerre_transformation(temp_df['Weighted_Maturity'], L)\n",
    "    \n",
    "    X = temp_df[['X1', 'X2']]\n",
    "    y = temp_df['ytw']\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dea37ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X: np.array, Y: float):\n",
    "    '''This function train a regression model to estimate the Nelson-Siegel coefficients.'''\n",
    "    scaler = StandardScaler()    # used to set the mean and std dev to 0 and 1 respectively of a dataset; `scalar` stores the mean and standard deviation as attributes\n",
    "    X = scaler.fit_transform(X)\n",
    "    model = Ridge(alpha=0.001, random_state = 1).fit(X , Y)\n",
    "    return scaler, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc22c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSchema_model():\n",
    "    '''This function returns the schema required for the bigquery table storing the nelson siegel coefficients.'''\n",
    "    schema = [bigquery.SchemaField('date', 'DATE', 'REQUIRED'),\n",
    "              bigquery.SchemaField('const', 'FLOAT', 'REQUIRED'),\n",
    "              bigquery.SchemaField('exponential', 'FLOAT', 'REQUIRED'),\n",
    "              bigquery.SchemaField('laguerre', 'FLOAT', 'REQUIRED')]\n",
    "    return schema\n",
    "\n",
    "\n",
    "def getSchema_scaler():\n",
    "    '''This function returns the schema required for the bigquery table storing the sklearn StandardScaler's parameters siegel coefficients.'''\n",
    "    schema = [bigquery.SchemaField('date', 'DATE', 'REQUIRED'),\n",
    "              bigquery.SchemaField('exponential_mean','FLOAT', 'REQUIRED'),\n",
    "              bigquery.SchemaField('exponential_std','FLOAT', 'REQUIRED'),\n",
    "              bigquery.SchemaField('laguerre_mean','FLOAT', 'REQUIRED'),\n",
    "              bigquery.SchemaField('laguerre_std','FLOAT', 'REQUIRED')]\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24bd82b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_data(df: pd.DataFrame, table_id: str):\n",
    "    '''This function upload the coefficient and scalar dataframe to BigQuery. `table_id` is the path of the bigquery table to upload to.'''\n",
    "    client = bigquery.Client()\n",
    "    \n",
    "    if table_id == TABLE_ID_MODEL:\n",
    "        schema = getSchema_model()\n",
    "    elif table_id == TABLE_ID_SCALER:\n",
    "        schema = getSchema_scaler()\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(schema=schema, write_disposition='WRITE_APPEND')\n",
    "    job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
    "    try:\n",
    "        job.result()\n",
    "        print(f'Upload Successful to {table_id}')\n",
    "    except Exception as e:\n",
    "        print(f'Failed to Upload to {table_id}')\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5202f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shape_parameter() -> float:\n",
    "    '''This function grabs the latest shape parameters for the Nelson Siegel.'''\n",
    "    query = f'''SELECT L FROM `{PROJECT_ID}.{DATASET_NAME}.shape_parameters` ORDER BY date DESC LIMIT 1'''    # takes the most recent `date` which refers to the effective date from S&P\n",
    "    df = pd.read_gbq(query, project_id=PROJECT_ID, dialect='standard')\n",
    "    return df.loc[0].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1c32cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    '''Grab the index YTW values and the weighted average duration, and calculate the Nelson-Siegel coefficients. \n",
    "    Store the Nelson-Siegel coefficients in BigQuery and Redis.'''\n",
    "    maturity_data = load_maturity_data()\n",
    "    index_data = load_index_data()\n",
    "    L = load_shape_parameter()\n",
    "\n",
    "    coefficient_df = pd.DataFrame()\n",
    "    scaler_df = pd.DataFrame()\n",
    "    for target_date in list(maturity_data.index.astype(str)):    # `for` loop runs only once and it is when `target_date == TARGET_DATE`; TODO: refactor this code to remove the `for` loop\n",
    "        if target_date != TARGET_DATE: continue\n",
    "        print(f'Calculating the coefficients for {target_date}')\n",
    "        maturity_dict = get_maturity_dict(maturity_data, target_date)\n",
    "        yield_curve_maturity_df = get_yield_curve_maturity_df(index_data, target_date, maturity_dict)\n",
    "\n",
    "        # creating the inputs for the model\n",
    "        X, Y = get_model_inputs(yield_curve_maturity_df, L)\n",
    "        scaler, model = train_model(X, Y)\n",
    "\n",
    "        # retrieve model parameters\n",
    "        const = model.intercept_\n",
    "        exponential = model.coef_[0]\n",
    "        laguerre = model.coef_[1]\n",
    "\n",
    "        # retrieve scaler parameters, used to standardize the data\n",
    "        exponential_mean = scaler.mean_[0]\n",
    "        exponential_std = np.sqrt(scaler.var_[0])\n",
    "        laguerre_mean = scaler.mean_[1]\n",
    "        laguerre_std = np.sqrt(scaler.var_[1])\n",
    "\n",
    "        # convert date to datetime object\n",
    "        date = pd.to_datetime(target_date).date()    # pd.to_datetime(datetime.datetime.now()).date()\n",
    "\n",
    "        temp_coefficient_df = pd.DataFrame({'date': date, \n",
    "                                            'const': const, \n",
    "                                            'exponential': exponential, \n",
    "                                            'laguerre': laguerre}, index=[0])\n",
    "\n",
    "        temp_scaler_df = pd.DataFrame({'date': date, \n",
    "                                       'exponential_mean': exponential_mean, \n",
    "                                       'exponential_std': exponential_std, \n",
    "                                       'laguerre_mean': laguerre_mean, \n",
    "                                       'laguerre_std': laguerre_std}, index=[0])\n",
    "\n",
    "        coefficient_df = coefficient_df.append(temp_coefficient_df)\n",
    "        scaler_df = scaler_df.append(temp_scaler_df)    \n",
    "    print(f'Uploading Data to {TABLE_ID_MODEL} and {TABLE_ID_SCALER}')\n",
    "    upload_data(coefficient_df, TABLE_ID_MODEL) \n",
    "    upload_data(scaler_df, TABLE_ID_SCALER)\n",
    "\n",
    "    print('Uploading data to redis')\n",
    "    string_date = date\n",
    "    string_date = string_date + (BUSINESS_DAY * 1)    # S&P publishes its indices at the end of the business day, so if a trade occurs on day 1, it should use the S&P indices published on day 0 and not day 1, therefore we add a business day to the S&P effective date so that we use the correct indices \n",
    "    string_date = string_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    coefficient_df.reset_index(inplace=True, drop=True)\n",
    "    scaler_df.reset_index(inplace=True, drop=True)\n",
    "    nelson_values = coefficient_df.set_index('date')\n",
    "    scalar_values = scaler_df.set_index('date')\n",
    "\n",
    "    temp_dict = {'nelson_values': nelson_values, 'scalar_values': scalar_values, 'shape_parameter': L}\n",
    "    redis_client = redis.Redis(host='10.227.69.60', port=6379, db=0)    # this `redis_client` is used in `ficc/app_engine/demo/server/modules/ficc/utils/yc_data.py` and possibly elsewhere\n",
    "    value = pickle.dumps(temp_dict, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    redis_client.set(string_date, value)\n",
    "    \n",
    "    return 'SUCCESS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e9f21ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the coefficients for 2023-06-16\n",
      "Uploading Data\n",
      "Upload Successful\n",
      "Upload Successful\n",
      "Uploading data to redis\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SUCCESS'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main('test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
