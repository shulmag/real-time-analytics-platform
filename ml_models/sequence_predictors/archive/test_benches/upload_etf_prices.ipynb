{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook corresponds to the cloud function: `update_daily_etf_prices`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ec15a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'ahmad_creds.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'EZR0IHAAL6MFWX4B'\n",
    "\n",
    "etfs = ['HYD', 'HYMB', 'IBMJ', 'IBMK', 'IBML', 'ITM', 'MLN', 'MUB', 'PZA', 'SHM', 'SHYD', 'SMB', 'SUB', 'TFI', 'VTEB', 'FMHI', 'MMIN']    # ETFs that we are using for our models\n",
    "\n",
    "PROJECT_ID = 'eng-reactor-287421'\n",
    "SP_ETF_DAILY_DATASET = 'ETF_daily_alphavantage'\n",
    "BQ_PROJECT_DATASET = 'eng-reactor-287421.ETF_daily_alphavantage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71e5f910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_daily_etf_prices_bq():\n",
    "    '''Loads the maturity data from the specified bigquery tables in the global etfs list and returns a dictionary \n",
    "    with keys corresponding to the ETF names.'''\n",
    "    etf_data  = {}\n",
    "    \n",
    "    for table_name in etfs:\n",
    "        query = f'''SELECT * FROM {SP_ETF_DAILY_DATASET}.{table_name}  ORDER BY Date DESC LIMIT 1 '''    # takes the most recent `date` which refers to the date coming from AlphaVantage\n",
    "        df = pd.read_gbq(query, project_id=PROJECT_ID, dialect='standard')\n",
    "        \n",
    "        df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "        df.sort_values('Date', inplace=True)\n",
    "        df.set_index('Date', inplace=True, drop=True)    # `drop=True` removes the column that is to be used as the index\n",
    "        etf_data[table_name] = df \n",
    "        \n",
    "    assert list(etf_data.keys()) == etfs\n",
    "    return etf_data\n",
    "\n",
    "\n",
    "def download_daily_prices():\n",
    "    '''Downloads the daily ETF price data from the AlphaVantage API, and saves it to a dictionary of dataframes.'''\n",
    "    dataframes = {}\n",
    "    for etf in etfs: \n",
    "        print(f'Downloading daily ETF prices for etf: {etf}')\n",
    "        url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&outputsize=full&symbol={etf}&apikey={API_KEY}&adjusted=False'\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        df = pd.DataFrame(data['Time Series (Daily)']).T\n",
    "\n",
    "        for col in df:\n",
    "            df[col] = df[col].astype(float)\n",
    "\n",
    "        df.index.rename('Date', inplace=True)\n",
    "        df = df.rename({'1. open': 'Open', '2. high': 'High', '3. low': 'Low', '4. close': 'Close', '5. volume': 'Volume'}, axis=1)\n",
    "        df.columns = df.columns + '_' + etf\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        dataframes[etf] = df\n",
    "\n",
    "        time.sleep(15)        # have `time.sleep(15)` so that we do not want to keep hitting the AlphaVantage API, otherwise they will block our access\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "def get_col_names(df: pd.DataFrame):\n",
    "    '''Retrieves each of columns in `df`. The columns of each ETF's dataframe have a naming prefix (ie Open_MUB, Close_MUB, etc).'''\n",
    "    return (df.filter(regex='Open').columns[0], \n",
    "            df.filter(regex='Close').columns[0], \n",
    "            df.filter(regex='Volume').columns[0],\n",
    "            df.filter(regex='High').columns[0],\n",
    "            df.filter(regex='Low').columns[0])\n",
    "\n",
    "\n",
    "def get_schema(Open: str, Close: str, Volume: str, High: str, Low: str):\n",
    "    '''Returns the schema of the bigquery table for each ETF. The names of the Open, Close, Volume, High and Low columns are taken \n",
    "    as input because they are prefixed with the ETF name.'''\n",
    "    job_config = bigquery.LoadJobConfig(schema=[bigquery.SchemaField('Date', bigquery.enums.SqlTypeNames.DATE),\n",
    "                                                bigquery.SchemaField(Open, bigquery.enums.SqlTypeNames.FLOAT),\n",
    "                                                bigquery.SchemaField(Close, bigquery.enums.SqlTypeNames.FLOAT),\n",
    "                                                bigquery.SchemaField(Volume, bigquery.enums.SqlTypeNames.FLOAT),\n",
    "                                                bigquery.SchemaField(High, bigquery.enums.SqlTypeNames.FLOAT),\n",
    "                                                bigquery.SchemaField(Low, bigquery.enums.SqlTypeNames.FLOAT)],\n",
    "                                        write_disposition='WRITE_APPEND')\n",
    "    return job_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    '''First download the bigquery tables of existing ETF price data, then download the daily data from AlphaVantage. \n",
    "    Afterwards, check if the observations in the downloaded data are in bigquery: if they are, we make a note and print \n",
    "    those with data already available, and if not, upload that data to bigquery.'''\n",
    "    bq_data = load_daily_etf_prices_bq()\n",
    "    daily_data = download_daily_prices()\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    excluded = []    # list to keep track of ETFs that might already have today's data to avoid duplicates\n",
    "    for name, df in daily_data.items():    # for each ETF in the data downloaded from AlphaVantage, check if it is already available, then upload if needed\n",
    "        print(name)\n",
    "        df = df.sort_index(ascending=True)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        data_last_date = str(df.index[-1].date())    # date of the most recent entry in the downloaded data \n",
    "        \n",
    "        dates = pd.to_datetime(bq_data[name].index)\n",
    "        last_date = str(dates[0].date())    # date of the most recent entry in the bigquery table \n",
    "        \n",
    "        if data_last_date == last_date:    # if the data is already available, then we move on to the next etf\n",
    "            excluded.append(name)\n",
    "            continue\n",
    "\n",
    "        df = df.loc[data_last_date:]    # if the data is not available, we get all the data from the last date as a dataframe\n",
    "        \n",
    "        # retrieve the column names and schema, and then upload to Bigquery\n",
    "        Open, Close, Volume, High, Low = get_col_names(df)\n",
    "        job_config = get_schema(Open, Close, Volume, High, Low)\n",
    "        table_id = BQ_PROJECT_DATASET + '.' + name\n",
    "        df = df.reset_index(drop=False).sort_values(by='Date')\n",
    "        job = client.load_table_from_dataframe(df, table_id, job_config=job_config)  \n",
    "        job.result()\n",
    "\n",
    "    if excluded:\n",
    "        return f'Data for {excluded} already available'\n",
    "    else:\n",
    "        return 'Upload Successful for all ETFs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "734c8fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYD\n",
      "HYMB\n",
      "IBMJ\n",
      "IBMK\n",
      "IBML\n",
      "IBMM\n",
      "ITM\n",
      "MLN\n",
      "MUB\n",
      "PZA\n",
      "SHM\n",
      "SHYD\n",
      "SMB\n",
      "SUB\n",
      "TFI\n",
      "VTEB\n",
      "FMHI\n",
      "MMIN\n",
      "HYD\n",
      "HYMB\n",
      "IBMJ\n",
      "IBMK\n",
      "IBML\n",
      "IBMM\n",
      "ITM\n",
      "MLN\n",
      "MUB\n",
      "PZA\n",
      "SHM\n",
      "SHYD\n",
      "SMB\n",
      "SUB\n",
      "TFI\n",
      "VTEB\n",
      "FMHI\n",
      "MMIN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Data for ['IBMJ', 'IBMK'] already available\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924f9ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
