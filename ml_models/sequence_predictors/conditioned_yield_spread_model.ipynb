{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring yield spread models conditioned on the calc-date\n",
    "\n",
    "## Import all required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ficc.utils.auxiliary_variables import PREDICTORS, IDENTIFIERS, CATEGORICAL_FEATURES, NON_CAT_FEATURES, BINARY\n",
    "from ficc.models import get_model_instance\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "\n",
    "import shutil\n",
    "\n",
    "import wandb\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed the random-number generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the random seed, for consistency\n",
    "SEED = 10\n",
    "\n",
    "pl.utilities.seed.seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 5\n",
    "NUM_FEATURES = 5\n",
    "NUM_EPOCHS = 1500\n",
    "BATCH_SIZE = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_data.pkl', 'rb') as f:\n",
    "    df = pd.read_pickle(f)\n",
    "\n",
    "df = df[~df.purpose_sub_class.isin([6, 20, 21, 22, 44, 57, 90, 106])]\n",
    "df = df[~df.called_redemption_type.isin([18, 19])]\n",
    "\n",
    "# Add additional features\n",
    "processed_data = df[IDENTIFIERS + PREDICTORS + ['trade_datetime']]\n",
    "\n",
    "# A few features such as the initial issue amount cannot be filled with their logical counterparts as their values are not known and hence are dropped.\n",
    "processed_data = processed_data.dropna()\n",
    "unprocessed_data = processed_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the date into train and test set\n",
    "test_dataframe = processed_data[processed_data.trade_datetime >= '2022-05-21']\n",
    "train_dataframe = processed_data[processed_data.trade_datetime < '2022-05-21']\n",
    "unprocessed_test_df = unprocessed_data[unprocessed_data.trade_datetime >= '2022-05-21']\n",
    "\n",
    "# Split the training data in to train and validation set\n",
    "val_dataframe = train_dataframe[train_dataframe.trade_datetime >= '2022-05-07']\n",
    "train_dataframe = train_dataframe[train_dataframe.trade_datetime < '2022-05-07']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Uniform normalization\n",
    "normalizers = []\n",
    "for f in NON_CAT_FEATURES + BINARY:\n",
    "    normalizers.append(preprocessing.StandardScaler())\n",
    "    processed_data.loc[:, f] = normalizers[-1].fit_transform(processed_data[f].to_numpy().astype('float32').reshape(-1, 1))\n",
    "\n",
    "# Fitting encoders to the categorical features. These encoders are then used to encode the categorical features of the train and test set\n",
    "encoders = {}\n",
    "fmax = {}\n",
    "for f in CATEGORICAL_FEATURES + ['calc_day_cat']:\n",
    "    fprep = preprocessing.LabelEncoder().fit(\n",
    "        processed_data[f].drop_duplicates())\n",
    "    fmax[f] = np.max(fprep.transform(fprep.classes_))\n",
    "    encoders[f] = fprep\n",
    "\n",
    "def create_input(df, with_cdc=False):\n",
    "    sdf = df[IDENTIFIERS + PREDICTORS]\n",
    "\n",
    "    global encoders\n",
    "    datalist = []\n",
    "    datalist.append(torch.tensor(np.stack(sdf['trade_history'])).float())\n",
    "\n",
    "    noncat_and_binary = []\n",
    "    for f in NON_CAT_FEATURES + BINARY:\n",
    "        noncat_and_binary.append(np.expand_dims(\n",
    "            sdf[f].to_numpy().astype('float32'), axis=1))\n",
    "    datalist.append(torch.tensor(np.concatenate(noncat_and_binary, axis=-1)))\n",
    "\n",
    "    cat_features = CATEGORICAL_FEATURES if not with_cdc else CATEGORICAL_FEATURES + ['calc_day_cat']\n",
    "    for f in cat_features:\n",
    "        encoded = encoders[f].transform(sdf[f])\n",
    "        datalist.append(torch.tensor(encoded).long())\n",
    "\n",
    "    return datalist\n",
    "\n",
    "def create_label(df):\n",
    "    sdf = df[IDENTIFIERS + PREDICTORS]\n",
    "    return torch.tensor(sdf.yield_spread.to_numpy()).float()\n",
    "\n",
    "# Create all the datasets and dataloaders, appending _with_cdc to the name to indicate if the calc-day categorical feature is included\n",
    "train_ds = TensorDataset(*create_input(train_dataframe), create_label(train_dataframe))\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, num_workers=os.cpu_count())\n",
    "train_ds_with_cdc = TensorDataset(*create_input(train_dataframe, with_cdc=True), create_label(train_dataframe))\n",
    "train_loader_with_cdc = DataLoader(train_ds_with_cdc, batch_size=BATCH_SIZE, num_workers=os.cpu_count(), persistent_workers=True)\n",
    "\n",
    "val_ds = TensorDataset(*create_input(val_dataframe), create_label(val_dataframe))\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, num_workers=os.cpu_count())\n",
    "val_ds_with_cdc = TensorDataset(*create_input(val_dataframe, with_cdc=True), create_label(val_dataframe))\n",
    "val_loader_with_cdc = DataLoader(val_ds_with_cdc, batch_size=BATCH_SIZE, num_workers=os.cpu_count(), persistent_workers=True)\n",
    "\n",
    "test_ds = TensorDataset(*create_input(test_dataframe), create_label(test_dataframe))\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, num_workers=os.cpu_count())\n",
    "test_ds_with_cdc = TensorDataset(*create_input(test_dataframe, with_cdc=True), create_label(test_dataframe))\n",
    "test_loader_with_cdc = DataLoader(test_ds_with_cdc, batch_size=BATCH_SIZE, num_workers=os.cpu_count(), persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'num_trade_history_features': NUM_FEATURES,\n",
    "    'non_categorical_size': NON_CAT_FEATURES + BINARY,\n",
    "    'category_sizes': fmax,\n",
    "    'lstm_sizes': [50, 100],\n",
    "    'embed_sizes': 15,\n",
    "    'tabular_sizes': [400, 200, 100],\n",
    "    'tabular_resblocks': 1,\n",
    "    'final_sizes': [300, 100],\n",
    "    'final_resblocks': 0,\n",
    "    'dropout': 0.4,\n",
    "    'learning_schedule': 'constant', \n",
    "    'learning_rate': 3e-04, \n",
    "    'weight_decay': 0.0004\n",
    "}\n",
    "\n",
    "model = get_model_instance(\n",
    "    \"lstm_yield_spread_model_pytorch\",\n",
    "    **model_params)\n",
    "\n",
    "wandb = WandbLogger(project=\"pytorch-models\", entity=\"ficc-ai\", name=f\"Conditioned Model with dropout {model_params['dropout']}\")\n",
    "wandb.watch(model, log=\"all\")\n",
    "\n",
    "checkpoint = ModelCheckpoint(monitor=\"val_mae\")\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1 if torch.cuda.is_available() else 0,\n",
    "    max_epochs=NUM_EPOCHS,\n",
    "    callbacks=[\n",
    "        checkpoint\n",
    "    ],\n",
    "    logger=wandb,\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader_with_cdc, val_loader_with_cdc)\n",
    "\n",
    "shutil.copyfile(checkpoint.best_model_path, f\"best_ys_model-{model_params['dropout']}.ckpt\")\n",
    "\n",
    "wandb.experiment.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the calc-date model and use it to calculate probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anis/anaconda3/envs/ficc/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py:91: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "fmax_without_cd = fmax.copy()\n",
    "fmax_without_cd.pop('calc_day_cat')\n",
    "\n",
    "# Drop the calc-date categorical feature from fmax\n",
    "model_params = {\n",
    "    'num_trade_history_features': NUM_FEATURES,\n",
    "    'non_categorical_size': NON_CAT_FEATURES + BINARY,\n",
    "    'category_sizes': fmax_without_cd,\n",
    "    'lstm_sizes': [50, 100],\n",
    "    'embed_sizes': 15,\n",
    "    'tabular_sizes': [400, 200, 100],\n",
    "    'tabular_resblocks': 1,\n",
    "    'final_sizes': [300, 100],\n",
    "    'final_resblocks': 0,\n",
    "    'dropout': 0.4,\n",
    "    'learning_schedule': 'constant', \n",
    "    'learning_rate': 3e-04, \n",
    "    'weight_decay': 0.0004\n",
    "}\n",
    "\n",
    "model = get_model_instance(\n",
    "    \"lstm_calc_date_model_pytorch\",\n",
    "    **model_params)\n",
    "\n",
    "# Reload the checkpoint of the best model, to this point\n",
    "model = model.load_from_checkpoint(\n",
    "    checkpoint_path=\"best_cd_model.ckpt\",\n",
    "    **model_params\n",
    ")\n",
    "model.eval()\n",
    "model = model.cuda()\n",
    "\n",
    "inputs = [x.cuda() for x in create_input(test_dataframe)]\n",
    "with torch.no_grad():\n",
    "    logits = model(*inputs)\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "del inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the yield-spread model with calc-day-cat set to 0, 1, 2, 3. Then weight all the results by the calculated calc-date probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calc-date 0\n",
      "\tPredictions complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anis/anaconda3/envs/ficc/lib/python3.9/site-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 332.90 GiB (GPU 0; 23.70 GiB total capacity; 311.14 MiB already allocated; 18.94 GiB free; 1.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/anis/ficc.ai/ficc/ml_models/sequence_predictors/conditioned_yield_spread_model.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anis/ficc.ai/ficc/ml_models/sequence_predictors/conditioned_yield_spread_model.ipynb#ch0000014?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mPredictions complete\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anis/ficc.ai/ficc/ml_models/sequence_predictors/conditioned_yield_spread_model.ipynb#ch0000014?line=22'>23</a>\u001b[0m \u001b[39mif\u001b[39;00m yield_spread \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/anis/ficc.ai/ficc/ml_models/sequence_predictors/conditioned_yield_spread_model.ipynb#ch0000014?line=23'>24</a>\u001b[0m     yield_spread \u001b[39m=\u001b[39m current_yield_spread \u001b[39m*\u001b[39;49m probs[:, calc_date]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anis/ficc.ai/ficc/ml_models/sequence_predictors/conditioned_yield_spread_model.ipynb#ch0000014?line=24'>25</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anis/ficc.ai/ficc/ml_models/sequence_predictors/conditioned_yield_spread_model.ipynb#ch0000014?line=25'>26</a>\u001b[0m     yield_spread \u001b[39m=\u001b[39m yield_spread \u001b[39m+\u001b[39m current_yield_spread \u001b[39m*\u001b[39m probs[:, calc_date]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 332.90 GiB (GPU 0; 23.70 GiB total capacity; 311.14 MiB already allocated; 18.94 GiB free; 1.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Reset the category sizes to include the calc-date categorical feature, for the yield-spread model conditioned upon calc-date\n",
    "model_params['category_sizes'] = fmax\n",
    "\n",
    "model = get_model_instance(\n",
    "    \"lstm_yield_spread_model_pytorch\",\n",
    "    **model_params)\n",
    "model = model.load_from_checkpoint(\n",
    "    checkpoint_path=f\"best_ys_model-{model_params['dropout']}.ckpt\",\n",
    "    **model_params\n",
    ")\n",
    "model.eval()\n",
    "model = model.cuda()\n",
    "\n",
    "yield_spread = None\n",
    "inputs_with_cdc = [x.cuda() for x in create_input(test_dataframe, with_cdc=True)]\n",
    "for calc_date in range(4):\n",
    "    print (f\"Calc-date {calc_date}\")\n",
    "    test_dataframe.loc[:, 'calc_day_cat'] = calc_date\n",
    "\n",
    "    with torch.no_grad():\n",
    "        current_yield_spread = model.cuda()(*inputs_with_cdc)\n",
    "        print(\"\\tPredictions complete\")\n",
    "        if yield_spread is None:\n",
    "            yield_spread = current_yield_spread * probs[:, calc_date]\n",
    "        else:\n",
    "            yield_spread = yield_spread + current_yield_spread * probs[:, calc_date]\n",
    "\n",
    "print(f\"Conditioned model attains a yield-spread MAE of {(create_label(test_dataframe) - yield_spread).abs().mean().cpu().numpy()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ficc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2266f9d190a7b880f073f634cb88d3a7b9a2324a600bb63b91ff505e59b5d8d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
