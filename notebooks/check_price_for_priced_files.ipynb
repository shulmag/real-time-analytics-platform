{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7806c7c",
   "metadata": {},
   "source": [
    "# Check Price for Files Priced Using Point-in-Time Pricing\n",
    "\n",
    "\n",
    "This notebook checks prices for the case when we are pricing CUSIPs that have traded, but not for the specific quantities or sides that traded. The procedure for pricing the trades is as follows: (1) get all of the trades that occurred on a specified date, and (2) use the archived model for that specific day to price these trades at specified quantities and trade types. See `point_in_time_pricing_actual_trades.py` for more details on this procedure. To evaluate our accuracy for these trades, we take the trades that occured and check the MAE for the closest prediction for the same trade direction and a similar trade size. We expect the MAE will be higher than the MAE would be if we compared a trade to a prediction with the same size and direction. The intent is simply to check if the MAE is within the expected range, in this case 10-15 bps. \n",
    "\n",
    "A secondary concern was to put files into a more user-friendly format by creating bid-ask spreads and by eliminating cases where we price some sizes and directions but not others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee34cf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab865ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '../creds.json'\n",
    "bq_client = bigquery.Client()\n",
    "\n",
    "project = 'eng-reactor-287421'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2274c4",
   "metadata": {},
   "source": [
    "We remove rows with error values and trades that are close to maturity as these will skew the MAE numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513ca1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_error_rows(df):\n",
    "    df['ytw'] = pd.to_numeric(df['ytw'], errors='coerce')\n",
    "    df['trade_datetime'] = pd.to_datetime(df['trade_datetime'])\n",
    "    df = df[~df['ytw'].isna() & (df['ytw'] != -1)]\n",
    "    df = df[(pd.to_datetime(df['yield_to_worst_date']) - pd.to_datetime(df['trade_datetime'])).dt.days >= 180]\n",
    "    df = df.drop('yield_to_worst_date', axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f20b9c3",
   "metadata": {},
   "source": [
    "We keep only CUSIPs where we have six prices, for both directions and three sizes (assuming that the file was priced with three different quantities). The reason that we may need to do this is because some error messages (e.g., \"CUSIP is maturing very soon or has already matured\" depends on the calc date which is something we learn after pricing) only affect a subset of the quantity, trade type pairs. We then create a bid-ask spread. These steps were intended to regularize the file for the end user, but may not be necessary.  If we do use them, we may want to run these before we remove the error rows, above, as the error rows are important for the end user. We may want to run only these two functions to create a cleaner file for the end user--the MAE testing should be distinct from cleaning the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0da156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_only_cusips_with_all_six_quantities(df):\n",
    "    df['cusip_trade_datetime'] = df['cusip'] + '_' + df['trade_datetime'].astype(str)\n",
    "    df['combination_counts'] = df['cusip_trade_datetime'].map(df['cusip_trade_datetime'].value_counts())\n",
    "    df = df[df['combination_counts'] == 6]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a66546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bid_ask_spread(df):\n",
    "    offered_df = df[df['trade_type'] == 'Offered Side'].copy()\n",
    "    bid_df = df[df['trade_type'] == 'Bid Side'].copy()\n",
    "\n",
    "    # Rename 'ytw' and 'price' columns in each DataFrame to reflect trade_type\n",
    "    offered_df.rename(columns={'ytw': 'ytw_offered', 'price': 'price_offered'}, inplace=True)\n",
    "    bid_df.rename(columns={'ytw': 'ytw_bid', 'price': 'price_bid'}, inplace=True)\n",
    "\n",
    "    # Drop 'trade_type' from both DataFrames since it will be redundant post-merge\n",
    "    offered_df.drop('trade_type', axis=1, inplace=True)\n",
    "    bid_df.drop('trade_type', axis=1, inplace=True)\n",
    "\n",
    "    assert len(bid_df) == len(offered_df)\n",
    "\n",
    "    # Perform an outer merge to ensure all combinations are preserved\n",
    "    combined_df = pd.merge(offered_df, bid_df, on=['cusip', 'quantity', 'trade_datetime', 'cusip_trade_datetime', 'combination_counts'], how='outer')\n",
    "    assert len(combined_df) * 2 == len(df)\n",
    "    combined_df['bid_ask_spread'] = combined_df['ytw_bid'] - combined_df['ytw_offered']\n",
    "    new_column_order = [\n",
    "        'cusip',\n",
    "        'quantity',\n",
    "        'ytw_offered',\n",
    "        'price_offered',\n",
    "        'ytw_bid',\n",
    "        'price_bid',\n",
    "        'bid_ask_spread',    # Placing ytw_bid, price_bid, bid_ask_spread after price_offered\n",
    "        'trade_datetime',\n",
    "        'cusip_trade_datetime',\n",
    "    ]\n",
    "\n",
    "    # Reorder the DataFrame according to the new column order\n",
    "    combined_df = combined_df[new_column_order]\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a932584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_file(df):\n",
    "    df = keep_only_cusips_with_all_six_quantities(df)\n",
    "    df = create_bid_ask_spread(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425e5267",
   "metadata": {},
   "source": [
    "Create helper functions for loading the results of the SQL query to a dataframe and for setting the quantity correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqltodf(sql, limit=''):\n",
    "    if limit != '': limit = f' ORDER BY RAND() LIMIT {limit}'\n",
    "    bqr = bq_client.query(sql + limit).result()\n",
    "    return bqr.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779eb9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_quantity(par_traded, quantities):\n",
    "    for quantity in quantities:\n",
    "        if par_traded <= quantity:\n",
    "            return quantity\n",
    "    return quantities[-1]    # Return the last quantity if none match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26747956",
   "metadata": {},
   "source": [
    "Get the MAE for a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be751c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae(df, filename):\n",
    "    '''`filename` is used solely for printing.'''\n",
    "    trade_date = df.iloc[0].trade_datetime.date()\n",
    "    query = f'''SELECT * FROM auxiliary_views.msrb_final WHERE trade_date = \"{trade_date}\" and publish_date = \"{trade_date}\"'''\n",
    "    trades = sqltodf(query)\n",
    "    trades['cusip_trade_datetime'] = trades['cusip'] + '_' + trades['trade_datetime'].astype(str)\n",
    "    quantities = sorted(df['quantity'].unique().tolist())\n",
    "    trades['quantity'] = trades['par_traded'].apply(lambda x: assign_quantity(x, quantities))\n",
    "    merged_df = trades.merge(df, on=['cusip_trade_datetime', 'quantity'], how='left')\n",
    "    bid = merged_df[merged_df['trade_type'] == 'D'][['yield', 'ytw_bid', 'cusip_trade_datetime']]\n",
    "    offer = merged_df[merged_df['trade_type'] == 'S'][['yield', 'ytw_offered', 'cusip_trade_datetime']]\n",
    "    offer = offer.dropna(subset=['ytw_offered'])\n",
    "    bid = bid.dropna(subset=['ytw_bid'])\n",
    "    offer['diff'] = abs(offer['yield'] - offer['ytw_offered'])\n",
    "    bid['diff'] = abs(bid['yield'] - bid['ytw_bid'])\n",
    "    offer = offer.sort_values(by='diff', ascending=False)\n",
    "    bid = bid.sort_values(by='diff', ascending=False)\n",
    "    offer_mae = round((offer['diff'].mean() * 100), 3)\n",
    "    bid_mae = round((bid['diff'].mean() * 100), 3)\n",
    "    print(f'''Offer MAE for {filename} is : {offer_mae} bps.''')\n",
    "    print(f'''Bid MAE for {filename} is : {bid_mae} bps.''')\n",
    "    return (offer_mae, bid_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852c09b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file(df, filename):\n",
    "    '''`filename` is used solely for printing.'''\n",
    "    df = remove_error_rows(df)\n",
    "    df = keep_only_cusips_with_all_six_quantities(df)\n",
    "    df = create_bid_ask_spread(df)\n",
    "    get_mae(df, filename)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920eaf8b",
   "metadata": {},
   "source": [
    "Specify the path to your CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ce9ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = '/Users/user/desktop/BMO'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2219b9d",
   "metadata": {},
   "source": [
    "Loop through each file in `directory_path` and analyze the MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8170b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "    # Check if the current item is a file and not a directory/subdirectory\n",
    "    if os.path.isfile(file_path) and filename.endswith('.csv') and filename != '.DS_Store':\n",
    "        print(f'Processing {file_path}...')\n",
    "\n",
    "        # Attempt to read the file with different encodings\n",
    "        successful_read = False\n",
    "        for encoding in ['utf-8', 'ISO-8859-1', 'windows-1252']:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding=encoding)\n",
    "                successful_read = True\n",
    "                print(f'Successfully read {filename} with encoding {encoding}')\n",
    "                # Optionally, display the first few rows or filename to track progress\n",
    "                # print(df.head())  # Uncomment to see the first few rows of each file\n",
    "\n",
    "                # Run your function on the DataFrame (Assuming `check_file` is defined elsewhere)\n",
    "                check_file(df, filename)\n",
    "                break    # Exit the encoding loop on success\n",
    "            except UnicodeDecodeError as e:\n",
    "                print(f'Error reading {filename} with encoding {encoding}: {e}')\n",
    "            except Exception as e:\n",
    "                print(f'Unexpected error while reading {filename}: {e}')\n",
    "                break    # Exit the encoding loop on unexpected error\n",
    "\n",
    "        if not successful_read:\n",
    "            print(f'Failed to read {filename} with tried encodings.')\n",
    "    else:\n",
    "        print(f'Skipping {filename}...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
