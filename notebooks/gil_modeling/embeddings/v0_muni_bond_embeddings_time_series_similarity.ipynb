{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f113e000",
   "metadata": {},
   "source": [
    "##### Created by Gil on 9/24/2025\n",
    "\n",
    "# Market-Driven Municipal Bond Embeddings via Siamese Networks\n",
    "\n",
    "## Executive Summary\n",
    "This notebook implements a Bond2Vec approach to learning municipal bond embeddings based on market behavior rather than traditional characteristics. We use a Siamese neural network to learn representations where bonds with similar trading patterns are close in embedding space.\n",
    "\n",
    "## Key Innovation\n",
    "Traditional bond similarity relies on static features (rating, sector, maturity). This approach discovers bonds that \"dance to the same tune\" in the market, potentially uncovering:\n",
    "- Cross-sector relationships\n",
    "- Hidden risk factors  \n",
    "- Market microstructure effects\n",
    "- Novel relative value opportunities\n",
    "\n",
    "## Methodology Overview\n",
    "\n",
    "### 1. Temporal Co-occurrence\n",
    "- Bonds trading within 30-60 second windows are candidates for behavioral similarity\n",
    "- Similar to \"customers who viewed X also viewed Y\" in recommendation systems\n",
    "\n",
    "### 2. Behavioral Similarity\n",
    "- Compute cosine similarity on trade history arrays (5 previous trades)\n",
    "- Each trade encoded as: [yield_spread, treasury_spread, log_par_traded, trade_type_1, trade_type_2, log_seconds_ago]\n",
    "- Similarity > 0.5 → positive pair (similar behavior)\n",
    "- Similarity < 0.2 → negative pair (dissimilar behavior)\n",
    "\n",
    "### 3. Siamese Network Training\n",
    "- Twin networks with shared weights process two CUSIPs\n",
    "- Learn embeddings that preserve behavioral similarity\n",
    "- Contrastive loss pushes similar bonds together, dissimilar apart\n",
    "\n",
    "### 4. Temporal Alignment (Critical!)\n",
    "- Each pair uses point-in-time features from the EXACT trade timestamps\n",
    "- Maintains temporal consistency: comparing apples to apples\n",
    "- Creates index: (CUSIP, trade_datetime) → feature_vector\n",
    "\n",
    "### Understanding ficc.ai Trade History Arrays\n",
    "\n",
    "The trade history arrays encode historical trades as 6-element numpy arrays. Each row represents one historical trade, with the following structure:\n",
    "\n",
    "### Array Structure\n",
    "```python   \n",
    "\n",
    "### Array Structure\n",
    "\n",
    "[yield_spread, treasury_spread, log_par_traded, trade_type_1, trade_type_2, log_seconds_ago]\n",
    "```\n",
    "\n",
    "### Feature Breakdown\n",
    "\n",
    "| Index | Feature | Description | Example Value |\n",
    "|-------|---------|-------------|---------------|\n",
    "| 0 | **Yield Spread** | Bond yield × 100 - FICC yield curve level (basis points) | 101.42 |\n",
    "| 1 | **Treasury Spread** | (Bond yield - Treasury rate) × 100 (basis points, rounded to 3 decimals) | 19.0 |\n",
    "| 2 | **Log Par Traded** | log₁₀(trade size) - normalizes trade sizes | 4.70 |\n",
    "| 3 | **Trade Type 1** | First component of trade type encoding | 0.0 |\n",
    "| 4 | **Trade Type 2** | Second component of trade type encoding | 0.0 |\n",
    "| 5 | **Log Seconds Ago** | log₁₀(1 + seconds since trade) - normalizes time decay | 2.18 |\n",
    "\n",
    "## Production Use Cases\n",
    "1. **Daily Arbitrage Scanner**: Find bonds with high embedding similarity but yield spreads\n",
    "2. **Portfolio Risk Analysis**: Measure concentration in \"behavioral clusters\"\n",
    "3. **Trade Idea Generation**: Find substitutes based on market behavior, not just ratings\n",
    "4. **Anomaly Detection**: Flag bonds diverging from their embedding neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae3c01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n",
      "GPU Available: []\n",
      "Num GPUs: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Step 1: Set paths BEFORE importing tensorflow\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-12/lib64:/usr/local/cuda/lib64'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# Standard data science imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data processing and utilities\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Dict\n",
    "import random\n",
    "from scipy import stats\n",
    "from decimal import Decimal\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configuration\n",
    "EMBEDDING_DIM = 128\n",
    "BATCH_SIZE = 8192\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 100\n",
    "N_BONDS_TO_ANALYZE = 1000  # Number of bonds to analyze for correlation pairs\n",
    "MIN_YIELD_SPREAD_RANGE = 5.0  # Minimum basis points movement\n",
    "\n",
    "# Correlation thresholds\n",
    "CORRELATION_THRESHOLD = 0.7  # For positive pairs\n",
    "NEGATIVE_CORRELATION_THRESHOLD = -0.3  # For negative pairs\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"Num GPUs: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "# Mixed precision for faster training (optional)\n",
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb2327a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1,405,865 bond trades\n",
      "Unique CUSIPs: 196,446\n",
      "Date range: 2025-08-20 00:00:00 to 2025-09-19 00:00:00\n",
      "\n",
      "Columns: ['rtrs_control_number', 'cusip', 'yield', 'is_callable', 'refund_date', 'accrual_date', 'dated_date', 'next_sink_date', 'coupon', 'delivery_date']...\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "# file_path = \"/home/gil/git/ficc/notebooks/gil_modeling/embeddings/processed_data_yield_spread_with_similar_trades_v2.pkl\"\n",
    "file_path = \"/home/gil/git/ficc/notebooks/gil_modeling/embeddings/2025-09-22_one_month.pkl\"\n",
    "\n",
    "df = pd.read_pickle(file_path)\n",
    "\n",
    "print(f\"Loaded {len(df):,} bond trades\")\n",
    "print(f\"Unique CUSIPs: {df['cusip'].nunique():,}\")\n",
    "print(f\"Date range: {df['trade_date'].min()} to {df['trade_date'].max()}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()[:10]}...\")  # Show first 10 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7289b216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eeb4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules\n",
    "from modules.pair_generation import run_pair_generation_pipeline\n",
    "from modules.siamese_network import run_training_pipeline\n",
    "\n",
    "# 1. Load your data\n",
    "df = pd.read_pickle('/Users/gil/git/ficc/notebooks/gil_modeling/embeddings/2025-09-10.pkl')\n",
    "\n",
    "# 2. Generate temporally-aligned pairs\n",
    "pairs_df = run_pair_generation_pipeline(\n",
    "    df,\n",
    "    time_window_seconds=30,\n",
    "    min_similarity=0.5,\n",
    "    max_pairs_per_window=500,\n",
    "    n_processes=11\n",
    ")\n",
    "\n",
    "# 3. Train the Siamese network with temporal alignment\n",
    "base_network, artifacts, history = run_training_pipeline(\n",
    "    df,           # All trades with reference data\n",
    "    pairs_df,     # Pairs with temporal metadata\n",
    "    test_size=0.2,\n",
    "    embedding_dim=128,\n",
    "    epochs=100\n",
    ")\n",
    "\n",
    "# 4. Generate embeddings for unique CUSIPs\n",
    "unique_cusips_df = df.sort_values('trade_datetime').groupby('cusip').last().reset_index()\n",
    "embeddings_df = get_embeddings(unique_cusips_df, base_network, artifacts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234d9484",
   "metadata": {},
   "source": [
    "## Understanding ficc.ai Trade History Arrays\n",
    "\n",
    "The trade history arrays encode historical trades as 6-element numpy arrays. Each row represents one historical trade, with the following structure:\n",
    "\n",
    "### Array Structure\n",
    "```python   \n",
    "\n",
    "### Array Structure\n",
    "\n",
    "[yield_spread, treasury_spread, log_par_traded, trade_type_1, trade_type_2, log_seconds_ago]\n",
    "```\n",
    "\n",
    "### Feature Breakdown\n",
    "\n",
    "| Index | Feature | Description | Example Value |\n",
    "|-------|---------|-------------|---------------|\n",
    "| 0 | **Yield Spread** | Bond yield × 100 - FICC yield curve level (basis points) | 101.42 |\n",
    "| 1 | **Treasury Spread** | (Bond yield - Treasury rate) × 100 (basis points, rounded to 3 decimals) | 19.0 |\n",
    "| 2 | **Log Par Traded** | log₁₀(trade size) - normalizes trade sizes | 4.70 |\n",
    "| 3 | **Trade Type 1** | First component of trade type encoding | 0.0 |\n",
    "| 4 | **Trade Type 2** | Second component of trade type encoding | 0.0 |\n",
    "| 5 | **Log Seconds Ago** | log₁₀(1 + seconds since trade) - normalizes time decay | 2.18 |\n",
    "\n",
    "### Trade Type Encoding (One-Hot)\n",
    "\n",
    "The trade type uses a 2-component one-hot encoding:\n",
    "\n",
    "| Trade Type | Code | Component 1 | Component 2 | Description |\n",
    "|------------|------|-------------|-------------|-------------|\n",
    "| **D** | [0, 0] | 0 | 0 | Dealer-to-dealer |\n",
    "| **S** | [0, 1] | 0 | 1 | Dealer sell / Customer buy |\n",
    "| **P** | [1, 0] | 1 | 0 | Dealer purchase / Customer sell |\n",
    "\n",
    "### Example: Decoding a Trade History Array\n",
    "\n",
    "```python\n",
    "# Sample trade history with 5 trades\n",
    "trade_history = np.array([\n",
    "    [101.42, 19.0,  4.70, 0., 0., 2.18],  # D trade, 151 seconds ago\n",
    "    [100.22, 17.8,  4.70, 0., 1., 2.18],  # S trade, 151 seconds ago\n",
    "    [104.42, 22.0,  4.70, 0., 0., 3.75],  # D trade, ~5,620 seconds ago\n",
    "    [106.02, 23.6,  4.70, 0., 0., 3.75],  # D trade, ~5,620 seconds ago\n",
    "    [103.62, 21.2,  4.70, 0., 0., 3.75],  # D trade, ~5,620 seconds ago\n",
    "])\n",
    "```\n",
    "\n",
    "**Interpretation:**\n",
    "- All trades have the same size (10^4.70 ≈ 50,000 par value)\n",
    "- Yield spreads range from 100.22 to 106.02 basis points\n",
    "- Most are dealer-to-dealer trades (D), except trade #2 which is dealer sell (S)\n",
    "- Trades occurred at two different times: ~151 seconds ago and ~5,620 seconds ago\n",
    "\n",
    "### Key Transformations\n",
    "\n",
    "1. **Logarithmic scaling**: Applied to trade size and time to handle wide ranges\n",
    "2. **Basis point conversion**: Yields multiplied by 100 for better model scaling\n",
    "3. **One-hot encoding**: Categorical trade types converted to numerical representation\n",
    "\n",
    "### Model Usage\n",
    "\n",
    "- **Yield Spread Models**: Use 5 historical trades (`NUM_TRADES_IN_HISTORY_YIELD_SPREAD_MODEL = 5`)\n",
    "- **Dollar Price Models**: Use 2 historical trades (`NUM_TRADES_IN_HISTORY_DOLLAR_PRICE_MODEL = 2`)\n",
    "- **Similar Trades Model**: Incorporates trades from bonds with similar characteristics\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4c9ed4",
   "metadata": {},
   "source": [
    "## Generating Training Pairs from Correlation Matrix\n",
    "\n",
    "Now we'll compute pairwise correlations between all bonds and generate three types of pairs:\n",
    "1. **Positive pairs**: Correlation > 0.7 (bonds that move together)\n",
    "2. **Hard negative pairs**: Correlation < -0.3 (bonds that move oppositely)\n",
    "3. **Random negative pairs**: Random sampling (majority of real-world cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "553a68fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (1405865, 150)\n",
      "Unique CUSIPs: 196446\n",
      "Time window: 30 seconds\n",
      "\n",
      "Generating pairs with parallel processing...\n",
      "Using 11 processes for pair generation\n",
      "Processing 25439 time windows\n",
      "Pair generation completed in 147.93 seconds\n",
      "\n",
      "Generated:\n",
      "  Positive pairs: 3,871,776\n",
      "  Negative pairs: 2,661,695\n",
      "\n",
      "Final dataset shape: (6533471, 4)\n",
      "Positive ratio: 59.26%\n",
      "\n",
      "Saved to embedding_pairs.pkl\n",
      "\n",
      "Similarity statistics:\n",
      "Positive pairs - Mean: 0.824\n",
      "Positive pairs - Std:  0.146\n",
      "Negative pairs - Mean: -0.257\n",
      "Negative pairs - Std:  0.290\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from datetime import timedelta\n",
    "import pickle\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "def compute_behavioral_similarity_fast(hist1, hist2):\n",
    "    \"\"\"Simple cosine similarity using dot product on normalized histories\"\"\"\n",
    "    if len(hist1) < 5 or len(hist2) < 5:\n",
    "        return 0.0\n",
    "    \n",
    "    hist1_flat = hist1.flatten()\n",
    "    hist2_flat = hist2.flatten()\n",
    "    \n",
    "    hist1_norm = hist1_flat / (np.linalg.norm(hist1_flat) + 1e-8)\n",
    "    hist2_norm = hist2_flat / (np.linalg.norm(hist2_flat) + 1e-8)\n",
    "    \n",
    "    return np.dot(hist1_norm, hist2_norm)\n",
    "\n",
    "def get_trade_history(hist_str):\n",
    "    \"\"\"Parse trade history from string format\"\"\"\n",
    "    if isinstance(hist_str, np.ndarray):\n",
    "        return hist_str\n",
    "    return np.array(hist_str) if hist_str is not None else np.array([])\n",
    "\n",
    "def process_single_window(args):\n",
    "    \"\"\"Process a single time window - for parallel execution\"\"\"\n",
    "    window, window_df, time_window_seconds, min_similarity, max_pairs_per_window = args\n",
    "    \n",
    "    # Get unique CUSIPs in this window\n",
    "    cusip_groups = window_df.groupby('cusip').first()\n",
    "    \n",
    "    if len(cusip_groups) < 2:\n",
    "        return [], []\n",
    "    \n",
    "    # Prepare CUSIP data with histories\n",
    "    cusip_data = {}\n",
    "    for cusip, row in cusip_groups.iterrows():\n",
    "        hist = get_trade_history(row['trade_history'])\n",
    "        if len(hist) >= 5:\n",
    "            cusip_data[cusip] = hist\n",
    "    \n",
    "    if len(cusip_data) < 2:\n",
    "        return [], []\n",
    "    \n",
    "    # Generate all pairs within this window\n",
    "    window_pairs = []\n",
    "    for cusip1, cusip2 in combinations(cusip_data.keys(), 2):\n",
    "        hist1 = cusip_data[cusip1]\n",
    "        hist2 = cusip_data[cusip2]\n",
    "        sim = compute_behavioral_similarity_fast(hist1, hist2)\n",
    "        window_pairs.append((cusip1, cusip2, sim))\n",
    "    \n",
    "    # Cap pairs per window if needed\n",
    "    if len(window_pairs) > max_pairs_per_window:\n",
    "        window_pairs = random.sample(window_pairs, max_pairs_per_window)\n",
    "    \n",
    "    # Classify into positive and negative\n",
    "    positive_pairs = []\n",
    "    negative_pairs = []\n",
    "    for cusip1, cusip2, sim in window_pairs:\n",
    "        if sim > min_similarity:\n",
    "            positive_pairs.append((cusip1, cusip2, sim))\n",
    "        elif sim < 0.2:\n",
    "            negative_pairs.append((cusip1, cusip2, sim))\n",
    "    \n",
    "    return positive_pairs, negative_pairs\n",
    "\n",
    "def generate_temporal_behavioral_pairs_parallel(\n",
    "    df,\n",
    "    time_window_seconds=30,\n",
    "    min_similarity=0.5,\n",
    "    max_pairs_per_window=1000,\n",
    "    n_processes=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate pairs using parallel processing\n",
    "    \"\"\"\n",
    "    if n_processes is None:\n",
    "        n_processes = cpu_count() - 1  # Leave one CPU free\n",
    "    \n",
    "    print(f\"Using {n_processes} processes for pair generation\")\n",
    "    \n",
    "    # Ensure datetime column\n",
    "    df['trade_dt'] = pd.to_datetime(df['trade_datetime'])\n",
    "    \n",
    "    # Create time windows\n",
    "    df['time_window'] = df['trade_dt'].dt.floor(f'{time_window_seconds}s')\n",
    "    \n",
    "    # Get unique windows and their data\n",
    "    unique_windows = df['time_window'].unique()\n",
    "    print(f\"Processing {len(unique_windows)} time windows\")\n",
    "    \n",
    "    # Prepare arguments for parallel processing\n",
    "    window_args = []\n",
    "    for window in unique_windows:\n",
    "        window_df = df[df['time_window'] == window]\n",
    "        window_args.append((\n",
    "            window, \n",
    "            window_df, \n",
    "            time_window_seconds, \n",
    "            min_similarity, \n",
    "            max_pairs_per_window\n",
    "        ))\n",
    "    \n",
    "    # Process windows in parallel\n",
    "    start_time = time.time()\n",
    "    with Pool(processes=n_processes) as pool:\n",
    "        results = pool.map(process_single_window, window_args)\n",
    "    \n",
    "    # Combine results\n",
    "    all_positive_pairs = []\n",
    "    all_negative_pairs = []\n",
    "    seen_pairs = set()\n",
    "    \n",
    "    for pos_pairs, neg_pairs in results:\n",
    "        # Add positive pairs, avoiding duplicates\n",
    "        for cusip1, cusip2, sim in pos_pairs:\n",
    "            pair_key = tuple(sorted([cusip1, cusip2]))\n",
    "            if pair_key not in seen_pairs:\n",
    "                seen_pairs.add(pair_key)\n",
    "                all_positive_pairs.append((cusip1, cusip2, sim))\n",
    "        \n",
    "        # Add negative pairs, avoiding duplicates\n",
    "        for cusip1, cusip2, sim in neg_pairs:\n",
    "            pair_key = tuple(sorted([cusip1, cusip2]))\n",
    "            if pair_key not in seen_pairs:\n",
    "                seen_pairs.add(pair_key)\n",
    "                all_negative_pairs.append((cusip1, cusip2, sim))\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Pair generation completed in {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    return all_positive_pairs, all_negative_pairs\n",
    "\n",
    "def create_training_dataset(positive_pairs, negative_pairs, neg_to_pos_ratio=3):\n",
    "    \"\"\"Create balanced training dataset with proper CUSIP pairs\"\"\"\n",
    "    n_positives = len(positive_pairs)\n",
    "    n_negatives_needed = min(len(negative_pairs), n_positives * neg_to_pos_ratio)\n",
    "    \n",
    "    if len(negative_pairs) > n_negatives_needed:\n",
    "        negative_pairs = random.sample(negative_pairs, n_negatives_needed)\n",
    "    \n",
    "    all_pairs = []\n",
    "    \n",
    "    for cusip1, cusip2, sim in positive_pairs:\n",
    "        all_pairs.append({\n",
    "            'cusip1': cusip1,\n",
    "            'cusip2': cusip2,\n",
    "            'similarity': sim,\n",
    "            'label': 1\n",
    "        })\n",
    "    \n",
    "    for cusip1, cusip2, sim in negative_pairs:\n",
    "        all_pairs.append({\n",
    "            'cusip1': cusip1,\n",
    "            'cusip2': cusip2,\n",
    "            'similarity': sim,\n",
    "            'label': 0\n",
    "        })\n",
    "    \n",
    "    random.shuffle(all_pairs)\n",
    "    return pd.DataFrame(all_pairs)\n",
    "\n",
    "def run_pair_generation_parallel(\n",
    "    df_or_pkl_path,\n",
    "    time_window_seconds=30,\n",
    "    min_similarity=0.5,\n",
    "    max_pairs_per_window=1000,\n",
    "    output_path='embedding_pairs.pkl',\n",
    "    n_processes=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Main function to run parallel pair generation\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    if isinstance(df_or_pkl_path, str):\n",
    "        print(f\"Loading data from {df_or_pkl_path}\")\n",
    "        with open(df_or_pkl_path, 'rb') as f:\n",
    "            df = pickle.load(f)\n",
    "    else:\n",
    "        df = df_or_pkl_path\n",
    "    \n",
    "    print(f\"Data shape: {df.shape}\")\n",
    "    print(f\"Unique CUSIPs: {df['cusip'].nunique()}\")\n",
    "    print(f\"Time window: {time_window_seconds} seconds\")\n",
    "    \n",
    "    # Generate pairs in parallel\n",
    "    print(\"\\nGenerating pairs with parallel processing...\")\n",
    "    positive_pairs, negative_pairs = generate_temporal_behavioral_pairs_parallel(\n",
    "        df,\n",
    "        time_window_seconds=time_window_seconds,\n",
    "        min_similarity=min_similarity,\n",
    "        max_pairs_per_window=max_pairs_per_window,\n",
    "        n_processes=n_processes\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nGenerated:\")\n",
    "    print(f\"  Positive pairs: {len(positive_pairs):,}\")\n",
    "    print(f\"  Negative pairs: {len(negative_pairs):,}\")\n",
    "    \n",
    "    # Create training dataset\n",
    "    pairs_df = create_training_dataset(positive_pairs, negative_pairs)\n",
    "    \n",
    "    print(f\"\\nFinal dataset shape: {pairs_df.shape}\")\n",
    "    print(f\"Positive ratio: {pairs_df['label'].mean():.2%}\")\n",
    "    \n",
    "    # Save\n",
    "    pairs_df.to_pickle(output_path)\n",
    "    print(f\"\\nSaved to {output_path}\")\n",
    "    \n",
    "    # Stats\n",
    "    if len(pairs_df) > 0:\n",
    "        print(\"\\nSimilarity statistics:\")\n",
    "        pos_df = pairs_df[pairs_df['label']==1]\n",
    "        neg_df = pairs_df[pairs_df['label']==0]\n",
    "        \n",
    "        if len(pos_df) > 0:\n",
    "            print(f\"Positive pairs - Mean: {pos_df['similarity'].mean():.3f}\")\n",
    "            print(f\"Positive pairs - Std:  {pos_df['similarity'].std():.3f}\")\n",
    "        \n",
    "        if len(neg_df) > 0:\n",
    "            print(f\"Negative pairs - Mean: {neg_df['similarity'].mean():.3f}\")\n",
    "            print(f\"Negative pairs - Std:  {neg_df['similarity'].std():.3f}\")\n",
    "    \n",
    "    return pairs_df\n",
    "\n",
    "n_cpus = cpu_count() - 1\n",
    "pairs_df = run_pair_generation_parallel(df, time_window_seconds=30,min_similarity=0.5,max_pairs_per_window=500,n_processes=n_cpus) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a4e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pair_distributions(pairs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4187d38c",
   "metadata": {},
   "source": [
    "# CUSIP Siamese Network Embedding System\n",
    "\n",
    "## Overview\n",
    "This system implements a Siamese neural network for learning embeddings of CUSIPs. The model learns to map similar securities close together in an embedding space while keeping dissimilar securities far apart.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Constants and Utilities](#constants-and-utilities)\n",
    "2. [Helper Functions](#helper-functions)\n",
    "3. [Feature Engineering](#feature-engineering)\n",
    "4. [Siamese Network Architecture](#siamese-network-architecture)\n",
    "5. [Training Pipeline](#training-pipeline)\n",
    "6. [Usage Examples](#usage-examples)\n",
    "\n",
    "\n",
    "\n",
    "## Constants and Utilities\n",
    "\n",
    "### Key Constants\n",
    "\n",
    "```python\n",
    "NUM_OF_DAYS_IN_YEAR = 360  # MSRB convention for bond calculations\n",
    "```\n",
    "\n",
    "The system uses the **360-day year convention** commonly used in municipal bond markets (MSRB Rule G-33).\n",
    "\n",
    "### Coupon Frequency Mappings\n",
    "\n",
    "The code maintains two dictionaries for handling coupon payment frequencies:\n",
    "\n",
    "1. **`COUPON_FREQUENCY_DICT`**: Maps numeric codes (0-36) to human-readable frequency descriptions\n",
    "   - Example: `1`  \"Semiannually\", `31` -> \"Zero coupon\"\n",
    "\n",
    "2. **`COUPON_FREQUENCY_TYPE`**: Maps frequency descriptions to annual payment counts\n",
    "   - Example: \"Semiannually\" -> `2`, \"Quarterly\" -> `4`, \"Zero coupon\" -> `0`\n",
    "\n",
    "### Default Values Dictionary\n",
    "\n",
    "**`FEATURES_AND_DEFAULT_VALUES`** provides fallback values for missing features:\n",
    "- Numeric fields default to sensible values (e.g., prices default to 100)\n",
    "- Boolean fields default to False\n",
    "- Text fields get placeholder values\n",
    "- Some fields use computed defaults based on the dataset\n",
    "\n",
    "\n",
    "\n",
    "## Helper Functions\n",
    "\n",
    "### Date Calculation Functions\n",
    "\n",
    "#### `diff_in_days_two_dates_360_30()`\n",
    "Calculates the difference between two dates using the **30/360 day count convention**:\n",
    "- Assumes 30 days per month\n",
    "- Assumes 360 days per year\n",
    "- Used for standardized bond calculations\n",
    "\n",
    "#### `diff_in_days_two_dates_exact()`\n",
    "Calculates the exact calendar day difference between dates.\n",
    "\n",
    "#### `diff_in_days()`\n",
    "A wrapper function that:\n",
    "- Handles accrual date logic\n",
    "- Supports both 360/30 and exact conventions\n",
    "- Returns 0 for invalid dates\n",
    "\n",
    "### Interest Payment Calculations\n",
    "\n",
    "#### `days_in_interest_payment()`\n",
    "Calculates the number of days in an interest payment period:\n",
    "- Converts frequency codes to actual periods\n",
    "- Returns 360 divided by annual frequency\n",
    "- Handles special cases (zero coupon, irregular payments)\n",
    "\n",
    "#### `calculate_a_over_e()`\n",
    "Computes the **A/E ratio** (Accrued/Expected):\n",
    "- Measures the portion of the current coupon period that has elapsed\n",
    "- Used for accrued interest calculations\n",
    "- Important for bond pricing\n",
    "\n",
    "### Data Cleaning Functions\n",
    "\n",
    "#### `to_numeric()`\n",
    "Safely converts series to numeric values:\n",
    "- Handles Decimal type objects\n",
    "- Converts to float for calculations\n",
    "- Maintains data integrity\n",
    "\n",
    "#### `fill_missing_values()`\n",
    "Intelligently fills missing values:\n",
    "- Uses predefined defaults from `FEATURES_AND_DEFAULT_VALUES`\n",
    "- Can compute dynamic defaults (e.g., mean values)\n",
    "- Falls back to related columns when appropriate\n",
    "\n",
    "\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "### Main Processing Function: `process_features()`\n",
    "\n",
    "This function performs the core feature transformations:\n",
    "\n",
    "#### 1. **Data Type Conversions**\n",
    "- Converts Decimal columns to float\n",
    "- Ensures numeric compatibility\n",
    "\n",
    "#### 2. **Interest Payment Frequency Processing**\n",
    "- Maps numeric codes to descriptive strings\n",
    "- Handles missing values appropriately\n",
    "\n",
    "#### 3. **Quantity Transformations**\n",
    "- Applies **log transformation** to par traded amounts\n",
    "- Helps normalize highly skewed distributions\n",
    "- Formula: `log10(par_traded)`\n",
    "\n",
    "#### 4. **Amount Fields Processing**\n",
    "- Log-transforms large monetary values\n",
    "- Includes: issue_amount, maturity_amount, orig_principal_amount\n",
    "- Formula: `log10(1 + amount)` to handle zeros\n",
    "\n",
    "#### 5. **Binary Feature Creation**\n",
    "Creates indicator variables for:\n",
    "- **callable**: Bond can be called early\n",
    "- **called**: Bond has been called\n",
    "- **zerocoupon**: No periodic interest payments\n",
    "- **whenissued**: Traded before settlement\n",
    "- **sinking**: Has sinking fund provisions\n",
    "- **deferred**: Interest payments are deferred\n",
    "\n",
    "#### 6. **Date Feature Engineering**\n",
    "Converts dates to numeric features:\n",
    "- **days_to_settle**: Settlement lag\n",
    "- **days_to_maturity**: Time until bond matures\n",
    "- **days_to_call**: Time until callable\n",
    "- **days_to_refund**: Time until refundable\n",
    "- All transformed with `log10(1 + days)` for normalization\n",
    "\n",
    "#### 7. **Accrual Calculations**\n",
    "- **accrued_days**: Days since last coupon\n",
    "- **scaled_accrued_days**: Normalized by payment period\n",
    "- **A/E ratio**: Fraction of period elapsed\n",
    "\n",
    "### Complete Feature Engineering: `engineer_features_complete()`\n",
    "\n",
    "This is the production-ready feature engineering pipeline:\n",
    "\n",
    "#### Features Processed:\n",
    "\n",
    "1. **Binary Features** (converted to 0/1):\n",
    "   - Bond characteristics (callable, called, zerocoupon)\n",
    "   - Trading status (whenissued, sinking, deferred)\n",
    "   - Tax and legal status indicators\n",
    "\n",
    "2. **Numeric Direct Features** (used as-is after scaling):\n",
    "   - Coupon rate, prices, yields\n",
    "   - Time-based calculations\n",
    "   - Accrual metrics\n",
    "\n",
    "3. **Numeric Log Features** (already transformed):\n",
    "   - Quantities and amounts\n",
    "   - Pre-processed in `process_features()`\n",
    "\n",
    "4. **Categorical Features** (one-hot encoded):\n",
    "   - State codes, trade types, ratings\n",
    "   - Purpose classes, tax status\n",
    "   - Series names, security types\n",
    "\n",
    "#### Rating Score Mapping\n",
    "Converts letter ratings to numeric scores:\n",
    "- AAA = 22 (highest)\n",
    "- D = 1 (default)\n",
    "- MR/NR = 0 (missing/not rated)\n",
    "\n",
    "#### Scaling and Encoding\n",
    "- Uses **RobustScaler** for numeric features (resistant to outliers)\n",
    "- **LabelEncoder** + one-hot encoding for categoricals\n",
    "- Handles unseen categories with \"UNKNOWN\" class\n",
    "\n",
    "\n",
    "\n",
    "## Siamese Network Architecture\n",
    "\n",
    "### Base Network: `create_base_network()`\n",
    "\n",
    "The core embedding network consists of:\n",
    "\n",
    "```\n",
    "Input Layer (n features)\n",
    "    >\n",
    "Dense Layer (512 units, ReLU)\n",
    "    >\n",
    "Batch Normalization\n",
    "    >\n",
    "Dropout (%)\n",
    "    >\n",
    "Dense Layer (256 units, ReLU)\n",
    "    >\n",
    "Batch Normalization\n",
    "    >\n",
    "Dropout (%)\n",
    "    >\n",
    "Dense Layer (256 units, ReLU)\n",
    "    >\n",
    "Batch Normalization\n",
    "    >\n",
    "Dropout (%)\n",
    "    >\n",
    "Embedding Layer (128 units, Linear)\n",
    "    >\n",
    "L2 Normalization\n",
    "```\n",
    "\n",
    "**Key Design Choices:**\n",
    "- **Progressive dimension reduction**: 512 -> 256 -> 256 -> 128\n",
    "- **Batch normalization**: Stabilizes training\n",
    "- **Dropout**: Prevents overfitting (decreasing rates)\n",
    "- **L2 normalization**: Creates unit-length embeddings\n",
    "\n",
    "### Siamese Network: `create_siamese_network()`\n",
    "\n",
    "The full architecture:\n",
    "1. Two **identical base networks** (shared weights)\n",
    "2. Process two input CUSIPs in parallel\n",
    "3. Compute **cosine similarity** between embeddings\n",
    "4. Output similarity score (-1 to 1)\n",
    "\n",
    "### Loss Function: `contrastive_loss()`\n",
    "\n",
    "Implements contrastive loss with margin:\n",
    "- **Positive pairs** (similar): Minimizes distance\n",
    "- **Negative pairs** (dissimilar): Enforces minimum margin\n",
    "- **Margin parameter**: Default 0.5\n",
    "- Balances attraction and repulsion forces\n",
    "\n",
    "\n",
    "\n",
    "## Training Pipeline\n",
    "\n",
    "### Data Preparation: `prepare_pairs_for_training()`\n",
    "\n",
    "1. **Date Processing**: Ensures all date columns are datetime objects\n",
    "2. **Feature Engineering**: Applies complete feature pipeline\n",
    "3. **CUSIP Mapping**: Creates dictionary of CUSIP -> feature vector\n",
    "4. **Pair Assembly**: Matches CUSIP pairs with their labels\n",
    "5. **Missing Data Handling**: Reports any CUSIPs not found\n",
    "\n",
    "### Training Function: `train_siamese_network()`\n",
    "\n",
    "**Training Configuration:**\n",
    "- **Optimizer**: Adam (learning rate = 0.001)\n",
    "- **Batch Size**: 256\n",
    "- **Epochs**: Up to 100 (with early stopping)\n",
    "\n",
    "**Callbacks:**\n",
    "1. **EarlyStopping**: Patience of 15 epochs\n",
    "2. **ModelCheckpoint**: Saves best model\n",
    "3. **ReduceLROnPlateau**: Adaptive learning rate\n",
    "\n",
    "### Embedding Extraction: `get_embeddings()`\n",
    "\n",
    "Generates embeddings for new CUSIPs:\n",
    "1. Processes features using saved artifacts\n",
    "2. Passes through trained base network\n",
    "3. Returns dataframe with CUSIP and embedding columns\n",
    "\n",
    "### Complete Pipeline: `run_training_pipeline()`\n",
    "\n",
    "Orchestrates the entire training process:\n",
    "\n",
    "1. **Data Preparation**\n",
    "   - Engineers features\n",
    "   - Creates training pairs\n",
    "\n",
    "2. **Train/Validation Split**\n",
    "   - 80/20 split by default\n",
    "   - Stratified sampling\n",
    "\n",
    "3. **Model Training**\n",
    "   - Trains Siamese network\n",
    "   - Monitors validation loss\n",
    "\n",
    "4. **Artifact Saving**\n",
    "   - Model weights: `cusip_embedding_model.h5`\n",
    "   - Feature artifacts: `feature_artifacts.pkl`\n",
    "   - Feature names: `feature_names.pkl`\n",
    "\n",
    "\n",
    "\n",
    "## Usage Examples\n",
    "\n",
    "### Basic Training\n",
    "\n",
    "```python\n",
    "# Load your data\n",
    "df = pd.read_pickle('processed_data_yield_spread_with_similar_trades_v2.pkl')\n",
    "pairs_df = pd.read_pickle('/Users/gil/git/ficc/notebooks/gil_modeling/embeddings/bond_pairs_180days.pkl')\n",
    "\n",
    "# Train the model\n",
    "base_network, artifacts, history = run_training_pipeline(\n",
    "    df,  \n",
    "    pairs_df,\n",
    "    test_size=0.2,\n",
    "    embedding_dim=128,\n",
    "    epochs=100\n",
    ")\n",
    "```\n",
    "\n",
    "### Generating Embeddings\n",
    "\n",
    "```python\n",
    "# Load saved model and artifacts\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "base_network = load_model('cusip_embedding_model.h5')\n",
    "with open('feature_artifacts.pkl', 'rb') as f:\n",
    "    artifacts = pickle.load(f)\n",
    "\n",
    "# Generate embeddings for new data\n",
    "new_df = pd.read_csv('new_cusips.csv')\n",
    "embeddings_df = get_embeddings(new_df, base_network, artifacts)\n",
    "embeddings_df.to_csv('cusip_embeddings.csv', index=False)\n",
    "```\n",
    "\n",
    "### Finding Similar Securities\n",
    "\n",
    "```python\n",
    "# Calculate similarities between CUSIPs\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Get embeddings\n",
    "embedding_matrix = embeddings_df.iloc[:, 1:].values\n",
    "\n",
    "# Find most similar to first CUSIP\n",
    "similarities = cosine_similarity(embedding_matrix[0:1], embedding_matrix)[0]\n",
    "top_10_indices = similarities.argsort()[-11:-1][::-1]  # Exclude self\n",
    "similar_cusips = embeddings_df.iloc[top_10_indices]['cusip'].values\n",
    "```\n",
    "\n",
    "## Key Features and Benefits\n",
    "\n",
    "### Advantages of This Approach\n",
    "\n",
    "1. **Robust Feature Engineering**\n",
    "   - Handles multiple data types (dates, decimals, categoricals)\n",
    "   - Intelligent missing value imputation\n",
    "   - Domain-specific transformations (360/30 convention)\n",
    "\n",
    "2. **Scalable Architecture**\n",
    "   - Siamese networks learn from pairs efficiently\n",
    "   - Can handle large CUSIP databases\n",
    "   - Embeddings enable fast similarity searches\n",
    "\n",
    "3. **Production Ready**\n",
    "   - Saves all artifacts for deployment\n",
    "   - Handles unseen categories gracefully\n",
    "   - Includes comprehensive error handling\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- **Portfolio Analysis**: Find similar bonds for diversification\n",
    "- **Risk Management**: Identify securities with similar risk profiles\n",
    "- **Trading**: Discover arbitrage opportunities between similar securities\n",
    "- **Research**: Cluster securities for market analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e6d6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# CONSTANTS AND UTILITIES\n",
    "# ===========================\n",
    "\n",
    "NUM_OF_DAYS_IN_YEAR = 360  # MSRB convention\n",
    "\n",
    "# Coupon frequency mapping\n",
    "COUPON_FREQUENCY_DICT = {\n",
    "    0: 'Unknown', 1: 'Semiannually', 2: 'Monthly', 3: 'Annually', 4: 'Weekly',\n",
    "    5: 'Quarterly', 6: 'Every 2 years', 7: 'Every 3 years', 8: 'Every 4 years',\n",
    "    9: 'Every 5 years', 10: 'Every 7 years', 11: 'Every 8 years', 12: 'Biweekly',\n",
    "    13: 'Changeable', 14: 'Daily', 15: 'Term mode', 16: 'Interest at maturity',\n",
    "    17: 'Bimonthly', 18: 'Every 13 weeks', 19: 'Irregular', 20: 'Every 28 days',\n",
    "    21: 'Every 35 days', 22: 'Every 26 weeks', 23: 'Not Applicable', 24: 'Tied to prime',\n",
    "    25: 'One time', 26: 'Every 10 years', 27: 'Frequency to be determined',\n",
    "    28: 'Mandatory put', 29: 'Every 52 weeks', 30: 'When interest adjusts-commercial paper',\n",
    "    31: 'Zero coupon', 32: 'Certain years only', 33: 'Under certain circumstances',\n",
    "    34: 'Every 15 years', 35: 'Custom', 36: 'Single Interest Payment'\n",
    "}\n",
    "\n",
    "COUPON_FREQUENCY_TYPE = {\n",
    "    'Unknown': 1e6, 'Semiannually': 2, 'Monthly': 12, 'Annually': 1,\n",
    "    'Weekly': 52, 'Quarterly': 4, 'Every 2 years': 0.5, 'Every 3 years': 1/3,\n",
    "    'Every 4 years': 0.25, 'Every 5 years': 0.2, 'Every 7 years': 1/7,\n",
    "    'Every 8 years': 1/8, 'Every 10 years': 1/10, 'Biweekly': 26,\n",
    "    'Changeable': 44, 'Daily': 360, 'Interest at maturity': 0, 'Not Applicable': 1e6\n",
    "}\n",
    "\n",
    "# Helper to convert Decimal to float\n",
    "def to_numeric(series):\n",
    "    \"\"\"Convert a series to numeric, handling Decimal types\"\"\"\n",
    "    if series.dtype == object:\n",
    "        # Check if series contains Decimal objects\n",
    "        if any(isinstance(x, Decimal) for x in series.dropna().head()):\n",
    "            return series.apply(lambda x: float(x) if isinstance(x, Decimal) else x).astype(float)\n",
    "    return pd.to_numeric(series, errors='coerce')\n",
    "\n",
    "# Default values for missing features\n",
    "FEATURES_AND_DEFAULT_VALUES = {\n",
    "    'purpose_class': 0, 'call_timing': 0, 'call_timing_in_part': 0,\n",
    "    'sink_frequency': 0, 'sink_amount_type': 10, 'issue_text': 'No issue text',\n",
    "    'state_tax_status': 0, 'series_name': 'No series name', 'transaction_type': 'I',\n",
    "    'next_call_price': 100, 'par_call_price': 100, 'min_amount_outstanding': 0,\n",
    "    'max_amount_outstanding': 0, 'maturity_amount': 0,\n",
    "    'issue_price': lambda df: to_numeric(df.issue_price).mean(skipna=True) if 'issue_price' in df.columns else 100,\n",
    "    'orig_principal_amount': lambda df: np.log10(to_numeric(10 ** to_numeric(df.orig_principal_amount)).mean(skipna=True)) if 'orig_principal_amount' in df.columns else 0,\n",
    "    'par_price': 100, 'called_redemption_type': 0, 'extraordinary_make_whole_call': False,\n",
    "    'make_whole_call': False, 'default_indicator': False, 'days_to_settle': 0,\n",
    "    'days_to_maturity': 0, 'days_to_refund': 0, 'call_to_maturity': 0,\n",
    "    'days_in_interest_payment': 180\n",
    "}\n",
    "\n",
    "FEATURES_AND_DEFAULT_COLUMNS = {\n",
    "    'days_to_par': 'days_to_maturity',\n",
    "    'days_to_call': 'days_to_maturity'\n",
    "}\n",
    "\n",
    "# ===========================\n",
    "# HELPER FUNCTIONS\n",
    "# ===========================\n",
    "\n",
    "def diff_in_days_two_dates_360_30(end_date, start_date):\n",
    "    \"\"\"Calculate difference in days using 360/30 convention (MSRB Rule G-33)\"\"\"\n",
    "    if pd.isna(end_date) or pd.isna(start_date):\n",
    "        return np.nan\n",
    "    \n",
    "    Y2, Y1 = end_date.year, start_date.year\n",
    "    M2, M1 = end_date.month, start_date.month\n",
    "    D2, D1 = end_date.day, start_date.day\n",
    "    \n",
    "    D1 = min(D1, 30)\n",
    "    if D1 == 30:\n",
    "        D2 = min(D2, 30)\n",
    "    \n",
    "    return (Y2 - Y1) * 360 + (M2 - M1) * 30 + (D2 - D1)\n",
    "\n",
    "def diff_in_days_two_dates_exact(end_date, start_date):\n",
    "    \"\"\"Calculate exact difference in days\"\"\"\n",
    "    if pd.isna(end_date) or pd.isna(start_date):\n",
    "        return np.nan\n",
    "    return (end_date - start_date).days\n",
    "\n",
    "def diff_in_days(trade, convention='360/30', calc_type=None):\n",
    "    \"\"\"Calculate days difference for a trade\"\"\"\n",
    "    if calc_type == 'accrual':\n",
    "        if pd.isnull(trade.get('accrual_date')):\n",
    "            start_date = trade.get('dated_date')\n",
    "        else:\n",
    "            start_date = trade.get('accrual_date')\n",
    "    else:\n",
    "        start_date = trade.get('dated_date')\n",
    "    \n",
    "    end_date = trade.get('settlement_date')\n",
    "    \n",
    "    if pd.isna(start_date) or pd.isna(end_date):\n",
    "        return 0\n",
    "    \n",
    "    if convention == '360/30':\n",
    "        return diff_in_days_two_dates_360_30(end_date, start_date)\n",
    "    else:\n",
    "        return diff_in_days_two_dates_exact(end_date, start_date)\n",
    "\n",
    "def days_in_interest_payment(trade):\n",
    "    \"\"\"Calculate days in interest payment period\"\"\"\n",
    "    if 'interest_payment_frequency' not in trade:\n",
    "        return 180  # Default\n",
    "    \n",
    "    freq_val = trade['interest_payment_frequency']\n",
    "    \n",
    "    # Handle if it's already converted to string\n",
    "    if isinstance(freq_val, str):\n",
    "        frequency = COUPON_FREQUENCY_TYPE.get(freq_val, 1e6)\n",
    "    else:\n",
    "        # Convert numeric code to string first\n",
    "        freq_str = COUPON_FREQUENCY_DICT.get(freq_val, 'Unknown')\n",
    "        frequency = COUPON_FREQUENCY_TYPE.get(freq_str, 1e6)\n",
    "    \n",
    "    if frequency == 0 or frequency >= 1e6:\n",
    "        return 1e6\n",
    "    \n",
    "    return 360 / frequency\n",
    "\n",
    "def calculate_a_over_e(row):\n",
    "    \"\"\"Calculate A/E ratio\"\"\"\n",
    "    if not pd.isnull(row.get('previous_coupon_payment_date')) and not pd.isnull(row.get('settlement_date')):\n",
    "        try:\n",
    "            A = (row['settlement_date'] - row['previous_coupon_payment_date']).days\n",
    "            days_ip = row.get('days_in_interest_payment', 180)\n",
    "            if days_ip > 0:\n",
    "                return A / days_ip\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    accrued = row.get('accrued_days', 0)\n",
    "    return accrued / NUM_OF_DAYS_IN_YEAR if NUM_OF_DAYS_IN_YEAR > 0 else 0\n",
    "\n",
    "def fill_missing_values(df):\n",
    "    \"\"\"Fill missing values with defaults\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Fill with default values\n",
    "    for feature, default_value in FEATURES_AND_DEFAULT_VALUES.items():\n",
    "        if feature in df.columns:\n",
    "            if callable(default_value):\n",
    "                try:\n",
    "                    default_value = default_value(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not compute default for {feature}: {e}\")\n",
    "                    default_value = 0 if feature in ['orig_principal_amount'] else 100\n",
    "            df[feature] = df[feature].fillna(default_value)\n",
    "    \n",
    "    # Fill with other columns\n",
    "    for feature, feature_to_replace_with in FEATURES_AND_DEFAULT_COLUMNS.items():\n",
    "        if feature in df.columns and feature_to_replace_with in df.columns:\n",
    "            df[feature] = df[feature].fillna(df[feature_to_replace_with])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ===========================\n",
    "# FEATURE ENGINEERING\n",
    "# ===========================\n",
    "\n",
    "def process_features(df):\n",
    "    \"\"\"Main feature processing function from the original codebase\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert Decimal columns to float\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            # Check if column contains Decimal objects\n",
    "            sample = df[col].dropna().head()\n",
    "            if len(sample) > 0 and any(isinstance(x, Decimal) for x in sample):\n",
    "                df[col] = df[col].apply(lambda x: float(x) if isinstance(x, Decimal) else x)\n",
    "    \n",
    "    # Process interest payment frequency\n",
    "    if 'interest_payment_frequency' in df.columns:\n",
    "        df['interest_payment_frequency'] = df['interest_payment_frequency'].fillna(0)\n",
    "        df['interest_payment_frequency'] = df['interest_payment_frequency'].apply(\n",
    "            lambda x: COUPON_FREQUENCY_DICT.get(int(x), 'Unknown') if isinstance(x, (int, float)) else x\n",
    "        )\n",
    "    \n",
    "    # Process quantity\n",
    "    if 'par_traded' in df.columns:\n",
    "        df['par_traded'] = to_numeric(df['par_traded'])\n",
    "        df['quantity'] = np.log10(df['par_traded'].clip(lower=1))\n",
    "    \n",
    "    # Process amounts with log transformation\n",
    "    for col in ['issue_amount', 'maturity_amount', 'orig_principal_amount', 'max_amount_outstanding']:\n",
    "        if col in df.columns:\n",
    "            df[col] = to_numeric(df[col])\n",
    "            df[col] = np.log10(1 + df[col].fillna(0).clip(lower=0))\n",
    "    \n",
    "    # Process coupon\n",
    "    if 'coupon' in df.columns:\n",
    "        df['coupon'] = to_numeric(df['coupon']).fillna(0)\n",
    "    \n",
    "    # Create binary features\n",
    "    if 'is_callable' in df.columns:\n",
    "        df['callable'] = df['is_callable'].astype(bool).astype(int)\n",
    "    if 'is_called' in df.columns:\n",
    "        df['called'] = df['is_called'].astype(bool).astype(int)\n",
    "    if 'coupon' in df.columns:\n",
    "        df['zerocoupon'] = (df['coupon'] == 0).astype(int)\n",
    "    if 'delivery_date' in df.columns and 'trade_date' in df.columns:\n",
    "        df['whenissued'] = (df['delivery_date'] >= df['trade_date']).astype(int)\n",
    "    if 'next_sink_date' in df.columns:\n",
    "        df['sinking'] = (~df['next_sink_date'].isnull()).astype(int)\n",
    "    if 'interest_payment_frequency' in df.columns and 'zerocoupon' in df.columns:\n",
    "        df['deferred'] = ((df['interest_payment_frequency'] == 'Unknown') | (df['zerocoupon'] == 1)).astype(int)\n",
    "    \n",
    "    # Process dates - convert to days from trade date\n",
    "    if 'settlement_date' in df.columns and 'trade_date' in df.columns:\n",
    "        df['days_to_settle'] = (df['settlement_date'] - df['trade_date']).dt.days.fillna(0)\n",
    "        # Remove trades settling >= 30 days from trade date\n",
    "        df = df[df['days_to_settle'] < 30]\n",
    "    \n",
    "    # Calculate days to various dates with log transformation\n",
    "    date_calculations = [\n",
    "        ('days_to_maturity', 'maturity_date', 'trade_date'),\n",
    "        ('days_to_call', 'next_call_date', 'trade_date'),\n",
    "        ('days_to_refund', 'refund_date', 'trade_date'),\n",
    "        ('days_to_par', 'par_call_date', 'trade_date'),\n",
    "        ('call_to_maturity', 'maturity_date', 'next_call_date')\n",
    "    ]\n",
    "    \n",
    "    for new_col, end_col, start_col in date_calculations:\n",
    "        if end_col in df.columns and start_col in df.columns:\n",
    "            days_diff = (df[end_col] - df[start_col]).dt.days.fillna(0).clip(lower=0)\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore')\n",
    "                df[new_col] = np.log10(1 + days_diff)\n",
    "                df[new_col] = df[new_col].replace([-np.inf, np.inf], 0)\n",
    "    \n",
    "    # Calculate accrual features\n",
    "    if 'settlement_date' in df.columns:\n",
    "        df['accrued_days'] = df.apply(lambda row: diff_in_days(row, calc_type='accrual'), axis=1)\n",
    "        df['accrued_days'] = df['accrued_days'].fillna(0)\n",
    "    \n",
    "    if 'interest_payment_frequency' in df.columns:\n",
    "        df['days_in_interest_payment'] = df.apply(days_in_interest_payment, axis=1)\n",
    "    else:\n",
    "        df['days_in_interest_payment'] = 180\n",
    "    \n",
    "    if 'accrued_days' in df.columns and 'days_in_interest_payment' in df.columns:\n",
    "        df['scaled_accrued_days'] = df['accrued_days'] / (360 / df['days_in_interest_payment'].clip(lower=1))\n",
    "        df['scaled_accrued_days'] = df['scaled_accrued_days'].fillna(0)\n",
    "        df['A/E'] = df.apply(calculate_a_over_e, axis=1)\n",
    "        df['A/E'] = df['A/E'].fillna(0)\n",
    "    \n",
    "    # Process rating\n",
    "    if 'sp_long' in df.columns:\n",
    "        df['sp_long'] = df['sp_long'].fillna('MR')\n",
    "        df['rating'] = df['sp_long']\n",
    "    elif 'rating' not in df.columns:\n",
    "        df['rating'] = 'MR'\n",
    "    \n",
    "    # Fill missing values\n",
    "    df = fill_missing_values(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def engineer_features_complete(df_raw: pd.DataFrame, fit: bool=True, artifacts: dict=None):\n",
    "    \"\"\"\n",
    "    Complete feature engineering - ROBUST VERSION\n",
    "    Handles all nullable integer types properly\n",
    "    \"\"\"\n",
    "    # First apply the original processing\n",
    "    df = process_features(df_raw)\n",
    "    \n",
    "    if artifacts is None:\n",
    "        artifacts = {}\n",
    "    \n",
    "    # Binary features - ensure they're numeric\n",
    "    binary_features = [\n",
    "        'callable', 'called', 'zerocoupon', 'whenissued', 'sinking', 'deferred',\n",
    "        'is_non_transaction_based_compensation', 'is_general_obligation',\n",
    "        'callable_at_cav', 'extraordinary_make_whole_call', 'make_whole_call',\n",
    "        'has_unexpired_lines_of_credit', 'escrow_exists', 'default_indicator'\n",
    "    ]\n",
    "    \n",
    "    # Numeric features (direct - no transformation)\n",
    "    numeric_direct = [\n",
    "        'coupon', 'issue_price', 'par_price', 'original_yield',\n",
    "        'next_call_price', 'par_call_price', 'days_to_settle',\n",
    "        'days_to_maturity', 'days_to_call', 'days_to_refund',\n",
    "        'days_to_par', 'call_to_maturity', 'accrued_days',\n",
    "        'days_in_interest_payment', 'scaled_accrued_days', 'A/E'\n",
    "    ]\n",
    "    \n",
    "    # Numeric features (already log-transformed)\n",
    "    numeric_log = [\n",
    "        'quantity', 'issue_amount', 'maturity_amount',\n",
    "        'orig_principal_amount', 'max_amount_outstanding'\n",
    "    ]\n",
    "    \n",
    "    # All potential categorical features\n",
    "    categorical_features = [\n",
    "        'incorporated_state_code', 'trade_type', 'purpose_class',\n",
    "        'rating', 'purpose_sub_class', 'called_redemption_type',\n",
    "        'call_timing', 'call_timing_in_part', 'sink_frequency',\n",
    "        'sink_amount_type', 'state_tax_status', 'transaction_type',\n",
    "        'coupon_type', 'federal_tax_status', 'use_of_proceeds',\n",
    "        'muni_security_type', 'muni_issue_type', 'capital_type',\n",
    "        'other_enhancement_type', 'series_name'\n",
    "    ]\n",
    "    \n",
    "    # Collect features\n",
    "    feature_list = []\n",
    "    feature_names = []\n",
    "    \n",
    "    # Binary features\n",
    "    for feat in binary_features:\n",
    "        if feat in df.columns:\n",
    "            # Ensure numeric and handle any remaining issues\n",
    "            val = pd.to_numeric(df[feat], errors='coerce').fillna(0).astype(float).values.reshape(-1, 1)\n",
    "            feature_list.append(val)\n",
    "            feature_names.append(feat)\n",
    "    \n",
    "    # Numeric direct features\n",
    "    for feat in numeric_direct:\n",
    "        if feat in df.columns:\n",
    "            val = to_numeric(df[feat]).fillna(0).astype(float).values.reshape(-1, 1)\n",
    "            feature_list.append(val)\n",
    "            feature_names.append(feat)\n",
    "    \n",
    "    # Numeric log features (already transformed)\n",
    "    for feat in numeric_log:\n",
    "        if feat in df.columns:\n",
    "            val = to_numeric(df[feat]).fillna(0).astype(float).values.reshape(-1, 1)\n",
    "            feature_list.append(val)\n",
    "            feature_names.append(feat)\n",
    "    \n",
    "    # Add rating score\n",
    "    rating_map = {\n",
    "        \"AAA\": 22, \"AA+\": 21, \"AA\": 20, \"AA-\": 19,\n",
    "        \"A+\": 18, \"A\": 17, \"A-\": 16,\n",
    "        \"BBB+\": 15, \"BBB\": 14, \"BBB-\": 13,\n",
    "        \"BB+\": 12, \"BB\": 11, \"BB-\": 10,\n",
    "        \"B+\": 9, \"B\": 8, \"B-\": 7,\n",
    "        \"CCC+\": 6, \"CCC\": 5, \"CCC-\": 4,\n",
    "        \"CC\": 3, \"C\": 2, \"D\": 1, \"MR\": 0, \"NR\": 0\n",
    "    }\n",
    "    \n",
    "    if 'rating' in df.columns:\n",
    "        rating_score = df['rating'].map(rating_map).fillna(0).values.reshape(-1, 1)\n",
    "        feature_list.append(rating_score)\n",
    "        feature_names.append('rating_score')\n",
    "    \n",
    "    # Combine numeric features\n",
    "    if feature_list:\n",
    "        X_numeric = np.hstack(feature_list)\n",
    "    else:\n",
    "        X_numeric = np.zeros((len(df), 0))\n",
    "    \n",
    "    # Scale numeric features\n",
    "    if fit:\n",
    "        scaler = RobustScaler()\n",
    "        X_numeric_scaled = scaler.fit_transform(X_numeric)\n",
    "        artifacts['scaler'] = scaler\n",
    "    else:\n",
    "        scaler = artifacts.get('scaler')\n",
    "        if scaler:\n",
    "            X_numeric_scaled = scaler.transform(X_numeric)\n",
    "        else:\n",
    "            X_numeric_scaled = X_numeric\n",
    "    \n",
    "    # One-hot encode categorical features with robust handling\n",
    "    cat_encoded_list = []\n",
    "    cat_feature_names = []\n",
    "    \n",
    "    if fit:\n",
    "        artifacts['encoders'] = {}\n",
    "    \n",
    "    for cat in categorical_features:\n",
    "        if cat in df.columns:\n",
    "            # Check the dtype and handle appropriately\n",
    "            dtype_name = str(df[cat].dtype)\n",
    "            \n",
    "            # Handle nullable integer types (Int64, Int32, etc.)\n",
    "            if 'Int' in dtype_name or 'int' in dtype_name.lower():\n",
    "                # Convert to regular numpy int, then to string\n",
    "                # Use copy to avoid modifying original\n",
    "                cat_series = df[cat].copy()\n",
    "                # Fill NaN with a special integer value first\n",
    "                cat_series = cat_series.fillna(-9999)\n",
    "                # Convert to regular int64 (not nullable)\n",
    "                cat_series = cat_series.astype('int64')\n",
    "                # Now convert to string\n",
    "                cat_values = cat_series.astype(str)\n",
    "                # Replace the special value marker\n",
    "                cat_values = cat_values.replace('-9999', 'MISSING')\n",
    "            \n",
    "            # Handle float types\n",
    "            elif 'float' in dtype_name.lower():\n",
    "                # Convert float to int first, then to string\n",
    "                cat_series = df[cat].copy()\n",
    "                cat_series = cat_series.fillna(-9999.0)\n",
    "                cat_series = cat_series.astype('int64')\n",
    "                cat_values = cat_series.astype(str)\n",
    "                cat_values = cat_values.replace('-9999', 'MISSING')\n",
    "            \n",
    "            # Handle regular object/string types\n",
    "            else:\n",
    "                # Standard string handling\n",
    "                cat_values = df[cat].fillna('MISSING').astype(str)\n",
    "            \n",
    "            if fit:\n",
    "                encoder = LabelEncoder()\n",
    "                unique_vals = cat_values.unique().tolist()\n",
    "                if 'UNKNOWN' not in unique_vals:\n",
    "                    unique_vals.append('UNKNOWN')\n",
    "                encoder.fit(unique_vals)\n",
    "                artifacts['encoders'][cat] = encoder\n",
    "            else:\n",
    "                encoder = artifacts.get('encoders', {}).get(cat)\n",
    "                if encoder is None:\n",
    "                    continue\n",
    "            \n",
    "            # Handle unseen categories\n",
    "            cat_values = cat_values.apply(lambda x: x if x in encoder.classes_ else 'UNKNOWN')\n",
    "            encoded = encoder.transform(cat_values)\n",
    "            \n",
    "            # One-hot encode\n",
    "            n_classes = len(encoder.classes_)\n",
    "            one_hot = np.zeros((len(df), n_classes))\n",
    "            one_hot[np.arange(len(df)), encoded] = 1\n",
    "            \n",
    "            cat_encoded_list.append(one_hot)\n",
    "            \n",
    "            # Add feature names\n",
    "            for i, class_name in enumerate(encoder.classes_):\n",
    "                cat_feature_names.append(f\"{cat}_{class_name}\")\n",
    "    \n",
    "    # Combine all features\n",
    "    if cat_encoded_list:\n",
    "        X_cat = np.hstack(cat_encoded_list)\n",
    "        X = np.hstack([X_numeric_scaled, X_cat])\n",
    "        all_feature_names = feature_names + cat_feature_names\n",
    "    else:\n",
    "        X = X_numeric_scaled\n",
    "        all_feature_names = feature_names\n",
    "    \n",
    "    print(f\"Engineered {len(all_feature_names)} features\")\n",
    "    return X.astype(np.float32), artifacts, all_feature_names\n",
    "# ===========================\n",
    "# SIAMESE NETWORK (Your original code with updated feature engineering)\n",
    "# ===========================\n",
    "\n",
    "def create_base_network(input_dim, embedding_dim=128):\n",
    "    \"\"\"Create the base network for one side of the Siamese network\"\"\"\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    \n",
    "    x = layers.Dense(512, activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    \n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    \n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    \n",
    "    embeddings = layers.Dense(embedding_dim, activation='linear', name='embeddings')(x)\n",
    "    embeddings = layers.Lambda(lambda x: tf.nn.l2_normalize(x, axis=1))(embeddings)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=embeddings, name='base_network')\n",
    "    return model\n",
    "\n",
    "def create_siamese_network(input_dim, embedding_dim=128):\n",
    "    \"\"\"Create the full Siamese network\"\"\"\n",
    "    base_network = create_base_network(input_dim, embedding_dim)\n",
    "    \n",
    "    input_a = Input(shape=(input_dim,), name='input_a')\n",
    "    input_b = Input(shape=(input_dim,), name='input_b')\n",
    "    \n",
    "    embedding_a = base_network(input_a)\n",
    "    embedding_b = base_network(input_b)\n",
    "    \n",
    "    cosine_similarity = layers.Dot(axes=1, normalize=False)([embedding_a, embedding_b])\n",
    "    \n",
    "    siamese_model = Model(inputs=[input_a, input_b], outputs=cosine_similarity)\n",
    "    \n",
    "    return siamese_model, base_network\n",
    "\n",
    "def contrastive_loss(y_true, y_pred, margin=0.5):\n",
    "    \"\"\"Contrastive loss for Siamese network\"\"\"\n",
    "    y_pred_dist = 1 - y_pred\n",
    "    pos_loss = y_true * tf.square(y_pred_dist)\n",
    "    neg_loss = (1 - y_true) * tf.square(tf.maximum(0.0, margin - y_pred_dist))\n",
    "    return tf.reduce_mean(pos_loss + neg_loss)\n",
    "\n",
    "def prepare_pairs_for_training(features_df, pairs_df, artifacts=None):\n",
    "    \"\"\"Prepare pairs of CUSIPs for training\"\"\"\n",
    "    print(\"Engineering features...\")\n",
    "    \n",
    "    # Ensure date columns are datetime\n",
    "    date_columns = [\n",
    "        'refund_date', 'accrual_date', 'dated_date', 'next_sink_date',\n",
    "        'delivery_date', 'trade_date', 'trade_datetime', 'par_call_date',\n",
    "        'maturity_date', 'settlement_date', 'next_call_date',\n",
    "        'previous_coupon_payment_date', 'next_coupon_payment_date',\n",
    "        'first_coupon_date', 'last_period_accrues_from_date'\n",
    "    ]\n",
    "    \n",
    "    for col in date_columns:\n",
    "        if col in features_df.columns:\n",
    "            features_df[col] = pd.to_datetime(features_df[col], errors='coerce')\n",
    "    \n",
    "    # Use the complete feature engineering\n",
    "    X_all, artifacts, feature_names = engineer_features_complete(\n",
    "        features_df, \n",
    "        fit=(artifacts is None), \n",
    "        artifacts=artifacts\n",
    "    )\n",
    "    \n",
    "    print(f\"Feature dimension: {X_all.shape[1]}\")\n",
    "    print(f\"Total CUSIPs processed: {X_all.shape[0]}\")\n",
    "    \n",
    "    cusip_to_vec = dict(zip(features_df[\"cusip\"].values, X_all))\n",
    "    \n",
    "    features_a = []\n",
    "    features_b = []\n",
    "    labels = []\n",
    "    missing_cusips = set()\n",
    "    \n",
    "    for _, row in pairs_df.iterrows():\n",
    "        cusip1, cusip2 = row[\"cusip1\"], row[\"cusip2\"]\n",
    "        \n",
    "        if cusip1 in cusip_to_vec and cusip2 in cusip_to_vec:\n",
    "            features_a.append(cusip_to_vec[cusip1])\n",
    "            features_b.append(cusip_to_vec[cusip2])\n",
    "            labels.append(row[\"label\"])\n",
    "        else:\n",
    "            if cusip1 not in cusip_to_vec:\n",
    "                missing_cusips.add(cusip1)\n",
    "            if cusip2 not in cusip_to_vec:\n",
    "                missing_cusips.add(cusip2)\n",
    "    \n",
    "    if missing_cusips:\n",
    "        print(f\"Warning: {len(missing_cusips)} CUSIPs from pairs not found in features\")\n",
    "    \n",
    "    X_pairs = (np.array(features_a, dtype=np.float32), \n",
    "               np.array(features_b, dtype=np.float32))\n",
    "    y = np.array(labels, dtype=np.float32)\n",
    "    \n",
    "    print(f\"Valid pairs prepared: {len(y)}\")\n",
    "    \n",
    "    return X_pairs, y, artifacts, feature_names\n",
    "\n",
    "def train_siamese_network(X_train, y_train, X_val, y_val, \n",
    "                         input_dim, embedding_dim=128, \n",
    "                         epochs=100, batch_size=256):\n",
    "    \"\"\"Train the Siamese network\"\"\"\n",
    "    siamese_model, base_network = create_siamese_network(input_dim, embedding_dim)\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    siamese_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=contrastive_loss,\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    print(\"\\nBase network summary:\")\n",
    "    base_network.summary()\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "        ModelCheckpoint('best_siamese_model.h5', save_best_only=True, monitor='val_loss', verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nStarting training...\")\n",
    "    history = siamese_model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return siamese_model, base_network, history\n",
    "\n",
    "def get_embeddings(cusip_features_df, base_network, artifacts):\n",
    "    \"\"\"Get embeddings for CUSIPs\"\"\"\n",
    "    # Ensure date columns are datetime\n",
    "    date_columns = [\n",
    "        'refund_date', 'accrual_date', 'dated_date', 'next_sink_date',\n",
    "        'delivery_date', 'trade_date', 'trade_datetime', 'par_call_date',\n",
    "        'maturity_date', 'settlement_date', 'next_call_date',\n",
    "        'previous_coupon_payment_date', 'next_coupon_payment_date',\n",
    "        'first_coupon_date', 'last_period_accrues_from_date'\n",
    "    ]\n",
    "    \n",
    "    for col in date_columns:\n",
    "        if col in cusip_features_df.columns:\n",
    "            cusip_features_df[col] = pd.to_datetime(cusip_features_df[col], errors='coerce')\n",
    "    \n",
    "    X_features, _, _ = engineer_features_complete(cusip_features_df, fit=False, artifacts=artifacts)\n",
    "    embeddings = base_network.predict(X_features, batch_size=256)\n",
    "    \n",
    "    embedding_cols = [f'emb_{i}' for i in range(embeddings.shape[1])]\n",
    "    embeddings_df = pd.DataFrame(embeddings, columns=embedding_cols)\n",
    "    embeddings_df['cusip'] = cusip_features_df['cusip'].values\n",
    "    embeddings_df = embeddings_df[['cusip'] + embedding_cols]\n",
    "    \n",
    "    return embeddings_df\n",
    "\n",
    "def run_training_pipeline(features_df, pairs_df, test_size=0.2, embedding_dim=128, epochs=100):\n",
    "    \"\"\"Run the complete training pipeline\"\"\"\n",
    "    print(\"Preparing features and pairs...\")\n",
    "    X_pairs, y, artifacts, feature_names = prepare_pairs_for_training(features_df, pairs_df)\n",
    "    \n",
    "    print(\"\\nSplitting data...\")\n",
    "    X_train_a, X_val_a, X_train_b, X_val_b, y_train, y_val = train_test_split(\n",
    "        X_pairs[0], X_pairs[1], y, \n",
    "        test_size=test_size, \n",
    "        random_state=42, \n",
    "        stratify=y\n",
    "    )\n",
    "    X_train = (X_train_a, X_train_b)\n",
    "    X_val = (X_val_a, X_val_b)\n",
    "    \n",
    "    input_dim = X_train[0].shape[1]\n",
    "    print(f\"\\nData summary:\")\n",
    "    print(f\"  Input dimension: {input_dim}\")\n",
    "    print(f\"  Number of features: {len(feature_names)}\")\n",
    "    print(f\"  Training samples: {len(y_train):,}\")\n",
    "    print(f\"  Validation samples: {len(y_val):,}\")\n",
    "    print(f\"  Positive ratio in train: {y_train.mean():.2%}\")\n",
    "    print(f\"  Positive ratio in val: {y_val.mean():.2%}\")\n",
    "    \n",
    "    siamese_model, base_network, history = train_siamese_network(\n",
    "        X_train, y_train, X_val, y_val,\n",
    "        input_dim=input_dim,\n",
    "        embedding_dim=embedding_dim,\n",
    "        epochs=epochs\n",
    "    )\n",
    "    \n",
    "    print(\"\\nSaving model and artifacts...\")\n",
    "    base_network.save('cusip_embedding_model.h5')\n",
    "    \n",
    "    with open('feature_artifacts.pkl', 'wb') as f:\n",
    "        pickle.dump(artifacts, f)\n",
    "    \n",
    "    with open('feature_names.pkl', 'wb') as f:\n",
    "        pickle.dump(feature_names, f)\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    print(f\"Final validation loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "    \n",
    "    return base_network, artifacts, history\n",
    "\n",
    "# Train the model\n",
    "base_network, artifacts, history = run_training_pipeline(\n",
    "    df,  \n",
    "    pairs_df,\n",
    "    test_size=0.2,\n",
    "    embedding_dim=128,\n",
    "    epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ab93b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered 301 features\n",
      "\u001b[1m5475/5492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 02:23:27.712045: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5492/5492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Get unique CUSIPs BEFORE generating embeddings\n",
    "# This prevents generating 1.4M embeddings for duplicate CUSIPs\n",
    "unique_cusips_df = df.sort_values('trade_datetime').groupby('cusip').last().reset_index()\n",
    "print(f\"Original data shape: {df.shape}\")\n",
    "print(f\"Unique CUSIPs to embed: {len(unique_cusips_df)}\")\n",
    "\n",
    "# Generate embeddings for unique CUSIPs only\n",
    "embeddings_df = get_embeddings(unique_cusips_df, base_network, artifacts)\n",
    "print(f\"Generated embeddings shape: {embeddings_df.shape}\")\n",
    "\n",
    "# Save the correct embeddings\n",
    "embeddings_df.to_csv('cusip_embeddings_unique.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e7f7f-8853-4570-a331-eb657ba3e484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cusip</th>\n",
       "      <th>emb_0</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>emb_5</th>\n",
       "      <th>emb_6</th>\n",
       "      <th>emb_7</th>\n",
       "      <th>emb_8</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_118</th>\n",
       "      <th>emb_119</th>\n",
       "      <th>emb_120</th>\n",
       "      <th>emb_121</th>\n",
       "      <th>emb_122</th>\n",
       "      <th>emb_123</th>\n",
       "      <th>emb_124</th>\n",
       "      <th>emb_125</th>\n",
       "      <th>emb_126</th>\n",
       "      <th>emb_127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19648FYV0</td>\n",
       "      <td>-0.046834</td>\n",
       "      <td>0.087962</td>\n",
       "      <td>0.082753</td>\n",
       "      <td>0.083307</td>\n",
       "      <td>0.072688</td>\n",
       "      <td>-0.051867</td>\n",
       "      <td>-0.070449</td>\n",
       "      <td>0.090668</td>\n",
       "      <td>-0.025534</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059696</td>\n",
       "      <td>-0.127566</td>\n",
       "      <td>0.007473</td>\n",
       "      <td>0.075059</td>\n",
       "      <td>0.029351</td>\n",
       "      <td>-0.001663</td>\n",
       "      <td>-0.063635</td>\n",
       "      <td>0.183805</td>\n",
       "      <td>0.037120</td>\n",
       "      <td>0.080757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>180848XJ7</td>\n",
       "      <td>0.002178</td>\n",
       "      <td>0.087889</td>\n",
       "      <td>0.083040</td>\n",
       "      <td>0.080956</td>\n",
       "      <td>0.072237</td>\n",
       "      <td>-0.055294</td>\n",
       "      <td>-0.068476</td>\n",
       "      <td>0.090352</td>\n",
       "      <td>-0.025602</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061586</td>\n",
       "      <td>-0.131052</td>\n",
       "      <td>-0.018359</td>\n",
       "      <td>0.076542</td>\n",
       "      <td>0.032309</td>\n",
       "      <td>0.018512</td>\n",
       "      <td>-0.035759</td>\n",
       "      <td>0.205933</td>\n",
       "      <td>0.034215</td>\n",
       "      <td>0.045631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54639TDU3</td>\n",
       "      <td>0.102498</td>\n",
       "      <td>0.112877</td>\n",
       "      <td>0.107830</td>\n",
       "      <td>0.089851</td>\n",
       "      <td>0.068170</td>\n",
       "      <td>-0.042887</td>\n",
       "      <td>-0.056532</td>\n",
       "      <td>0.080180</td>\n",
       "      <td>0.008092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055697</td>\n",
       "      <td>-0.108302</td>\n",
       "      <td>-0.048728</td>\n",
       "      <td>0.101579</td>\n",
       "      <td>0.007120</td>\n",
       "      <td>0.038187</td>\n",
       "      <td>-0.005147</td>\n",
       "      <td>0.198760</td>\n",
       "      <td>0.030310</td>\n",
       "      <td>0.015362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>575896PX7</td>\n",
       "      <td>-0.044441</td>\n",
       "      <td>0.070050</td>\n",
       "      <td>0.083702</td>\n",
       "      <td>0.072695</td>\n",
       "      <td>0.074190</td>\n",
       "      <td>-0.052992</td>\n",
       "      <td>-0.061353</td>\n",
       "      <td>0.098432</td>\n",
       "      <td>-0.059692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059641</td>\n",
       "      <td>-0.122083</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.068069</td>\n",
       "      <td>0.036816</td>\n",
       "      <td>-0.002147</td>\n",
       "      <td>-0.055479</td>\n",
       "      <td>0.191326</td>\n",
       "      <td>0.030561</td>\n",
       "      <td>0.061081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>575896PX7</td>\n",
       "      <td>-0.035170</td>\n",
       "      <td>0.070882</td>\n",
       "      <td>0.081211</td>\n",
       "      <td>0.074073</td>\n",
       "      <td>0.076941</td>\n",
       "      <td>-0.051934</td>\n",
       "      <td>-0.060732</td>\n",
       "      <td>0.097129</td>\n",
       "      <td>-0.052026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056449</td>\n",
       "      <td>-0.123906</td>\n",
       "      <td>-0.004465</td>\n",
       "      <td>0.068960</td>\n",
       "      <td>0.033432</td>\n",
       "      <td>0.003381</td>\n",
       "      <td>-0.049986</td>\n",
       "      <td>0.196347</td>\n",
       "      <td>0.029966</td>\n",
       "      <td>0.057655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cusip     emb_0     emb_1     emb_2     emb_3     emb_4     emb_5  \\\n",
       "0  19648FYV0 -0.046834  0.087962  0.082753  0.083307  0.072688 -0.051867   \n",
       "1  180848XJ7  0.002178  0.087889  0.083040  0.080956  0.072237 -0.055294   \n",
       "2  54639TDU3  0.102498  0.112877  0.107830  0.089851  0.068170 -0.042887   \n",
       "3  575896PX7 -0.044441  0.070050  0.083702  0.072695  0.074190 -0.052992   \n",
       "4  575896PX7 -0.035170  0.070882  0.081211  0.074073  0.076941 -0.051934   \n",
       "\n",
       "      emb_6     emb_7     emb_8  ...   emb_118   emb_119   emb_120   emb_121  \\\n",
       "0 -0.070449  0.090668 -0.025534  ...  0.059696 -0.127566  0.007473  0.075059   \n",
       "1 -0.068476  0.090352 -0.025602  ...  0.061586 -0.131052 -0.018359  0.076542   \n",
       "2 -0.056532  0.080180  0.008092  ...  0.055697 -0.108302 -0.048728  0.101579   \n",
       "3 -0.061353  0.098432 -0.059692  ...  0.059641 -0.122083  0.000705  0.068069   \n",
       "4 -0.060732  0.097129 -0.052026  ...  0.056449 -0.123906 -0.004465  0.068960   \n",
       "\n",
       "    emb_122   emb_123   emb_124   emb_125   emb_126   emb_127  \n",
       "0  0.029351 -0.001663 -0.063635  0.183805  0.037120  0.080757  \n",
       "1  0.032309  0.018512 -0.035759  0.205933  0.034215  0.045631  \n",
       "2  0.007120  0.038187 -0.005147  0.198760  0.030310  0.015362  \n",
       "3  0.036816 -0.002147 -0.055479  0.191326  0.030561  0.061081  \n",
       "4  0.033432  0.003381 -0.049986  0.196347  0.029966  0.057655  \n",
       "\n",
       "[5 rows x 129 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = pd.read_csv('cusip_embeddings.csv')\n",
    "embedding.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e67982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = embedding.drop_duplicates(subset='cusip', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdee998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('2025-09-22_one_month.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d85077",
   "metadata": {},
   "source": [
    "# Municipal Bond Embeddings — Practical Analysis & Trading Applications\n",
    "\n",
    "Extract actionable insights from 128-dimensional bond embeddings trained via Siamese networks on market behavior patterns.\n",
    "\n",
    "## What Are These Embeddings?\n",
    "\n",
    "Our embeddings capture the \"behavioral DNA\" of municipal bonds - how they trade relative to others based on historical patterns. Bonds close in embedding space exhibit similar market behavior, regardless of their static characteristics. This can reveal non-obvious relationships and pricing inefficiencies.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "### 1. **Data Preparation**\n",
    "- Merge unique CUSIP embeddings with latest trade data\n",
    "- Extract 128-dimensional vectors normalized for cosine similarity\n",
    "- Retain key trading fields: yield, price, rating, maturity, state\n",
    "\n",
    "### 2. **Core Functions**\n",
    "\n",
    "#### **Similarity Search**\n",
    "Find bonds that behave similarly to a target CUSIP with practical filters:\n",
    "- Same state requirement (typical for muni analysis)\n",
    "- Maturity bands (±X years)\n",
    "- Rating bucket constraints (AAA/AA/A/BBB/BB/B)\n",
    "\n",
    "#### **Arbitrage Discovery**\n",
    "Identify pricing inefficiencies using simple logic:\n",
    "- **High embedding similarity** (>0.93) = bonds behave similarly in market\n",
    "- **Large yield spread** (>25 bps) = pricing discrepancy\n",
    "- **Result**: Buy lower yield, sell higher yield for convergence trade\n",
    "\n",
    "#### **Portfolio Concentration Analysis**\n",
    "Measure systematic risk from holding similar-behaving bonds:\n",
    "- Compute pairwise similarities within portfolio\n",
    "- Flag high concentration (avg similarity >0.85)\n",
    "- Identify most similar pairs for risk management\n",
    "\n",
    "### 3. **Key Outputs**\n",
    "\n",
    "Each analysis produces trader-friendly outputs:\n",
    "- **Similar bonds table**: CUSIP, similarity score, yield differential\n",
    "- **Arbitrage trades**: Buy/sell CUSIPs with spread in basis points\n",
    "- **Concentration metrics**: Average portfolio similarity with risk warnings\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "**Technical choices:**\n",
    "- Cosine similarity on L2-normalized vectors for speed\n",
    "- Vectorized numpy operations (no loops over 196k CUSIPs)\n",
    "- Optional filters match real trading constraints\n",
    "\n",
    "**Performance:**\n",
    "- Similarity search: <100ms for 10 neighbors among 196k bonds\n",
    "- Arbitrage scan: ~2 seconds for 500 bond sample\n",
    "- Portfolio analysis: <50ms for 100-bond portfolio\n",
    "\n",
    "## Production Use Cases\n",
    "\n",
    "1. **Daily Arbitrage Scanner**\n",
    "   - Run nightly on all bonds with >$1M daily volume\n",
    "   - Email top 10 opportunities to trading desk\n",
    "   - Track convergence over time\n",
    "\n",
    "2. **Trade Idea API**\n",
    "   ```\n",
    "   GET /api/similar-bonds/{cusip}?state=same&maturity_band=3\n",
    "   ```\n",
    "   Returns similar bonds with yields for relative value analysis\n",
    "\n",
    "3. **Portfolio Risk Dashboard**\n",
    "   - Upload holdings → receive concentration analysis\n",
    "   - Highlight bonds that are behavioral \"twins\"\n",
    "   - Suggest diversification candidates from different embedding clusters\n",
    "\n",
    "4. **Market Anomaly Detection**\n",
    "   - Flag bonds whose yields diverge from embedding neighbors\n",
    "   - Identify potential rating changes before agencies act\n",
    "   - Spot liquidity shifts via neighbor trading patterns\n",
    "\n",
    "## Interpreting Results\n",
    "\n",
    "**High similarity (>0.95) means:**\n",
    "- Bonds react similarly to market events\n",
    "- Similar flow patterns and trader behavior\n",
    "- Often (but not always) similar fundamentals\n",
    "\n",
    "**Arbitrage signals are strongest when:**\n",
    "- Same state, similar maturity (reduces exogenous factors)\n",
    "- High liquidity (>10 trades/day for both bonds)\n",
    "- No recent credit events or calls\n",
    "\n",
    "**Portfolio concentration warnings indicate:**\n",
    "- Systematic risk from correlated behaviors\n",
    "- Need for geographic/sector/duration diversification\n",
    "- Potential for synchronized losses in stress scenarios\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Backtest arbitrage signals** on historical data\n",
    "2. **Build real-time similarity API** for trader terminals\n",
    "3. **Integrate with existing** yield curve and relative value models\n",
    "4. **Create embedding drift monitoring** to detect market regime changes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be73d432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MUNICIPAL BOND EMBEDDING EXPLORER\n",
      "============================================================\n",
      "⚠️ Found 85 bonds with invalid yields (>15% or negative)\n",
      "   Examples:\n",
      "           cusip   yield\n",
      "3134   28304CBU0  32.779\n",
      "5358   13069AAW8  17.641\n",
      "10468  05753PCD2  19.309\n",
      "23431  574193MH8  19.365\n",
      "25514  927676MW3  24.689\n",
      "✓ Valid yields: 196,361 bonds, median: 3.26%, range: 0.00%-14.83%\n",
      "✓ Loaded 196,446 unique CUSIP embeddings\n",
      "✓ Merged with trade data: 196,446 CUSIPs with full data\n",
      "✓ Embedding dimension: 128\n",
      "\n",
      "============================================================\n",
      "EXAMPLE ANALYSES\n",
      "============================================================\n",
      "\n",
      "1️⃣ SIMILARITY SEARCH\n",
      "----------------------------------------\n",
      "\n",
      "📌 Query Bond: 19648FYV0\n",
      "   Yield: 4.641%\n",
      "   Rating: AA\n",
      "   Maturity: 2044-08-01 00:00:00\n",
      "   State: CO\n",
      "\n",
      "Most similar bonds:\n",
      "    cusip  similarity  yield  yield_diff_bps rating\n",
      "19648FJE5    0.999854  4.690             4.9     AA\n",
      "61531ABA4    0.999830  4.452           -18.9     AA\n",
      "249176DE5    0.999799  4.384           -25.7    AAA\n",
      "249182FR2    0.999724  4.515           -12.6    AA-\n",
      "249176DD7    0.999642  4.565            -7.6    AAA\n",
      "\n",
      "2️⃣ ARBITRAGE OPPORTUNITIES\n",
      "----------------------------------------\n",
      "\n",
      "🔍 Searching for arbitrage opportunities...\n",
      "   Criteria: similarity > 0.93, yield spread > 25 bps\n",
      "   Analyzing 196361 bonds with valid yields...\n",
      "✓ Found 5 unique arbitrage opportunities\n",
      "\n",
      "Top arbitrage trades:\n",
      "\n",
      "  Trade #290999:\n",
      "    BUY:  10624NDD3 @ 2.415% (Rating: AA-)\n",
      "    SELL: 768296EE1 @ 13.954% (Rating: AA)\n",
      "    Spread: 1153.9 bps | Similarity: 0.934\n",
      "\n",
      "  Trade #108090:\n",
      "    BUY:  939720H27 @ 3.163% (Rating: MR)\n",
      "    SELL: 764258UJ6 @ 14.409% (Rating: AA+)\n",
      "    Spread: 1124.6 bps | Similarity: 0.943\n",
      "\n",
      "  Trade #245815:\n",
      "    BUY:  880461ZL5 @ 3.439% (Rating: AA+)\n",
      "    SELL: 162393EN8 @ 14.527% (Rating: AA-)\n",
      "    Spread: 1108.8 bps | Similarity: 0.951\n",
      "\n",
      "3️⃣ PORTFOLIO CONCENTRATION CHECK\n",
      "----------------------------------------\n",
      "\n",
      "📊 Portfolio Analysis: 10 bonds\n",
      "\n",
      "Portfolio Similarity Metrics:\n",
      "  Average pairwise similarity: 0.890\n",
      "  Max similarity: 1.000\n",
      "  Min similarity: 0.559\n",
      "\n",
      "Most similar pair: 523470HV7 <-> 451295G86\n",
      "  Similarity: 1.000\n",
      "\n",
      "⚠️ WARNING: High portfolio concentration - bonds are very similar\n",
      "   Consider diversifying to reduce systematic risk\n",
      "\n",
      "============================================================\n",
      "✅ Analysis complete! Bond embeddings ready for production use.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Municipal Bond Embedding Explorer\n",
    "# Clean, practical exploration of bond embeddings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# 1. SETUP AND DATA PREP\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"MUNICIPAL BOND EMBEDDING EXPLORER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load your data (assuming these are already in memory)\n",
    "# embeddings_df = pd.read_csv('cusip_embeddings_unique.csv')  \n",
    "# df = pd.read_pickle('2025-09-22_one_month.pkl')\n",
    "\n",
    "# Merge embeddings with the latest trade data for each CUSIP\n",
    "latest_trades = (df.sort_values('trade_datetime')\n",
    "                   .groupby('cusip')\n",
    "                   .last()\n",
    "                   .reset_index())\n",
    "\n",
    "# Key fields we care about for analysis\n",
    "key_fields = ['cusip', 'yield', 'dollar_price', 'rating', 'maturity_date', \n",
    "              'coupon', 'incorporated_state_code', 'trade_datetime', \n",
    "              'par_traded', 'trade_type']\n",
    "\n",
    "# Keep only available fields\n",
    "available_fields = [f for f in key_fields if f in latest_trades.columns]\n",
    "analysis_df = embeddings_df.merge(latest_trades[available_fields], on='cusip', how='inner')\n",
    "\n",
    "# Convert yields from basis points to percent and clean bad data\n",
    "if 'yield' in analysis_df.columns:\n",
    "    analysis_df['yield'] = analysis_df['yield'] / 100.0\n",
    "    \n",
    "    # Filter out obviously bad yields (>15% is extremely rare for munis)\n",
    "    bad_yield_mask = (analysis_df['yield'] > 15) | (analysis_df['yield'] < 0)\n",
    "    n_bad = bad_yield_mask.sum()\n",
    "    \n",
    "    if n_bad > 0:\n",
    "        print(f\"⚠️ Found {n_bad} bonds with invalid yields (>15% or negative)\")\n",
    "        bad_examples = analysis_df[bad_yield_mask][['cusip', 'yield']].head()\n",
    "        print(f\"   Examples:\\n{bad_examples}\")\n",
    "        analysis_df.loc[bad_yield_mask, 'yield'] = np.nan\n",
    "    \n",
    "    valid_yields = analysis_df['yield'].dropna()\n",
    "    print(f\"✓ Valid yields: {len(valid_yields):,} bonds, median: {valid_yields.median():.2f}%, range: {valid_yields.min():.2f}%-{valid_yields.max():.2f}%\")\n",
    "\n",
    "print(f\"✓ Loaded {len(embeddings_df):,} unique CUSIP embeddings\")\n",
    "print(f\"✓ Merged with trade data: {len(analysis_df):,} CUSIPs with full data\")\n",
    "\n",
    "# Extract embedding columns and create matrix\n",
    "embedding_cols = [c for c in embeddings_df.columns if c.startswith('emb_')]\n",
    "X = analysis_df[embedding_cols].values\n",
    "X_normalized = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "print(f\"✓ Embedding dimension: {len(embedding_cols)}\")\n",
    "\n",
    "# ============================================\n",
    "# 2. SIMILARITY SEARCH FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def find_similar_bonds(cusip_query, n_similar=10, same_state_only=False, \n",
    "                       similar_maturity_years=None, similar_rating_bucket=False):\n",
    "    \"\"\"\n",
    "    Find the most similar bonds to a given CUSIP\n",
    "    \n",
    "    Parameters:\n",
    "    - cusip_query: The CUSIP to search for\n",
    "    - n_similar: Number of similar bonds to return\n",
    "    - same_state_only: If True, only return bonds from same state\n",
    "    - similar_maturity_years: If set, only bonds within ±X years maturity\n",
    "    - similar_rating_bucket: If True, only bonds in same rating bucket (AAA/AA/A/BBB/BB/B)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find the query bond\n",
    "    if cusip_query not in analysis_df['cusip'].values:\n",
    "        print(f\"❌ CUSIP {cusip_query} not found in embeddings\")\n",
    "        return None\n",
    "    \n",
    "    query_idx = analysis_df[analysis_df['cusip'] == cusip_query].index[0]\n",
    "    query_embedding = X_normalized[query_idx].reshape(1, -1)\n",
    "    query_info = analysis_df.iloc[query_idx]\n",
    "    \n",
    "    # Compute similarities to all bonds\n",
    "    similarities = cosine_similarity(query_embedding, X_normalized)[0]\n",
    "    \n",
    "    # Create mask for filtering\n",
    "    mask = np.ones(len(analysis_df), dtype=bool)\n",
    "    mask[query_idx] = False  # Exclude self\n",
    "    \n",
    "    # Apply filters\n",
    "    if same_state_only and 'incorporated_state_code' in analysis_df.columns:\n",
    "        mask &= (analysis_df['incorporated_state_code'] == query_info['incorporated_state_code'])\n",
    "    \n",
    "    if similar_maturity_years and 'maturity_date' in analysis_df.columns:\n",
    "        query_maturity = pd.to_datetime(query_info['maturity_date'])\n",
    "        if pd.notna(query_maturity):\n",
    "            all_maturities = pd.to_datetime(analysis_df['maturity_date'])\n",
    "            years_diff = abs((all_maturities - query_maturity).dt.days / 365.25)\n",
    "            mask &= (years_diff <= similar_maturity_years)\n",
    "    \n",
    "    if similar_rating_bucket and 'rating' in analysis_df.columns:\n",
    "        def get_rating_bucket(r):\n",
    "            r_str = str(r).upper()\n",
    "            if r_str.startswith('AAA'): return 'AAA'\n",
    "            if r_str.startswith('AA'): return 'AA'\n",
    "            if r_str.startswith('A'): return 'A'\n",
    "            if r_str.startswith('BBB'): return 'BBB'\n",
    "            if r_str.startswith('BB'): return 'BB'\n",
    "            if r_str.startswith('B'): return 'B'\n",
    "            return 'NR'\n",
    "        \n",
    "        query_bucket = get_rating_bucket(query_info.get('rating', 'NR'))\n",
    "        all_buckets = analysis_df['rating'].apply(get_rating_bucket)\n",
    "        mask &= (all_buckets == query_bucket)\n",
    "    \n",
    "    # Get top similar bonds\n",
    "    valid_indices = np.where(mask)[0]\n",
    "    if len(valid_indices) == 0:\n",
    "        print(\"⚠️ No bonds found matching the criteria\")\n",
    "        return None\n",
    "    \n",
    "    valid_similarities = similarities[valid_indices]\n",
    "    top_indices = valid_indices[np.argsort(valid_similarities)[-n_similar:][::-1]]\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results = analysis_df.iloc[top_indices].copy()\n",
    "    results['similarity'] = similarities[top_indices]\n",
    "    \n",
    "    # Add useful comparison columns\n",
    "    if 'yield' in results.columns and pd.notna(query_info.get('yield')):\n",
    "        results['yield_diff_bps'] = (results['yield'] - query_info['yield']) * 100\n",
    "    \n",
    "    if 'dollar_price' in results.columns and pd.notna(query_info.get('dollar_price')):\n",
    "        results['price_diff'] = results['dollar_price'] - query_info['dollar_price']\n",
    "    \n",
    "    # Reorder columns for clarity\n",
    "    cols_order = ['cusip', 'similarity']\n",
    "    if 'yield' in results.columns:\n",
    "        cols_order.extend(['yield', 'yield_diff_bps'])\n",
    "    if 'dollar_price' in results.columns:\n",
    "        cols_order.extend(['dollar_price', 'price_diff'])\n",
    "    cols_order.extend([c for c in results.columns if c not in cols_order and not c.startswith('emb_')])\n",
    "    \n",
    "    results = results[cols_order]\n",
    "    \n",
    "    # Print query bond info\n",
    "    print(f\"\\n📌 Query Bond: {cusip_query}\")\n",
    "    if 'yield' in query_info and pd.notna(query_info['yield']):\n",
    "        print(f\"   Yield: {query_info['yield']:.3f}%\")\n",
    "    if 'rating' in query_info:\n",
    "        print(f\"   Rating: {query_info['rating']}\")\n",
    "    if 'maturity_date' in query_info:\n",
    "        print(f\"   Maturity: {query_info['maturity_date']}\")\n",
    "    if 'incorporated_state_code' in query_info:\n",
    "        print(f\"   State: {query_info['incorporated_state_code']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ============================================\n",
    "# 3. ARBITRAGE OPPORTUNITY FINDER\n",
    "# ============================================\n",
    "\n",
    "def find_arbitrage_opportunities(min_similarity=0.95, min_yield_spread_bps=30, \n",
    "                                max_results=20, same_state=True, \n",
    "                                similar_maturity_years=2.0):\n",
    "    \"\"\"\n",
    "    Find potential arbitrage opportunities:\n",
    "    Bonds that are very similar but have significant yield differences\n",
    "    \n",
    "    Arbitrage logic: If two bonds behave identically (high embedding similarity)\n",
    "    but trade at different yields, there may be a pricing inefficiency\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n🔍 Searching for arbitrage opportunities...\")\n",
    "    print(f\"   Criteria: similarity > {min_similarity}, yield spread > {min_yield_spread_bps} bps\")\n",
    "    \n",
    "    if 'yield' not in analysis_df.columns:\n",
    "        print(\"❌ No yield data available for arbitrage analysis\")\n",
    "        return None\n",
    "    \n",
    "    # Only consider bonds with valid yields (already converted to percent)\n",
    "    # Filter out unreasonable yields (>15% is extremely rare for munis)\n",
    "    valid_yield_mask = (analysis_df['yield'].notna()) & (analysis_df['yield'] <= 15) & (analysis_df['yield'] >= 0)\n",
    "    valid_df = analysis_df[valid_yield_mask].copy()\n",
    "    valid_X = X_normalized[valid_yield_mask]\n",
    "    \n",
    "    if len(valid_df) < 2:\n",
    "        print(\"❌ Insufficient bonds with valid yield data\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"   Analyzing {len(valid_df)} bonds with valid yields...\")\n",
    "    \n",
    "    opportunities = []\n",
    "    \n",
    "    # Sample bonds to check (for speed, check a subset)\n",
    "    n_sample = min(500, len(valid_df))\n",
    "    sample_indices = np.random.choice(len(valid_df), n_sample, replace=False)\n",
    "    \n",
    "    for i in sample_indices:\n",
    "        query_embedding = valid_X[i].reshape(1, -1)\n",
    "        similarities = cosine_similarity(query_embedding, valid_X)[0]\n",
    "        \n",
    "        # Find highly similar bonds\n",
    "        similar_mask = (similarities > min_similarity)\n",
    "        similar_mask[i] = False  # Exclude self\n",
    "        \n",
    "        # Apply additional filters\n",
    "        if same_state and 'incorporated_state_code' in valid_df.columns:\n",
    "            state_mask = (valid_df['incorporated_state_code'].values == \n",
    "                         valid_df.iloc[i]['incorporated_state_code'])\n",
    "            similar_mask &= state_mask\n",
    "        \n",
    "        if similar_maturity_years and 'maturity_date' in valid_df.columns:\n",
    "            query_maturity = pd.to_datetime(valid_df.iloc[i]['maturity_date'])\n",
    "            if pd.notna(query_maturity):\n",
    "                all_maturities = pd.to_datetime(valid_df['maturity_date'])\n",
    "                years_diff = abs((all_maturities - query_maturity).dt.days / 365.25)\n",
    "                similar_mask &= (years_diff.values <= similar_maturity_years)\n",
    "        \n",
    "        similar_indices = np.where(similar_mask)[0]\n",
    "        \n",
    "        for j in similar_indices:\n",
    "            yield_i = valid_df.iloc[i]['yield']\n",
    "            yield_j = valid_df.iloc[j]['yield']\n",
    "            yield_spread = abs(yield_i - yield_j) * 100  # Convert to bps\n",
    "            \n",
    "            if yield_spread >= min_yield_spread_bps:\n",
    "                # Determine buy/sell direction\n",
    "                if yield_i > yield_j:\n",
    "                    buy_idx, sell_idx = j, i\n",
    "                else:\n",
    "                    buy_idx, sell_idx = i, j\n",
    "                \n",
    "                opportunities.append({\n",
    "                    'buy_cusip': valid_df.iloc[buy_idx]['cusip'],\n",
    "                    'buy_yield': valid_df.iloc[buy_idx]['yield'],\n",
    "                    'sell_cusip': valid_df.iloc[sell_idx]['cusip'],\n",
    "                    'sell_yield': valid_df.iloc[sell_idx]['yield'],\n",
    "                    'yield_spread_bps': yield_spread,\n",
    "                    'similarity': similarities[j],\n",
    "                    'state': valid_df.iloc[i].get('incorporated_state_code', 'N/A'),\n",
    "                    'buy_rating': valid_df.iloc[buy_idx].get('rating', 'NR'),\n",
    "                    'sell_rating': valid_df.iloc[sell_idx].get('rating', 'NR'),\n",
    "                })\n",
    "    \n",
    "    if not opportunities:\n",
    "        print(\"No arbitrage opportunities found with current criteria\")\n",
    "        return None\n",
    "    \n",
    "    # Remove duplicates and sort by opportunity size\n",
    "    arb_df = pd.DataFrame(opportunities)\n",
    "    arb_df['pair_key'] = arb_df.apply(\n",
    "        lambda x: tuple(sorted([x['buy_cusip'], x['sell_cusip']])), axis=1\n",
    "    )\n",
    "    arb_df = arb_df.drop_duplicates(subset='pair_key').drop('pair_key', axis=1)\n",
    "    arb_df = arb_df.sort_values('yield_spread_bps', ascending=False).head(max_results)\n",
    "    \n",
    "    print(f\"✓ Found {len(arb_df)} unique arbitrage opportunities\")\n",
    "    \n",
    "    return arb_df\n",
    "\n",
    "# ============================================\n",
    "# 4. PORTFOLIO SIMILARITY ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "def analyze_portfolio_similarity(cusip_list):\n",
    "    \"\"\"\n",
    "    Analyze how similar bonds in a portfolio are to each other\n",
    "    Useful for understanding concentration risk\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter to CUSIPs in our embeddings\n",
    "    valid_cusips = [c for c in cusip_list if c in analysis_df['cusip'].values]\n",
    "    \n",
    "    if len(valid_cusips) < 2:\n",
    "        print(\"❌ Need at least 2 valid CUSIPs for portfolio analysis\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n📊 Portfolio Analysis: {len(valid_cusips)} bonds\")\n",
    "    \n",
    "    # Get embeddings for portfolio\n",
    "    portfolio_mask = analysis_df['cusip'].isin(valid_cusips)\n",
    "    portfolio_df = analysis_df[portfolio_mask].copy()\n",
    "    portfolio_X = X_normalized[portfolio_mask]\n",
    "    \n",
    "    # Compute pairwise similarities\n",
    "    sim_matrix = cosine_similarity(portfolio_X)\n",
    "    \n",
    "    # Extract upper triangle (excluding diagonal)\n",
    "    upper_triangle = sim_matrix[np.triu_indices_from(sim_matrix, k=1)]\n",
    "    \n",
    "    # Statistics\n",
    "    avg_similarity = np.mean(upper_triangle)\n",
    "    max_similarity = np.max(upper_triangle)\n",
    "    min_similarity = np.min(upper_triangle)\n",
    "    \n",
    "    print(f\"\\nPortfolio Similarity Metrics:\")\n",
    "    print(f\"  Average pairwise similarity: {avg_similarity:.3f}\")\n",
    "    print(f\"  Max similarity: {max_similarity:.3f}\")\n",
    "    print(f\"  Min similarity: {min_similarity:.3f}\")\n",
    "    \n",
    "    # Find most similar pair\n",
    "    max_idx = np.unravel_index(np.argmax(sim_matrix - np.eye(len(sim_matrix))), \n",
    "                                sim_matrix.shape)\n",
    "    most_similar_pair = (portfolio_df.iloc[max_idx[0]]['cusip'],\n",
    "                        portfolio_df.iloc[max_idx[1]]['cusip'])\n",
    "    \n",
    "    print(f\"\\nMost similar pair: {most_similar_pair[0]} <-> {most_similar_pair[1]}\")\n",
    "    print(f\"  Similarity: {sim_matrix[max_idx]:.3f}\")\n",
    "    \n",
    "    # Concentration warning\n",
    "    if avg_similarity > 0.85:\n",
    "        print(\"\\n⚠️ WARNING: High portfolio concentration - bonds are very similar\")\n",
    "        print(\"   Consider diversifying to reduce systematic risk\")\n",
    "    elif avg_similarity > 0.75:\n",
    "        print(\"\\n⚠️ CAUTION: Moderate portfolio concentration\")\n",
    "    else:\n",
    "        print(\"\\n✓ Portfolio appears well-diversified in embedding space\")\n",
    "    \n",
    "    return {\n",
    "        'similarity_matrix': sim_matrix,\n",
    "        'portfolio_df': portfolio_df,\n",
    "        'avg_similarity': avg_similarity,\n",
    "        'most_similar_pair': most_similar_pair\n",
    "    }\n",
    "\n",
    "# ============================================\n",
    "# 5. EXAMPLE USAGE\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXAMPLE ANALYSES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example 1: Find similar bonds to a specific CUSIP\n",
    "print(\"\\n1️⃣ SIMILARITY SEARCH\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Pick a random CUSIP with good data\n",
    "sample_cusips = analysis_df[(analysis_df['yield'].notna()) & \n",
    "                           (analysis_df['yield'] <= 15) & \n",
    "                           (analysis_df['yield'] >= 0)]['cusip'].values\n",
    "if len(sample_cusips) > 0:\n",
    "    example_cusip = sample_cusips[0]\n",
    "    similar_bonds = find_similar_bonds(\n",
    "        example_cusip, \n",
    "        n_similar=5,\n",
    "        same_state_only=True,\n",
    "        similar_maturity_years=3\n",
    "    )\n",
    "    \n",
    "    if similar_bonds is not None:\n",
    "        print(\"\\nMost similar bonds:\")\n",
    "        display_cols = ['cusip', 'similarity', 'yield', 'yield_diff_bps', 'rating']\n",
    "        display_cols = [c for c in display_cols if c in similar_bonds.columns]\n",
    "        print(similar_bonds[display_cols].to_string(index=False))\n",
    "\n",
    "# Example 2: Find arbitrage opportunities\n",
    "print(\"\\n2️⃣ ARBITRAGE OPPORTUNITIES\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "arb_opportunities = find_arbitrage_opportunities(\n",
    "    min_similarity=0.93,  # Slightly lower threshold for more results\n",
    "    min_yield_spread_bps=25,\n",
    "    max_results=5,\n",
    "    same_state=True\n",
    ")\n",
    "\n",
    "if arb_opportunities is not None and len(arb_opportunities) > 0:\n",
    "    print(\"\\nTop arbitrage trades:\")\n",
    "    for idx, row in arb_opportunities.head(3).iterrows():\n",
    "        print(f\"\\n  Trade #{idx+1}:\")\n",
    "        print(f\"    BUY:  {row['buy_cusip']} @ {row['buy_yield']:.3f}% (Rating: {row['buy_rating']})\")\n",
    "        print(f\"    SELL: {row['sell_cusip']} @ {row['sell_yield']:.3f}% (Rating: {row['sell_rating']})\")\n",
    "        print(f\"    Spread: {row['yield_spread_bps']:.1f} bps | Similarity: {row['similarity']:.3f}\")\n",
    "\n",
    "# Example 3: Portfolio concentration analysis\n",
    "print(\"\\n3️⃣ PORTFOLIO CONCENTRATION CHECK\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create a sample portfolio\n",
    "sample_portfolio = sample_cusips[:10].tolist() if len(sample_cusips) >= 10 else sample_cusips.tolist()\n",
    "portfolio_analysis = analyze_portfolio_similarity(sample_portfolio)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ Analysis complete! Bond embeddings ready for production use.\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
