{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01beff96",
   "metadata": {},
   "source": [
    "Author: Gil Shulman\n",
    "Date: 2024-11-15\n",
    "Last Edit Date: 2024-11-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4426d2d3-21aa-4263-aae7-4ca9c5eb58b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LARGE_NUMBER = 1e6\n",
    "COUPON_FREQUENCY_TYPE = {'Unknown': LARGE_NUMBER,\n",
    "                         'Semiannually': 2,\n",
    "                         'Monthly': 12,\n",
    "                         'Annually': 1,\n",
    "                         'Weekly': 52,\n",
    "                         'Quarterly': 4,\n",
    "                         'Every 2 years': 0.5,\n",
    "                         'Every 3 years': 1/3,\n",
    "                         'Every 4 years': 0.25,\n",
    "                         'Every 5 years': 0.2,\n",
    "                         'Every 7 years': 1/7,\n",
    "                         'Every 8 years': 1/8,\n",
    "                         'Every 10 years': 1/10,\n",
    "                         'Biweekly':  26,\n",
    "                         'Changeable': 44,\n",
    "                         'Daily': 360,\n",
    "                         'Interest at maturity': 0,\n",
    "                         'Not Applicable': LARGE_NUMBER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4dc5aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/gil/git/ficc/creds.json\"\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from google.cloud import bigquery\n",
    "from deloitte_ycl import add_yield_curve\n",
    "\n",
    "bqclient = bigquery.Client()\n",
    "project = \"eng-reactor-287421\"\n",
    "\n",
    "def sqltodf(sql,limit = \"\"):\n",
    "    if limit != \"\": \n",
    "        limit = f\" ORDER BY RAND() LIMIT {limit}\"\n",
    "    bqr = bqclient.query(sql + limit).result()\n",
    "    return bqr.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8788de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = sqltodf(f'''\n",
    "# SELECT\n",
    "#   msrb.trade_date,\n",
    "#   msrb.time_of_trade,\n",
    "#   msrb.trade_datetime,\n",
    "#   msrb.dollar_price,\n",
    "#   msrb.yield,\n",
    "#   msrb.trade_type,\n",
    "#   msrb.cusip,\n",
    "#   msrb.par_traded,\n",
    "#   msrb.settlement_date,\n",
    "#   ref_data_v1.file_received_from_provider_timestamp,\n",
    "#   ref_data_v1.accrual_date,\n",
    "#   ref_data_v1.next_coupon_payment_date,\n",
    "#   ref_data_v1.interest_payment_frequency,\n",
    "#   ref_data_v1.current_coupon_rate AS coupon,\n",
    "#   ref_data_v1.incorporated_state_code,\n",
    "#   ref_data_v1.coupon_type,\n",
    "#   ref_data_v1.is_callable,\n",
    "#   ref_data_v1.sink_indicator,\n",
    "#   ref_data_v1.is_general_obligation,\n",
    "#   ref_data_v1.callable_at_cav,\n",
    "#   ref_data_v1.sp_long,\n",
    "#   ref_data_v1.purpose_class,\n",
    "#   ref_data_v1.maturity_date AS maturity_date,\n",
    "#   ref_data_v1.next_call_date,\n",
    "#   ref_data_v1.par_call_date,\n",
    "#   ref_data_v1.instrument_primary_name\n",
    "# FROM\n",
    "#   `auxiliary_views_v2.msrb_final` msrb\n",
    "# INNER JOIN\n",
    "#   `reference_data_v1.reference_data_flat` ref_data_v1\n",
    "# ON\n",
    "#   msrb.cusip = ref_data_v1.cusip\n",
    "#   AND timestamp(msrb.trade_datetime, \"America/New_York\") BETWEEN ref_data_v1.ref_valid_from_date AND ref_data_v1.ref_valid_to_date\n",
    "#   AND msrb.trade_date > \"2023-01-01\"\n",
    "# ''')\n",
    "\n",
    "# df.to_pickle(\"lgbm_data_file.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d72cd5",
   "metadata": {},
   "source": [
    "# Light GBM Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac13dd3a",
   "metadata": {},
   "source": [
    "# Ensemble Model for Bond Price Prediction\n",
    "\n",
    "## Model Architecture\n",
    "This study employs an ensemble of Light Gradient Boosting Machine (LightGBM) regressors for predicting municipal bond prices. The ensemble is constructed using a Voting Regressor, which aggregates predictions from multiple base models to enhance generalization and mitigate overfitting.\n",
    "\n",
    "## Key Components\n",
    "1. **Base Model**: LightGBM Regressor\n",
    "   - Objective: Mean Absolute Error (MAE) minimization\n",
    "   - Key parameters: Dynamic (configurable) max_depth, num_leaves, and n_estimators\n",
    "   - Incorporates subsampling for robustness\n",
    "\n",
    "2. **Ensemble Method**: Voting Regressor\n",
    "   - Combines multiple LightGBM models with varied random seeds\n",
    "   - Leverages parallel processing for efficient computation\n",
    "\n",
    "3. **Feature Engineering**\n",
    "   - Limitted set of ref data\n",
    "   - Label encoding for categorical\n",
    "   - Days to for dates\n",
    "\n",
    "4. **Temporal Considerations**\n",
    "   - Implements time series cross-validation for robust performance estimation\n",
    "\n",
    "## Methodology Highlights\n",
    "- Gradient boosting \n",
    "- Ensemble methods to reduce model variance and enhance predictive stability\n",
    "\n",
    "## Performance Evaluation\n",
    "- Primary metric: Mean Absolute Error (MAE)\n",
    "- Secondary analysis: Feature importance quantification\n",
    "- Cross-validation strategy accounting for temporal structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c93b494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# # Load the data\n",
    "# data = pd.read_pickle('/Users/gil/git/ficc/notebooks/gil_modeling/lgbm_data_file.pkl')\n",
    "\n",
    "# # Convert trade_date to datetime if it's not already\n",
    "# data['trade_date'] = pd.to_datetime(data['trade_date'])\n",
    "\n",
    "# # Calculate the date 7 months ago from the most recent date in the dataset\n",
    "# most_recent_date = data['trade_date'].max()\n",
    "# seven_months_ago = most_recent_date - timedelta(days=9*30)  # Approximating 7 months as 7*30 days\n",
    "\n",
    "# recent_data = data[data['trade_date'] > seven_months_ago]\n",
    "\n",
    "# print(f\"Original dataset size: {len(data)}\")\n",
    "# print(f\"Recent dataset size (last 9 months): {len(recent_data)}\")\n",
    "# print(f\"Date range: from {recent_data['trade_date'].min()} to {recent_data['trade_date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fda60f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recent_data.to_pickle(\"nine_months_lgbm_data_file.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d837ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('/Users/gil/git/ficc/notebooks/gil_modeling/lgbm_data_file.pkl')\n",
    "# data = add_yield_curve(data)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9719974-d294-48b5-aa47-04faf174f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data\n",
    "# data = pd.read_pickle('/Users/gil/git/ficc/notebooks/gil_modeling/seven_months_lgbm_data_file.pkl')\n",
    "\n",
    "print(f\"Number of rows in the dataset: {len(data)}\")\n",
    "\n",
    "# Data preprocessing (keep this part as is)\n",
    "data['quantity'] = np.log10(data.par_traded.astype(np.float32))\n",
    "data['has_sinking_fund'] = data['sink_indicator'].notna().astype(int)\n",
    "data['is_zerocoupon'] = (data['coupon'] == 0).astype(int)\n",
    "data['interest_payment_frequency'] = data['interest_payment_frequency'].map(COUPON_FREQUENCY_TYPE)\n",
    "data['interest_payment_frequency'] = data['interest_payment_frequency'].fillna(COUPON_FREQUENCY_TYPE['Unknown'])\n",
    "\n",
    "# Feature categories\n",
    "BINARY = ['is_callable', 'has_sinking_fund', 'is_zerocoupon', 'is_general_obligation', 'callable_at_cav']\n",
    "CATEGORICAL_FEATURES = ['sp_long', 'incorporated_state_code', 'purpose_class', 'coupon_type','trade_type']\n",
    "NON_CAT_FEATURES = ['coupon', 'interest_payment_frequency', 'quantity']\n",
    "DATE_COLS = ['trade_date', 'accrual_date', 'next_coupon_payment_date', 'maturity_date', 'next_call_date', 'par_call_date']\n",
    "\n",
    "# Filter features\n",
    "BINARY = [col for col in BINARY if col in data.columns]\n",
    "CATEGORICAL_FEATURES = [col for col in CATEGORICAL_FEATURES if col in data.columns]\n",
    "NON_CAT_FEATURES = [col for col in NON_CAT_FEATURES if col in data.columns]\n",
    "DATE_COLS = [col for col in DATE_COLS if col in data.columns]\n",
    "\n",
    "# Handle missing values\n",
    "float_cols = data.select_dtypes(include=['float64']).columns\n",
    "int_cols = data.select_dtypes(include=['int64', 'int32']).columns\n",
    "data[float_cols] = data[float_cols].fillna(data[float_cols].mean())\n",
    "data[int_cols] = data[int_cols].fillna(data[int_cols].median().astype(int))\n",
    "\n",
    "categorical_cols = data.select_dtypes(include=['object', 'category']).columns\n",
    "for col in categorical_cols:\n",
    "    data[col] = data[col].fillna(data[col].mode().iloc[0])\n",
    "\n",
    "# Convert features\n",
    "for col in BINARY:\n",
    "    data[col] = data[col].fillna(0).astype(int)\n",
    "for col in CATEGORICAL_FEATURES:\n",
    "    data[col] = data[col].astype('category')\n",
    "for col in DATE_COLS:\n",
    "    data[col] = pd.to_datetime(data[col], errors='coerce')\n",
    "\n",
    "# Feature engineering\n",
    "for col in DATE_COLS:\n",
    "    if col != 'trade_date':\n",
    "        data[f'days_to_{col}'] = (data[col] - data['trade_date']).dt.days\n",
    "        NON_CAT_FEATURES.append(f'days_to_{col}')\n",
    "\n",
    "# Combine all features\n",
    "features = BINARY + CATEGORICAL_FEATURES + NON_CAT_FEATURES\n",
    "\n",
    "# Time-based split:\n",
    "data['trade_date'] = pd.to_datetime(data['trade_date'])\n",
    "split_date = pd.to_datetime('2024-09-29')\n",
    "train_mask = data['trade_date'] <= split_date\n",
    "X_train = data.loc[train_mask, features].copy()\n",
    "y_train = data.loc[train_mask, 'dollar_price'].copy()\n",
    "X_test = data.loc[~train_mask, features].copy()\n",
    "y_test = data.loc[~train_mask, 'dollar_price'].copy()\n",
    "\n",
    "# Handle remaining missing values\n",
    "for dataset in [X_train, X_test]:\n",
    "    for col in dataset.columns:\n",
    "        if dataset[col].dtype.name == 'category':\n",
    "            dataset[col] = dataset[col].cat.add_categories('Unknown').fillna('Unknown')\n",
    "        elif dataset[col].dtype in ['int64', 'Int64']:\n",
    "            dataset[col] = dataset[col].fillna(dataset[col].median()).astype(int)\n",
    "        else:\n",
    "            dataset[col] = dataset[col].fillna(dataset[col].mean())\n",
    "\n",
    "# Encode categorical variables using LabelEncoder\n",
    "label_encoders = {}\n",
    "for col in CATEGORICAL_FEATURES:\n",
    "    le = LabelEncoder()\n",
    "    X_train[col] = le.fit_transform(X_train[col])\n",
    "    X_test[col] = le.transform(X_test[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Define myLGBM and mkensemble functions\n",
    "def myLGBM(seed=11, depth=8):\n",
    "    return LGBMRegressor(\n",
    "        max_depth=depth,\n",
    "        num_leaves=depth*10,\n",
    "        n_estimators=depth*30,\n",
    "        objective='mae',\n",
    "        verbosity=-1,\n",
    "        subsample=0.5,\n",
    "        subsample_freq=10,\n",
    "        random_state=seed\n",
    "    )\n",
    "\n",
    "def mkensemble(n=10, seed=11, depth=10):\n",
    "    regressors = []\n",
    "    for j in range(1, n+1):\n",
    "        regressors.append(('m'+str(j), myLGBM(seed+j, depth)))\n",
    "    return VotingRegressor(regressors, n_jobs=-1, verbose=False)\n",
    "\n",
    "# Create the ensemble model\n",
    "ensemble_model = mkensemble(n=10, seed=42, depth=10)\n",
    "\n",
    "# Fit the ensemble model\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'Mean Absolute Error: {mae:.2f}')\n",
    "\n",
    "# Feature importance (average over ensemble)\n",
    "importances = np.mean([\n",
    "    est.feature_importances_ for name, est in ensemble_model.named_estimators_.items()\n",
    "], axis=0)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': importances\n",
    "})\n",
    "feature_importance = feature_importance.sort_values(by='importance', ascending=False)\n",
    "print(feature_importance.head(20))\n",
    "\n",
    "# Time-based cross-validation with ensemble\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "cv_scores = []\n",
    "\n",
    "for train_index, val_index in tscv.split(X_train):\n",
    "    X_train_cv, X_val_cv = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_cv, y_val_cv = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    ensemble_cv = mkensemble(n=10, seed=42, depth=10)\n",
    "    ensemble_cv.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "    y_pred_cv = ensemble_cv.predict(X_val_cv)\n",
    "    cv_scores.append(mean_absolute_error(y_val_cv, y_pred_cv))\n",
    "\n",
    "print(f\"Cross-validation MAE scores: {cv_scores}\")\n",
    "print(f\"Mean CV MAE: {np.mean(cv_scores):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca547a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from joblib import dump\n",
    "\n",
    "# Method 1: Using pickle\n",
    "def save_model_pickle(model, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "# Save the model\n",
    "save_model_pickle(ensemble_model, 'ensemble_model.pkl')\n",
    "\n",
    "# Method 2: Using joblib (recommended for large arrays)\n",
    "def save_model_joblib(model, filename):\n",
    "    dump(model, filename)\n",
    "\n",
    "# Save the model\n",
    "save_model_joblib(ensemble_model, 'ensemble_model.joblib')\n",
    "\n",
    "# To save label encoders as well\n",
    "def save_label_encoders(label_encoders, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(label_encoders, file)\n",
    "\n",
    "# Save label encoders\n",
    "save_label_encoders(label_encoders, 'label_encoders.pkl')\n",
    "\n",
    "print(\"Model and label encoders saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb1229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqltodf(f'''\n",
    "SELECT\n",
    "  ref_data_v1.cusip,\n",
    "  ref_data_v1.file_received_from_provider_timestamp,\n",
    "  ref_data_v1.accrual_date,\n",
    "  ref_data_v1.next_coupon_payment_date,\n",
    "  ref_data_v1.interest_payment_frequency,\n",
    "  ref_data_v1.current_coupon_rate AS coupon,\n",
    "  ref_data_v1.incorporated_state_code,\n",
    "  ref_data_v1.coupon_type,\n",
    "  ref_data_v1.is_callable,\n",
    "  ref_data_v1.sink_indicator,\n",
    "  ref_data_v1.is_general_obligation,\n",
    "  ref_data_v1.callable_at_cav,\n",
    "  ref_data_v1.sp_long,\n",
    "  ref_data_v1.purpose_class,\n",
    "  ref_data_v1.maturity_date AS maturity_date,\n",
    "  ref_data_v1.next_call_date,\n",
    "  ref_data_v1.par_call_date,\n",
    "  ref_data_v1.instrument_primary_name\n",
    "FROM\n",
    "  `reference_data_v1.reference_data_flat` ref_data_v1\n",
    "WHERE\n",
    "  cusip IN (\n",
    "  SELECT\n",
    "    cusip\n",
    "  FROM\n",
    "    `jesse_test.cusips_deloitte` )\n",
    "AND \"2024-09-29\" BETWEEN ref_data_v1.ref_valid_from_date AND ref_data_v1.ref_valid_to_date\n",
    "''')\n",
    "\n",
    "df.to_pickle(\"deloitte.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e7ff04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from joblib import load\n",
    "\n",
    "# # Load model saved with pickle\n",
    "# def load_model_pickle(filename):\n",
    "#     with open(filename, 'rb') as file:\n",
    "#         return pickle.load(file)\n",
    "\n",
    "# # Load the pickle model\n",
    "# loaded_model_pickle = load_model_pickle('ensemble_model.pkl')\n",
    "\n",
    "# # Load model saved with joblib\n",
    "# def load_model_joblib(filename):\n",
    "#     return load(filename)\n",
    "\n",
    "# # Load the joblib model\n",
    "# loaded_model_joblib = load_model_joblib('ensemble_model.joblib')\n",
    "\n",
    "# # Load label encoders\n",
    "# def load_label_encoders(filename):\n",
    "#     with open(filename, 'rb') as file:\n",
    "#         return pickle.load(file)\n",
    "\n",
    "# # Load the label encoders\n",
    "# loaded_label_encoders = load_label_encoders('label_encoders.pkl')\n",
    "\n",
    "# print(\"Model and label encoders loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c12f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec65440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define the exact 18 features the model expects\n",
    "EXPECTED_FEATURES = [\n",
    "    'is_callable', 'has_sinking_fund', 'is_zerocoupon', 'is_general_obligation', 'callable_at_cav',\n",
    "    'sp_long', 'incorporated_state_code', 'purpose_class', 'coupon_type', 'trade_type',\n",
    "    'coupon', 'interest_payment_frequency', 'quantity',\n",
    "    'days_to_accrual_date', 'days_to_next_coupon_payment_date', 'days_to_maturity_date',\n",
    "    'days_to_next_call_date', 'days_to_par_call_date'\n",
    "]\n",
    "\n",
    "def preprocess_input_data(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Data preprocessing\n",
    "    df['quantity'] = np.log10(df['par_traded'].astype(np.float32))\n",
    "    df['has_sinking_fund'] = df['sink_indicator'].notna().astype(int)\n",
    "    df['is_zerocoupon'] = (df['coupon'] == 0).astype(int)\n",
    "    df['interest_payment_frequency'] = df['interest_payment_frequency'].map(COUPON_FREQUENCY_TYPE)\n",
    "    df['interest_payment_frequency'] = df['interest_payment_frequency'].fillna(COUPON_FREQUENCY_TYPE['Unknown'])\n",
    "\n",
    "    # Handle missing values and convert features\n",
    "    for col in EXPECTED_FEATURES:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Warning: {col} is missing. Adding it with default values.\")\n",
    "            df[col] = 0  # or another appropriate default value\n",
    "        elif df[col].dtype == 'object':\n",
    "            df[col] = df[col].fillna(df[col].mode().iloc[0])\n",
    "        elif df[col].dtype == 'float64':\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "        elif df[col].dtype in ['int64', 'Int64']:\n",
    "            df[col] = df[col].fillna(df[col].median()).astype(int)\n",
    "\n",
    "    # Feature engineering for date columns\n",
    "    date_cols = ['accrual_date', 'next_coupon_payment_date', 'maturity_date', 'next_call_date', 'par_call_date']\n",
    "    for col in date_cols:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        df[f'days_to_{col}'] = (df[col] - df['trade_date']).dt.days.fillna(0).astype(int)\n",
    "\n",
    "    # Encode categorical variables\n",
    "    categorical_features = ['sp_long', 'incorporated_state_code', 'purpose_class', 'coupon_type', 'trade_type']\n",
    "    for col in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "    return df[EXPECTED_FEATURES]\n",
    "\n",
    "# Main prediction code\n",
    "inputs_df = pd.read_pickle(\"deloitte.pkl\")\n",
    "\n",
    "# Add trade_type, par_traded, and trade_date to the input dataframe\n",
    "inputs_df['trade_type'] = 'P'  # Bid Side\n",
    "inputs_df['par_traded'] = 1000000  # As specified\n",
    "inputs_df['trade_date'] = pd.to_datetime('2024-08-31')  # Set the trade date for all rows\n",
    "# inputs_df = add_yield_curve(inputs_df)\n",
    "\n",
    "# Preprocess the input data\n",
    "X_pred = preprocess_input_data(inputs_df)\n",
    "print(\"X_pred columns:\", X_pred.columns)\n",
    "print(\"X_pred shape:\", X_pred.shape)\n",
    "print(\"X_pred dtypes:\", X_pred.dtypes)\n",
    "\n",
    "# Make predictions using the ensemble model\n",
    "predictions = ensemble_model.predict(X_pred)\n",
    "\n",
    "# Add predictions to the original dataframe\n",
    "inputs_df['predicted_dollar_price'] = predictions\n",
    "\n",
    "# Display results\n",
    "print(inputs_df[['cusip', 'trade_type', 'par_traded', 'predicted_dollar_price']])\n",
    "\n",
    "# Optionally, save the results to a CSV file\n",
    "inputs_df[['cusip', 'trade_type', 'par_traded', 'predicted_dollar_price']].to_csv('deloitte_predictions.csv', index=False)\n",
    "print(\"deloitte_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f34db18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the stats\n",
    "mean_price = inputs_df.predicted_dollar_price.mean()\n",
    "median_price = inputs_df.predicted_dollar_price.median()\n",
    "min_price = inputs_df.predicted_dollar_price.min()\n",
    "max_price = inputs_df.predicted_dollar_price.max()\n",
    "std_dev = inputs_df.predicted_dollar_price.std()\n",
    "count = inputs_df.predicted_dollar_price.count()\n",
    "negative_values_count = (inputs_df.predicted_dollar_price < 0).sum()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Predicted Dollar Price Statistics:\")\n",
    "print(f\"Mean: {mean_price:.2f}\")\n",
    "print(f\"Median: {median_price:.2f}\")\n",
    "print(f\"Min: {min_price:.2f}\")\n",
    "print(f\"Max: {max_price:.2f}\")\n",
    "print(f\"Standard Deviation: {std_dev:.2f}\")\n",
    "print(f\"Total Count: {count}\")\n",
    "print(f\"Negative Values Count: {negative_values_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eca511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the actual trades data\n",
    "actual_trades = pd.read_csv('msrb_res.csv')\n",
    "\n",
    "# Assuming inputs_df is your dataframe with predictions\n",
    "# If not, load it here:\n",
    "# inputs_df = pd.read_csv('deloitte_predictions_ycl.csv')\n",
    "\n",
    "# Convert trade_date to datetime for both dataframes\n",
    "actual_trades['trade_date'] = pd.to_datetime(actual_trades['trade_date'])\n",
    "inputs_df['trade_date'] = pd.to_datetime(inputs_df['trade_date'])\n",
    "\n",
    "# Merge the predictions with actual trades\n",
    "merged_df = pd.merge(inputs_df, actual_trades, on=['cusip'], how='inner', suffixes=('_pred', '_actual'))\n",
    "\n",
    "# Calculate the price delta\n",
    "merged_df['price_delta'] = merged_df['predicted_dollar_price'] - merged_df['dollar_price']\n",
    "\n",
    "# Calculate the absolute error\n",
    "merged_df['absolute_error'] = abs(merged_df['price_delta'])\n",
    "\n",
    "# Calculate the Mean Absolute Error\n",
    "mae = merged_df['absolute_error'].mean()\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "\n",
    "# Display some statistics about the price delta\n",
    "print(\"\\nPrice Delta Statistics:\")\n",
    "print(merged_df['price_delta'].describe())\n",
    "\n",
    "# Count how many predictions were within 1, 2, and 5 points of the actual price\n",
    "within_1 = (merged_df['absolute_error'] <= 1).sum()\n",
    "within_2 = (merged_df['absolute_error'] <= 2).sum()\n",
    "within_5 = (merged_df['absolute_error'] <= 5).sum()\n",
    "\n",
    "total_predictions = len(merged_df)\n",
    "\n",
    "print(f\"\\nOut of {total_predictions} matched predictions:\")\n",
    "print(f\"Within 1 point: {within_1} ({within_1/total_predictions:.2%})\")\n",
    "print(f\"Within 2 points: {within_2} ({within_2/total_predictions:.2%})\")\n",
    "print(f\"Within 5 points: {within_5} ({within_5/total_predictions:.2%})\")\n",
    "\n",
    "# Optionally, save the merged results to a CSV for further analysis\n",
    "merged_df.to_csv('prediction_vs_actual_comparison.csv', index=False)\n",
    "print(\"\\nDetailed comparison saved to 'prediction_vs_actual_comparison.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09f7dc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure date columns are in datetime format\n",
    "inputs_df['trade_date'] = pd.to_datetime(inputs_df['trade_date'], errors='coerce')\n",
    "inputs_df['maturity_date'] = pd.to_datetime(inputs_df['maturity_date'], errors='coerce')\n",
    "\n",
    "# Calculate years to maturity\n",
    "inputs_df['years_to_maturity'] = (inputs_df['maturity_date'] - inputs_df['trade_date']).dt.days / 365.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff71281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create coupon rate bins\n",
    "coupon_bins = [0, 1, 2, 3, 4, 5, np.inf]\n",
    "coupon_labels = ['0-1%', '1-2%', '2-3%', '3-4%', '4-5%', '5%+']\n",
    "\n",
    "# Create maturity bins (in years)\n",
    "maturity_bins = [0, 5, 10, 15, 20, 30, np.inf]\n",
    "maturity_labels = ['0-5 years', '5-10 years', '10-15 years', '15-20 years', '20-30 years', '30+ years']\n",
    "\n",
    "# Create the groupings\n",
    "inputs_df['coupon_group'] = pd.cut(inputs_df['coupon'], bins=coupon_bins, labels=coupon_labels, include_lowest=True)\n",
    "inputs_df['maturity_group'] = pd.cut(inputs_df['years_to_maturity'], bins=maturity_bins, labels=maturity_labels, include_lowest=True)\n",
    "\n",
    "# Group by coupon and maturity, and calculate average predicted price\n",
    "grouped_df = inputs_df.groupby(['coupon_group', 'maturity_group']).agg({\n",
    "    'predicted_dollar_price': 'mean',\n",
    "    'cusip': 'count'  # This gives us the count of bonds in each group\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the count column\n",
    "grouped_df = grouped_df.rename(columns={'cusip': 'bond_count'})\n",
    "\n",
    "# Sort the results\n",
    "grouped_df = grouped_df.sort_values(['coupon_group', 'maturity_group'])\n",
    "\n",
    "# Display the results\n",
    "print(grouped_df.to_string(index=False))\n",
    "\n",
    "# Optionally, save to CSV\n",
    "grouped_df.to_csv('grouped_bond_predictions.csv', index=False)\n",
    "print(\"\\nGrouped predictions saved to 'grouped_bond_predictions.csv'\")\n",
    "\n",
    "# Create a pivot table for easier viewing\n",
    "pivot_df = grouped_df.pivot(index='coupon_group', columns='maturity_group', values='predicted_dollar_price')\n",
    "print(\"\\nPivot Table of Average Predicted Prices:\")\n",
    "print(pivot_df.round(2).to_string())\n",
    "\n",
    "# Optionally, save pivot table to CSV\n",
    "pivot_df.to_csv('pivot_bond_predictions.csv')\n",
    "print(\"\\nPivot table saved to 'pivot_bond_predictions.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lgbm venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
