{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point in time pricing\n",
    "Last updated by Developer on 2024-11-15.\n",
    "\n",
    "**NOTE**: This notebook needs to be run on a VM so that the yield curve redis can be accessed which is necessary for `process_data(...)`. The error that will be raised otherwise is a `TimeoutError`.\n",
    "\n",
    "This notebook allows one to see what prices we would have returned on a specified date for a list of CUSIPs. The user specifies the date and time in `DATETIME_OF_INTEREST` and the file with the list of CUSIPs, and optionally quantities and trade types, in `FILE_TO_BE_PRICED`. The sequence of events is as follows: \n",
    "1. create a trade history data file where the most recent trade is not after `DATETIME_OF_INTEREST`, \n",
    "2. create a reference data file where the data is the reference features for each CUSIP at the `DATETIME_OF_INTEREST`, and \n",
    "3. use the archived deployed models for the same day if the time is before 5pm PT or the business day after `DATETIME_OF_INTEREST`, since after business hours, we consider the model that was trained up until two business days before the day it is deployed and validated on the business day before it is deployed. \n",
    "\n",
    "The core idea is to use as much code that is deployed i.e., that in `app_engine/demo/server/modules/finance.py`, as possible to maintain consistencies to what is deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, time\n",
    "from typing import List, Dict     # importing types for hinting\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar    # used to create a business day defined on the US federal holiday calendar that can be added or subtracted to a datetime\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "from modules.ficc.utils.auxiliary_variables import CATEGORICAL_FEATURES, \\\n",
    "                                                   NON_CAT_FEATURES, \\\n",
    "                                                   BINARY, \\\n",
    "                                                   NON_CAT_FEATURES_DOLLAR_PRICE, \\\n",
    "                                                   CATEGORICAL_FEATURES_DOLLAR_PRICE, \\\n",
    "                                                   BINARY_DOLLAR_PRICE\n",
    "\n",
    "\n",
    "__file__ = os.path.abspath('point_in_time_pricing.ipynb')    # in a Jupyter Notebook, the `__file__` variable is not automatically defined because notebooks do not run as standard Python scripts\n",
    "server_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', 'app_engine', 'demo', 'server'))    # get the directory containing the 'app_engine/demo/server' package\n",
    "sys.path.append(server_dir)    # add the directory to sys.path\n",
    "\n",
    "\n",
    "from modules.get_creds import get_creds\n",
    "get_creds()\n",
    "\n",
    "\n",
    "from modules.auxiliary_variables import SEQUENCE_LENGTH, \\\n",
    "                                        SEQUENCE_LENGTH_DOLLAR_PRICE, \\\n",
    "                                        NUMERICAL_ERROR\n",
    "from modules.auxiliary_functions import get_outstanding_amount\n",
    "from modules.data_preparation_for_pricing import cusip_is_invalid, \\\n",
    "                                                 calculate_cusip_check_digit, \\\n",
    "                                                 convert_isin_to_cusip, \\\n",
    "                                                 fix_cusip_improperly_formatted_from_excel_automatic_scientific_notation, \\\n",
    "                                                 get_encoders\n",
    "from modules.batch_pricing import prepare_batch_pricing_results_for_logging, \\\n",
    "                                  prepare_batch_pricing_results_to_output_to_user, \\\n",
    "                                  price_cusips_list\n",
    "\n",
    "import modules.pricing_functions    # used to modify the original functions inside finance.py to use the newly created ones in this notebook\n",
    "import modules.data_preparation_for_pricing    # used to modify the original functions inside finance.py to use the newly created ones in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from `cloud_functions/fast_trade_history_redis_update/main.py` and used to convert the trade history represented as a list of dictionaries to a pandas dataframe\n",
    "FEATURES_FOR_EACH_TRADE_IN_HISTORY = {'msrb_valid_from_date': 'DATETIME', \n",
    "                                      'msrb_valid_to_date': 'DATETIME', \n",
    "                                      'rtrs_control_number': 'INTEGER', \n",
    "                                      'trade_datetime': 'DATETIME', \n",
    "                                      'publish_datetime': 'DATETIME', \n",
    "                                      'yield': 'FLOAT', \n",
    "                                      'dollar_price': 'FLOAT', \n",
    "                                      'par_traded': 'NUMERIC', \n",
    "                                      'trade_type': 'STRING', \n",
    "                                      'is_non_transaction_based_compensation': 'BOOLEAN', \n",
    "                                      'is_lop_or_takedown': 'BOOLEAN', \n",
    "                                      'brokers_broker': 'STRING', \n",
    "                                      'is_alternative_trading_system': 'BOOLEAN', \n",
    "                                      'is_weighted_average_price': 'BOOLEAN', \n",
    "                                      'settlement_date': 'DATE', \n",
    "                                      'calc_date': 'DATE', \n",
    "                                      'calc_day_cat': 'INTEGER', \n",
    "                                      'maturity_date': 'DATE', \n",
    "                                      'next_call_date': 'DATE', \n",
    "                                      'par_call_date': 'DATE', \n",
    "                                      'refund_date': 'DATE', \n",
    "                                      'transaction_type': 'STRING', \n",
    "                                      'sequence_number': 'INTEGER'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUSINESS_DAY = CustomBusinessDay(calendar=USFederalHolidayCalendar())    # used to skip over holidays when adding or subtracting business days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATETIME_OF_INTEREST = '2024-02-08T22:00:00'    # modify to be the datetime at which the pricing occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_TO_BE_PRICED = 'cusips_to_be_priced_on_02_08_2024.csv'    # modify to be the file containing the list of CUSIPs to be priced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks if file is csv or xlsx\n",
    "if FILE_TO_BE_PRICED.lower().endswith('.csv'):\n",
    "    to_be_priced_df = pd.read_csv(FILE_TO_BE_PRICED, header=None)\n",
    "elif FILE_TO_BE_PRICED.lower().endswith(('.xls', '.xlsx')):\n",
    "    to_be_priced_df = pd.read_excel(FILE_TO_BE_PRICED, header=None)\n",
    "else:\n",
    "    raise ValueError('Unsupported file format')\n",
    "    \n",
    "to_be_priced_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_priced_df = to_be_priced_df.rename(columns={to_be_priced_df.columns[0]: 'cusip'})    # rename the first column to 'cusip'\n",
    "to_be_priced_df['cusip'] = to_be_priced_df['cusip'].apply(convert_isin_to_cusip)\n",
    "to_be_priced_df['cusip'] = to_be_priced_df['cusip'].apply(lambda cusip: cusip + calculate_cusip_check_digit(cusip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column names\n",
    "assert len(to_be_priced_df.columns) <= 3, 'Too many columns in `to_be_priced_df`'\n",
    "column_names = ['cusip', 'quantity', 'trade_type']\n",
    "\n",
    "if len(to_be_priced_df.columns) < 2:\n",
    "    to_be_priced_df['quantity'] = 500    # set default value\n",
    "else:\n",
    "    to_be_priced_df.iloc[:, 1] = to_be_priced_df.iloc[:, 1].astype(int)\n",
    "\n",
    "if len(to_be_priced_df.columns) < 3: to_be_priced_df['trade_type'] = 'S'    # set default value\n",
    "\n",
    "to_be_priced_df.columns = column_names\n",
    "to_be_priced_df['quantity'] = to_be_priced_df['quantity'] * 1000\n",
    "\n",
    "to_be_priced_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the pseudocode\n",
    "# if DATETIME_OF_INTEREST is a date with no time:\n",
    "#     add time to DATETIME_OF_INTEREST\n",
    "DATETIME_OF_INTEREST = datetime.strptime(DATETIME_OF_INTEREST, '%Y-%m-%dT%H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create trade history dataset and reference data dataset for `DATETIME_OF_INTEREST`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATETIME_OF_INTEREST_AS_TABLE_STRING = DATETIME_OF_INTEREST.strftime('%Y_%m_%d_%H_%M_%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_CLIENT = bigquery.Client()\n",
    "\n",
    "PROJECT_ID = 'eng-reactor-287421'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_view(dataset, name, sql_query):\n",
    "    db = f'{PROJECT_ID}.{dataset}.'\n",
    "    name = db + name\n",
    "    BQ_CLIENT.delete_table(name, not_found_ok=True) \n",
    "    view = bigquery.Table(name)\n",
    "    view.view_query = sql_query\n",
    "    view = BQ_CLIENT.create_table(view)\n",
    "    return name\n",
    "\n",
    "\n",
    "def sqltodf(sql_query, limit=''):\n",
    "    if limit != '': limit = f' ORDER BY RAND() LIMIT {limit}'\n",
    "    return BQ_CLIENT.query(sql_query + limit).result().to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following view creates trade history for a given CUSIP. The goal is to use trades up to a point in time. For this to happen, we need to do the following: look at all the trade messages and add a valid_to and valid_from timestamp to them in order to get the most up-to-date trade for a given timestamp. This procedure is done in a [notebook](https://github.com/Ficc-ai/ficc/blob/dev/SQL_examples/Create_trade_history_with_reference_data.ipynb) that creates a table called `msrb_final`. The table is `msrb_final` is always up-to-date, since it is a view that is created further upstream to the `trade_history_same_issue_5_yr_mat_bucket_1_materialized` table. This view is joined to a table containing calculation dates for each trade. The value par_traded is assumed to be $5MM when the field par_traded is null and the is_trade_with_a_par_amount_over_5MM flag is true. The exclusions are as follows:\n",
    "\n",
    "1) Trades with a par_traded under $10k, which we have found to be not useful for prediction.\n",
    "2) Trades with no dollar_price or yield.\n",
    "\n",
    "Note that these are only restrictions for trade data; we would still handle these CUSIPs if they are present in the ICE data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trade_history_query(up_until_datetime):\n",
    "    return f'''\n",
    "        SELECT \n",
    "            a.cusip,\n",
    "            ARRAY_AGG( STRUCT(\n",
    "                a.msrb_valid_from_date, \n",
    "                a.msrb_valid_to_date, \n",
    "                a.rtrs_control_number, \n",
    "                a.trade_datetime, \n",
    "                a.publish_datetime, \n",
    "                a.yield, \n",
    "                a.dollar_price,   \n",
    "                CASE\n",
    "                    WHEN a.par_traded IS NULL AND is_trade_with_a_par_amount_over_5MM IS TRUE THEN 5000000\n",
    "                    ELSE\n",
    "                    a.par_traded\n",
    "                END\n",
    "                    AS par_traded,\n",
    "                trade_type, \n",
    "                is_non_transaction_based_compensation, \n",
    "                is_lop_or_takedown, \n",
    "                brokers_broker, \n",
    "                is_alternative_trading_system, \n",
    "                is_weighted_average_price, \n",
    "                CASE\n",
    "                    WHEN a.settlement_date IS NULL AND a.assumed_settlement_date IS NOT NULL  THEN a.assumed_settlement_date\n",
    "                ELSE\n",
    "                    a.settlement_date\n",
    "                END AS settlement_date,\n",
    "                b.calc_date, \n",
    "                b.calc_date_selection AS calc_day_cat, \n",
    "                a.maturity_date, \n",
    "                next_call_date, \n",
    "                par_call_date, \n",
    "                refund_date,\n",
    "                a.sequence_number,\n",
    "                a.transaction_type)\n",
    "                ORDER BY\n",
    "                    a.trade_datetime DESC\n",
    "                LIMIT 32) AS recent\n",
    "        FROM `auxiliary_views_v2.msrb_final` a LEFT JOIN (select distinct * from eng-reactor-287421.auxiliary_views_v2.calculation_date_and_price_v2) b\n",
    "        ON a.rtrs_control_number = b.rtrs_control_number\n",
    "           AND a.trade_datetime = b.trade_datetime\n",
    "           AND a.publish_datetime = b.publish_datetime\n",
    "           -- AND a.msrb_valid_to_date = b.msrb_valid_to_date\n",
    "        WHERE a.msrb_valid_to_date > CURRENT_DATETIME('America/New_York')\n",
    "              AND b.msrb_valid_to_date > CURRENT_DATETIME('America/New_York')\n",
    "              AND a.dollar_price IS NOT NULL\n",
    "              AND (a.par_traded IS NULL OR a.par_traded >= 10000)\n",
    "              AND (a.transaction_type <> \"C\" or a.transaction_type is null)\n",
    "              AND a.trade_datetime < \"{up_until_datetime}\"\n",
    "        GROUP BY a.cusip'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trade_history_table_name = create_view('point_in_time', \n",
    "                                       f'trade_history_groupby_{DATETIME_OF_INTEREST_AS_TABLE_STRING}', \n",
    "                                       create_trade_history_query(DATETIME_OF_INTEREST))\n",
    "print(trade_history_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cusip_list = to_be_priced_df['cusip'].unique().tolist()    # used to filter the query to consider only CUSIPs that we will price later; prefer to do it in the query so we are working with a lot less data and have lower memory usage\n",
    "cusip_list_as_tuple_string = str(tuple(cusip_list)) if len(cusip_list) > 1 else f'(\"{cusip_list[0]}\")'\n",
    "cusip_in_cusip_list_clause = f'WHERE cusip IN {cusip_list_as_tuple_string}'    # use `tuple(...)` to have the string representation with parentheses instead of square brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_history_table_df_query = f'SELECT * FROM {trade_history_table_name} {cusip_in_cusip_list_clause}'\n",
    "print('query:', trade_history_table_df_query)\n",
    "trade_history_table_df = sqltodf(trade_history_table_df_query)\n",
    "trade_history_table_df.head(10)\n",
    "# trade_history_table_df = trade_history_table_df[trade_history_table_df['cusip'].isin(cusip_list)]    # consider only CUSIPs that we will price later; commented out since we perform the filtering in the query itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following view joins the reference data to the trade history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_trade_history_to_reference_data_query(trade_history_table_name, up_until_datetime):\n",
    "    return f'''\n",
    "        SELECT\n",
    "            ref_data_v1.current_coupon_rate AS coupon,\n",
    "            ref_data_v1.issue_key as series_id,\n",
    "            CONCAT(IFNULL(organization_primary_name, ''), ' ', IFNULL(instrument_primary_name, ''), ' ', IFNULL(conduit_obligor_name, '')) AS security_description,\n",
    "            ref_data_v1.cusip,\n",
    "            ref_valid_from_date,\n",
    "            ref_valid_to_date,\n",
    "            incorporated_state_code,\n",
    "            organization_primary_name,\n",
    "            instrument_primary_name,\n",
    "            issue_key,\n",
    "            issue_text,\n",
    "            conduit_obligor_name,\n",
    "            is_called,\n",
    "            is_callable,\n",
    "            is_escrowed_or_pre_refunded,\n",
    "            first_call_date,\n",
    "            call_date_notice,\n",
    "            callable_at_cav,\n",
    "            par_price,\n",
    "            call_defeased,\n",
    "            call_timing,\n",
    "            call_timing_in_part,\n",
    "            extraordinary_make_whole_call,\n",
    "            extraordinary_redemption,\n",
    "            make_whole_call,\n",
    "            next_call_date,\n",
    "            next_call_price,\n",
    "            call_redemption_id,\n",
    "            first_optional_redemption_code,\n",
    "            second_optional_redemption_code,\n",
    "            third_optional_redemption_code,\n",
    "            first_mandatory_redemption_code,\n",
    "            second_mandatory_redemption_code,\n",
    "            third_mandatory_redemption_code,\n",
    "            par_call_date,\n",
    "            par_call_price,\n",
    "            maximum_call_notice_period,\n",
    "            called_redemption_type,\n",
    "            muni_issue_type,\n",
    "            refund_date,\n",
    "            refund_price,\n",
    "            redemption_cav_flag,\n",
    "            max_notification_days,\n",
    "            min_notification_days,\n",
    "            next_put_date,\n",
    "            put_end_date,\n",
    "            put_feature_price,\n",
    "            put_frequency,\n",
    "            put_start_date,\n",
    "            put_type,\n",
    "            maturity_date,\n",
    "            sp_long,\n",
    "            sp_stand_alone,\n",
    "            sp_icr_school,\n",
    "            sp_prelim_long,\n",
    "            sp_outlook_long,\n",
    "            sp_watch_long,\n",
    "            sp_Short_Rating,\n",
    "            sp_Credit_Watch_Short_Rating,\n",
    "            sp_Recovery_Long_Rating,\n",
    "            moodys_long,\n",
    "            moodys_short,\n",
    "            moodys_Issue_Long_Rating,\n",
    "            moodys_Issue_Short_Rating,\n",
    "            moodys_Credit_Watch_Long_Rating,\n",
    "            moodys_Credit_Watch_Short_Rating,\n",
    "            moodys_Enhanced_Long_Rating,\n",
    "            moodys_Enhanced_Short_Rating,\n",
    "            moodys_Credit_Watch_Long_Outlook_Rating,\n",
    "            has_sink_schedule,\n",
    "            next_sink_date,\n",
    "            sink_indicator,\n",
    "            sink_amount_type_text,\n",
    "            sink_amount_type_type,\n",
    "            sink_frequency,\n",
    "            sink_defeased,\n",
    "            additional_next_sink_date,\n",
    "            sink_amount_type,\n",
    "            additional_sink_frequency,\n",
    "            min_amount_outstanding,\n",
    "            max_amount_outstanding,\n",
    "            default_exists,\n",
    "            has_unexpired_lines_of_credit,\n",
    "            years_to_loc_expiration,\n",
    "            escrow_exists,\n",
    "            escrow_obligation_percent,\n",
    "            escrow_obligation_agent,\n",
    "            escrow_obligation_type,\n",
    "            child_linkage_exists,\n",
    "            put_exists,\n",
    "            floating_rate_exists,\n",
    "            bond_insurance_exists,\n",
    "            is_general_obligation,\n",
    "            has_zero_coupons,\n",
    "            delivery_date,\n",
    "            issue_price,\n",
    "            primary_market_settlement_date,\n",
    "            issue_date,\n",
    "            outstanding_indicator,\n",
    "            federal_tax_status,\n",
    "            maturity_amount,\n",
    "            available_denom,\n",
    "            denom_increment_amount,\n",
    "            min_denom_amount,\n",
    "            accrual_date,\n",
    "            bond_insurance,\n",
    "            coupon_type,\n",
    "            current_coupon_rate,\n",
    "            daycount_basis_type,\n",
    "            debt_type,\n",
    "            default_indicator,\n",
    "            first_coupon_date,\n",
    "            interest_payment_frequency,\n",
    "            issue_amount,\n",
    "            last_period_accrues_from_date,\n",
    "            next_coupon_payment_date,\n",
    "            odd_first_coupon_date,\n",
    "            orig_principal_amount,\n",
    "            original_yield,\n",
    "            outstanding_amount,\n",
    "            previous_coupon_payment_date,\n",
    "            sale_type,\n",
    "            settlement_type,\n",
    "            additional_project_txt,\n",
    "            asset_claim_code,\n",
    "            additional_state_code,\n",
    "            backed_underlying_security_id,\n",
    "            bank_qualified,\n",
    "            capital_type,\n",
    "            conditional_call_date,\n",
    "            conditional_call_price,\n",
    "            designated_termination_date,\n",
    "            DTCC_status,\n",
    "            first_execution_date,\n",
    "            formal_award_date,\n",
    "            maturity_description_code,\n",
    "            muni_security_type,\n",
    "            mtg_insurance,\n",
    "            orig_cusip_status,\n",
    "            orig_instrument_enhancement_type,\n",
    "            other_enhancement_type,\n",
    "            other_enhancement_company,\n",
    "            pac_bond_indicator,\n",
    "            project_name,\n",
    "            purpose_class,\n",
    "            purpose_sub_class,\n",
    "            refunding_issue_key,\n",
    "            refunding_dated_date,\n",
    "            sale_date,\n",
    "            sec_regulation,\n",
    "            secured,\n",
    "            series_name,\n",
    "            sink_fund_redemption_method,\n",
    "            state_tax_status,\n",
    "            tax_credit_frequency,\n",
    "            tax_credit_percent,\n",
    "            use_of_proceeds,\n",
    "            use_of_proceeds_supplementary,\n",
    "            rating_downgrade,\n",
    "            rating_upgrade,\n",
    "            rating_downgrade_to_junk,\n",
    "            min_sp_rating_this_year,\n",
    "            max_sp_rating_this_year,\n",
    "            min_moodys_rating_this_year, \n",
    "            max_moodys_rating_this_year,\n",
    "                latest.* EXCEPT(cusip)\n",
    "        FROM `reference_data_v1.reference_data_flat` ref_data_v1 LEFT JOIN {trade_history_table_name} latest\n",
    "        ON latest.cusip = ref_data_v1.cusip   \n",
    "        WHERE ref_data_v1.cusip IS NOT NULL\n",
    "              AND timestamp (\"{up_until_datetime}\") BETWEEN ref_data_v1.ref_valid_from_date\n",
    "              AND ref_data_v1.ref_valid_to_date'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trade_history_joined_to_reference_data_table_name = create_view('point_in_time', \n",
    "                                                                f'trade_history_latest_ref_data_minimal_exclusions_{DATETIME_OF_INTEREST_AS_TABLE_STRING}', \n",
    "                                                                join_trade_history_to_reference_data_query(trade_history_table_name, DATETIME_OF_INTEREST))\n",
    "print(trade_history_joined_to_reference_data_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trade_history_joined_to_reference_data_table_df_query = f'SELECT * FROM {trade_history_joined_to_reference_data_table_name} {cusip_in_cusip_list_clause}'\n",
    "print('query:', trade_history_joined_to_reference_data_table_df_query)\n",
    "trade_history_joined_to_reference_data_table_df = sqltodf(trade_history_joined_to_reference_data_table_df_query)\n",
    "trade_history_joined_to_reference_data_table_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this file exists, then we can use it instead of running everything above this line to create the dataframe. Uncomment the below cell to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('point_in_time_ref_data_10_31.pkl', 'rb') as pickle_file:\n",
    "#     trade_history_joined_to_reference_data_table_df = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all quantities that are greater than the outstanding amount for that CUSIP, replace the value with the outstanding amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cusip_with_trade_history_and_reference_data = to_be_priced_df.merge(trade_history_joined_to_reference_data_table_df, on='cusip', how='left')\n",
    "\n",
    "# the following 5 lines of code are inspired by `price_cusips_list(...)` in `finance.py`\n",
    "outstanding_amount = get_outstanding_amount(cusip_with_trade_history_and_reference_data, batch_pricing=True)\n",
    "outstanding_amount = outstanding_amount.fillna(np.inf)    # ensures that the condition of whether the quantity is greater than the amount outstanding will always be `False` if `outstanding_amount` does not exist\n",
    "outstanding_amount = outstanding_amount.replace(0, np.inf)    # ensures that the condition of whether the quantity is greater than the amount outstanding will always be `False` if `outstanding_amount` is 0\n",
    "quantity_greater_than_outstanding_amount = cusip_with_trade_history_and_reference_data['quantity'] > outstanding_amount\n",
    "cusip_with_trade_history_and_reference_data.loc[quantity_greater_than_outstanding_amount, 'quantity'] = outstanding_amount[quantity_greater_than_outstanding_amount]\n",
    "\n",
    "cusip_with_trade_history_and_reference_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cusip_list = cusip_with_trade_history_and_reference_data['cusip'].tolist()\n",
    "quantity_list = cusip_with_trade_history_and_reference_data['quantity'].tolist()\n",
    "trade_type_list = cusip_with_trade_history_and_reference_data['trade_type'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cusip_list[:10]\\n', cusip_list[:10])\n",
    "print('quantity_list[:10]\\n', quantity_list[:10])\n",
    "print('trade_type_list[:10]\\n', trade_type_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this dataset to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DATETIME = DATETIME_OF_INTEREST\n",
    "if DATETIME_OF_INTEREST.time() > time(17, 0, 0): MODEL_DATETIME = DATETIME_OF_INTEREST + (BUSINESS_DAY * 1)    # after business hours (17 comes from converting 5pm to military time)\n",
    "MODEL_DATE_AS_STRING = MODEL_DATETIME.strftime('%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function finds the appropriate model, either in the automated_training directory, or in a special directory. TODO: clean up the way we store models on cloud storage by unifying the folders and naming convention and adding the year to the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WEEK_DAYS_IN_THE_PAST_TO_CHECK = 10    # denotes the maximum number of week days back that we go to search for the model before raising an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(folder, bucket='gs://automated_training'):\n",
    "    assert folder in ('yield_spread_model', 'dollar_price_model')\n",
    "    model_prefix = '' if folder == 'yield_spread_model' else 'dollar-'\n",
    "\n",
    "    for num_business_days_in_the_past in range(MAX_NUM_WEEK_DAYS_IN_THE_PAST_TO_CHECK):\n",
    "        model_date_string = (MODEL_DATETIME - (BUSINESS_DAY * num_business_days_in_the_past)).strftime('%m-%d')\n",
    "        bucket_folder_model_path = os.path.join(os.path.join(bucket, folder), f'{model_prefix}model-{model_date_string}')    # create path of the form: <bucket>/<folder>/<model>\n",
    "        base_model_path = os.path.join(bucket, f'{model_prefix}model-{model_date_string}')    # create path of the form: <bucket>/<model>\n",
    "        for model_path in (bucket_folder_model_path, base_model_path):    # iterate over possible paths and try to load the model\n",
    "            print(f'Attempting to load model from {model_path}')\n",
    "            try:\n",
    "                model = keras.models.load_model(model_path)\n",
    "                print(f'Model loaded from {model_path}')\n",
    "                return model\n",
    "            except Exception as e:\n",
    "                print(f'Model failed to load from {model_path} with exception: {e}')\n",
    "\n",
    "    raise FileNotFoundError(f'No model for {folder} was found from {MODEL_DATE_AS_STRING} to {model_date_string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yield_spread_model = load_model(folder='yield_spread_model')\n",
    "dollar_price_model = load_model(folder='dollar_price_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace `get_data_from_redis(...)` to instead grab data from the file generated in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_redis(cusips):\n",
    "    '''Return the data found in the redis from a list of cusips. If `return_cusips_not_found` \n",
    "    is True, then we return the data not found in the redis.\n",
    "    NOTE: experiments with parallelization for getting the data from redis did not give any speedup.'''\n",
    "    # reference_data_redis_client = redis.Redis(host='10.14.140.37', port=6379, db=0)    # do not need this in the notebook since we are not using the redis\n",
    "    # trade_history_redis_client = redis.Redis(host='10.75.46.228', port=6379, db=0)    # do not need this in the notebook since we are not using the redis\n",
    "    if type(cusips) != list: cusips = [cusips]    # this means that a single cusip was passed in, but not in a list\n",
    "\n",
    "    cusips_can_be_priced_df = []\n",
    "    cusips_cannot_be_priced_df = []\n",
    "\n",
    "    # TODO: take this function outside of `get_data_from_redis(...)` in `finance.py` taking an argument of `reference_data` and `trade_history_data` to make code much more DRY\n",
    "    def get_data_for_single_cusip(cusip_idx, cusip):\n",
    "        '''Get redis data for a single CUSIP. Put the data into the correct list\n",
    "        based on if the data was found or not.'''\n",
    "        if len(cusip) == 0: return None    # ignore an empty line\n",
    "        cusip = cusip.strip()        # remove leading and trailing whitespaces from CUSIP column\n",
    "\n",
    "        def missing_important_dates(single_cusip_data):\n",
    "            '''Checks whether important dates needed for pricing are null. To price a CUSIP, we need \n",
    "            `next_coupon_payment_date`, `maturity_date`, `first_coupon_date` and `accrual_date`. However, \n",
    "            we only need these features if the CUSIP is neither called nor a zero coupon bond.'''\n",
    "            interest_payment_frequency = single_cusip_data['interest_payment_frequency']\n",
    "            coupon_type = single_cusip_data['coupon_type']\n",
    "            \n",
    "            is_called = single_cusip_data['is_called'] is True\n",
    "            is_zero_coupon = coupon_type is 17 or interest_payment_frequency is 16 or single_cusip_data['coupon'] is 0    # changed `!=` to `is` since `pd.NA == 16` returns `pd.NA` instead of the expected `False` (fixed by using `is`), leading to Error: boolean value of NA is ambiguous\n",
    "            needs_important_dates = interest_payment_frequency is not 23 and interest_payment_frequency is not 16 and coupon_type is not 4\n",
    "            is_missing_important_dates = pd.isna(single_cusip_data['next_coupon_payment_date']) or \\\n",
    "                                         pd.isna(single_cusip_data['maturity_date']) or \\\n",
    "                                         pd.isna(single_cusip_data['first_coupon_date']) or \\\n",
    "                                         pd.isna(single_cusip_data['accrual_date'])\n",
    "            return not is_called and (not is_zero_coupon or needs_important_dates) and is_missing_important_dates\n",
    "\n",
    "        def get_trade_history_yields(trade_history):\n",
    "            if len(trade_history) == 0: return []\n",
    "            trade_history = trade_history[:SEQUENCE_LENGTH]    # only consider the last `SEQUENCE_LENGTH` trades\n",
    "            return [trade['yield'] for trade in trade_history]\n",
    "\n",
    "        def get_trade_history_dollar_prices(trade_history):\n",
    "            if len(trade_history) == 0: return []\n",
    "            trade_history = trade_history[:SEQUENCE_LENGTH_DOLLAR_PRICE]    # only consider the last `SEQUENCE_LENGTH_DOLLAR_PRICE` trades\n",
    "            return [trade['dollar_price'] for trade in trade_history]\n",
    "\n",
    "        def yield_in_history_is_high(trade_history_yields):\n",
    "            '''Checks whether the MSRB reported yield is greater than 10, in which case, we \n",
    "            should initially decline to price with a descriptive message for the user. \n",
    "            TODO: handle pricing for this case, possibly with the dollar price model.'''\n",
    "            return trade_history_yields != [] and None not in trade_history_yields and any(trade_history_yield > 10 for trade_history_yield in trade_history_yields)\n",
    "\n",
    "        def dollar_price_in_history_is_null(trade_history_dollar_prices):\n",
    "            '''Checks whether the MSRB reported dollar price is null, in which case, we should decline to price.'''\n",
    "            return trade_history_dollar_prices != [] and any([pd.isna(dollar_price) for dollar_price in trade_history_dollar_prices])\n",
    "\n",
    "        def irregular_coupon_rate(single_cusip_data):\n",
    "            '''Checks whether the coupon rate is irregular / variable or whether the interest payment \n",
    "            frequency is one that we cannot yet handle.'''\n",
    "            interest_payment_frequency = single_cusip_data['interest_payment_frequency']\n",
    "            coupon_type = single_cusip_data['coupon_type']\n",
    "            return interest_payment_frequency not in (1, 2, 3, 5, 16) or coupon_type not in (3, 4, 8, 10, 17, 23, 24)    # `coupon_type == 3` corresponds to bonds that have an initial period with a fixed coupon\n",
    "\n",
    "        get_cusip_cannot_be_priced_series = lambda cusip_idx, cusip, message: pd.Series({'cusip': cusip, 'message': message}, name=cusip_idx)\n",
    "\n",
    "        if cusip_is_invalid(cusip):    # all cusips must have length >= 8\n",
    "            cusips_cannot_be_priced_df.append(get_cusip_cannot_be_priced_series(cusip_idx, cusip, 'invalid'))\n",
    "        else:\n",
    "            if len(cusip) == 8:\n",
    "                check_digit = calculate_cusip_check_digit(cusip)\n",
    "                orig_cusip = cusip    # `orig_cusip` used for print statement\n",
    "                cusip = cusip + str(check_digit)\n",
    "                print(f'*** 8 digit CUSIP of {orig_cusip} was converted to 9 digit CUSIP: {cusip} ***')\n",
    "            try:\n",
    "                cusip = convert_isin_to_cusip(cusip)\n",
    "                cusip = fix_cusip_improperly_formatted_from_excel_automatic_scientific_notation(cusip)\n",
    "                reference_data = cusip_with_trade_history_and_reference_data.loc[cusip_idx]    # access by index of the dataframe with `cusip_idx` to avoid issue with same CUSIP but different `quantity` or `trade_type`\n",
    "                trade_history_data = reference_data['recent']\n",
    "                trade_history_yields = get_trade_history_yields(trade_history_data)\n",
    "                trade_history_dollar_prices = get_trade_history_dollar_prices(trade_history_data)\n",
    "                trade_history_data = pd.DataFrame.from_records(trade_history_data)[list(FEATURES_FOR_EACH_TRADE_IN_HISTORY.keys())].to_numpy() if len(trade_history_data) > 0 else np.array([])    # create trade history as a dataframe and convert to numpy to represent the data as it comes from the redis\n",
    "                reference_data['recent'] = trade_history_data\n",
    "                if reference_data['outstanding_indicator'] is False:\n",
    "                    cusips_cannot_be_priced_df.append(get_cusip_cannot_be_priced_series(cusip_idx, cusip, 'not_outstanding'))\n",
    "                elif pd.isna(reference_data['coupon']) or pd.isna(reference_data['interest_payment_frequency']) or pd.isna(reference_data['default_indicator']) or missing_important_dates(reference_data):\n",
    "                    cusips_cannot_be_priced_df.append(get_cusip_cannot_be_priced_series(cusip_idx, cusip, 'insufficient_data'))\n",
    "                elif reference_data['default_exists'] is True or reference_data['default_indicator'] is True:\n",
    "                    cusips_cannot_be_priced_df.append(get_cusip_cannot_be_priced_series(cusip_idx, cusip, 'defaulted'))\n",
    "                elif yield_in_history_is_high(trade_history_yields):\n",
    "                    cusips_cannot_be_priced_df.append(get_cusip_cannot_be_priced_series(cusip_idx, cusip, 'high_yield_in_history'))\n",
    "                elif dollar_price_in_history_is_null(trade_history_dollar_prices):\n",
    "                    cusips_cannot_be_priced_df.append(get_cusip_cannot_be_priced_series(cusip_idx, cusip, 'null_dollar_price_in_history'))\n",
    "                elif irregular_coupon_rate(reference_data):\n",
    "                    cusips_cannot_be_priced_df.append(get_cusip_cannot_be_priced_series(cusip_idx, cusip, 'irregular_coupon_rate'))\n",
    "                else:    # no problems with this cusip\n",
    "                    # reference_data['recent'] = trade_history_data    # do not need to do this in the notebook since we are working specifically with the reference data and trade history together\n",
    "                    cusips_can_be_priced_df.append(reference_data.rename(cusip_idx))    # change name to be `cusip_idx` in order to preserve original ordering\n",
    "            except Exception as e:    # this means that the cusip was not found in the data file\n",
    "                print(f'{cusip} not in dataframe.\\n\\tError: {e}')\n",
    "                cusips_cannot_be_priced_df.append(get_cusip_cannot_be_priced_series(cusip_idx, cusip, 'not_found'))\n",
    "\n",
    "    for cusip_idx, cusip in enumerate(cusips):\n",
    "        get_data_for_single_cusip(cusip_idx, cusip)\n",
    "    cusips_can_be_priced_df = pd.concat(cusips_can_be_priced_df, axis=1).T if cusips_can_be_priced_df != [] else pd.DataFrame()    # list of series to dataframe: https://stackoverflow.com/questions/55478191/list-of-series-to-dataframe\n",
    "    cusips_cannot_be_priced_df = pd.concat(cusips_cannot_be_priced_df, axis=1).T if cusips_cannot_be_priced_df != [] else pd.DataFrame()     # list of series to dataframe: https://stackoverflow.com/questions/55478191/list-of-series-to-dataframe\n",
    "    \n",
    "    return cusips_can_be_priced_df, cusips_cannot_be_priced_df\n",
    "modules.data_preparation_for_pricing.get_data_from_redis = get_data_from_redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the historical yield spread model and dollar price model for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10000    # empirically determined to give fast batch predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_spread(instances: List[Dict]) -> List[List]:\n",
    "    '''Retrieves yield spread estimates from the yield spread model on given a set of `instances`.'''\n",
    "    return yield_spread_model.predict(instances, batch_size=BATCH_SIZE)\n",
    "modules.pricing_functions.predict_spread = predict_spread\n",
    "\n",
    "\n",
    "def predict_dollar_price(instances: List[Dict]) -> List[List]:\n",
    "    '''Retrieves dollar price estimates from the dollar price model on given a set of `instances`.'''\n",
    "    return dollar_price_model.predict(instances, batch_size=BATCH_SIZE)\n",
    "modules.pricing_functions.predict_dollar_price = predict_dollar_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `features_for_input_to_nn(...)` and `get_inputs_for_nn(...)` need to be changed since the original functions returns inputs for the neural network as an array list to call the model directly, instead of a list of dictionaries as necessary for vertex AI. Since we get the model as a file in this notebook instead of calling the currently deployed model on vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_for_input_to_nn(df, use_dollar_price_model):\n",
    "    '''Returns inputs for the neural network as an array list to call the model directly, \n",
    "    instead of a list of dictionaries as necessary for vertex AI. This is because we get \n",
    "    the model as a file in this notebook instead of calling the currently deployed model \n",
    "    on vertex AI.'''\n",
    "    encoders = get_encoders(use_dollar_price_model)    # do not make `encoders` a global variable because the only way to use the updated encoders is to re-deploy the server, and we would like to use updated encoders even if there is no server code change\n",
    "    datalist = []\n",
    "    non_cat_features = NON_CAT_FEATURES_DOLLAR_PRICE if use_dollar_price_model else NON_CAT_FEATURES\n",
    "    binary = BINARY_DOLLAR_PRICE if use_dollar_price_model else BINARY\n",
    "    noncat_and_binary = [np.expand_dims(df[f].to_numpy().astype('float64'), axis=1) for f in non_cat_features + binary]\n",
    "    datalist.append(np.concatenate(noncat_and_binary, axis=-1))\n",
    "\n",
    "    categorical_features = CATEGORICAL_FEATURES_DOLLAR_PRICE if use_dollar_price_model else CATEGORICAL_FEATURES\n",
    "    for f in categorical_features:\n",
    "        encoded = encoders[f].transform(df[f])\n",
    "        datalist.append(encoded.astype('float64'))\n",
    "    return datalist\n",
    "modules.data_preparation_for_pricing.features_for_input_to_nn = features_for_input_to_nn\n",
    "\n",
    "\n",
    "def get_inputs_for_nn(df, use_dollar_price_model):\n",
    "    '''Returns inputs for the neural network as an array list to call the model directly, \n",
    "    instead of a list of dictionaries as necessary for vertex AI. This is because we get \n",
    "    the model as a file in this notebook instead of calling the currently deployed model \n",
    "    on vertex AI.'''\n",
    "    trade_history_input = df['trade_history_dollar_price'] if use_dollar_price_model else df['trade_history']\n",
    "    trade_history_input = np.stack(trade_history_input.to_numpy())\n",
    "\n",
    "    target_attention_features_input = np.stack(df['target_attention_features'].to_numpy())\n",
    "    \n",
    "    return [trade_history_input, target_attention_features_input] + features_for_input_to_nn(df, use_dollar_price_model)\n",
    "modules.data_preparation_for_pricing.get_inputs_for_nn = get_inputs_for_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priced_df = price_cusips_list(cusip_list, quantity_list, trade_type_list, DATETIME_OF_INTEREST)\n",
    "priced_df = prepare_batch_pricing_results_to_output_to_user(prepare_batch_pricing_results_for_logging(priced_df))\n",
    "priced_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the results and save the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "did_not_price = priced_df[priced_df['ytw'] == NUMERICAL_ERROR]\n",
    "did_not_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priced_df.to_csv(f'priced_{MODEL_DATE_AS_STRING}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving to an excel spreadsheet causes the kernel to crash even though the cell works. Uncomment the below cell if excel spreadsheet format is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# priced_df.to_excel(f'priced_{MODEL_DATE_AS_STRING}.xlsx', index=False)    # save the DataFrame to an excel file"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
