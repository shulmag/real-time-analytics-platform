{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " # @ Create Time: 2022-01-14 17:44:00\n",
    " # @ Modified by: Gil\n",
    " # @ Modified time: 2024-08-12 11:55am PT\n",
    " # @ Description: This file implements functions from the pricing module\n",
    " \n",
    " '''\n",
    "# ensures that any changes to the modules will be reloaded when this cell is run\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from ficc.utils.auxiliary_functions import sqltodf\n",
    "from ficc.utils.process_features import process_features\n",
    "from ficc.pricing.price import compute_price\n",
    "from ficc.pricing.yield_rate import compute_yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/Users/gil/git/ficc/creds.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditions that we tested on:\n",
    "* `yield > 0`: conceptually, this notebook should work for any yield, but in practice, bonds with zero yield indicates a problem with MSRB data, and negative yields haven't been tested on\n",
    "* `is_non_transaction_based_compensation is false`, `is_lop_or_takedown is false`: conceptually, this notebook should work without these filters, but in practice, away from market prices sometimes have anomolous yields reported in the MSRB data\n",
    "* `callable_at_cav is false`: this notebook will not be correct for bonds callable at cav; future work will incorporate the special case of cav bonds that have call prices from old reference data\n",
    "* `ref_valid_to_date > timestamp(publish_datetime, \"America/New_York\")`, `timestamp(publish_datetime, \"America/New_York\") >= ref_valid_from_date`, `msrb_valid_to_date > publish_datetime`, `msrb_valid_from_date <= publish_datetime`: conditions to correctly join old reference data and MSRB\n",
    "* `par_call_price = 100`: conceptually this is identical to the fourth bullet point, but sometimes there are issues in the old reference data\n",
    "* `interest_payment_frequency is not NULL`: if interest payment frequency is NULL, then the coupon frequency is unknown; possibly the field `coupon_type` could be used to determine the interest payment frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = lambda trade_date: ''' \n",
    "SELECT\n",
    "  IFNULL(settlement_date, assumed_settlement_date) AS settlement_date,\n",
    "  trade_date,\n",
    "  cusip,\n",
    "  dated_date,\n",
    "  par_traded,\n",
    "  accrual_date,\n",
    "  dollar_price,\n",
    "  issue_price,\n",
    "  coupon,\n",
    "  interest_payment_frequency,\n",
    "  next_call_date,\n",
    "  par_call_date,\n",
    "  next_call_price,\n",
    "  par_call_price,\n",
    "  maturity_date,\n",
    "  previous_coupon_payment_date,\n",
    "  next_coupon_payment_date,\n",
    "  first_coupon_date,\n",
    "  coupon_type,\n",
    "  muni_security_type,\n",
    "  called_redemption_type,\n",
    "  refund_date,\n",
    "  refund_price,\n",
    "  is_callable,\n",
    "  is_called,\n",
    "  call_timing,\n",
    "  yield,\n",
    "  rtrs_control_number,\n",
    "  has_zero_coupons,\n",
    "  last_period_accrues_from_date,\n",
    "  call_defeased,\n",
    "  issue_amount,\n",
    "  -- needed for `process_features(...)`\n",
    "  maturity_amount,\n",
    "  -- needed for `process_features(...)`\n",
    "  orig_principal_amount,\n",
    "  -- needed for `process_features(...)`\n",
    "  max_amount_outstanding,\n",
    "  -- needed for `process_features(...)`\n",
    "  delivery_date,\n",
    "  -- needed for `process_features(...)`\n",
    "  next_sink_date    -- needed for `process_features(...)`\n",
    "FROM\n",
    "  `eng-reactor-287421.jesse_tests.msrb_sp_joined`\n",
    "  -- `eng-reactor-287421.auxiliary_views_v2.trade_history_same_issue_5_yr_mat_bucket_1_materialized`\n",
    "WHERE\n",
    "  -- yield > 0 and\n",
    "  -- is_non_transaction_based_compensation is false and\n",
    "  -- callable_at_cav is false and\n",
    "  -- is_lop_or_takedown is false and\n",
    "  DATETIME(ref_valid_to_date) > publish_datetime\n",
    "  AND publish_datetime >= DATETIME(ref_valid_from_date)\n",
    "  AND msrb_valid_to_date > publish_datetime\n",
    "  AND msrb_valid_from_date <= publish_datetime\n",
    "  AND interest_payment_frequency IN (1, 2, 3, 5, 16, 31) -- Semiannual, Monthly, Annually, Quarterly, AtMat, zcb\n",
    "  and default_exists is false\n",
    "  -- par_call_price is not NULL and\n",
    "  -- par_call_price = 100 and\n",
    "  -- interest_payment_frequency is not NULL and\n",
    "  -- interest_payment_frequency = 16 and\n",
    "  -- coupon > 0 and\n",
    "  AND trade_date >= \\'''' + trade_date + '''\\'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2024-08-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "muni_df_query = query(date)\n",
    "\n",
    "using_saved_muni_df = False\n",
    "MUNI_DF_FILE_NAME = 'sp_joined.pkl' #'ice_joined.pkl' \n",
    "if os.path.exists(MUNI_DF_FILE_NAME):\n",
    "    with open(MUNI_DF_FILE_NAME, 'rb') as file:\n",
    "        muni_df_query_from_pkl, muni_df = pickle.load(file)\n",
    "    if muni_df_query == muni_df_query_from_pkl:\n",
    "        using_saved_muni_df = True\n",
    "\n",
    "if not using_saved_muni_df:\n",
    "    bqclient = bigquery.Client()\n",
    "    muni_df = sqltodf(muni_df_query, bqclient)\n",
    "    with open(MUNI_DF_FILE_NAME, 'wb') as file:\n",
    "        pickle.dump((muni_df_query, muni_df), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "muni_df = muni_df.dropna(subset=['accrual_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muni_df[muni_df.cusip == '67868NCJ6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muni_df = process_features(muni_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "muni_df = muni_df.rename(columns={'yield': 'ficc_ytw'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the `compute_price` function to every trade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold off - Convert all decimals in the df to float\n",
    "# muni_df = muni_df[~(muni_df.is_callable & pd.isnull(muni_df.next_call_date))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not a great solution: \n",
    "muni_df = muni_df[~(pd.isnull(muni_df.last_period_accrues_from_date))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muni_df[muni_df.cusip == '6500357E2'][['called_redemption_type','call_defeased']]\n",
    "#muni_df[pd.isna(muni_df.maturity_date)]\n",
    "\n",
    "print(len(muni_df))\n",
    "\n",
    "muni_df = muni_df[muni_df.cusip != '882723J34']\n",
    "muni_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muni_df['price_calc_from_yield'] = muni_df.apply(lambda x: compute_price(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the price from the reference data on the x-axis and our computed price on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muni_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muni_df['price_from_yield'] = [x[0] for x in muni_df['price_calc_from_yield']]\n",
    "muni_df.plot.scatter(x='dollar_price', y='price_from_yield', c='DarkBlue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the sum and mean of the errors where the error is defined as the absolute distance between our computed price and the reference price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "muni_df['price_delta'] = abs(muni_df['price_from_yield'] - muni_df['dollar_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f'Sum of errors: {np.sum(muni_df['price_delta'])}')\n",
    "print(f'Mean of errors: {np.mean(muni_df['price_delta'])}')\n",
    "\n",
    "# Check for infinite and NaN values in 'price_delta'\n",
    "num_infinite_values = np.isinf(muni_df['price_delta']).sum()\n",
    "num_nan_values = muni_df['price_delta'].isna().sum()\n",
    "\n",
    "print(f'{num_infinite_values} infinite values found in \\'price_delta\\'')\n",
    "print(f'{num_nan_values} NaN values found in \\'price_delta\\'')\n",
    "\n",
    "# Filter out rows with infinite or NaN values in 'price_delta'\n",
    "muni_df_filtered = muni_df[~np.isinf(muni_df['price_delta']) & muni_df['price_delta'].notna()]\n",
    "\n",
    "# Calculate the sum and mean of errors in the filtered DataFrame\n",
    "sum_errors = np.sum(muni_df_filtered['price_delta'])\n",
    "mean_errors = np.mean(muni_df_filtered['price_delta'])\n",
    "\n",
    "print(f'Sum of errors: {sum_errors}')\n",
    "print(f'Mean of errors: {mean_errors}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#muni_df.price_delta.value_counts()\n",
    "\n",
    "# Define the bins\n",
    "bins = [0, 0.1, 5, np.inf]\n",
    "\n",
    "# Bin the 'price_delta' values\n",
    "binned_price_delta = pd.cut(muni_df['price_delta'], bins=bins)\n",
    "\n",
    "# Count the occurrences in each bin\n",
    "binned_price_delta.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muni_df[muni_df['price_delta'] > 10][['cusip','ficc_ytw','dollar_price','price_calc_from_yield','price_from_yield','price_delta']].sort_values(by='price_delta', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "muni_df.to_pickle('sp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muni_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S&P Compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the relevant pickled data files\n",
    "sp_df = pd.read_pickle('sp.pkl')\n",
    "ice_df = pd.read_pickle('ice.pkl')\n",
    "\n",
    "def analyze_and_plot(data, title):\n",
    "    # Calculate price_from_yield\n",
    "    data['price_from_yield'] = [x[0] for x in data['price_calc_from_yield']]\n",
    "    \n",
    "    # Calculate price_delta\n",
    "    data['price_delta'] = abs(data['price_from_yield'] - data['dollar_price'])\n",
    "    \n",
    "    # Normalize price_delta for color mapping\n",
    "    norm = plt.Normalize(data['price_delta'].min(), data['price_delta'].max())\n",
    "    \n",
    "    # Create a custom colormap with darker colors\n",
    "    colors = plt.cm.Purples(np.linspace(0.3, 1, 256))  # Adjusted to use darker shades\n",
    "    custom_cmap = plt.cm.colors.LinearSegmentedColormap.from_list(\"custom\", colors)\n",
    "    \n",
    "    # Apply the custom colormap to the price_delta values\n",
    "    color_values = custom_cmap(norm(data['price_delta']))\n",
    "    \n",
    "    # Plot the scatter plot with the custom colormap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(data['dollar_price'], data['price_from_yield'], \n",
    "                          color=color_values, edgecolors='none', alpha=0.7, s=30)\n",
    "    \n",
    "    # Label major outliers without duplicates\n",
    "    threshold = data['price_delta'].quantile(0.9995)  # Top 0.03% of outliers\n",
    "    outliers = data[data['price_delta'] > threshold].sort_values('price_delta', ascending=False)\n",
    "    labeled_cusips = set()\n",
    "    for _, row in outliers.iterrows():\n",
    "        if row['cusip'] not in labeled_cusips:\n",
    "            plt.annotate(row['cusip'], \n",
    "                         (row['dollar_price'], row['price_from_yield']),\n",
    "                         xytext=(5, 5), textcoords='offset points', \n",
    "                         fontsize=8, alpha=0.7)\n",
    "            labeled_cusips.add(row['cusip'])\n",
    "    \n",
    "    # Adding plot details\n",
    "    plt.xlabel('Dollar Price')\n",
    "    plt.ylabel('Price from Yield')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and print the sum and mean of errors\n",
    "    sum_errors = np.sum(data['price_delta'])\n",
    "    mean_errors = np.mean(data['price_delta'])\n",
    "    print(f'Sum of errors: {sum_errors}')\n",
    "    print(f'Mean of errors: {mean_errors}')\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Analyze and plot for S&P\n",
    "sp_analysis = analyze_and_plot(sp_df, 'S&P Data Analysis')\n",
    "\n",
    "# Analyze and plot for old reference data\n",
    "ice_analysis = analyze_and_plot(ice_df, 'old reference data Data Analysis')\n",
    "\n",
    "# Identify and compare outliers for both datasets, ensuring distinct cusips\n",
    "sp_outliers = sp_analysis[sp_analysis['price_delta'] > 10][['cusip', 'ficc_ytw', 'dollar_price', 'price_calc_from_yield', 'price_from_yield', 'price_delta']].sort_values(by='price_delta', ascending=False).drop_duplicates(subset=['cusip'])\n",
    "ice_outliers = ice_analysis[ice_analysis['price_delta'] > 10][['cusip', 'ficc_ytw', 'dollar_price', 'price_calc_from_yield', 'price_from_yield', 'price_delta']].sort_values(by='price_delta', ascending=False).drop_duplicates(subset=['cusip'])\n",
    "\n",
    "# Check if the outliers are the same in both datasets\n",
    "common_outliers = pd.merge(sp_outliers, ice_outliers, on='cusip', suffixes=('_sp', '_ice'))\n",
    "\n",
    "# Display the common outliers\n",
    "print(common_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the relevant pickled data files\n",
    "sp_df = pd.read_pickle('sp.pkl')\n",
    "\n",
    "sp_df = sp_df.drop_duplicates(subset=['cusip'])\n",
    "\n",
    "sp_lst = sp_df[sp_df.price_delta > 1].cusip\n",
    "\n",
    "ice_df = pd.read_pickle('ice.pkl')\n",
    "ice_df = ice_df.drop_duplicates(subset=['cusip'])\n",
    "\n",
    "# Merge the dataframes on the 'cusip' field\n",
    "combined_df = pd.merge(ice_df, sp_df, on='cusip', suffixes=('_ice', '_sp'))\n",
    "# combined_df = combined_df.drop_duplicates(subset=['cusip'])\n",
    "\n",
    "print(len(sp_df[sp_df.price_delta > 1]))\n",
    "\n",
    "#print([ice_df[ice_df.cusip == cusip] for cusip in sp_lst])\n",
    "\n",
    "merged_df = pd.merge(ice_df, sp_df, on='cusip', suffixes=('_ice', '_sp'))\n",
    "merged_df = merged_df.drop_duplicates(subset=['cusip'])\n",
    "\n",
    "print(len(merged_df[merged_df.price_delta_sp > 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the relevant pickled data files\n",
    "sp_df = pd.read_pickle('sp.pkl')\n",
    "ice_df = pd.read_pickle('ice.pkl')\n",
    "\n",
    "# Merge the dataframes on the 'cusip' field\n",
    "combined_df = pd.merge(ice_df, sp_df, on='cusip', suffixes=('_ice', '_sp'))\n",
    "combined_df = combined_df.drop_duplicates(subset=['cusip'])\n",
    "\n",
    "# print(f\" combined_df[combined_df.price_delta_sp > 1])\n",
    "\n",
    "# Identify CUSIPs that are outliers in SP but not in old reference data\n",
    "ice_good_sp_bad = combined_df[\n",
    "    (combined_df['price_delta_ice'] <= 10) & \n",
    "    (combined_df['price_delta_sp'] > 1)\n",
    "]\n",
    "\n",
    "# List of key fields to compare (ensure these fields are available in both dataframes)\n",
    "key_fields = [\n",
    "    'dollar_price', \n",
    "    'price_from_yield',\n",
    "    'price_delta',\n",
    "    'next_call_date', \n",
    "    'interest_payment_frequency', \n",
    "    'first_coupon_date', \n",
    "    'maturity_date', \n",
    "    'dated_date',\n",
    "    'accrual_date', \n",
    "    'issue_price', \n",
    "    'coupon', \n",
    "    'par_call_date', \n",
    "    'par_call_price', \n",
    "    'next_coupon_payment_date', \n",
    "    'coupon_type', \n",
    "    'muni_security_type', \n",
    "    'issue_amount', \n",
    "    'maturity_amount', \n",
    "    'delivery_date', \n",
    "    'next_sink_date'\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with side-by-side comparison\n",
    "comparison_df = pd.DataFrame()\n",
    "comparison_df['cusip'] = ice_good_sp_bad['cusip']\n",
    "\n",
    "for field in key_fields:\n",
    "    comparison_df[f'{field}_ice'] = ice_good_sp_bad[f'{field}_ice']\n",
    "    comparison_df[f'{field}_sp'] = ice_good_sp_bad[f'{field}_sp']\n",
    "\n",
    "# Display the comparison\n",
    "print(\"Comparison of Key Fields for Outliers\")\n",
    "display(comparison_df)\n",
    "comparison_df.to_csv('sp_ytp_data_comparison_analysis.csv')\n",
    "\n",
    "# Create a dataframe to store differences\n",
    "diff_comparison_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each cusip to find differences\n",
    "for cusip in ice_good_sp_bad['cusip']:\n",
    "    cusip_row = ice_good_sp_bad[ice_good_sp_bad['cusip'] == cusip]\n",
    "    diff_row = {'cusip': cusip}\n",
    "    for field in key_fields:\n",
    "        ice_value = cusip_row[f\"{field}_ice\"].values[0]\n",
    "        sp_value = cusip_row[f\"{field}_sp\"].values[0]\n",
    "        if ice_value != sp_value:\n",
    "            diff_row[f\"{field}_ice\"] = ice_value\n",
    "            diff_row[f\"{field}_sp\"] = sp_value\n",
    "    diff_comparison_df = diff_comparison_df.append(diff_row, ignore_index=True)\n",
    "\n",
    "# Display the comparison of fields with differences\n",
    "print(\"Comparison of Key Fields for Outliers with Differences\")\n",
    "display(diff_comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df[comparison_df.dollar_price_ice == comparison_df.dollar_price_sp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the yield from the reference data on the x-axis and our computed yield on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muni_df['yield_from_price'] = [x[0] for x in muni_df['yield_and_calc_date']]\n",
    "muni_df['yield_calc_date'] = [x[1] for x in muni_df['yield_and_calc_date']]\n",
    "# when we get close to redemption, it's difficult to compute the yield from the price\n",
    "muni_df.plot.scatter(x='yield', y='yield_from_price', c='DarkBlue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the sum and mean of the errors where the error is defined as the absolute distance between our computed yield and the reference yield."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muni_df['ytw_delta'] = abs(muni_df['yield_from_price'] - muni_df['yield'])\n",
    "print(f'Sum of errors: {np.sum(muni_df['ytw_delta'])}')\n",
    "print(f'Mean of errors: {np.mean(muni_df['ytw_delta'])}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
