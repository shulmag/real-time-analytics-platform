{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11345108-947c-4d4e-b702-661ca2190fbb",
   "metadata": {},
   "source": [
    "This notebook has been superceded by https://github.com/Ficc-ai/ficc/blob/dev/app_engine/demo/server/point_in_time_pricing.ipynb.\n",
    "\n",
    "**This notebook serves as a template for the workflow to retrieve point in time predictions for historical trades. Models archived in cloud storage automated_training bucket [here](https://console.cloud.google.com/storage/browser/automated_training;tab=objects?forceOnBucketsSortingFiltering=true&authuser=1&project=eng-reactor-287421&prefix=&forceOnObjectsSortingFiltering=false) are dated as of the day of deployment, meaning they are tested on the day prior. For example, if I want predictions for 11-01, retrieve the model titled 11-01 - that model will be trained up till 10-30 and tested on 10-31.** \n",
    "\n",
    "The following steps are needed to get point-in-time predictions: \n",
    "1. Load model deployed on target date\n",
    "2. Load point-in-time data for target date \n",
    "3. Process point-in-time data for target date so that it can be fed to the model \n",
    "4. Run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "061514eb-8de1-4a82-add3-7280d95bf98b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "TF Version: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import gcsfs\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from ficc.utils.auxiliary_functions import sqltodf\n",
    "from ficc.data.process_data import *\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "print(f'TF Version: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce7061eb-34a3-4630-8064-7d234c845746",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/home/jupyter/ficc/isaac_creds.json\"\n",
    "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "    \n",
    "bq_client = bigquery.Client()\n",
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16378aa3-174c-4585-b403-55034d9c9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "BINARY = ['callable',\n",
    "          'sinking',\n",
    "          'zerocoupon',\n",
    "          'is_non_transaction_based_compensation',\n",
    "          'is_general_obligation',\n",
    "          'callable_at_cav',\n",
    "          'extraordinary_make_whole_call',\n",
    "          'make_whole_call',\n",
    "          'has_unexpired_lines_of_credit',\n",
    "          'escrow_exists']\n",
    "\n",
    "CATEGORICAL_FEATURES = ['rating',\n",
    "                        'incorporated_state_code',\n",
    "                        'trade_type',\n",
    "                        'purpose_class',\n",
    "                        'max_ys_ttypes',\n",
    "                        'min_ys_ttypes',\n",
    "                        'max_qty_ttypes',\n",
    "                        'min_ago_ttypes',\n",
    "                        'D_min_ago_ttypes',\n",
    "                        'P_min_ago_ttypes',\n",
    "                        'S_min_ago_ttypes']\n",
    "\n",
    "NON_CAT_FEATURES = ['quantity',\n",
    "                     'days_to_maturity',\n",
    "                     'days_to_call',\n",
    "                     'coupon',\n",
    "                     'issue_amount',\n",
    "                     'last_seconds_ago',\n",
    "                     'last_yield_spread',\n",
    "                     'days_to_settle',\n",
    "                     'days_to_par',\n",
    "                     'maturity_amount',\n",
    "                     'issue_price',\n",
    "                     'orig_principal_amount',\n",
    "                     'max_amount_outstanding',\n",
    "                     'accrued_days',\n",
    "                     'days_in_interest_payment',\n",
    "                     'A/E',\n",
    "                     'ficc_treasury_spread',\n",
    "                     'max_ys_ys',\n",
    "                     'max_ys_ago',\n",
    "                     'max_ys_qdiff',\n",
    "                     'min_ys_ys',\n",
    "                     'min_ys_ago',\n",
    "                     'min_ys_qdiff',\n",
    "                     'max_qty_ys',\n",
    "                     'max_qty_ago',\n",
    "                     'max_qty_qdiff',\n",
    "                     'min_ago_ys',\n",
    "                     'min_ago_ago',\n",
    "                     'min_ago_qdiff',\n",
    "                     'D_min_ago_ys',\n",
    "                     'D_min_ago_ago',\n",
    "                     'D_min_ago_qdiff',\n",
    "                     'P_min_ago_ys',\n",
    "                     'P_min_ago_ago',\n",
    "                     'P_min_ago_qdiff',\n",
    "                     'S_min_ago_ys',\n",
    "                     'S_min_ago_ago',\n",
    "                     'S_min_ago_qdiff']\n",
    "\n",
    "ttype_dict = { (0,0):'D', (0,1):'S', (1,0):'P' }\n",
    "\n",
    "ys_variants = [\"max_ys\", \"min_ys\", \"max_qty\", \"min_ago\", \"D_min_ago\", \"P_min_ago\", \"S_min_ago\"]\n",
    "ys_feats = [\"_ys\", \"_ttypes\", \"_ago\", \"_qdiff\"]\n",
    "D_prev = dict()\n",
    "P_prev = dict()\n",
    "S_prev = dict()\n",
    "\n",
    "def get_trade_history_columns():\n",
    "    '''\n",
    "    This function is used to create a list of columns\n",
    "    '''\n",
    "    YS_COLS = []\n",
    "    for prefix in ys_variants:\n",
    "        for suffix in ys_feats:\n",
    "            YS_COLS.append(prefix + suffix)\n",
    "    return YS_COLS\n",
    "\n",
    "def extract_feature_from_trade(row, name, trade):\n",
    "    yield_spread = trade[0]\n",
    "    ttypes = ttype_dict[(trade[3],trade[4])] + row.trade_type\n",
    "    seconds_ago = trade[5]\n",
    "    quantity_diff = np.log10(1 + np.abs(10**trade[2] - 10**row.quantity))\n",
    "    return [yield_spread, ttypes,  seconds_ago, quantity_diff]\n",
    "\n",
    "def trade_history_derived_features(row):\n",
    "    trade_history = row.trade_history\n",
    "    trade = trade_history[0]\n",
    "    \n",
    "    D_min_ago_t = D_prev.get(row.cusip,trade)\n",
    "    D_min_ago = 9        \n",
    "\n",
    "    P_min_ago_t = P_prev.get(row.cusip,trade)\n",
    "    P_min_ago = 9\n",
    "    \n",
    "    S_min_ago_t = S_prev.get(row.cusip,trade)\n",
    "    S_min_ago = 9\n",
    "    \n",
    "    max_ys_t = trade; max_ys = trade[0]\n",
    "    min_ys_t = trade; min_ys = trade[0]\n",
    "    max_qty_t = trade; max_qty = trade[2]\n",
    "    min_ago_t = trade; min_ago = trade[5]\n",
    "    \n",
    "    for trade in trade_history[0:]:\n",
    "        #Checking if the first trade in the history is from the same block\n",
    "        if trade[5] == 0: \n",
    "            continue\n",
    " \n",
    "        if trade[0] > max_ys: \n",
    "            max_ys_t = trade\n",
    "            max_ys = trade[0]\n",
    "        elif trade[0] < min_ys: \n",
    "            min_ys_t = trade; \n",
    "            min_ys = trade[0]\n",
    "\n",
    "        if trade[2] > max_qty: \n",
    "            max_qty_t = trade \n",
    "            max_qty = trade[2]\n",
    "        if trade[5] < min_ago: \n",
    "            min_ago_t = trade; \n",
    "            min_ago = trade[5]\n",
    "            \n",
    "        side = ttype_dict[(trade[3],trade[4])]\n",
    "        if side == \"D\":\n",
    "            if trade[5] < D_min_ago: \n",
    "                D_min_ago_t = trade; D_min_ago = trade[5]\n",
    "                D_prev[row.cusip] = trade\n",
    "        elif side == \"P\":\n",
    "            if trade[5] < P_min_ago: \n",
    "                P_min_ago_t = trade; P_min_ago = trade[5]\n",
    "                P_prev[row.cusip] = trade\n",
    "        elif side == \"S\":\n",
    "            if trade[5] < S_min_ago: \n",
    "                S_min_ago_t = trade; S_min_ago = trade[5]\n",
    "                S_prev[row.cusip] = trade\n",
    "        else: \n",
    "            print(\"invalid side\", trade)\n",
    "    \n",
    "    trade_history_dict = {\"max_ys\":max_ys_t,\n",
    "                          \"min_ys\":min_ys_t,\n",
    "                          \"max_qty\":max_qty_t,\n",
    "                          \"min_ago\":min_ago_t,\n",
    "                          \"D_min_ago\":D_min_ago_t,\n",
    "                          \"P_min_ago\":P_min_ago_t,\n",
    "                          \"S_min_ago\":S_min_ago_t}\n",
    "\n",
    "    return_list = []\n",
    "    for variant in ys_variants:\n",
    "        feature_list = extract_feature_from_trade(row,variant,trade_history_dict[variant])\n",
    "        return_list += feature_list\n",
    "    \n",
    "    return return_list\n",
    "\n",
    "\n",
    "def create_input(df):\n",
    "    datalist = []\n",
    "    \n",
    "    datalist.append(np.stack(df['trade_history'].to_numpy()))\n",
    "    datalist.append(np.stack(df['target_attention_features'].to_numpy()))\n",
    "\n",
    "    noncat_and_binary = []\n",
    "    for f in NON_CAT_FEATURES + BINARY:\n",
    "        noncat_and_binary.append(np.expand_dims(df[f].to_numpy().astype('float64'), axis=1))\n",
    "    datalist.append(np.concatenate(noncat_and_binary, axis=-1))\n",
    "    \n",
    "    for f in CATEGORICAL_FEATURES:\n",
    "        encoded = encoders[f].transform(df[f])\n",
    "        datalist.append(encoded.astype('float32'))\n",
    "    return datalist\n",
    "\n",
    "def addflag(flag, condition, name):\n",
    "    empty = flag == \"none\"\n",
    "    flag[condition & empty] = name\n",
    "    flag[condition & ~empty] = flag[condition & ~empty] + \" & \" + name\n",
    "    \n",
    "def addcol(data, newname, newvals, warn=False):\n",
    "    if newname in data.columns:\n",
    "        if warn: print( f\"Warning: replacing duplicate column {newname}\" )\n",
    "        data[newname] = newvals\n",
    "    else:\n",
    "        newcol = pd.Series(newvals, index = data.index, name=newname)\n",
    "        data = pd.concat([data,newcol],axis=1)\n",
    "    return data\n",
    "\n",
    "def mkcases(df):\n",
    "    flag = pd.Series(\"none\", index=df.index)\n",
    "\n",
    "    addflag(flag, df.last_yield.isna(), \"no last yld\")\n",
    "    addflag(flag, df.last_yield < 150, \"last yld < 1.5%\")\n",
    "    addflag(flag, df.last_yield.between(150,700), \"1.5% <= last yld <= 7%\")\n",
    "    addflag(flag, df.last_yield > 700, \"last yld > 7%\")\n",
    "    addflag(flag, df.when_issued, \"when issued\")\n",
    "    \n",
    "    print( flag.value_counts(dropna=False) )\n",
    "    return flag.astype('category')\n",
    "\n",
    "def mean_absolute_deviation(pred, truth):\n",
    "    pred, truth = np.array(pred).reshape(-1,1), np.array(truth).reshape(-1,1)\n",
    "    err = abs(pred - truth)\n",
    "    return np.median(err)\n",
    "\n",
    "def compare_mae(df, prediction_cols, groupby_cols, target_variable):\n",
    "    \n",
    "    if not isinstance(prediction_cols, list):\n",
    "        raise TypeError(f'prediction_cols must be a list, got {type(prediction_cols)}, {type(groupby_cols)} instead')\n",
    "    \n",
    "    if groupby_cols and not isinstance(groupby_cols, list):\n",
    "        raise TypeError(f'groupby_cols must be a list or None, got {type(groupby_cols)} instead')\n",
    "    \n",
    "    print(f'{f\" Analysis for target: {target_variable} \":=^75}')\n",
    "    \n",
    "    nan_counts = df[prediction_cols].isna().sum() \n",
    "    \n",
    "    for x,y  in df[prediction_cols].isna().sum().iteritems():\n",
    "        print(f'Prediction col {x} has {y} nan values')\n",
    "    \n",
    "    df = df.dropna(subset=prediction_cols)\n",
    "\n",
    "    if groupby_cols:\n",
    "        temp = df[[target_variable, 'cases'] + prediction_cols + groupby_cols]\\\n",
    "                .groupby(groupby_cols, observed=True)\\\n",
    "                .apply(lambda x: [mean_absolute_error(x[target_variable], x[col]) for col in prediction_cols] + \\\n",
    "                        [mean_absolute_deviation(x[target_variable], x[col]) for col in prediction_cols] + [len(x)])   \n",
    "        temp = pd.DataFrame(temp.to_list(), index = zip(['Overall']*len(temp),temp.index))\n",
    "\n",
    "        temp2 = df[[target_variable, 'cases'] + prediction_cols + groupby_cols]\\\n",
    "                .groupby(['cases']+ groupby_cols, observed=True)\\\n",
    "                .apply(lambda x: [mean_absolute_error(x[target_variable], x[col]) for col in prediction_cols] + \\\n",
    "                        [mean_absolute_deviation(x[target_variable], x[col]) for col in prediction_cols] + [len(x)])   \n",
    "        temp2 = pd.DataFrame(temp2.to_list(), index = temp2.index)\n",
    "        summary = pd.concat([temp, temp2], axis=0)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        temp2 = df[[target_variable, 'cases'] + prediction_cols]\\\n",
    "                .groupby('cases', observed=True)\\\n",
    "                .apply(lambda x: [mean_absolute_error(x[target_variable], x[col]) for col in prediction_cols] + \\\n",
    "                        [mean_absolute_deviation(x[target_variable], x[col]) for col in prediction_cols] + [len(x)])   \n",
    "        temp2 = pd.DataFrame(temp2.to_list(), index = temp2.index)\n",
    "        \n",
    "        temp = pd.DataFrame([mean_absolute_error(df[target_variable], df[col]) for col in prediction_cols] + \\\n",
    "                        [mean_absolute_deviation(df[target_variable], df[col]) for col in prediction_cols] + [len(df)], columns=['Overall']).T\n",
    "    \n",
    "    summary = pd.concat([temp, temp2], axis=0)  \n",
    "    mae_col = ['Mean Absolute Error on YTW (bps)']*len(prediction_cols)\n",
    "    mad_col = ['Median Absolute Error on YTW (bps)']*len(prediction_cols)\n",
    "    prediction_cols = ['']*len(prediction_cols)\n",
    "    columns= list(zip(mae_col, prediction_cols)) + list(zip(mad_col, prediction_cols)) + [('', 'Number of Trades')]\n",
    "    summary.columns=pd.MultiIndex.from_tuples(columns)\n",
    "    \n",
    "    if groupby_cols:\n",
    "        summary.index=pd.MultiIndex.from_tuples(summary.index, names = ['cases']+groupby_cols)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    summary[('', 'Number of Trades')] = summary[('', 'Number of Trades')].astype(int)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a23f9e7f-181d-4140-abf2-ed192d1b5993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.0.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "fs = gcsfs.GCSFileSystem(project='eng-reactor-287421')\n",
    "with fs.open('automated_training/encoders.pkl') as gf:\n",
    "    encoders = pickle.load(gf)\n",
    "    \n",
    "fmax = {key: len(value.classes_) for key, value in encoders.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dad10ab-a722-4147-a5c6-477e4a05026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_date = '11-01'\n",
    "bucket = 'gs://automated_training'\n",
    "folder = 'yield_spread_model'\n",
    "\n",
    "def load_model(bucket, folder, target_date):\n",
    "    if folder: \n",
    "        prefix = os.path.join(bucket,folder)   \n",
    "    else:\n",
    "        prefix = bucket\n",
    "    model_path = os.path.join(prefix, f'model-{target_date}')\n",
    "    print(f'Attempting to load model from {model_path}')\n",
    "    try: \n",
    "        model = keras.models.load_model(model_path)\n",
    "        print('Model loaded')\n",
    "        return model        \n",
    "    except Exception as e: \n",
    "        print('Model failed to load with exception: ', e)\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "144e0890-998d-4791-8857-b1c2b85ec196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load model from gs://automated_training/yield_spread_model/model-11-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 22:40:03.949333: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-01 22:40:03.954855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-01 22:40:03.958678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-01 22:40:03.961546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-01 22:40:14.395670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-01 22:40:14.397912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-01 22:40:14.399802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-01 22:40:14.409815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13594 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "model = load_model(bucket, folder, target_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "id": "001ae578-a8b1-4aa8-8705-ab6e1f4b8db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ficc.data.process_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942a7fa5-be13-4539-8f7a-1e645303ea61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = process_data(QUERY, \n",
    "                    bq_client,\n",
    "                    SEQUENCE_LENGTH = 5,\n",
    "                    NUM_FEATURES = 6,\n",
    "                    \"raw_data.pkl\",\n",
    "                    'FICC_NEW',\n",
    "                    estimate_calc_date=False,\n",
    "                    remove_short_maturity=False,\n",
    "                    remove_non_transaction_based=False,\n",
    "                    remove_trade_type = [],\n",
    "                    trade_history_delay = 1,\n",
    "                    min_trades_in_history = 0,\n",
    "                    process_ratings=False,\n",
    "                    treasury_spread = True,\n",
    "                    add_previous_treasury_rate=True,\n",
    "                    add_previous_treasury_difference=True,\n",
    "                    add_flags=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "id": "d3fb8484-c293-4fc8-8e5d-7a7417cf5bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.1 ms, sys: 5.39 s, total: 5.43 s\n",
      "Wall time: 6.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "temp = df[['cusip','trade_history','quantity','trade_type']].parallel_apply(trade_history_derived_features, axis=1)\n",
    "YS_COLS = get_trade_history_columns()\n",
    "df[YS_COLS] = pd.DataFrame(temp.tolist(), index=df.index)\n",
    "\n",
    "df.target_attention_features = df.target_attention_features.apply(lambda x: x[[0], :])\n",
    "df['new_ys'] = df['yield'] - df['new_ficc_ycl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31230973-15d1-43d4-be21-a32e37d2bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(create_input(df), batch_size=10000)\n",
    "df['prediction'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "id": "651d48b5-28b4-43de-ab73-8711a07b86dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ficc.pricing.price import *\n",
    "from ficc.utils.auxiliary_variables import COUPON_FREQUENCY_TYPE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "id": "a7386bda-425d-426c-b0fe-ec3290e9bf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['interest_payment_frequency'] = df['interest_payment_frequency'].apply(lambda x: COUPON_FREQUENCY_TYPE[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a192465-b07c-4c0c-a6e3-06996f50f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trade_price(trade):\n",
    "    # compute price does not need to return the calc_date, if we are using the calc_date model: \n",
    "    try: \n",
    "        price, _ = compute_price(trade, trade.predicted_yield/100)\n",
    "        return price\n",
    "    except: \n",
    "        None\n",
    "    \n",
    "    # price, _ = compute_price(trade, trade.predicted_yield/100)\n",
    "    # return price\n",
    "\n",
    "%time df['predicted_price'] = df.parallel_apply(lambda x: get_trade_price(x), axis=1)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
