{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory mgmt experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery, storage\n",
    "import os\n",
    "import csv\n",
    "import xmltodict\n",
    "import json\n",
    "import gzip\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# pull schema information as stored in ice_bq_map for functions that conform xmls to bigquery schema\n",
    "# generate list of xmls and pass to json creater, then load to table\n",
    "\n",
    "PROJECT = 'eng-reactor-287421'\n",
    "dataset = 'reference_data'\n",
    "data_table = 'ice_nested'\n",
    "map_table = 'ice_bq_map'\n",
    "\n",
    "bqclient = bigquery.Client(project=PROJECT,)\n",
    "\n",
    "# get mapping schema from bigquery\n",
    "map_query = \"\"\"SELECT * FROM {}.{}\"\"\".format(dataset, map_table)\n",
    "\n",
    "bq_map_result = bqclient.query(map_query).result()\n",
    "bq_map_result = list(bq_map_result)\n",
    "ice_schema, map_version, leaf_nodes, repeated_record_nodes, address_key_list, bad_address_list = bq_map_result[\n",
    "    0]\n",
    "leaf_nodes = [[entry['path'], entry['type'], entry['mode']]\n",
    "              for entry in leaf_nodes]\n",
    "address_key_list = [[entry['path'], entry['key']]\n",
    "                    for entry in address_key_list]\n",
    "repeated_leafs = [entry[0] for entry in leaf_nodes if entry[2] == 'REPEATED']\n",
    "all_repeated_nodes = repeated_leafs + repeated_record_nodes\n",
    "\n",
    "\n",
    "def repeated_enforcement(address_list, xml_dict):\n",
    "# address_list is '/' separated string of nodes address\n",
    "# xml_dict is ice instrument in python dict\n",
    "# function checks all repeated nodes in address, if in the instrument, checks if instrument is wrapped in a list\n",
    "# if not, entry is wrapped in a list\n",
    "\n",
    "    split_address_list = [entry.split('/')[1:] for entry in address_list]\n",
    "\n",
    "    for address in split_address_list:\n",
    "        current_xml_level = xml_dict\n",
    "        for current_index in range(len(address)-1):\n",
    "            if address[current_index] not in current_xml_level:\n",
    "                break\n",
    "            current_xml_level = current_xml_level[address[current_index]]\n",
    "\n",
    "        if address[-1] not in current_xml_level:\n",
    "            continue\n",
    "        else:\n",
    "            if not isinstance(current_xml_level[address[-1]], list):\n",
    "                current_xml_level[address[-1]\n",
    "                                  ] = [current_xml_level[address[-1]]]\n",
    "\n",
    "\n",
    "def record_key_flatten(address_key_list, xml_dict):\n",
    "# address_key_list contains '/' separated string of nodes address and the key value to extract\n",
    "# key value in child node of node in address_key_list is extracted and put as value for node    \n",
    "\n",
    "    split_address_key_list = [\n",
    "        [entry[0].split('/')[1:], entry[1]] for entry in address_key_list]\n",
    "\n",
    "    for entry in split_address_key_list:\n",
    "        address, key = entry\n",
    "        current_xml_level = xml_dict\n",
    "        for current_index in range(len(address)-1):\n",
    "            if address[current_index] not in current_xml_level:\n",
    "                break\n",
    "            current_xml_level = current_xml_level[address[current_index]]\n",
    "\n",
    "        if address[-1] not in current_xml_level:\n",
    "            continue\n",
    "        else:\n",
    "            if isinstance(current_xml_level[address[-1]], dict):\n",
    "                current_xml_level[address[-1]\n",
    "                                  ] = current_xml_level[address[-1]][key]\n",
    "\n",
    "\n",
    "def derepeat_bad_fields(bad_address_list, xml_dict):\n",
    "# this is a one off on a poorly handled field in ICE data\n",
    "# usually not repeated, when repeated, just take the first entry in list\n",
    "    \n",
    "    split_bad_address_list = [entry.split(\n",
    "        '/')[1:] for entry in bad_address_list]\n",
    "\n",
    "    for address in split_bad_address_list:\n",
    "        current_xml_level = xml_dict\n",
    "        for current_index in range(len(address)-1):\n",
    "            if address[current_index] not in current_xml_level:\n",
    "                break\n",
    "            current_xml_level = current_xml_level[address[current_index]]\n",
    "\n",
    "        if address[-1] not in current_xml_level:\n",
    "            continue\n",
    "        else:\n",
    "            if isinstance(current_xml_level[address[-1]], list):\n",
    "                current_xml_level[address[-1]\n",
    "                                  ] = current_xml_level[address[-1]][0]\n",
    "\n",
    "\n",
    "def get_timestamp_from_file_name(file_name):\n",
    "    #regex to pull timestamp from file name\n",
    "    date = re.search(\n",
    "        '([0-9]{4}[0-9]{2}[0-9]{2}T[0-9]{4}-[0-9]{2})', file_name)[0]\n",
    "    date = date.replace('-', '')\n",
    "    return(datetime.strftime(datetime.strptime(date, '%Y%m%dT%H%M%S'), '%Y-%m-%dT%H:%M:%SZ'))\n",
    "\n",
    "\n",
    "def xmls_list_generator(gz_file_name):\n",
    "# with zip file name download to temp file, open and read through about 10mb of zip file size\n",
    "# parse to list of xml in string format and pass each iteration through generator\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket('ref_data_1')\n",
    "    # e.g. 'gsm_update_muni_APFICC_GSMF10I.35.1_1.20201221T0800-05.xml.gz')\n",
    "    blob = bucket.get_blob(gz_file_name)\n",
    "    file = blob.download_to_filename('/tmp/temp_gz_file.xml.gz')\n",
    "\n",
    "    start = True\n",
    "    last_partial_entry = ''\n",
    "    keep_reading = True\n",
    "    with gzip.open('/tmp/temp_gz_file.xml.gz', 'rt') as f:\n",
    "        while keep_reading:\n",
    "            # read_length equal to a zip file size of about 15mb\n",
    "            read_length = int(1e8)\n",
    "            file_str = f.read(read_length)\n",
    "\n",
    "            file_str = last_partial_entry + file_str\n",
    "            if len(file_str)<read_length:\n",
    "                file_str = file_str.split('</payload>')[0]\n",
    "            if start:\n",
    "                file_str = file_str.split('<payload>')[1]\n",
    "                start = False\n",
    "\n",
    "            xml_list = file_str.split('</instrument>')\n",
    "            #save last entry of the list for next loop, will be empty if string cleanly ends on an xml instrument\n",
    "            last_partial_entry = xml_list.pop()\n",
    "            \n",
    "            if not file_str:\n",
    "                keep_reading = False\n",
    "                \n",
    "           \n",
    "            yield xml_list\n",
    "\n",
    "\n",
    "\n",
    "def load_nested_table_uri_json(table_id, uri):\n",
    "# load from created json file into main table\n",
    "\n",
    "    bq_client = bigquery.Client()\n",
    "    ice_nested_table = bqclient.get_table(table_id)\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=ice_nested_table.schema,\n",
    "        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "    )\n",
    "\n",
    "    # Make an API request.\n",
    "    bq_client.load_table_from_uri(uri, table_id, job_config=job_config)\n",
    "    \n",
    "\n",
    "\n",
    "def get_nested_ndjson_as_uri(file_name, xmls_generator, timestamp):\n",
    "# with file name for save, xmls list generator and timestamp for recording time of upload to table\n",
    "\n",
    "    json_file_name = '%snest.json' % (file_name)\n",
    "\n",
    "    # save memory by writing one json string to file at a time rather than store to a variable\n",
    "    with open('/tmp/%s' % (json_file_name), 'wb+') as f:\n",
    "        \n",
    "        # loop through capped size xml lists\n",
    "        for xmls in xmls_generator:\n",
    "            for xml_entry in xmls:\n",
    "                xml_str = xml_entry+'</instrument>'\n",
    "                instrument_dict = xmltodict.parse(\n",
    "                    xml_str, attr_prefix='', cdata_key='text')\n",
    "\n",
    "                # add timestamp to dict, constant for now\n",
    "                instrument_dict['ice_file_date'] = timestamp\n",
    "\n",
    "                # conform fields with bq schema\n",
    "                derepeat_bad_fields(bad_address_list, instrument_dict)\n",
    "                record_key_flatten(address_key_list, instrument_dict)\n",
    "                repeated_enforcement(all_repeated_nodes, instrument_dict)\n",
    "\n",
    "                ndjson = json.dumps(instrument_dict)+'\\n'\n",
    "\n",
    "                f.write(ndjson.encode())\n",
    "        \n",
    "        f.seek(0)\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket('ice_ndjsons')\n",
    "        blob = bucket.blob(json_file_name)\n",
    "        blob.upload_from_filename('/tmp/%s' % (json_file_name))\n",
    "        f.close\n",
    "    uri = 'gs://ice_ndjsons/' + json_file_name\n",
    "    return(uri)\n",
    "\n",
    "def get_to_process_files():\n",
    "    \n",
    "    PROJECT = 'eng-reactor-287421'\n",
    "    dataset = 'reference_data'\n",
    "    process_table = 'ice_files_loading_processing'\n",
    "\n",
    "    bqclient = bigquery.Client(project=PROJECT,)\n",
    "\n",
    "    # get mapping schema from bigquery\n",
    "    get_files_query = \"\"\"SELECT zip_file_name FROM {}.{} GROUP BY zip_file_name HAVING MAX(status) = 0\"\"\".format(dataset, process_table)\n",
    "\n",
    "    get_files_result = bqclient.query(get_files_query).result()\n",
    "    get_files_result = list(get_files_result)\n",
    "    get_files_result = [entry[0] for entry in get_files_result]\n",
    "    \n",
    "    return get_files_result\n",
    "\n",
    "def update_ice_files_loading_processing_table(zip_file_name):\n",
    "    query = \"\"\"\n",
    "        INSERT INTO `eng-reactor-287421.reference_data.ice_files_loading_processing` \n",
    "        VALUES('\"\"\" + zip_file_name + \"\"\"',1, CURRENT_DATE(\"US/Eastern\"))\n",
    "    \"\"\"\n",
    "    query_job = bqclient.query(query)\n",
    "    query_job.result()\n",
    "    \n",
    "\n",
    "def main():\n",
    "\n",
    "    \n",
    "    files_to_process = get_to_process_files()\n",
    "    \n",
    "    for file_name in files_to_process:\n",
    "        xmls_generator = xmls_list_generator(file_name)\n",
    "        uri = get_nested_ndjson_as_uri(\n",
    "            file_name, xmls_generator, get_timestamp_from_file_name(file_name))\n",
    "        load_nested_table_uri_json(\n",
    "            'eng-reactor-287421.reference_data.ice_nested', uri)\n",
    "        update_ice_files_loading_processing_table(file_name)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsm_update_muni_APFICC_GSMF10I.156.1_1.20210215T1400-05.xml.gz\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5252830\n",
    "gsm_update_muni_APFICC_GSMF10I.158.1_1.20210216T0800-05.xml.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query to get names\n",
    "\n",
    "PROJECT = 'eng-reactor-287421'\n",
    "dataset = 'reference_data'\n",
    "data_table = 'ice_nested'\n",
    "map_table = 'ice_bq_map'\n",
    "process_table = 'ice_files_loading_processing'\n",
    "\n",
    "bqclient = bigquery.Client(project=PROJECT,)\n",
    "\n",
    "# get mapping schema from bigquery\n",
    "get_files_query = \"\"\"SELECT zip_file_name FROM {}.{} GROUP BY zip_file_name HAVING MAX(status) = 0\"\"\".format(dataset, process_table)\n",
    "\n",
    "get_files_result = bqclient.query(get_files_query).result()\n",
    "get_files_result = list(get_files_result)\n",
    "get_files_result = [entry[0] for entry in get_files_result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gsm_update_muni_APFICC_GSMF10I.158.1_1.20210216T0800-05.xml.gz',\n",
       " 'gsm_update_muni_APFICC_GSMF10I.156.1_1.20210215T1400-05.xml.gz',\n",
       " 'gsm_update_muni_APFICC_GSMF10I.161.1_1.20210217T0800-05.xml.gz',\n",
       " 'gsm_update_muni_APFICC_GSMF10I.155.1_1.20210215T0800-05.xml.gz',\n",
       " 'gsm_update_muni_APFICC_GSMF10I.159.1_1.20210216T1400-05.xml.gz',\n",
       " 'gsm_update_muni_APFICC_GSMF10I.157.1_1.20210215T2000-05.xml.gz',\n",
       " 'gsm_update_muni_APFICC_GSMF10I.163.1_1.20210217T2000-05.xml.gz',\n",
       " 'gsm_update_muni_APFICC_GSMF10I.160.1_1.20210216T2000-05.xml.gz',\n",
       " 'gsm_update_muni_APFICC_GSMF10I.162.1_1.20210217T1400-05.xml.gz',\n",
       " 'gsm_update_muni_APFICC_GSMF10I.164.1_1.20210218T0800-05.xml.gz']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_files_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.bigquery.job.LoadJob object at 0x7fe6bbf42cd0>\n"
     ]
    }
   ],
   "source": [
    "# elements of main() for testing\n",
    "\n",
    "PROJECT = 'eng-reactor-287421'\n",
    "dataset = 'reference_data'\n",
    "data_table = 'ice_nested'\n",
    "map_table = 'ice_bq_map'\n",
    "\n",
    "bqclient = bigquery.Client(project=PROJECT,)\n",
    "\n",
    "# get mapping schema from bigquery\n",
    "map_query = \"\"\"SELECT * FROM {}.{}\"\"\".format(dataset, map_table)\n",
    "\n",
    "bq_map_result = bqclient.query(map_query).result()\n",
    "bq_map_result = list(bq_map_result)\n",
    "ice_schema, map_version, leaf_nodes, repeated_record_nodes, address_key_list, bad_address_list = bq_map_result[\n",
    "    0]\n",
    "leaf_nodes = [[entry['path'], entry['type'], entry['mode']]\n",
    "              for entry in leaf_nodes]\n",
    "address_key_list = [[entry['path'], entry['key']]\n",
    "                    for entry in address_key_list]\n",
    "repeated_leafs = [entry[0] for entry in leaf_nodes if entry[2] == 'REPEATED']\n",
    "all_repeated_nodes = repeated_leafs + repeated_record_nodes\n",
    "\n",
    "\n",
    "file_name = 'gsm_update_muni_APFICC_GSMF10I.130.1_1.20210202T2000-05.xml.gz'\n",
    "xmls_generator = xmls_list_generator(file_name)\n",
    "uri = get_nested_ndjson_as_uri(\n",
    "    file_name, xmls_generator, get_timestamp_from_file_name(file_name))\n",
    "load_nested_table_uri_json(\n",
    "    'eng-reactor-287421.reference_data.ice_nested', uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interim work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before writing one at a time to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery, storage\n",
    "import os\n",
    "import csv\n",
    "import xmltodict\n",
    "import json\n",
    "import gzip\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "PROJECT = 'eng-reactor-287421'\n",
    "dataset = 'reference_data'\n",
    "data_table = 'ice_nested'\n",
    "map_table = 'ice_bq_map'\n",
    "\n",
    "bqclient = bigquery.Client(project=PROJECT,)\n",
    "\n",
    "# get mapping schema from bigquery\n",
    "map_query = \"\"\"\n",
    "SELECT * FROM {}.{}\n",
    "           \"\"\".format(dataset, map_table)\n",
    "\n",
    "bq_map_result = bqclient.query(map_query).result()\n",
    "bq_map_result = list(bq_map_result)\n",
    "ice_schema, map_version, leaf_nodes, repeated_record_nodes, address_key_list, bad_address_list = bq_map_result[\n",
    "    0]\n",
    "leaf_nodes = [[entry['path'], entry['type'], entry['mode']]\n",
    "              for entry in leaf_nodes]\n",
    "address_key_list = [[entry['path'], entry['key']]\n",
    "                    for entry in address_key_list]\n",
    "repeated_leafs = [entry[0] for entry in leaf_nodes if entry[2] == 'REPEATED']\n",
    "all_repeated_nodes = repeated_leafs + repeated_record_nodes\n",
    "\n",
    "# address_list is '/' seperated string of nodes address\n",
    "# xml_dict is ice instrument in python dict\n",
    "# function checks all repeated nodes in address, if in the instrument, checks if instrument is wrapped in a list\n",
    "# if not, entry is wrapped in a list\n",
    "\n",
    "\n",
    "def repeated_enforcement(address_list, xml_dict):\n",
    "    split_address_list = [entry.split('/')[1:] for entry in address_list]\n",
    "\n",
    "    for address in split_address_list:\n",
    "        current_xml_level = xml_dict\n",
    "        for current_index in range(len(address)-1):\n",
    "            if address[current_index] not in current_xml_level:\n",
    "                break\n",
    "            current_xml_level = current_xml_level[address[current_index]]\n",
    "\n",
    "        if address[-1] not in current_xml_level:\n",
    "            continue\n",
    "        else:\n",
    "            if not isinstance(current_xml_level[address[-1]], list):\n",
    "                current_xml_level[address[-1]\n",
    "                                  ] = [current_xml_level[address[-1]]]\n",
    "\n",
    "\n",
    "# address_key_list contains '/' seperated string of nodes address and the key value to extract\n",
    "# key value in child node of node in address_key_list is extracted and put as value for node\n",
    "\n",
    "def record_key_flatten(address_key_list, xml_dict):\n",
    "    split_address_key_list = [\n",
    "        [entry[0].split('/')[1:], entry[1]] for entry in address_key_list]\n",
    "\n",
    "    for entry in split_address_key_list:\n",
    "        address, key = entry\n",
    "        current_xml_level = xml_dict\n",
    "        for current_index in range(len(address)-1):\n",
    "            if address[current_index] not in current_xml_level:\n",
    "                break\n",
    "            current_xml_level = current_xml_level[address[current_index]]\n",
    "\n",
    "        if address[-1] not in current_xml_level:\n",
    "            continue\n",
    "        else:\n",
    "            if isinstance(current_xml_level[address[-1]], dict):\n",
    "                current_xml_level[address[-1]\n",
    "                                  ] = current_xml_level[address[-1]][key]\n",
    "\n",
    "# this is a one off on a poorly handled field in ICE data\n",
    "# usually not repeated, when repeated, just take the first entry in list\n",
    "\n",
    "\n",
    "def derepeat_bad_fields(bad_address_list, xml_dict):\n",
    "    split_bad_address_list = [entry.split(\n",
    "        '/')[1:] for entry in bad_address_list]\n",
    "\n",
    "    for address in split_bad_address_list:\n",
    "        current_xml_level = xml_dict\n",
    "        for current_index in range(len(address)-1):\n",
    "            if address[current_index] not in current_xml_level:\n",
    "                break\n",
    "            current_xml_level = current_xml_level[address[current_index]]\n",
    "\n",
    "        if address[-1] not in current_xml_level:\n",
    "            continue\n",
    "        else:\n",
    "            if isinstance(current_xml_level[address[-1]], list):\n",
    "                current_xml_level[address[-1]\n",
    "                                  ] = current_xml_level[address[-1]][0]\n",
    "\n",
    "\n",
    "def get_timestamp_from_file_name(file_name):\n",
    "    date = re.search(\n",
    "        '([0-9]{4}[0-9]{2}[0-9]{2}T[0-9]{4}-[0-9]{2})', file_name)[0]\n",
    "    date = date.replace('-', '')\n",
    "    return(datetime.strftime(datetime.strptime(date, '%Y%m%dT%H%M%S'), '%Y-%m-%dT%H:%M:%SZ'))\n",
    "\n",
    "\n",
    "def xmls_list_generator(gz_file_name):\n",
    "# with zip file name download to temp file, open and read through about 10mb of zip file size\n",
    "# parse to list of xml in string format and pass each iteration through generator\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket('ref_data_1')\n",
    "    # e.g. 'gsm_update_muni_APFICC_GSMF10I.35.1_1.20201221T0800-05.xml.gz')\n",
    "    blob = bucket.get_blob(gz_file_name)\n",
    "    file = blob.download_to_filename('/tmp/temp_gz_file.xml.gz')\n",
    "\n",
    "    start = True\n",
    "    last_partial_entry = ''\n",
    "    keep_reading = True\n",
    "    with gzip.open('/tmp/temp_gz_file.xml.gz', 'rt') as f:\n",
    "        while keep_reading:\n",
    "            # read_length equal to a zip file size of about 15mb\n",
    "            read_length = int(1e8)\n",
    "            file_str = f.read(read_length)\n",
    "\n",
    "            file_str = last_partial_entry + file_str\n",
    "            if len(file_str)<read_length:\n",
    "                file_str = file_str.split('</payload>')[0]\n",
    "            if start:\n",
    "                file_str = file_str.split('<payload>')[1]\n",
    "                start = False\n",
    "\n",
    "            xml_list = file_str.split('</instrument>')\n",
    "            #save last entry of the list for next loop, will be empty if string cleanly ends on an xml instrument\n",
    "            last_partial_entry = xml_list.pop()\n",
    "            \n",
    "            if not file_str:\n",
    "                keep_reading = False\n",
    "                \n",
    "           \n",
    "            yield xml_list\n",
    "\n",
    "\n",
    "\n",
    "def load_nested_table_uri_json(table_id, uri):\n",
    "\n",
    "    bq_client = bigquery.Client()\n",
    "    ice_nested_table = bqclient.get_table(table_id)\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=ice_nested_table.schema,\n",
    "        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "    )\n",
    "\n",
    "    # Make an API request.\n",
    "    job = bq_client.load_table_from_uri(uri, table_id, job_config=job_config)\n",
    "    print(job.result())\n",
    "\n",
    "\n",
    "def get_nested_ndjson_as_uri(file_name, xmls_generator, timestamp):\n",
    "    ndjson = ''\n",
    "\n",
    "    \n",
    "    for xmls in xmls_generator:\n",
    "        #total = len(xmls)\n",
    "\n",
    "        #for n in range(total):\n",
    "        while len(xmls)>0:\n",
    "            xml_entry = xmls.pop()\n",
    "            xml_str = xml_entry+'</instrument>'\n",
    "            instrument_dict = xmltodict.parse(\n",
    "                xml_str, attr_prefix='', cdata_key='text')\n",
    "\n",
    "            # add timestamp to dict, constant for now\n",
    "            instrument_dict['ice_file_date'] = timestamp\n",
    "\n",
    "            # conform fields with bq schema\n",
    "            derepeat_bad_fields(bad_address_list, instrument_dict)\n",
    "            record_key_flatten(address_key_list, instrument_dict)\n",
    "            repeated_enforcement(all_repeated_nodes, instrument_dict)\n",
    "\n",
    "            ndjson += json.dumps(instrument_dict)+'\\n'\n",
    "\n",
    "        \n",
    "        \n",
    "    json_file_name = '%snest.json' % (file_name)\n",
    "\n",
    "    with open('/tmp/%s' % (json_file_name), 'wb+') as f:\n",
    "        f.write(ndjson.encode())\n",
    "        f.seek(0)\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket('ice_ndjsons')\n",
    "        blob = bucket.blob(json_file_name)\n",
    "        blob.upload_from_filename('/tmp/%s' % (json_file_name))\n",
    "        f.close\n",
    "    uri = 'gs://ice_ndjsons/' + json_file_name\n",
    "    return(uri)\n",
    "\n",
    "def main(event, context):\n",
    "    PROJECT = 'eng-reactor-287421'\n",
    "    dataset = 'reference_data'\n",
    "    data_table = 'ice_nested'\n",
    "    map_table = 'ice_bq_map'\n",
    "\n",
    "    bqclient = bigquery.Client(project=PROJECT,)\n",
    "\n",
    "    # get mapping schema from bigquery\n",
    "    map_query = \"\"\"SELECT * FROM {}.{}\"\"\".format(dataset, map_table)\n",
    "\n",
    "    bq_map_result = bqclient.query(map_query).result()\n",
    "    bq_map_result = list(bq_map_result)\n",
    "    ice_schema, map_version, leaf_nodes, repeated_record_nodes, address_key_list, bad_address_list = bq_map_result[\n",
    "        0]\n",
    "    leaf_nodes = [[entry['path'], entry['type'], entry['mode']]\n",
    "                  for entry in leaf_nodes]\n",
    "    address_key_list = [[entry['path'], entry['key']]\n",
    "                        for entry in address_key_list]\n",
    "    repeated_leafs = [entry[0] for entry in leaf_nodes if entry[2] == 'REPEATED']\n",
    "    all_repeated_nodes = repeated_leafs + repeated_record_nodes\n",
    "    \n",
    "    \n",
    "    file_name = event['name']\n",
    "    xmls = get_xmls_list(file_name)\n",
    "    uri = get_nested_ndjson_as_uri(\n",
    "        file_name, xmls, get_timestamp_from_file_name(file_name))\n",
    "    load_nested_table_uri_json(\n",
    "        'eng-reactor-287421.reference_data.ice_nested', uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery, storage\n",
    "import os\n",
    "import csv\n",
    "import xmltodict\n",
    "import json\n",
    "import gzip\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "PROJECT = 'eng-reactor-287421'\n",
    "dataset = 'reference_data'\n",
    "data_table = 'ice_nested'\n",
    "map_table = 'ice_bq_map'\n",
    "\n",
    "bqclient = bigquery.Client(project=PROJECT,)\n",
    "\n",
    "# get mapping schema from bigquery\n",
    "map_query = \"\"\"\n",
    "SELECT * FROM {}.{}\n",
    "           \"\"\".format(dataset, map_table)\n",
    "\n",
    "bq_map_result = bqclient.query(map_query).result()\n",
    "bq_map_result = list(bq_map_result)\n",
    "ice_schema, map_version, leaf_nodes, repeated_record_nodes, address_key_list, bad_address_list = bq_map_result[\n",
    "    0]\n",
    "leaf_nodes = [[entry['path'], entry['type'], entry['mode']]\n",
    "              for entry in leaf_nodes]\n",
    "address_key_list = [[entry['path'], entry['key']]\n",
    "                    for entry in address_key_list]\n",
    "repeated_leafs = [entry[0] for entry in leaf_nodes if entry[2] == 'REPEATED']\n",
    "all_repeated_nodes = repeated_leafs + repeated_record_nodes\n",
    "\n",
    "# address_list is '/' seperated string of nodes address\n",
    "# xml_dict is ice instrument in python dict\n",
    "# function checks all repeated nodes in address, if in the instrument, checks if instrument is wrapped in a list\n",
    "# if not, entry is wrapped in a list\n",
    "\n",
    "\n",
    "def repeated_enforcement(address_list, xml_dict):\n",
    "    split_address_list = [entry.split('/')[1:] for entry in address_list]\n",
    "\n",
    "    for address in split_address_list:\n",
    "        current_xml_level = xml_dict\n",
    "        for current_index in range(len(address)-1):\n",
    "            if address[current_index] not in current_xml_level:\n",
    "                break\n",
    "            current_xml_level = current_xml_level[address[current_index]]\n",
    "\n",
    "        if address[-1] not in current_xml_level:\n",
    "            continue\n",
    "        else:\n",
    "            if not isinstance(current_xml_level[address[-1]], list):\n",
    "                current_xml_level[address[-1]\n",
    "                                  ] = [current_xml_level[address[-1]]]\n",
    "\n",
    "\n",
    "# address_key_list contains '/' seperated string of nodes address and the key value to extract\n",
    "# key value in child node of node in address_key_list is extracted and put as value for node\n",
    "\n",
    "def record_key_flatten(address_key_list, xml_dict):\n",
    "    split_address_key_list = [\n",
    "        [entry[0].split('/')[1:], entry[1]] for entry in address_key_list]\n",
    "\n",
    "    for entry in split_address_key_list:\n",
    "        address, key = entry\n",
    "        current_xml_level = xml_dict\n",
    "        for current_index in range(len(address)-1):\n",
    "            if address[current_index] not in current_xml_level:\n",
    "                break\n",
    "            current_xml_level = current_xml_level[address[current_index]]\n",
    "\n",
    "        if address[-1] not in current_xml_level:\n",
    "            continue\n",
    "        else:\n",
    "            if isinstance(current_xml_level[address[-1]], dict):\n",
    "                current_xml_level[address[-1]\n",
    "                                  ] = current_xml_level[address[-1]][key]\n",
    "\n",
    "# this is a one off on a poorly handled field in ICE data\n",
    "# usually not repeated, when repeated, just take the first entry in list\n",
    "\n",
    "\n",
    "def derepeat_bad_fields(bad_address_list, xml_dict):\n",
    "    split_bad_address_list = [entry.split(\n",
    "        '/')[1:] for entry in bad_address_list]\n",
    "\n",
    "    for address in split_bad_address_list:\n",
    "        current_xml_level = xml_dict\n",
    "        for current_index in range(len(address)-1):\n",
    "            if address[current_index] not in current_xml_level:\n",
    "                break\n",
    "            current_xml_level = current_xml_level[address[current_index]]\n",
    "\n",
    "        if address[-1] not in current_xml_level:\n",
    "            continue\n",
    "        else:\n",
    "            if isinstance(current_xml_level[address[-1]], list):\n",
    "                current_xml_level[address[-1]\n",
    "                                  ] = current_xml_level[address[-1]][0]\n",
    "\n",
    "\n",
    "def get_timestamp_from_file_name(file_name):\n",
    "    date = re.search(\n",
    "        '([0-9]{4}[0-9]{2}[0-9]{2}T[0-9]{4}-[0-9]{2})', file_name)[0]\n",
    "    date = date.replace('-', '')\n",
    "    return(datetime.strftime(datetime.strptime(date, '%Y%m%dT%H%M%S'), '%Y-%m-%dT%H:%M:%SZ'))\n",
    "\n",
    "\n",
    "def get_xmls_list(gz_file_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket('ref_data_1')\n",
    "    # e.g. 'gsm_update_muni_APFICC_GSMF10I.35.1_1.20201221T0800-05.xml.gz')\n",
    "    blob = bucket.get_blob(gz_file_name)\n",
    "    file = blob.download_to_filename('/tmp/temp_gz_file.xml.gz')\n",
    "\n",
    "    f = gzip.open('/tmp/temp_gz_file.xml.gz', 'rt')\n",
    "    file_str = f.read()\n",
    "    clean_file = file_str.split('<payload>')[1]\n",
    "    return (clean_file.split('</instrument>'))\n",
    "\n",
    "\n",
    "def load_nested_table_uri_json(table_id, uri):\n",
    "\n",
    "    bq_client = bigquery.Client()\n",
    "    ice_nested_table = bqclient.get_table(table_id)\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=ice_nested_table.schema,\n",
    "        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "    )\n",
    "\n",
    "    # Make an API request.\n",
    "    job = bq_client.load_table_from_uri(uri, table_id, job_config=job_config)\n",
    "    print(job.result())\n",
    "\n",
    "\n",
    "def get_nested_ndjson_as_uri(file_name, xmls, timestamp):\n",
    "    ndjson = ''\n",
    "    total = len(xmls)-1\n",
    "    xmls = gen_list\n",
    "    total = len(gen_list)\n",
    "\n",
    "    for n in range(total):\n",
    "        xml_str = xmls[n]+'</instrument>'\n",
    "        instrument_dict = xmltodict.parse(\n",
    "            xml_str, attr_prefix='', cdata_key='text')\n",
    "\n",
    "        # add timestamp to dict, constant for now\n",
    "        instrument_dict['ice_file_date'] = timestamp\n",
    "\n",
    "        # conform fields with bq schema\n",
    "        derepeat_bad_fields(bad_address_list, instrument_dict)\n",
    "        record_key_flatten(address_key_list, instrument_dict)\n",
    "        repeated_enforcement(all_repeated_nodes, instrument_dict)\n",
    "\n",
    "        ndjson += json.dumps(instrument_dict)+'\\n'\n",
    "\n",
    "    json_file_name = '%snest.json' % (file_name)\n",
    "    with open('/tmp/%s' % (json_file_name), 'wb+') as f:\n",
    "        f.write(ndjson.encode())\n",
    "        f.seek(0)\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket('ice_ndjsons')\n",
    "        blob = bucket.blob(json_file_name)\n",
    "        blob.upload_from_filename('/tmp/%s' % (json_file_name))\n",
    "        f.close\n",
    "    uri = 'gs://ice_ndjsons/' + json_file_name\n",
    "    return(uri)\n",
    "\n",
    "def main(event, context):\n",
    "    PROJECT = 'eng-reactor-287421'\n",
    "    dataset = 'reference_data'\n",
    "    data_table = 'ice_nested'\n",
    "    map_table = 'ice_bq_map'\n",
    "\n",
    "    bqclient = bigquery.Client(project=PROJECT,)\n",
    "\n",
    "    # get mapping schema from bigquery\n",
    "    map_query = \"\"\"SELECT * FROM {}.{}\"\"\".format(dataset, map_table)\n",
    "\n",
    "    bq_map_result = bqclient.query(map_query).result()\n",
    "    bq_map_result = list(bq_map_result)\n",
    "    ice_schema, map_version, leaf_nodes, repeated_record_nodes, address_key_list, bad_address_list = bq_map_result[\n",
    "        0]\n",
    "    leaf_nodes = [[entry['path'], entry['type'], entry['mode']]\n",
    "                  for entry in leaf_nodes]\n",
    "    address_key_list = [[entry['path'], entry['key']]\n",
    "                        for entry in address_key_list]\n",
    "    repeated_leafs = [entry[0] for entry in leaf_nodes if entry[2] == 'REPEATED']\n",
    "    all_repeated_nodes = repeated_leafs + repeated_record_nodes\n",
    "    \n",
    "    \n",
    "    file_name = event['name']\n",
    "    xmls = get_xmls_list(file_name)\n",
    "    uri = get_nested_ndjson_as_uri(\n",
    "        file_name, xmls, get_timestamp_from_file_name(file_name))\n",
    "    load_nested_table_uri_json(\n",
    "        'eng-reactor-287421.reference_data.ice_nested', uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.bigquery.job.LoadJob object at 0x7fe6ba902850>\n"
     ]
    }
   ],
   "source": [
    "PROJECT = 'eng-reactor-287421'\n",
    "dataset = 'reference_data'\n",
    "data_table = 'ice_nested'\n",
    "map_table = 'ice_bq_map'\n",
    "\n",
    "bqclient = bigquery.Client(project=PROJECT,)\n",
    "\n",
    "# get mapping schema from bigquery\n",
    "map_query = \"\"\"SELECT * FROM {}.{}\"\"\".format(dataset, map_table)\n",
    "\n",
    "bq_map_result = bqclient.query(map_query).result()\n",
    "bq_map_result = list(bq_map_result)\n",
    "ice_schema, map_version, leaf_nodes, repeated_record_nodes, address_key_list, bad_address_list = bq_map_result[\n",
    "    0]\n",
    "leaf_nodes = [[entry['path'], entry['type'], entry['mode']]\n",
    "              for entry in leaf_nodes]\n",
    "address_key_list = [[entry['path'], entry['key']]\n",
    "                    for entry in address_key_list]\n",
    "repeated_leafs = [entry[0] for entry in leaf_nodes if entry[2] == 'REPEATED']\n",
    "all_repeated_nodes = repeated_leafs + repeated_record_nodes\n",
    "\n",
    "\n",
    "file_name = 'gsm_update_muni_APFICC_GSMF10I.130.1_1.20210202T2000-05.xml.gz'\n",
    "xmls = get_xmls_list(file_name)\n",
    "uri = get_nested_ndjson_as_uri(\n",
    "    file_name, xmls, get_timestamp_from_file_name(file_name))\n",
    "load_nested_table_uri_json(\n",
    "    'eng-reactor-287421.reference_data.ice_nested', uri)\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.mnightly-2021-02-12-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:mnightly-2021-02-12-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
