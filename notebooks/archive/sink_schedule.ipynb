{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-8083338b643a>:18: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  tqdm_notebook().pandas()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7bdda099114c88b78f1b6b71848e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/FICC.AI/opt/anaconda3/lib/python3.8/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import bigquery\n",
    "# from google.cloud import bigquery_storage\n",
    "\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "# import table_schema as table_schema\n",
    "import pytz\n",
    "# import qfrm\n",
    "from datetime import datetime,timedelta\n",
    "# from send_email import send_error_email\n",
    "# from google.api_core.exceptions import BadRequest\n",
    "# from google.cloud import secretmanager\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import os\n",
    "tqdm_notebook().pandas()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy.optimize as optimize\n",
    "sns.set()\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import math\n",
    "from pandas import NaT\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "from io import StringIO, BytesIO\n",
    "from google.cloud import bigquery\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../gsm_init_muni_APFICC_GSMF10I.1.1_1.20201203T1300-05.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_call_schedule(author):\n",
    "    for doc in author.iter('instrument'):\n",
    "        # print(doc.attrib[\"id\"])\n",
    "        doc_dict = {}\n",
    "        call_details_info = doc.find(\".//debt/call_details\")\n",
    "        if call_details_info is not None:\n",
    "            for element in call_details_info:\n",
    "                if element.tag == \"call_schedule\":\n",
    "#                 new_dict.update({element.tag:element.text})\n",
    "                    new_dict = {}\n",
    "                    new_dict.update({\"instrument_id\":doc.attrib[\"id\"]})\n",
    "                    new_dict.update({element.find(\".//call_price\").tag:element.find(\".//call_price\").text})\n",
    "                    new_dict.update({element.find(\".//call_date\").tag:element.find(\".//call_date\").text})\n",
    "                    if element.find(\".//cav_call_price\") is not None:\n",
    "                        new_dict.update({element.find(\".//cav_call_price\").tag:element.find(\".//cav_call_price\").text})\n",
    "                    yield new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sink_schedule(author):\n",
    "    for doc in author.iter('instrument'):\n",
    "        sink_schedule = doc.iterfind(\".//debt/sink_schedule\")\n",
    "        if sink_schedule is not None:\n",
    "            for element in sink_schedule:\n",
    "                new_dict = {}\n",
    "                new_dict.update({\"instrument_id\":doc.attrib[\"id\"]})\n",
    "                if element.find(\".//mandatory_sink_amount\") is not None:\n",
    "                    new_dict.update({element.find(\".//mandatory_sink_amount\").tag:element.find(\".//mandatory_sink_amount\").text})\n",
    "                if element.find(\".//mandatory_sink_date\") is not None:\n",
    "                    new_dict.update({element.find(\".//mandatory_sink_date\").tag:element.find(\".//mandatory_sink_date\").text})\n",
    "                if element.find(\".//mandatory_sink_price\") is not None:\n",
    "                    new_dict.update({element.find(\".//mandatory_sink_price\").tag:element.find(\".//mandatory_sink_price\").text})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"../creds.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'eng-reactor-287421'\n",
    "dataset = 'FULL_ICE'\n",
    "table = 'sink_schedule'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bqclient = bigquery.Client(project=PROJECT,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_with_schema(bq,project_id,dataset,table_id,schema = []):\n",
    "    PROJECT = project_id\n",
    "    bq = bq\n",
    "    table_id = '{}.{}.{}'.format(PROJECT,dataset,table_id)\n",
    "    table = bq.create_table(table_id, exists_ok=True)\n",
    "    print('{} created on {}'.format(table.table_id, table.created))\n",
    "    table = bq.get_table(table_id)\n",
    "    table.schema = schema\n",
    "    table = bq.update_table(table, [\"schema\"])\n",
    "\n",
    "def load_data(bq,data,project,dataset,table):\n",
    "    bq = bq\n",
    "    table_id = '{}.{}.{}'.format(project,dataset,table)\n",
    "    job_config = bigquery.LoadJobConfig(schema =[])\n",
    "    job = bq.load_table_from_dataframe(data, table_id,job_config=job_config)\n",
    "\n",
    "    try:\n",
    "        job.result() # Waits for the job to complete.\n",
    "        return 'success'  \n",
    "    except BadRequest as ex:\n",
    "        print(ex) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sink_schedule created on 2020-12-14 17:30:54.125000+00:00\n"
     ]
    }
   ],
   "source": [
    "create_table_with_schema(bqclient,PROJECT,dataset,table,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loaded_threading(bqclient,doc_df,PROJECT,dataset,table,count):\n",
    "    try:\n",
    "        test = load_data(bqclient,doc_df,PROJECT,dataset,table)\n",
    "        print(test)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        for columns in doc_df.columns:\n",
    "            print(columns)\n",
    "        print(doc_df.head)\n",
    "        print(count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sink_schedule(new_elem,final_list):\n",
    "    \n",
    "    list_of_elem = new_elem.xpath(\"//instrument/debt\")\n",
    "    instrument_id = new_elem.xpath(\"//instrument\")[0].attrib[\"id\"]\n",
    "    if len(list_of_elem)>0:\n",
    "        for element in list_of_elem[0].getchildren():\n",
    "            if element.tag == \"sink_schedule\":\n",
    "                new_dict = {}\n",
    "                new_dict.update({\"instrument_id\":instrument_id})\n",
    "#                 print(\"inside call schedule\")\n",
    "#                 print(len(element.getchildren()))\n",
    "#                 new_dict.update({element.tag:element.text})\n",
    "                for childs in element.getchildren():\n",
    "# \n",
    "                    new_dict.update({childs.tag:childs.text})\n",
    "                final_list.append(new_dict)\n",
    "                \n",
    "            \n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_sink_schedule(new_elem,final_list):\n",
    "#     new_dict = {}\n",
    "    \n",
    "    sink_schedule(new_elem,final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_dataframe(final_list,count,data_timestamp):\n",
    "    doc_df = pd.DataFrame(final_list)\n",
    "    #             print(doc_df)\n",
    "    new_columns = []\n",
    "    for columns in doc_df.columns:\n",
    "        columns = columns.replace(\"&\",\"\")\n",
    "        columns = columns.replace(\" \",\"_\")\n",
    "        columns = columns.replace(\"-\",\"_\")\n",
    "        new_columns.append(columns)\n",
    "    doc_df.columns = new_columns\n",
    "    doc_df[\"upload_date\"] = my_timezone.localize(datetime.now()).date() \n",
    "    doc_df[\"data_timestamp\"] = pd.to_datetime(data_timestamp[0])\n",
    "    doc_df[\"data_timestamp\"] = doc_df[\"data_timestamp\"].dt.date\n",
    "    doc_df.replace(np.nan,None,inplace = True)\n",
    "    doc_df.replace([np.nan],[None],inplace = True)\n",
    "    doc_df.replace(\"nan\",None,inplace = True)\n",
    "    print(len(doc_df))\n",
    "\n",
    "    #             doc_df.to_pickle(storage_path+\"_\"+str(count)+\".pkl\")\n",
    "    data_loaded_threading(bqclient,doc_df,PROJECT,dataset,table,count)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "from datetime import datetime\n",
    "\n",
    "my_timezone = pytz.timezone('America/New_York')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "12359\n",
      "success\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "11773\n",
      "success\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "12226\n",
      "success\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "12387\n",
      "success\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "11696\n",
      "success\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "11854\n",
      "success\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "12664\n",
      "success\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "12048\n",
      "success\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "11943\n",
      "success\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "11826\n",
      "success\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "12091\n",
      "success\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "11984\n",
      "success\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "12290\n",
      "success\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "11063\n",
      "success\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "12288\n",
      "success\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "12073\n",
      "success\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "12188\n",
      "success\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "12155\n",
      "success\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "11623\n",
      "success\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "12092\n",
      "success\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "12050\n",
      "success\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "12801\n",
      "success\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "13067\n",
      "success\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "11966\n",
      "success\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "12378\n",
      "success\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "12272\n",
      "success\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "12353\n",
      "success\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "11770\n",
      "success\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "12377\n",
      "success\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "12291\n",
      "success\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "11981\n",
      "success\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "12089\n",
      "success\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "12312\n",
      "success\n",
      "1000000\n",
      "1010000\n",
      "1020000\n",
      "11700\n",
      "success\n",
      "1030000\n",
      "1040000\n",
      "1050000\n",
      "12074\n",
      "success\n",
      "1060000\n",
      "1070000\n",
      "1080000\n",
      "12033\n",
      "success\n",
      "2618\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "# count = 0\n",
    "total = 1086089\n",
    "columns_list = []\n",
    "data_timestamp = []\n",
    "head_context = etree.iterparse(path,events=(\"start\",),tag = \"header\")\n",
    "for event,elem in head_context:\n",
    "    new_elem = etree.parse(BytesIO(etree.tostring(elem)))\n",
    "    list_of_elem = elem.xpath(\"//header\")\n",
    "    if len(list_of_elem)>0:\n",
    "        for element in list_of_elem[0].getchildren():\n",
    "#             print(element.tag)\n",
    "            if element.tag == \"timestamp\":\n",
    "                print(\"hello\")\n",
    "                data_timestamp.append(element.text)\n",
    "    break\n",
    "\n",
    "\n",
    "def new_fast_iter(context,data_timestamp, *args, **kwargs):\n",
    "  \n",
    "    processes = []\n",
    "    final_list = []\n",
    "    count = 0\n",
    "    jobs = []\n",
    "    for event,elem in context:\n",
    "        new_elem = etree.parse(BytesIO(etree.tostring(elem)))\n",
    "        create_df_sink_schedule(new_elem,final_list)\n",
    "        \n",
    "        count += 1\n",
    "    #         elem.clear()\n",
    "    #         elem.clear()\n",
    "        while elem.getprevious() is not None:\n",
    "            del elem.getparent()[0]\n",
    "#         if count == 10:\n",
    "#             break\n",
    "#         print(\"******\")\n",
    "\n",
    "        if count%10000 == 0:\n",
    "    #             print(elem.attrib)\n",
    "            print(count)\n",
    "        if count %30000== 0:\n",
    "            upload_dataframe(final_list,count,data_timestamp)\n",
    "#             thread = threading.Thread(target = threaded_stuff(final_list))\n",
    "\n",
    "            final_list = []\n",
    "\n",
    "\n",
    "        if count == total:\n",
    "            upload_dataframe(final_list,count,data_timestamp)\n",
    "#             thread = threading.Thread(target = threaded_stuff(final_list))\n",
    "\n",
    "            final_list = []\n",
    "            \n",
    "\n",
    "# def process_element(elem,final_list):\n",
    "# #     new_elem = elem\n",
    "    \n",
    "#     new_elem = etree.parse(BytesIO(etree.tostring(elem)))\n",
    "# #     print(etree.tostring(elem, pretty_print=True))\n",
    "# #     print(etree.dump(new_elem))\n",
    "#     create_df_call_schedule(new_elem,final_list)\n",
    "\n",
    "\n",
    "\n",
    "context = etree.iterparse(path,events=(\"start\",),tag = \"instrument\")\n",
    "new_fast_iter(context,data_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
