{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move MSRB Intraday Trade Files into Folders Grouped By Date\n",
    "Last updated by Developer on 2024-02-20.\n",
    "\n",
    "This notebook was used to convert every json file in the Google Cloud bucket `msrb_intraday_real_time_trade_files` into a folder that contained the date that it was created. This was done because the bucket had over 900k+ json files at the time of performing this cleanup, which made it so we could not locate or search for files in the bucket as Google filtering does not work when there are a large (number unknown) number of files in the bucket. \n",
    "\n",
    "Perhaps this notebook can be used in the future for other Google Cloud bucket cleanup tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocess as mp    # using `multiprocess` instead of `multiprocessing` because function to be called in `map` is in the same file as the function which is calling it: https://stackoverflow.com/questions/41385708/multiprocessing-example-giving-attributeerror\n",
    "\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/user/ficc/mitas_creds.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_file(bucket_name, old_file_name, new_file_name):\n",
    "    client = storage.Client()    # initialize a client\n",
    "    bucket = client.get_bucket(bucket_name)    # get bucket reference\n",
    "    blob = bucket.blob(old_file_name)    # get blob (file) reference\n",
    "    bucket.copy_blob(blob, bucket, new_file_name)    # copy the blob to the new destination with the new name\n",
    "    blob.delete()    # delete the original blob\n",
    "    # print(f\"File '{old_file_name}' renamed to '{new_file_name}' successfully.\")    # comment out otherwise too many print statements eventually causes the notebook to crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client()    # initialize a client\n",
    "bucket = client.get_bucket('msrb_intraday_real_time_trade_files')    # get bucket reference\n",
    "\n",
    "blobs = bucket.list_blobs()    # list all blobs (files) in the bucket\n",
    "blobs = [blob for blob in blobs if '/' not in blob.name]\n",
    "\n",
    "count = 10\n",
    "for blob in blobs:    # iterate through the first `count` blobs and print their names\n",
    "    print(blob.name)\n",
    "    count -= 1\n",
    "    if count == 0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total blobs:', len(blobs))    # use for monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_from_filename(filename, suffix):\n",
    "    prefix = 'real_time_msrb_file_'\n",
    "    num_characters_in_prefix = len(prefix)\n",
    "    num_characters_in_suffix = len(suffix)\n",
    "    assert filename[:num_characters_in_prefix] == prefix, f'filename: {filename} does not have the correct prefix'\n",
    "    assert filename[-num_characters_in_suffix:] == suffix, f'filename: {filename} does not have the correct suffix'\n",
    "    assert len(filename) == num_characters_in_prefix + num_characters_in_suffix + 10 + 9, f'filename: {filename} does not have the right format'\n",
    "    date_as_string = filename[num_characters_in_prefix : num_characters_in_prefix + 10]\n",
    "    return date_as_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_to_blobs_dict(suffix: str = '.json') -> dict:\n",
    "    date_to_filenames_dict = {}\n",
    "    for blob in blobs:\n",
    "        try:\n",
    "            date_as_string = date_from_filename(blob.name, suffix)\n",
    "        except AssertionError as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        if date_as_string not in date_to_filenames_dict: date_to_filenames_dict[date_as_string] = []\n",
    "        date_to_filenames_dict[date_as_string].append(blob)\n",
    "    return date_to_filenames_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blobs_to_filenames(date_to_filenames_dict: dict) -> dict:\n",
    "    '''Mutates `date_to_filenames_dict`. Need to call this function before using `multiprocessing` \n",
    "    since blobs are not picklable, a list of blobs cannot be used in multiprocessing, so need to \n",
    "    convert them to a list of strings (in this case, filenames).'''\n",
    "    total_blobs = 0    # use for monitoring\n",
    "    for date, blob_list in date_to_filenames_dict.items():\n",
    "        total_blobs += len(blob_list)\n",
    "        date_to_filenames_dict[date] = [blob.name for blob in blob_list]\n",
    "    print('Total blobs:', total_blobs)\n",
    "    return date_to_filenames_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle all files with format: `real_time_msrb_file_YYYY-MM-DD_HH:MM:SS.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_to_filenames = blobs_to_filenames(create_date_to_blobs_dict())\n",
    "date_to_filenames    # preview the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_file_with_date_as_folder(date, filename_list: list):\n",
    "    print('date:', date)    # use for monitoring\n",
    "    for filename in filename_list:\n",
    "        rename_file('msrb_intraday_real_time_trade_files', filename, f'{date}/{filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without multiprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files(date_to_filenames: dict):\n",
    "    for date, filename_list in date_to_filenames.items():\n",
    "        rename_file_with_date_as_folder(date, filename_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_files(date_to_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use multiprocessing for the outer `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files_with_multiprocessing(date_to_filenames_dict: dict):\n",
    "    with mp.Pool() as pool_object:    # using template from https://docs.python.org/3/library/multiprocessing.html\n",
    "        pool_object.starmap(rename_file_with_date_as_folder, list(date_to_filenames_dict.items()))    # need to use starmap since `upload_trade_history_to_trade_history_redis` has multiple arguments: https://stackoverflow.com/questions/5442910/how-to-use-multiprocessing-pool-map-with-multiple-arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_files_with_multiprocessing(date_to_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle all files with format: `real_time_msrb_file_YYYY-MM-DD_HH:MM:SS_from_fast_redis_update.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_to_filenames_from_fast_redis_update = blobs_to_filenames(create_date_to_blobs_dict(suffix='_from_fast_redis_update.json'))\n",
    "date_to_filenames_from_fast_redis_update    # preview the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without multiprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_files(date_to_filenames_from_fast_redis_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use multiprocessing for the outer `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_files_with_multiprocessing(date_to_filenames_from_fast_redis_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling incorrectly renamed blobs (blobs that were renamed incorrectly during first pass of this code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client()    # initialize a client\n",
    "bucket = client.get_bucket('msrb_intraday_real_time_trade_files')    # get bucket reference\n",
    "\n",
    "damaged_blobs = bucket.list_blobs()    # list all blobs (files) in the bucket\n",
    "damaged_blobs = [damaged_blob for damaged_blob in damaged_blobs if '2021-05-04/msrb_intraday_real_time_trade_files/2021-05-04/' in damaged_blob.name]\n",
    "\n",
    "count = 10\n",
    "for damaged_blob in damaged_blobs:    # iterate through the first `count` blobs and print their names\n",
    "    print(damaged_blob.name)\n",
    "    count -= 1\n",
    "    if count == 0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(damaged_blobs)    # use for monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2021-05-04'\n",
    "for blob in damaged_blobs:\n",
    "    blob_name = blob.name\n",
    "    last_slash = blob_name.rfind('/')\n",
    "    blob_name_without_directories = blob_name[last_slash + 1:]\n",
    "    rename_file('msrb_intraday_real_time_trade_files', blob_name, f'{date}/{blob_name_without_directories}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
