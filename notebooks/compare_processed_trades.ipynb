{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "971c1abd",
   "metadata": {},
   "source": [
    "# Compare Processed Trades\n",
    "\n",
    "This notebook checks to see if trades priced with archived models are similar to the historical predictions table. The intent is simply to check if the pricing is similar, not to produce precisely the same results. The reason for minor discrepancy is that the historical predictions table, while supposed to have the results of the archived models, is sometimes not up-to-date since the archived models may need to be re-trained in the event of an issue with automated training and in this case, the model may be re-trained with slightly different training data and so could be slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e59ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523d68b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '../creds.json'\n",
    "bq_client = bigquery.Client()\n",
    "\n",
    "project = 'eng-reactor-287421'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad84f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file\n",
    "file_path = '/Users/user/desktop/BMO3.csv'\n",
    "\n",
    "# Read the CSV into a pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "df['trade_datetime'] = pd.to_datetime(df['trade_datetime'])\n",
    "df['ytw'] = pd.to_numeric(df['ytw'], errors='coerce')\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify it's loaded correctly\n",
    "print(df.head())\n",
    "print(f'Length of dataframe at {file_path}: {len(df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d414ecf5",
   "metadata": {},
   "source": [
    "Here is the query for getting trades for a certain day: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e27241",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''SELECT * FROM ( SELECT\n",
    "      a.cusip,\n",
    "      a.trade_date,\n",
    "      a.rtrs_control_number,\n",
    "      a.dollar_price,\n",
    "      a.yield AS msrb_reported_yield_in_bps,\n",
    "      a.new_ficc_ycl + new_ys_prediction AS ficc_yield_prediction_in_bps,\n",
    "      ABS((new_ficc_ycl + new_ys_prediction)-a.yield) AS prediction_error_in_bps\n",
    "    FROM\n",
    "      `eng-reactor-287421.historic_predictions.historical_predictions`a\n",
    "    LEFT JOIN\n",
    "      `auxiliary_views_v2.trade_history_same_issue_5_yr_mat_bucket_1_materialized` b\n",
    "    ON\n",
    "      a.rtrs_control_number = b.rtrs_control_number\n",
    "    WHERE\n",
    "      a.trade_date = \"2024-02-20\"\n",
    "      AND\n",
    "      b.calc_date > \"2025-01-01\") '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2565b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file\n",
    "file_path = '/Users/user/desktop/bmo_cusips_3_21_2.csv'\n",
    "\n",
    "# Read the CSV into a pandas DataFrame\n",
    "df2 = pd.read_csv(file_path)\n",
    "df2['trade_datetime'] = pd.to_datetime(df2['trade_datetime'])\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify it's loaded correctly\n",
    "print(df2.head())\n",
    "print(f'Length of dataframe at {file_path}: {len(df2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e962fd34",
   "metadata": {},
   "source": [
    "Join the two dataframes on the RTRS control number. A left join is performed since `df2` most likely has fewer trades than `df` since we choose a subset of all of the trades to price when sending the data to customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e5b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df2) <= len(df)\n",
    "merged_df = pd.merge(df2, df, on=['rtrs_control_number'], how='left')\n",
    "print(f'Length of joined dataframe: {len(merged_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e482f7",
   "metadata": {},
   "source": [
    "Remove all error trades for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874795bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[merged_df['ytw'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e7d84b",
   "metadata": {},
   "source": [
    "Create columns for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10293d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['new_prediction_in_bps'] = merged_df['ytw'] * 100\n",
    "merged_df['new_prediction_error_in_bps'] = abs(merged_df['msrb_reported_yield_in_bps'] - (merged_df['new_prediction_in_bps']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb156f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prediction_error_in_bps_mean = merged_df['new_prediction_error_in_bps'].mean()\n",
    "prediction_error_in_bps_mean = merged_df['prediction_error_in_bps'].mean()\n",
    "print(f'Mean of new_prediction_error_in_bps: {new_prediction_error_in_bps_mean}')\n",
    "print(f'Mean of prediction_error_in_bps: {prediction_error_in_bps_mean}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583967c4",
   "metadata": {},
   "source": [
    "Change column names and values to send to customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f062788",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['ficc_yield_prediction_in_bps'] = merged_df['ytw'] * 100\n",
    "merged_df['cusip'] = merged_df['cusip_x']    # the '_x' suffix comes from joining the dataframes in the `pd.merge(...)`\n",
    "merged_df['par_traded'] = merged_df['quantity_x']    # the '_x' suffix comes from joining the dataframes in the `pd.merge(...)`\n",
    "merged_df['trade_datetime'] = merged_df['trade_datetime_x']    # the '_x' suffix comes from joining the dataframes in the `pd.merge(...)`\n",
    "result_df = merged_df[['cusip', 'trade_datetime', 'rtrs_control_number', 'par_traded', 'trade_type', 'msrb_reported_yield_in_bps', 'ficc_yield_prediction_in_bps']]\n",
    "result_df = result_df.sort_values(by=['trade_datetime'], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809b2e17",
   "metadata": {},
   "source": [
    "Create the CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f1b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('2024-02-20_ficcai_predictions_for_msrb_trades.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e90061",
   "metadata": {},
   "source": [
    "## Additional analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc62cce3",
   "metadata": {},
   "source": [
    "### Issues further down the dataframe\n",
    "See if there is a change in error as the index increases in `merged_df`. This indicates an indexing issue with the data which is exacerbated further down the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6329cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming merged_df is your DataFrame and it's already defined\n",
    "n = 25  # Number of rows in each chunk\n",
    "\n",
    "# Calculate the number of chunks\n",
    "num_chunks = len(merged_df) // n + (1 if len(merged_df) % n else 0)\n",
    "\n",
    "# Initialize a list to store the MAE for each chunk\n",
    "mae_per_chunk = []\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    start_row = i * n\n",
    "    end_row = start_row + n\n",
    "    # Calculate MAE for the current chunk and append to the list\n",
    "    mae = merged_df['new_prediction_error_in_bps'][start_row:end_row].mean()\n",
    "    mae_per_chunk.append(mae)\n",
    "\n",
    "# Now, mae_per_chunk contains the MAE for each 1000-row chunk\n",
    "# for i, mae in enumerate(mae_per_chunk, 1):\n",
    "#     print(f'MAE for chunk {i}: {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6db7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mae_per_chunk, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6638e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_new_predictions_greater_than_0 = merged_df[merged_df[\"new_prediction_in_bps\"] > 0]\n",
    "merged_df_new_predictions_greater_than_0 = merged_df_new_predictions_greater_than_0.sort_values(by='new_prediction_error_in_bps', ascending=False)\n",
    "merged_df_new_predictions_greater_than_0[['prediction_error_in_bps', \"new_prediction_error_in_bps\", 'ficc_yield_prediction_in_bps', \"new_prediction_in_bps\", 'msrb_reported_yield_in_bps', 'cusip_x', 'trade_datetime_x']].tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadbd869",
   "metadata": {},
   "source": [
    "### RTRS control numbers with trades in the future\n",
    "Some RTRS control numbers have trades in the history with a negative `num_seconds_ago` feature which implies that the trade is in the future. Investigate these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e20376",
   "metadata": {},
   "source": [
    "Investigate `2024022007866600` specifically since this is causing issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49876121",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_new_predictions_greater_than_0[merged_df_new_predictions_greater_than_0['rtrs_control_number'] == 2024022007866600][['prediction_error_in_bps', 'new_prediction_error_in_bps']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed12b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to your text file\n",
    "file_path = '/Users/user/downloads/warnings.txt'\n",
    "\n",
    "# This regular expression matches sequences of digits that appear to represent the RTRS control numbers.\n",
    "# Adjust the pattern as necessary based on the actual format.\n",
    "rtrs_pattern = r'RTRS control number (\\d+)'\n",
    "\n",
    "# Initialize an empty list to store the RTRS control numbers\n",
    "rtrs_control_numbers = []\n",
    "\n",
    "# Open the file and read line by line\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Search for the pattern in the current line\n",
    "        match = re.search(rtrs_pattern, line)\n",
    "        if match:\n",
    "            # If a match is found, extract the control number and add it to the list\n",
    "            rtrs_control_numbers.append(match.group(1))\n",
    "\n",
    "# Now, rtrs_control_numbers contains all the RTRS control numbers found in the file\n",
    "\n",
    "rtrs_control_numbers = [int(number) for number in rtrs_control_numbers]\n",
    "print(rtrs_control_numbers)\n",
    "cusip_pattern = r'CUSIP (\\w{9})'\n",
    "cusips = []\n",
    "\n",
    "# Open the file and read line by line\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Search for the CUSIP pattern in the current line\n",
    "        matches = re.findall(cusip_pattern, line)\n",
    "        for match in matches:\n",
    "            # If matches are found, add them to the list (avoiding duplicates)\n",
    "            if match not in cusips:\n",
    "                cusips.append(match)\n",
    "\n",
    "# Now, cusips contains all the unique CUSIPs found in the file\n",
    "print(cusips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6f0d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrs_control_numbers = ', '.join(str(number) for number in rtrs_control_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6595a03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for control_number in rtrs_control_numbers:\n",
    "    # Filtering to get the rows for the specific RTRS control number\n",
    "    filtered_rows = merged_df_new_predictions_greater_than_0[merged_df_new_predictions_greater_than_0['rtrs_control_number'] == control_number][['prediction_error_in_bps', 'new_prediction_error_in_bps']]\n",
    "\n",
    "    # Checking where the new prediction error is worse than the old\n",
    "    worse_predictions = filtered_rows[(filtered_rows['new_prediction_error_in_bps'] - filtered_rows['prediction_error_in_bps']) >= 15]\n",
    "\n",
    "    # Print the result if there are any worse predictions\n",
    "    if not worse_predictions.empty:\n",
    "        print(f\"Worse predictions for RTRS control number {control_number}:\\n{worse_predictions}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896c5979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqltodf(sql, limit=''):\n",
    "    if limit != '': limit = f\" ORDER BY RAND() LIMIT {limit}\"\n",
    "    bqr = bq_client.query(sql + limit).result()\n",
    "    return bqr.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66fe195",
   "metadata": {},
   "outputs": [],
   "source": [
    "cusips_from_msrb_trade_messages = sqltodf(f'''SELECT distinct(cusip) FROM MSRB.msrb_trade_messages where rtrs_control_number in({rtrs_control_numbers})''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212600dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cusips_from_msrb_trade_messages = cusips_from_msrb_trade_messages.cusip.to_list()\n",
    "print(f'Number of CUSIPs in `cusips_from_msrb_trade_messages`: {len(cusips_from_msrb_trade_messages)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ab8ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cusips = set(cusips).intersection(set(cusips_from_msrb_trade_messages))\n",
    "print('Common CUSIPs in both lists:', common_cusips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eed9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame\n",
    "\n",
    "# Check for duplicated rows based on 'cusip', 'trade_datetime', and 'quantity'\n",
    "duplicates_exist = df.duplicated(subset=['cusip', 'trade_datetime', 'quantity', 'trade_type'], keep=False).any()\n",
    "\n",
    "if duplicates_exist:\n",
    "    print(\"There are duplicate combinations of 'cusip', 'trade_datetime', and 'quantity'.\")\n",
    "else:\n",
    "    print(\"All combinations of 'cusip', 'trade_datetime', and 'quantity' are unique.\")\n",
    "\n",
    "duplicate_rows = df[df.duplicated(subset=['cusip', 'trade_datetime', 'quantity'], keep=False)]\n",
    "print(duplicate_rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
